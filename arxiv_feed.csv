,title,title_detail,links,link,summary,summary_detail,id,guidislink,tags,published,published_parsed,arxiv_announce_type,rights,rights_detail,authors,author,author_detail,arxiv_doi,arxiv_journal_reference
0,Steering Large Text-to-Image Model for Abstract Art Synthesis: Preference-based Prompt Optimization and Visualization,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Steering Large Text-to-Image Model for Abstract Art Synthesis: Preference-based Prompt Optimization and Visualization'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14174'}]",https://arxiv.org/abs/2412.14174,"arXiv:2412.14174v1 Announce Type: new 
Abstract: With the advancement of neural generative capabilities, the art community has increasingly embraced GenAI (Generative Artificial Intelligence), particularly large text-to-image models, for producing aesthetically compelling results. However, the process often lacks determinism and requires a tedious trial-and-error process as users often struggle to devise effective prompts to achieve their desired outcomes. This paper introduces a prompting-free generative approach that applies a genetic algorithm and real-time iterative human feedback to optimize prompt generation, enabling the creation of user-preferred abstract art through a customized Artist Model. The proposed two-part approach begins with constructing an Artist Model capable of deterministically generating abstract art in specific styles, e.g., Kandinsky's Bauhaus style. The second phase integrates real-time user feedback to optimize the prompt generation and obtains an Optimized Prompting Model, which adapts to user preferences and generates prompts automatically. When combined with the Artist Model, this approach allows users to create abstract art tailored to their personal preferences and artistic style.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14174v1 Announce Type: new \nAbstract: With the advancement of neural generative capabilities, the art community has increasingly embraced GenAI (Generative Artificial Intelligence), particularly large text-to-image models, for producing aesthetically compelling results. However, the process often lacks determinism and requires a tedious trial-and-error process as users often struggle to devise effective prompts to achieve their desired outcomes. This paper introduces a prompting-free generative approach that applies a genetic algorithm and real-time iterative human feedback to optimize prompt generation, enabling the creation of user-preferred abstract art through a customized Artist Model. The proposed two-part approach begins with constructing an Artist Model capable of deterministically generating abstract art in specific styles, e.g., Kandinsky's Bauhaus style. The second phase integrates real-time user feedback to optimize the prompt generation and obtains an Optimized Prompting Model, which adapts to user preferences and generates prompts automatically. When combined with the Artist Model, this approach allows users to create abstract art tailored to their personal preferences and artistic style.""}",oai:arXiv.org:2412.14174v1,False,"[{'term': 'cs.HC', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Aven-Le Zhou, Wei Wu, Yu-Ao Wang, Kang Zhang'}]","Aven-Le Zhou, Wei Wu, Yu-Ao Wang, Kang Zhang","{'name': 'Aven-Le Zhou, Wei Wu, Yu-Ao Wang, Kang Zhang'}",,
1,BiTSA: Leveraging Time Series Foundation Model for Building Energy Analytics,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'BiTSA: Leveraging Time Series Foundation Model for Building Energy Analytics'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14175'}]",https://arxiv.org/abs/2412.14175,"arXiv:2412.14175v1 Announce Type: new 
Abstract: Incorporating AI technologies into digital infrastructure offers transformative potential for energy management, particularly in enhancing energy efficiency and supporting net-zero objectives. However, the complexity of IoT-generated datasets often poses a significant challenge, hindering the translation of research insights into practical, real-world applications. This paper presents the design of an interactive visualization tool, BiTSA. The tool enables building managers to interpret complex energy data quickly and take immediate, data-driven actions based on real-time insights. By integrating advanced forecasting models with an intuitive visual interface, our solution facilitates proactive decision-making, optimizes energy consumption, and promotes sustainable building management practices. BiTSA will empower building managers to optimize energy consumption, control demand-side energy usage, and achieve sustainability goals.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14175v1 Announce Type: new \nAbstract: Incorporating AI technologies into digital infrastructure offers transformative potential for energy management, particularly in enhancing energy efficiency and supporting net-zero objectives. However, the complexity of IoT-generated datasets often poses a significant challenge, hindering the translation of research insights into practical, real-world applications. This paper presents the design of an interactive visualization tool, BiTSA. The tool enables building managers to interpret complex energy data quickly and take immediate, data-driven actions based on real-time insights. By integrating advanced forecasting models with an intuitive visual interface, our solution facilitates proactive decision-making, optimizes energy consumption, and promotes sustainable building management practices. BiTSA will empower building managers to optimize energy consumption, control demand-side energy usage, and achieve sustainability goals.'}",oai:arXiv.org:2412.14175v1,False,"[{'term': 'cs.CE', 'scheme': None, 'label': None}, {'term': 'cs.CY', 'scheme': None, 'label': None}, {'term': 'cs.HC', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Xiachong Lin, Arian Prabowo, Imran Razzak, Hao Xue, Matthew Amos, Sam Behrens, Flora D. Salim'}]","Xiachong Lin, Arian Prabowo, Imran Razzak, Hao Xue, Matthew Amos, Sam Behrens, Flora D. Salim","{'name': 'Xiachong Lin, Arian Prabowo, Imran Razzak, Hao Xue, Matthew Amos, Sam Behrens, Flora D. Salim'}",,
2,A Panopticon on My Wrist: The Biopower of Big Data Visualization for Wearables,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'A Panopticon on My Wrist: The Biopower of Big Data Visualization for Wearables'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14176'}]",https://arxiv.org/abs/2412.14176,"arXiv:2412.14176v1 Announce Type: new 
Abstract: Big data visualization - the visual-spatial display of quantitative information culled from huge data sets - is now firmly embedded within the everyday experiences of people across the globe, yet scholarship on it remains surprisingly small. Within this literature, critical theorizations of big data visualizations are rare, as digital positivist perspectives dominate. This paper offers a critical, design-informed perspective on big data visualization in wearable health tracking ecosystems like FitBit. I argue that such visualizations are tools of individualized, neoliberal governance that operate largely through experiences of seduction and addiction to facilitate participation in the corporate capture and monetization of personal information. Exploration of my personal experience of the FitBit ecosystem illuminates this argument and emphasizes the capacity for harm to individuals using these ecosystems, leading to an exploration of the complex professional challenges for user experience designers working on visualizations within the ecosystems of wearables.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14176v1 Announce Type: new \nAbstract: Big data visualization - the visual-spatial display of quantitative information culled from huge data sets - is now firmly embedded within the everyday experiences of people across the globe, yet scholarship on it remains surprisingly small. Within this literature, critical theorizations of big data visualizations are rare, as digital positivist perspectives dominate. This paper offers a critical, design-informed perspective on big data visualization in wearable health tracking ecosystems like FitBit. I argue that such visualizations are tools of individualized, neoliberal governance that operate largely through experiences of seduction and addiction to facilitate participation in the corporate capture and monetization of personal information. Exploration of my personal experience of the FitBit ecosystem illuminates this argument and emphasizes the capacity for harm to individuals using these ecosystems, leading to an exploration of the complex professional challenges for user experience designers working on visualizations within the ecosystems of wearables.'}",oai:arXiv.org:2412.14176v1,False,"[{'term': 'cs.HC', 'scheme': None, 'label': None}, {'term': 'cs.CY', 'scheme': None, 'label': None}, {'term': 'cs.MM', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by-nc-nd/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-nd/4.0/'}",[{'name': 'KJ Hepworth'}],KJ Hepworth,{'name': 'KJ Hepworth'},10.1080/17547075.2019.1661723,
3,Revolutionizing QoE-Driven Network Management with Digital Agent Technology in 6G,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Revolutionizing QoE-Driven Network Management with Digital Agent Technology in 6G'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14177'}]",https://arxiv.org/abs/2412.14177,"arXiv:2412.14177v1 Announce Type: new 
Abstract: In this article, we propose a digital agent (DA)-assisted network management framework for future sixth generation (6G) networks considering users' quality of experience (QoE). Particularly, a novel QoE metric is defined by incorporating the impact of user behavior dynamics and environment complexity on quality of service (QoS). A two-level DA architecture is developed to assist the QoE-driven network orchestration and slicing, respectively. To further improve the performance of proposed framework, three potential solutions are presented from the perspectives of DA data collection, network scheduling algorithm selection, and DA deployment. A case study demonstrates that the proposed framework can effectively improve users' QoE compared with benchmark schemes.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14177v1 Announce Type: new \nAbstract: In this article, we propose a digital agent (DA)-assisted network management framework for future sixth generation (6G) networks considering users' quality of experience (QoE). Particularly, a novel QoE metric is defined by incorporating the impact of user behavior dynamics and environment complexity on quality of service (QoS). A two-level DA architecture is developed to assist the QoE-driven network orchestration and slicing, respectively. To further improve the performance of proposed framework, three potential solutions are presented from the perspectives of DA data collection, network scheduling algorithm selection, and DA deployment. A case study demonstrates that the proposed framework can effectively improve users' QoE compared with benchmark schemes.""}",oai:arXiv.org:2412.14177v1,False,"[{'term': 'cs.NI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Xuemin Shen, Xinyu Huang, Jianzhe Xue, Conghao Zhou, Xiufang Shi, Weihua Zhuang'}]","Xuemin Shen, Xinyu Huang, Jianzhe Xue, Conghao Zhou, Xiufang Shi, Weihua Zhuang","{'name': 'Xuemin Shen, Xinyu Huang, Jianzhe Xue, Conghao Zhou, Xiufang Shi, Weihua Zhuang'}",,
4,The GAIUS Experience: Powering a Hyperlocal Mobile Web for Communities in Emerging Regions,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'The GAIUS Experience: Powering a Hyperlocal Mobile Web for Communities in Emerging Regions'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14178'}]",https://arxiv.org/abs/2412.14178,"arXiv:2412.14178v1 Announce Type: new 
Abstract: Despite increasing mobile Internet penetration in developing regions, mobile users continue to experience a poor web experience due to two key factors: (i) lack of locally relevant content; (ii) poor web performance due to complex web pages and poor network conditions. In this paper, we describe our design, implementation and deployment experiences of GAIUS, a mobile content ecosystem that enables efficient creation and dissemination of locally relevant web content into hyperlocal communities in emerging markets. The basic building blocks of GAIUS are a lightweight content edge platform combined with a mobile application that collectively provide a Hyperlocal Web abstraction for mobile users to create and consume locally relevant content and interact with other users via a community abstraction. The GAIUS platform uses MAML, a web specification language that dramatically simplifies web pages to reduce the complexity of Web content within the GAIUS ecosystem, improve page load times and reduce network costs. In this paper, we describe our experiences deploying GAIUS across a large user base in India, Bangladesh and Kenya.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14178v1 Announce Type: new \nAbstract: Despite increasing mobile Internet penetration in developing regions, mobile users continue to experience a poor web experience due to two key factors: (i) lack of locally relevant content; (ii) poor web performance due to complex web pages and poor network conditions. In this paper, we describe our design, implementation and deployment experiences of GAIUS, a mobile content ecosystem that enables efficient creation and dissemination of locally relevant web content into hyperlocal communities in emerging markets. The basic building blocks of GAIUS are a lightweight content edge platform combined with a mobile application that collectively provide a Hyperlocal Web abstraction for mobile users to create and consume locally relevant content and interact with other users via a community abstraction. The GAIUS platform uses MAML, a web specification language that dramatically simplifies web pages to reduce the complexity of Web content within the GAIUS ecosystem, improve page load times and reduce network costs. In this paper, we describe our experiences deploying GAIUS across a large user base in India, Bangladesh and Kenya.'}",oai:arXiv.org:2412.14178v1,False,"[{'term': 'cs.NI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Rohail Asim, Arjuna Sathiaseelan, Arko Chatterjee, Mukund Lal, Yasir Zaki, Lakshmi Subramanian'}]","Rohail Asim, Arjuna Sathiaseelan, Arko Chatterjee, Mukund Lal, Yasir Zaki, Lakshmi Subramanian","{'name': 'Rohail Asim, Arjuna Sathiaseelan, Arko Chatterjee, Mukund Lal, Yasir Zaki, Lakshmi Subramanian'}",,
5,Benchmarking Harmonized Tariff Schedule Classification Models,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Benchmarking Harmonized Tariff Schedule Classification Models'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14179'}]",https://arxiv.org/abs/2412.14179,"arXiv:2412.14179v1 Announce Type: new 
Abstract: The Harmonized Tariff System (HTS) classification industry, essential to e-commerce and international trade, currently lacks standardized benchmarks for evaluating the effectiveness of classification solutions. This study establishes and tests a benchmark framework for imports to the United States, inspired by the benchmarking approaches used in language model evaluation, to systematically compare prominent HTS classification tools. The framework assesses key metrics--such as speed, accuracy, rationality, and HTS code alignment--to provide a comprehensive performance comparison. The study evaluates several industry-leading solutions, including those provided by Zonos, Tarifflo, Avalara, and WCO BACUDA, identifying each tool's strengths and limitations. Results highlight areas for industry-wide improvement and innovation, paving the way for more effective and standardized HTS classification solutions across the international trade and e-commerce sectors.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14179v1 Announce Type: new \nAbstract: The Harmonized Tariff System (HTS) classification industry, essential to e-commerce and international trade, currently lacks standardized benchmarks for evaluating the effectiveness of classification solutions. This study establishes and tests a benchmark framework for imports to the United States, inspired by the benchmarking approaches used in language model evaluation, to systematically compare prominent HTS classification tools. The framework assesses key metrics--such as speed, accuracy, rationality, and HTS code alignment--to provide a comprehensive performance comparison. The study evaluates several industry-leading solutions, including those provided by Zonos, Tarifflo, Avalara, and WCO BACUDA, identifying each tool's strengths and limitations. Results highlight areas for industry-wide improvement and innovation, paving the way for more effective and standardized HTS classification solutions across the international trade and e-commerce sectors.""}",oai:arXiv.org:2412.14179v1,False,"[{'term': 'cs.SE', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by-sa/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-sa/4.0/'}",[{'name': 'Bryce Judy'}],Bryce Judy,{'name': 'Bryce Judy'},,
6,"The Influence and Relationship between Computational Thinking, Learning Motivation, Attitude, and Achievement of Code.org in K-12 Programming Education","{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'The Influence and Relationship between Computational Thinking, Learning Motivation, Attitude, and Achievement of Code.org in K-12 Programming Education'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14180'}]",https://arxiv.org/abs/2412.14180,"arXiv:2412.14180v1 Announce Type: new 
Abstract: This study examined the impact of Code.org's block-based coding curriculum on primary school students' computational thinking, motivation, attitudes, and academic performance. Twenty students participated, and a range of tools was used: the Programming Computational Thinking Scale (PCTS) to evaluate computational thinking, the Instructional Materials Motivation Survey (IMMS) for motivation, the Attitude Scale of Computer Programming Learning (ASCOPL) for attitudes, and the Programming Achievement Test (PAT) for programming performance. The results revealed significant improvements in computational thinking, motivation, attitudes, and programming performance, with strong positive correlations among these factors. ANOVA analysis highlighted significant differences in computational concepts, perspectives, and motivational factors like attention and confidence, emphasizing their interdependence in programming success. This study highlights the interconnectedness of these factors and their importance in supporting programming achievement in primary school students, addressing gaps in the literature on block-based programming education.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14180v1 Announce Type: new \nAbstract: This study examined the impact of Code.org's block-based coding curriculum on primary school students' computational thinking, motivation, attitudes, and academic performance. Twenty students participated, and a range of tools was used: the Programming Computational Thinking Scale (PCTS) to evaluate computational thinking, the Instructional Materials Motivation Survey (IMMS) for motivation, the Attitude Scale of Computer Programming Learning (ASCOPL) for attitudes, and the Programming Achievement Test (PAT) for programming performance. The results revealed significant improvements in computational thinking, motivation, attitudes, and programming performance, with strong positive correlations among these factors. ANOVA analysis highlighted significant differences in computational concepts, perspectives, and motivational factors like attention and confidence, emphasizing their interdependence in programming success. This study highlights the interconnectedness of these factors and their importance in supporting programming achievement in primary school students, addressing gaps in the literature on block-based programming education.""}",oai:arXiv.org:2412.14180v1,False,"[{'term': 'cs.HC', 'scheme': None, 'label': None}, {'term': 'cs.CY', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Wan Chong Choi, Iek Chong Choi'}]","Wan Chong Choi, Iek Chong Choi","{'name': 'Wan Chong Choi, Iek Chong Choi'}",,
7,Automating Compliance in Government Organisations using eFLINT,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Automating Compliance in Government Organisations using eFLINT'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14183'}]",https://arxiv.org/abs/2412.14183,"arXiv:2412.14183v1 Announce Type: new 
Abstract: Ensuring compliance of norms and policies when working on administrative law cases can be difficult to manage for government organisations. Automating this process could save a lot of time, effort and ensure compliance. Prior research resulted in a method to formalize sources of norms. These can be turned into executable specifications using the domain-specific language eFLINT, which can be used for automating compliance. However, the current interface of eFLINT prevents adaption by legal experts. The aim of this research was to bridge this gap by developing a prototype based on eFLINT, for automating compliance within government organisations. To get a better understanding of the needs and requirements of potential users, qualitative research was conducted. This consisted of semi-structured interviews to gather requirements, which were analyzed using a thematic analysis method. Based on the analyzed data, a design for the interface of the prototype was made. The final prototype was evaluated in a user end study which included a cognitive walkthrough and user testing. The prototype proved to be a good first step in the right direction with a lot of room for further development.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14183v1 Announce Type: new \nAbstract: Ensuring compliance of norms and policies when working on administrative law cases can be difficult to manage for government organisations. Automating this process could save a lot of time, effort and ensure compliance. Prior research resulted in a method to formalize sources of norms. These can be turned into executable specifications using the domain-specific language eFLINT, which can be used for automating compliance. However, the current interface of eFLINT prevents adaption by legal experts. The aim of this research was to bridge this gap by developing a prototype based on eFLINT, for automating compliance within government organisations. To get a better understanding of the needs and requirements of potential users, qualitative research was conducted. This consisted of semi-structured interviews to gather requirements, which were analyzed using a thematic analysis method. Based on the analyzed data, a design for the interface of the prototype was made. The final prototype was evaluated in a user end study which included a cognitive walkthrough and user testing. The prototype proved to be a good first step in the right direction with a lot of room for further development.'}",oai:arXiv.org:2412.14183v1,False,"[{'term': 'cs.HC', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}",[{'name': 'Nina Verheijen'}],Nina Verheijen,{'name': 'Nina Verheijen'},,
8,Fabric Sensing of Intrinsic Hand Muscle Activity,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Fabric Sensing of Intrinsic Hand Muscle Activity'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14185'}]",https://arxiv.org/abs/2412.14185,"arXiv:2412.14185v1 Announce Type: new 
Abstract: Wearable robotics have the capacity to assist stroke survivors in assisting and rehabilitating hand function. Many devices that use surface electromyographic (sEMG) for control rely on extrinsic muscle signals, since sEMG sensors are relatively easy to place on the forearm without interfering with hand activity. In this work, we target the intrinsic muscles of the thumb, which are superficial to the skin and thus potentially more accessible via sEMG sensing. However, traditional, rigid electrodes can not be placed on the hand without adding bulk and affecting hand functionality. We thus present a novel sensing sleeve that uses textile electrodes to measure sEMG activity of intrinsic thumb muscles. We evaluate the sleeve's performance on detecting thumb movements and muscle activity during both isolated and isometric muscle contractions of the thumb and fingers. This work highlights the potential of textile-based sensors as a low-cost, lightweight, and non-obtrusive alternative to conventional sEMG sensors for wearable robotics.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14185v1 Announce Type: new \nAbstract: Wearable robotics have the capacity to assist stroke survivors in assisting and rehabilitating hand function. Many devices that use surface electromyographic (sEMG) for control rely on extrinsic muscle signals, since sEMG sensors are relatively easy to place on the forearm without interfering with hand activity. In this work, we target the intrinsic muscles of the thumb, which are superficial to the skin and thus potentially more accessible via sEMG sensing. However, traditional, rigid electrodes can not be placed on the hand without adding bulk and affecting hand functionality. We thus present a novel sensing sleeve that uses textile electrodes to measure sEMG activity of intrinsic thumb muscles. We evaluate the sleeve's performance on detecting thumb movements and muscle activity during both isolated and isometric muscle contractions of the thumb and fingers. This work highlights the potential of textile-based sensors as a low-cost, lightweight, and non-obtrusive alternative to conventional sEMG sensors for wearable robotics.""}",oai:arXiv.org:2412.14185v1,False,"[{'term': 'cs.HC', 'scheme': None, 'label': None}, {'term': 'cs.RO', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Katelyn Lee, Runsheng Wang, Ava Chen, Lauren Winterbottom, Ho Man Colman Leung, Lisa Maria DiSalvo, Iris Xu, Jingxi Xu, Dawn M. Nilsen, Joel Stein, Xia Zho, Matei Ciocarlie'}]","Katelyn Lee, Runsheng Wang, Ava Chen, Lauren Winterbottom, Ho Man Colman Leung, Lisa Maria DiSalvo, Iris Xu, Jingxi Xu, Dawn M. Nilsen, Joel Stein, Xia Zho, Matei Ciocarlie","{'name': 'Katelyn Lee, Runsheng Wang, Ava Chen, Lauren Winterbottom, Ho Man Colman Leung, Lisa Maria DiSalvo, Iris Xu, Jingxi Xu, Dawn M. Nilsen, Joel Stein, Xia Zho, Matei Ciocarlie'}",,
9,Towards AI-$45^{\circ}$ Law: A Roadmap to Trustworthy AGI,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Towards AI-$45^{\\circ}$ Law: A Roadmap to Trustworthy AGI'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14186'}]",https://arxiv.org/abs/2412.14186,"arXiv:2412.14186v1 Announce Type: new 
Abstract: Ensuring Artificial General Intelligence (AGI) reliably avoids harmful behaviors is a critical challenge, especially for systems with high autonomy or in safety-critical domains. Despite various safety assurance proposals and extreme risk warnings, comprehensive guidelines balancing AI safety and capability remain lacking. In this position paper, we propose the \textit{AI-\textbf{$45^{\circ}$} Law} as a guiding principle for a balanced roadmap toward trustworthy AGI, and introduce the \textit{Causal Ladder of Trustworthy AGI} as a practical framework. This framework provides a systematic taxonomy and hierarchical structure for current AI capability and safety research, inspired by Judea Pearl's ``Ladder of Causation''. The Causal Ladder comprises three core layers: the Approximate Alignment Layer, the Intervenable Layer, and the Reflectable Layer. These layers address the key challenges of safety and trustworthiness in AGI and contemporary AI systems. Building upon this framework, we define five levels of trustworthy AGI: perception, reasoning, decision-making, autonomy, and collaboration trustworthiness. These levels represent distinct yet progressive aspects of trustworthy AGI. Finally, we present a series of potential governance measures to support the development of trustworthy AGI.\footnote{In this paper, trustworthiness is generally considered a broad form of safety, and no explicit distinction is made between the two. However, in some contexts, safety and trustworthiness are treated as distinct: safety involves assurance of correct behavior, while trustworthiness refers to user confidence in the system's decision-making. In such cases, different terms or both may be used depending on the context.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14186v1 Announce Type: new \nAbstract: Ensuring Artificial General Intelligence (AGI) reliably avoids harmful behaviors is a critical challenge, especially for systems with high autonomy or in safety-critical domains. Despite various safety assurance proposals and extreme risk warnings, comprehensive guidelines balancing AI safety and capability remain lacking. In this position paper, we propose the \\textit{AI-\\textbf{$45^{\\circ}$} Law} as a guiding principle for a balanced roadmap toward trustworthy AGI, and introduce the \\textit{Causal Ladder of Trustworthy AGI} as a practical framework. This framework provides a systematic taxonomy and hierarchical structure for current AI capability and safety research, inspired by Judea Pearl's ``Ladder of Causation''. The Causal Ladder comprises three core layers: the Approximate Alignment Layer, the Intervenable Layer, and the Reflectable Layer. These layers address the key challenges of safety and trustworthiness in AGI and contemporary AI systems. Building upon this framework, we define five levels of trustworthy AGI: perception, reasoning, decision-making, autonomy, and collaboration trustworthiness. These levels represent distinct yet progressive aspects of trustworthy AGI. Finally, we present a series of potential governance measures to support the development of trustworthy AGI.\\footnote{In this paper, trustworthiness is generally considered a broad form of safety, and no explicit distinction is made between the two. However, in some contexts, safety and trustworthiness are treated as distinct: safety involves assurance of correct behavior, while trustworthiness refers to user confidence in the system's decision-making. In such cases, different terms or both may be used depending on the context.""}",oai:arXiv.org:2412.14186v1,False,"[{'term': 'cs.CY', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.CL', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Yang Chao, Lu Chaochao, Wang Yingchun, Zhou Bowen'}]","Yang Chao, Lu Chaochao, Wang Yingchun, Zhou Bowen","{'name': 'Yang Chao, Lu Chaochao, Wang Yingchun, Zhou Bowen'}",,
10,Detecting Dark Patterns in User Interfaces Using Logistic Regression and Bag-of-Words Representation,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Detecting Dark Patterns in User Interfaces Using Logistic Regression and Bag-of-Words Representation'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14187'}]",https://arxiv.org/abs/2412.14187,"arXiv:2412.14187v1 Announce Type: new 
Abstract: Dark patterns in user interfaces represent deceptive design practices intended to manipulate users' behavior, often leading to unintended consequences such as coerced purchases, involuntary data disclosures, or user frustration. Detecting and mitigating these dark patterns is crucial for promoting transparency, trust, and ethical design practices in digital environments. This paper proposes a novel approach for detecting dark patterns in user interfaces using logistic regression and bag-of-words representation. Our methodology involves collecting a diverse dataset of user interface text samples, preprocessing the data, extracting text features using the bag-of-words representation, training a logistic regression model, and evaluating its performance using various metrics such as accuracy, precision, recall, F1-score, and the area under the ROC curve (AUC). Experimental results demonstrate the effectiveness of the proposed approach in accurately identifying instances of dark patterns, with high predictive performance and robustness to variations in dataset composition and model parameters. The insights gained from this study contribute to the growing body of knowledge on dark patterns detection and classification, offering practical implications for designers, developers, and policymakers in promoting ethical design practices and protecting user rights in digital environments.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14187v1 Announce Type: new \nAbstract: Dark patterns in user interfaces represent deceptive design practices intended to manipulate users' behavior, often leading to unintended consequences such as coerced purchases, involuntary data disclosures, or user frustration. Detecting and mitigating these dark patterns is crucial for promoting transparency, trust, and ethical design practices in digital environments. This paper proposes a novel approach for detecting dark patterns in user interfaces using logistic regression and bag-of-words representation. Our methodology involves collecting a diverse dataset of user interface text samples, preprocessing the data, extracting text features using the bag-of-words representation, training a logistic regression model, and evaluating its performance using various metrics such as accuracy, precision, recall, F1-score, and the area under the ROC curve (AUC). Experimental results demonstrate the effectiveness of the proposed approach in accurately identifying instances of dark patterns, with high predictive performance and robustness to variations in dataset composition and model parameters. The insights gained from this study contribute to the growing body of knowledge on dark patterns detection and classification, offering practical implications for designers, developers, and policymakers in promoting ethical design practices and protecting user rights in digital environments.""}",oai:arXiv.org:2412.14187v1,False,"[{'term': 'cs.HC', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by-nc-nd/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-nd/4.0/'}","[{'name': 'Aliyu Umar, Maaruf Lawan, Adamu Lawan, Abdullahi Abdulkadir, Mukhtar Dahiru'}]","Aliyu Umar, Maaruf Lawan, Adamu Lawan, Abdullahi Abdulkadir, Mukhtar Dahiru","{'name': 'Aliyu Umar, Maaruf Lawan, Adamu Lawan, Abdullahi Abdulkadir, Mukhtar Dahiru'}",,
11,CogSimulator: A Model for Simulating User Cognition & Behavior with Minimal Data for Tailored Cognitive Enhancement,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'CogSimulator: A Model for Simulating User Cognition & Behavior with Minimal Data for Tailored Cognitive Enhancement'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14188'}]",https://arxiv.org/abs/2412.14188,"arXiv:2412.14188v1 Announce Type: new 
Abstract: The interplay between cognition and gaming, notably through educational games enhancing cognitive skills, has garnered significant attention in recent years. This research introduces the CogSimulator, a novel algorithm for simulating user cognition in small-group settings with minimal data, as the educational game Wordle exemplifies. The CogSimulator employs Wasserstein-1 distance and coordinates search optimization for hyperparameter tuning, enabling precise few-shot predictions in new game scenarios. Comparative experiments with the Wordle dataset illustrate that our model surpasses most conventional machine learning models in mean Wasserstein-1 distance, mean squared error, and mean accuracy, showcasing its efficacy in cognitive enhancement through tailored game design.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14188v1 Announce Type: new \nAbstract: The interplay between cognition and gaming, notably through educational games enhancing cognitive skills, has garnered significant attention in recent years. This research introduces the CogSimulator, a novel algorithm for simulating user cognition in small-group settings with minimal data, as the educational game Wordle exemplifies. The CogSimulator employs Wasserstein-1 distance and coordinates search optimization for hyperparameter tuning, enabling precise few-shot predictions in new game scenarios. Comparative experiments with the Wordle dataset illustrate that our model surpasses most conventional machine learning models in mean Wasserstein-1 distance, mean squared error, and mean accuracy, showcasing its efficacy in cognitive enhancement through tailored game design.'}",oai:arXiv.org:2412.14188v1,False,"[{'term': 'cs.HC', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'q-bio.NC', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Weizhen Bian, Yubo Zhou, Yuanhang Luo, Ming Mo, Siyan Liu, Yikai Gong, Renjie Wan, Ziyuan Luo, Aobo Wang'}]","Weizhen Bian, Yubo Zhou, Yuanhang Luo, Ming Mo, Siyan Liu, Yikai Gong, Renjie Wan, Ziyuan Luo, Aobo Wang","{'name': 'Weizhen Bian, Yubo Zhou, Yuanhang Luo, Ming Mo, Siyan Liu, Yikai Gong, Renjie Wan, Ziyuan Luo, Aobo Wang'}",,CogSci 2024
12,Toward Ethical Spatial Analysis: Addressing Endogenous Bias Through Visual Analytics,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Toward Ethical Spatial Analysis: Addressing Endogenous Bias Through Visual Analytics'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14189'}]",https://arxiv.org/abs/2412.14189,"arXiv:2412.14189v1 Announce Type: new 
Abstract: Spatial analysis can generate both exogenous and endogenous biases, which will lead to ethics issues. Exogenous biases arise from external factors or environments and are unrelated to internal operating mechanisms, while endogenous biases stem from internal processes or technologies. Although much attention has been given to exogenous biases, endogenous biases in spatial analysis have been largely overlooked, and a comprehensive methodology for addressing them is yet to be developed. To tackle this challenge, we propose that visual analytics can play a key role in understanding geographic data and improving the interpretation of analytical results. In this study, we conducted a preliminary investigation using various visualization techniques to explore endogenous biases. Our findings demonstrate the potentials of visual analytics to uncover hidden biases and identify associated issues. Additionally, we synthesized these visualization strategies into a framework that approximates a method for detecting endogenous biases. Through this work, we advocate for the integration of visualization at three critical stages of spatial analysis in order to minimize errors, address ethical concerns, and reduce misinterpretations associated with endogenous biases.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14189v1 Announce Type: new \nAbstract: Spatial analysis can generate both exogenous and endogenous biases, which will lead to ethics issues. Exogenous biases arise from external factors or environments and are unrelated to internal operating mechanisms, while endogenous biases stem from internal processes or technologies. Although much attention has been given to exogenous biases, endogenous biases in spatial analysis have been largely overlooked, and a comprehensive methodology for addressing them is yet to be developed. To tackle this challenge, we propose that visual analytics can play a key role in understanding geographic data and improving the interpretation of analytical results. In this study, we conducted a preliminary investigation using various visualization techniques to explore endogenous biases. Our findings demonstrate the potentials of visual analytics to uncover hidden biases and identify associated issues. Additionally, we synthesized these visualization strategies into a framework that approximates a method for detecting endogenous biases. Through this work, we advocate for the integration of visualization at three critical stages of spatial analysis in order to minimize errors, address ethical concerns, and reduce misinterpretations associated with endogenous biases.'}",oai:arXiv.org:2412.14189v1,False,"[{'term': 'cs.HC', 'scheme': None, 'label': None}, {'term': 'physics.data-an', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Chuan Chen, Peng Luo, Bo Zhao, Yu Feng, Liqiu Meng'}]","Chuan Chen, Peng Luo, Bo Zhao, Yu Feng, Liqiu Meng","{'name': 'Chuan Chen, Peng Luo, Bo Zhao, Yu Feng, Liqiu Meng'}",,
13,Lessons From an App Update at Replika AI: Identity Discontinuity in Human-AI Relationships,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Lessons From an App Update at Replika AI: Identity Discontinuity in Human-AI Relationships'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14190'}]",https://arxiv.org/abs/2412.14190,"arXiv:2412.14190v1 Announce Type: new 
Abstract: Can consumers form especially deep emotional bonds with AI and be vested in AI identities over time? We leverage a natural app-update event at Replika AI, a popular US-based AI companion, to shed light on these questions. We find that, after the app removed its erotic role play (ERP) feature, preventing intimate interactions between consumers and chatbots that were previously possible, this event triggered perceptions in customers that their AI companion's identity had discontinued. This in turn predicted negative consumer welfare and marketing outcomes related to loss, including mourning the loss, and devaluing the ""new"" AI relative to the ""original"". Experimental evidence confirms these findings. Further experiments find that AI companions users feel closer to their AI companion than even their best human friend, and mourn a loss of their AI companion more than a loss of various other inanimate products. In short, consumers are forming human-level relationships with AI companions; disruptions to these relationships trigger real patterns of mourning as well as devaluation of the offering; and the degree of mourning and devaluation are explained by perceived discontinuity in the AIs identity. Our results illustrate that relationships with AI are truly personal, creating unique benefits and risks for consumers and firms alike.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14190v1 Announce Type: new \nAbstract: Can consumers form especially deep emotional bonds with AI and be vested in AI identities over time? We leverage a natural app-update event at Replika AI, a popular US-based AI companion, to shed light on these questions. We find that, after the app removed its erotic role play (ERP) feature, preventing intimate interactions between consumers and chatbots that were previously possible, this event triggered perceptions in customers that their AI companion\'s identity had discontinued. This in turn predicted negative consumer welfare and marketing outcomes related to loss, including mourning the loss, and devaluing the ""new"" AI relative to the ""original"". Experimental evidence confirms these findings. Further experiments find that AI companions users feel closer to their AI companion than even their best human friend, and mourn a loss of their AI companion more than a loss of various other inanimate products. In short, consumers are forming human-level relationships with AI companions; disruptions to these relationships trigger real patterns of mourning as well as devaluation of the offering; and the degree of mourning and devaluation are explained by perceived discontinuity in the AIs identity. Our results illustrate that relationships with AI are truly personal, creating unique benefits and risks for consumers and firms alike.'}",oai:arXiv.org:2412.14190v1,False,"[{'term': 'cs.HC', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.CY', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Julian De Freitas, Noah Castelo, Ahmet Uguralp, Zeliha Uguralp'}]","Julian De Freitas, Noah Castelo, Ahmet Uguralp, Zeliha Uguralp","{'name': 'Julian De Freitas, Noah Castelo, Ahmet Uguralp, Zeliha Uguralp'}",,"Harvard Business School Working Paper, No. 25-018, October 2024"
14,Ontology-Aware RAG for Improved Question-Answering in Cybersecurity Education,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Ontology-Aware RAG for Improved Question-Answering in Cybersecurity Education'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14191'}]",https://arxiv.org/abs/2412.14191,"arXiv:2412.14191v1 Announce Type: new 
Abstract: Integrating AI into education has the potential to transform the teaching of science and technology courses, particularly in the field of cybersecurity. AI-driven question-answering (QA) systems can actively manage uncertainty in cybersecurity problem-solving, offering interactive, inquiry-based learning experiences. Large language models (LLMs) have gained prominence in AI-driven QA systems, offering advanced language understanding and user engagement. However, they face challenges like hallucinations and limited domain-specific knowledge, which reduce their reliability in educational settings. To address these challenges, we propose CyberRAG, an ontology-aware retrieval-augmented generation (RAG) approach for developing a reliable and safe QA system in cybersecurity education. CyberRAG employs a two-step approach: first, it augments the domain-specific knowledge by retrieving validated cybersecurity documents from a knowledge base to enhance the relevance and accuracy of the response. Second, it mitigates hallucinations and misuse by integrating a knowledge graph ontology to validate the final answer. Experiments on publicly available cybersecurity datasets show that CyberRAG delivers accurate, reliable responses aligned with domain knowledge, demonstrating the potential of AI tools to enhance education.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14191v1 Announce Type: new \nAbstract: Integrating AI into education has the potential to transform the teaching of science and technology courses, particularly in the field of cybersecurity. AI-driven question-answering (QA) systems can actively manage uncertainty in cybersecurity problem-solving, offering interactive, inquiry-based learning experiences. Large language models (LLMs) have gained prominence in AI-driven QA systems, offering advanced language understanding and user engagement. However, they face challenges like hallucinations and limited domain-specific knowledge, which reduce their reliability in educational settings. To address these challenges, we propose CyberRAG, an ontology-aware retrieval-augmented generation (RAG) approach for developing a reliable and safe QA system in cybersecurity education. CyberRAG employs a two-step approach: first, it augments the domain-specific knowledge by retrieving validated cybersecurity documents from a knowledge base to enhance the relevance and accuracy of the response. Second, it mitigates hallucinations and misuse by integrating a knowledge graph ontology to validate the final answer. Experiments on publicly available cybersecurity datasets show that CyberRAG delivers accurate, reliable responses aligned with domain knowledge, demonstrating the potential of AI tools to enhance education.'}",oai:arXiv.org:2412.14191v1,False,"[{'term': 'cs.CY', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Chengshuai Zhao, Garima Agrawal, Tharindu Kumarage, Zhen Tan, Yuli Deng, Ying-Chih Chen, Huan Liu'}]","Chengshuai Zhao, Garima Agrawal, Tharindu Kumarage, Zhen Tan, Yuli Deng, Ying-Chih Chen, Huan Liu","{'name': 'Chengshuai Zhao, Garima Agrawal, Tharindu Kumarage, Zhen Tan, Yuli Deng, Ying-Chih Chen, Huan Liu'}",,
15,Whom do Explanations Serve? A Systematic Literature Survey of User Characteristics in Explainable Recommender Systems Evaluation,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Whom do Explanations Serve? A Systematic Literature Survey of User Characteristics in Explainable Recommender Systems Evaluation'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14193'}]",https://arxiv.org/abs/2412.14193,"arXiv:2412.14193v1 Announce Type: new 
Abstract: Adding explanations to recommender systems is said to have multiple benefits, such as increasing user trust or system transparency. Previous work from other application areas suggests that specific user characteristics impact the users' perception of the explanation. However, we rarely find this type of evaluation for recommender systems explanations. This paper addresses this gap by surveying 124 papers in which recommender systems explanations were evaluated in user studies. We analyzed their participant descriptions and study results where the impact of user characteristics on the explanation effects was measured. Our findings suggest that the results from the surveyed studies predominantly cover specific users who do not necessarily represent the users of recommender systems in the evaluation domain. This may seriously hamper the generalizability of any insights we may gain from current studies on explanations in recommender systems. We further find inconsistencies in the data reporting, which impacts the reproducibility of the reported results. Hence, we recommend actions to move toward a more inclusive and reproducible evaluation.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14193v1 Announce Type: new \nAbstract: Adding explanations to recommender systems is said to have multiple benefits, such as increasing user trust or system transparency. Previous work from other application areas suggests that specific user characteristics impact the users' perception of the explanation. However, we rarely find this type of evaluation for recommender systems explanations. This paper addresses this gap by surveying 124 papers in which recommender systems explanations were evaluated in user studies. We analyzed their participant descriptions and study results where the impact of user characteristics on the explanation effects was measured. Our findings suggest that the results from the surveyed studies predominantly cover specific users who do not necessarily represent the users of recommender systems in the evaluation domain. This may seriously hamper the generalizability of any insights we may gain from current studies on explanations in recommender systems. We further find inconsistencies in the data reporting, which impacts the reproducibility of the reported results. Hence, we recommend actions to move toward a more inclusive and reproducible evaluation.""}",oai:arXiv.org:2412.14193v1,False,"[{'term': 'cs.HC', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.IR', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Kathrin Wardatzky, Oana Inel, Luca Rossetto, Abraham Bernstein'}]","Kathrin Wardatzky, Oana Inel, Luca Rossetto, Abraham Bernstein","{'name': 'Kathrin Wardatzky, Oana Inel, Luca Rossetto, Abraham Bernstein'}",,
16,"Detecting Cognitive Impairment and Psychological Well-being among Older Adults Using Facial, Acoustic, Linguistic, and Cardiovascular Patterns Derived from Remote Conversations","{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Detecting Cognitive Impairment and Psychological Well-being among Older Adults Using Facial, Acoustic, Linguistic, and Cardiovascular Patterns Derived from Remote Conversations'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14194'}]",https://arxiv.org/abs/2412.14194,"arXiv:2412.14194v1 Announce Type: new 
Abstract: INTRODUCTION: The aging society urgently requires scalable methods to monitor cognitive decline and identify social and psychological factors indicative of dementia risk in older adults. METHODS: Our machine learning models captured facial, acoustic, linguistic, and cardiovascular features from 39 individuals with normal cognition or Mild Cognitive Impairment derived from remote video conversations and classified cognitive status, social isolation, neuroticism, and psychological well-being. RESULTS: Our model could distinguish Clinical Dementia Rating Scale of 0.5 (vs. 0) with 0.78 area under the receiver operating characteristic curve (AUC), social isolation with 0.75 AUC, neuroticism with 0.71 AUC, and negative affect scales with 0.79 AUC. DISCUSSION: Our findings demonstrate the feasibility of remotely monitoring cognitive status, social isolation, neuroticism, and psychological well-being. Speech and language patterns were more useful for quantifying cognitive impairment, whereas facial expression and cardiovascular patterns using remote photoplethysmography were more useful for quantifying personality and psychological well-being.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14194v1 Announce Type: new \nAbstract: INTRODUCTION: The aging society urgently requires scalable methods to monitor cognitive decline and identify social and psychological factors indicative of dementia risk in older adults. METHODS: Our machine learning models captured facial, acoustic, linguistic, and cardiovascular features from 39 individuals with normal cognition or Mild Cognitive Impairment derived from remote video conversations and classified cognitive status, social isolation, neuroticism, and psychological well-being. RESULTS: Our model could distinguish Clinical Dementia Rating Scale of 0.5 (vs. 0) with 0.78 area under the receiver operating characteristic curve (AUC), social isolation with 0.75 AUC, neuroticism with 0.71 AUC, and negative affect scales with 0.79 AUC. DISCUSSION: Our findings demonstrate the feasibility of remotely monitoring cognitive status, social isolation, neuroticism, and psychological well-being. Speech and language patterns were more useful for quantifying cognitive impairment, whereas facial expression and cardiovascular patterns using remote photoplethysmography were more useful for quantifying personality and psychological well-being.'}",oai:arXiv.org:2412.14194v1,False,"[{'term': 'cs.HC', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by-nc-sa/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-sa/4.0/'}","[{'name': 'Xiaofan Mu, Salman Seyedi, Iris Zheng, Zifan Jiang, Liu Chen, Bolaji Omofojoye, Rachel Hershenberg, Allan I. Levey, Gari D. Clifford, Hiroko H. Dodge, Hyeokhyen Kwon'}]","Xiaofan Mu, Salman Seyedi, Iris Zheng, Zifan Jiang, Liu Chen, Bolaji Omofojoye, Rachel Hershenberg, Allan I. Levey, Gari D. Clifford, Hiroko H. Dodge, Hyeokhyen Kwon","{'name': 'Xiaofan Mu, Salman Seyedi, Iris Zheng, Zifan Jiang, Liu Chen, Bolaji Omofojoye, Rachel Hershenberg, Allan I. Levey, Gari D. Clifford, Hiroko H. Dodge, Hyeokhyen Kwon'}",,
17,IMPROVE: Impact of Mobile Phones on Remote Online Virtual Education,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'IMPROVE: Impact of Mobile Phones on Remote Online Virtual Education'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14195'}]",https://arxiv.org/abs/2412.14195,"arXiv:2412.14195v1 Announce Type: new 
Abstract: This work presents the IMPROVE dataset, designed to evaluate the effects of mobile phone usage on learners during online education. The dataset not only assesses academic performance and subjective learner feedback but also captures biometric, behavioral, and physiological signals, providing a comprehensive analysis of the impact of mobile phone use on learning. Multimodal data were collected from 120 learners in three groups with different phone interaction levels. A setup involving 16 sensors was implemented to collect data that have proven to be effective indicators for understanding learner behavior and cognition, including electroencephalography waves, videos, eye tracker, etc. The dataset includes metadata from the processed videos like face bounding boxes, facial landmarks, and Euler angles for head pose estimation. In addition, learner performance data and self-reported forms are included. Phone usage events were labeled, covering both supervisor-triggered and uncontrolled events. A semi-manual re-labeling system, using head pose and eye tracker data, is proposed to improve labeling accuracy. Technical validation confirmed signal quality, with statistical analyses revealing biometric changes during phone use.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14195v1 Announce Type: new \nAbstract: This work presents the IMPROVE dataset, designed to evaluate the effects of mobile phone usage on learners during online education. The dataset not only assesses academic performance and subjective learner feedback but also captures biometric, behavioral, and physiological signals, providing a comprehensive analysis of the impact of mobile phone use on learning. Multimodal data were collected from 120 learners in three groups with different phone interaction levels. A setup involving 16 sensors was implemented to collect data that have proven to be effective indicators for understanding learner behavior and cognition, including electroencephalography waves, videos, eye tracker, etc. The dataset includes metadata from the processed videos like face bounding boxes, facial landmarks, and Euler angles for head pose estimation. In addition, learner performance data and self-reported forms are included. Phone usage events were labeled, covering both supervisor-triggered and uncontrolled events. A semi-manual re-labeling system, using head pose and eye tracker data, is proposed to improve labeling accuracy. Technical validation confirmed signal quality, with statistical analyses revealing biometric changes during phone use.'}",oai:arXiv.org:2412.14195v1,False,"[{'term': 'cs.HC', 'scheme': None, 'label': None}, {'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Roberto Daza, Alvaro Becerra, Ruth Cobos, Julian Fierrez, Aythami Morales'}]","Roberto Daza, Alvaro Becerra, Ruth Cobos, Julian Fierrez, Aythami Morales","{'name': 'Roberto Daza, Alvaro Becerra, Ruth Cobos, Julian Fierrez, Aythami Morales'}",,
18,Advancing Vehicle Plate Recognition: Multitasking Visual Language Models with VehiclePaliGemma,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Advancing Vehicle Plate Recognition: Multitasking Visual Language Models with VehiclePaliGemma'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14197'}]",https://arxiv.org/abs/2412.14197,"arXiv:2412.14197v1 Announce Type: new 
Abstract: License plate recognition (LPR) involves automated systems that utilize cameras and computer vision to read vehicle license plates. Such plates collected through LPR can then be compared against databases to identify stolen vehicles, uninsured drivers, crime suspects, and more. The LPR system plays a significant role in saving time for institutions such as the police force. In the past, LPR relied heavily on Optical Character Recognition (OCR), which has been widely explored to recognize characters in images. Usually, collected plate images suffer from various limitations, including noise, blurring, weather conditions, and close characters, making the recognition complex. Existing LPR methods still require significant improvement, especially for distorted images. To fill this gap, we propose utilizing visual language models (VLMs) such as OpenAI GPT4o, Google Gemini 1.5, Google PaliGemma (Pathways Language and Image model + Gemma model), Meta Llama 3.2, Anthropic Claude 3.5 Sonnet, LLaVA, NVIDIA VILA, and moondream2 to recognize such unclear plates with close characters. This paper evaluates the VLM's capability to address the aforementioned problems. Additionally, we introduce ``VehiclePaliGemma'', a fine-tuned Open-sourced PaliGemma VLM designed to recognize plates under challenging conditions. We compared our proposed VehiclePaliGemma with state-of-the-art methods and other VLMs using a dataset of Malaysian license plates collected under complex conditions. The results indicate that VehiclePaliGemma achieved superior performance with an accuracy of 87.6\%. Moreover, it is able to predict the car's plate at a speed of 7 frames per second using A100-80GB GPU. Finally, we explored the multitasking capability of VehiclePaliGemma model to accurately identify plates containing multiple cars of various models and colors, with plates positioned and oriented in different directions.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14197v1 Announce Type: new \nAbstract: License plate recognition (LPR) involves automated systems that utilize cameras and computer vision to read vehicle license plates. Such plates collected through LPR can then be compared against databases to identify stolen vehicles, uninsured drivers, crime suspects, and more. The LPR system plays a significant role in saving time for institutions such as the police force. In the past, LPR relied heavily on Optical Character Recognition (OCR), which has been widely explored to recognize characters in images. Usually, collected plate images suffer from various limitations, including noise, blurring, weather conditions, and close characters, making the recognition complex. Existing LPR methods still require significant improvement, especially for distorted images. To fill this gap, we propose utilizing visual language models (VLMs) such as OpenAI GPT4o, Google Gemini 1.5, Google PaliGemma (Pathways Language and Image model + Gemma model), Meta Llama 3.2, Anthropic Claude 3.5 Sonnet, LLaVA, NVIDIA VILA, and moondream2 to recognize such unclear plates with close characters. This paper evaluates the VLM's capability to address the aforementioned problems. Additionally, we introduce ``VehiclePaliGemma'', a fine-tuned Open-sourced PaliGemma VLM designed to recognize plates under challenging conditions. We compared our proposed VehiclePaliGemma with state-of-the-art methods and other VLMs using a dataset of Malaysian license plates collected under complex conditions. The results indicate that VehiclePaliGemma achieved superior performance with an accuracy of 87.6\\%. Moreover, it is able to predict the car's plate at a speed of 7 frames per second using A100-80GB GPU. Finally, we explored the multitasking capability of VehiclePaliGemma model to accurately identify plates containing multiple cars of various models and colors, with plates positioned and oriented in different directions.""}",oai:arXiv.org:2412.14197v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Nouar AlDahoul, Myles Joshua Toledo Tan, Raghava Reddy Tera, Hezerul Abdul Karim, Chee How Lim, Manish Kumar Mishra, Yasir Zaki'}]","Nouar AlDahoul, Myles Joshua Toledo Tan, Raghava Reddy Tera, Hezerul Abdul Karim, Chee How Lim, Manish Kumar Mishra, Yasir Zaki","{'name': 'Nouar AlDahoul, Myles Joshua Toledo Tan, Raghava Reddy Tera, Hezerul Abdul Karim, Chee How Lim, Manish Kumar Mishra, Yasir Zaki'}",,
19,Designing Human and Generative AI Collaboration,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Designing Human and Generative AI Collaboration'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14199'}]",https://arxiv.org/abs/2412.14199,"arXiv:2412.14199v1 Announce Type: new 
Abstract: We examined the effectiveness of human-AI collaboration designs in creative work. Through a human subjects experiment in the context of creative writing, we found that while AI assistance improved productivity across all models, collaboration design significantly influenced output quality, user satisfaction, and content characteristics. Models incorporating human creative input delivered higher content interestingness and overall quality as well as greater task performer satisfaction compared to conditions where humans were limited to confirming AI outputs. Increased AI involvement encouraged creators to explore beyond personal experience but also led to greater story and genre similarities among participants. However, this effect was mitigated through human creative input. These findings underscore the importance of preserving the human creative role to ensure quality, satisfaction, and creative diversity in human-AI collaboration.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14199v1 Announce Type: new \nAbstract: We examined the effectiveness of human-AI collaboration designs in creative work. Through a human subjects experiment in the context of creative writing, we found that while AI assistance improved productivity across all models, collaboration design significantly influenced output quality, user satisfaction, and content characteristics. Models incorporating human creative input delivered higher content interestingness and overall quality as well as greater task performer satisfaction compared to conditions where humans were limited to confirming AI outputs. Increased AI involvement encouraged creators to explore beyond personal experience but also led to greater story and genre similarities among participants. However, this effect was mitigated through human creative input. These findings underscore the importance of preserving the human creative role to ensure quality, satisfaction, and creative diversity in human-AI collaboration.'}",oai:arXiv.org:2412.14199v1,False,"[{'term': 'cs.HC', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by-nc-sa/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-sa/4.0/'}","[{'name': 'Kartik Hosanagar, Daehwan Ahn'}]","Kartik Hosanagar, Daehwan Ahn","{'name': 'Kartik Hosanagar, Daehwan Ahn'}",,
20,ActiveAI: Enabling K-12 AI Literacy Education & Analytics at Scale,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'ActiveAI: Enabling K-12 AI Literacy Education & Analytics at Scale'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14200'}]",https://arxiv.org/abs/2412.14200,"arXiv:2412.14200v1 Announce Type: new 
Abstract: Interest in K-12 AI Literacy education has surged in the past year, yet large-scale learning data remains scarce despite considerable efforts in developing learning materials and running summer programs. To make larger scale dataset available and enable more replicable findings, we developed an intelligent online learning platform featuring AI Literacy modules and assessments, engaging 1,000 users from 12 secondary schools. Preliminary analysis of the data reveals patterns in prior knowledge levels of AI Literacy, gender differences in assessment scores, and the effectiveness of instructional activities. With open access to this de-identified dataset, researchers can perform secondary analyses, advancing the understanding in this emerging field of AI Literacy education.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14200v1 Announce Type: new \nAbstract: Interest in K-12 AI Literacy education has surged in the past year, yet large-scale learning data remains scarce despite considerable efforts in developing learning materials and running summer programs. To make larger scale dataset available and enable more replicable findings, we developed an intelligent online learning platform featuring AI Literacy modules and assessments, engaging 1,000 users from 12 secondary schools. Preliminary analysis of the data reveals patterns in prior knowledge levels of AI Literacy, gender differences in assessment scores, and the effectiveness of instructional activities. With open access to this de-identified dataset, researchers can perform secondary analyses, advancing the understanding in this emerging field of AI Literacy education.'}",oai:arXiv.org:2412.14200v1,False,"[{'term': 'cs.HC', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Ruiwei Xiao, Ying-Jui Tseng, Hanqi Li, Hsuan Nieu, Guanze Liao, John Stamper, Kenneth Koedinger'}]","Ruiwei Xiao, Ying-Jui Tseng, Hanqi Li, Hsuan Nieu, Guanze Liao, John Stamper, Kenneth Koedinger","{'name': 'Ruiwei Xiao, Ying-Jui Tseng, Hanqi Li, Hsuan Nieu, Guanze Liao, John Stamper, Kenneth Koedinger'}",,
21,"The ""Huh?"" Button: Improving Understanding in Educational Videos with Large Language Models","{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'The ""Huh?"" Button: Improving Understanding in Educational Videos with Large Language Models'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14201'}]",https://arxiv.org/abs/2412.14201,"arXiv:2412.14201v1 Announce Type: new 
Abstract: We propose a simple way to use large language models (LLMs) in education. Specifically, our method aims to improve individual comprehension by adding a novel feature to online videos. We combine the low threshold for interactivity in digital experiences with the benefits of rephrased and elaborated explanations typical of face-to-face interactions, thereby supporting to close knowledge gaps at scale. To demonstrate the technical feasibility of our approach, we conducted a proof-of-concept experiment and implemented a prototype which is available for testing online. Through the use case, we also show how caching can be applied in LLM-powered applications to reduce their carbon footprint.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14201v1 Announce Type: new \nAbstract: We propose a simple way to use large language models (LLMs) in education. Specifically, our method aims to improve individual comprehension by adding a novel feature to online videos. We combine the low threshold for interactivity in digital experiences with the benefits of rephrased and elaborated explanations typical of face-to-face interactions, thereby supporting to close knowledge gaps at scale. To demonstrate the technical feasibility of our approach, we conducted a proof-of-concept experiment and implemented a prototype which is available for testing online. Through the use case, we also show how caching can be applied in LLM-powered applications to reduce their carbon footprint.'}",oai:arXiv.org:2412.14201v1,False,"[{'term': 'cs.HC', 'scheme': None, 'label': None}, {'term': 'cs.CY', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Boris Ruf, Marcin Detyniecki'}]","Boris Ruf, Marcin Detyniecki","{'name': 'Boris Ruf, Marcin Detyniecki'}",,
22,BlenderLLM: Training Large Language Models for Computer-Aided Design with Self-improvement,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'BlenderLLM: Training Large Language Models for Computer-Aided Design with Self-improvement'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14203'}]",https://arxiv.org/abs/2412.14203,"arXiv:2412.14203v1 Announce Type: new 
Abstract: The application of Large Language Models (LLMs) in Computer-Aided Design (CAD) remains an underexplored area, despite their remarkable advancements in other domains. In this paper, we present BlenderLLM, a novel framework for training LLMs specifically for CAD tasks leveraging a self-improvement methodology. To support this, we developed a bespoke training dataset, BlendNet, and introduced a comprehensive evaluation suite, CADBench. Our results reveal that existing models demonstrate significant limitations in generating accurate CAD scripts. However, through minimal instruction-based fine-tuning and iterative self-improvement, BlenderLLM significantly surpasses these models in both functionality and accuracy of CAD script generation. This research establishes a strong foundation for the application of LLMs in CAD while demonstrating the transformative potential of self-improving models in advancing CAD automation. We encourage further exploration and adoption of these methodologies to drive innovation in the field. The dataset, model, benchmark, and source code are publicly available at https://github.com/FreedomIntelligence/BlenderLLM","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14203v1 Announce Type: new \nAbstract: The application of Large Language Models (LLMs) in Computer-Aided Design (CAD) remains an underexplored area, despite their remarkable advancements in other domains. In this paper, we present BlenderLLM, a novel framework for training LLMs specifically for CAD tasks leveraging a self-improvement methodology. To support this, we developed a bespoke training dataset, BlendNet, and introduced a comprehensive evaluation suite, CADBench. Our results reveal that existing models demonstrate significant limitations in generating accurate CAD scripts. However, through minimal instruction-based fine-tuning and iterative self-improvement, BlenderLLM significantly surpasses these models in both functionality and accuracy of CAD script generation. This research establishes a strong foundation for the application of LLMs in CAD while demonstrating the transformative potential of self-improving models in advancing CAD automation. We encourage further exploration and adoption of these methodologies to drive innovation in the field. The dataset, model, benchmark, and source code are publicly available at https://github.com/FreedomIntelligence/BlenderLLM'}",oai:arXiv.org:2412.14203v1,False,"[{'term': 'cs.HC', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Yuhao Du, Shunian Chen, Wenbo Zan, Peizhao Li, Mingxuan Wang, Dingjie Song, Bo Li, Yan Hu, Benyou Wang'}]","Yuhao Du, Shunian Chen, Wenbo Zan, Peizhao Li, Mingxuan Wang, Dingjie Song, Bo Li, Yan Hu, Benyou Wang","{'name': 'Yuhao Du, Shunian Chen, Wenbo Zan, Peizhao Li, Mingxuan Wang, Dingjie Song, Bo Li, Yan Hu, Benyou Wang'}",,
23,Large-scale Group Brainstorming using Conversational Swarm Intelligence (CSI) versus Traditional Chat,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Large-scale Group Brainstorming using Conversational Swarm Intelligence (CSI) versus Traditional Chat'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14205'}]",https://arxiv.org/abs/2412.14205,"arXiv:2412.14205v1 Announce Type: new 
Abstract: Conversational Swarm Intelligence (CSI) is an AI-facilitated method for enabling real-time conversational deliberations and prioritizations among networked human groups of potentially unlimited size. Based on the biological principle of Swarm Intelligence and modelled on the decision-making dynamics of fish schools, CSI has been shown in prior studies to amplify group intelligence, increase group participation, and facilitate productive collaboration among hundreds of participants at once. It works by dividing a large population into a set of small subgroups that are woven together by real-time AI agents called Conversational Surrogates. The present study focuses on the use of a CSI platform called Thinkscape to enable real-time brainstorming and prioritization among groups of 75 networked users. The study employed a variant of a common brainstorming intervention called an Alternative Use Task (AUT) and was designed to compare through subjective feedback, the experience of participants brainstorming using a CSI structure vs brainstorming in a single large chat room. This comparison revealed that participants significantly preferred brainstorming with the CSI structure and reported that it felt (i) more collaborative, (ii) more productive, and (iii) was better at surfacing quality answers. In addition, participants using the CSI structure reported (iv) feeling more ownership and more buy-in in the final answers the group converged on and (v) reported feeling more heard as compared to brainstorming in a traditional text chat environment. Overall, the results suggest that CSI is a very promising AI-facilitated method for brainstorming and prioritization among large-scale, networked human groups.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14205v1 Announce Type: new \nAbstract: Conversational Swarm Intelligence (CSI) is an AI-facilitated method for enabling real-time conversational deliberations and prioritizations among networked human groups of potentially unlimited size. Based on the biological principle of Swarm Intelligence and modelled on the decision-making dynamics of fish schools, CSI has been shown in prior studies to amplify group intelligence, increase group participation, and facilitate productive collaboration among hundreds of participants at once. It works by dividing a large population into a set of small subgroups that are woven together by real-time AI agents called Conversational Surrogates. The present study focuses on the use of a CSI platform called Thinkscape to enable real-time brainstorming and prioritization among groups of 75 networked users. The study employed a variant of a common brainstorming intervention called an Alternative Use Task (AUT) and was designed to compare through subjective feedback, the experience of participants brainstorming using a CSI structure vs brainstorming in a single large chat room. This comparison revealed that participants significantly preferred brainstorming with the CSI structure and reported that it felt (i) more collaborative, (ii) more productive, and (iii) was better at surfacing quality answers. In addition, participants using the CSI structure reported (iv) feeling more ownership and more buy-in in the final answers the group converged on and (v) reported feeling more heard as compared to brainstorming in a traditional text chat environment. Overall, the results suggest that CSI is a very promising AI-facilitated method for brainstorming and prioritization among large-scale, networked human groups.'}",oai:arXiv.org:2412.14205v1,False,"[{'term': 'cs.HC', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.SI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by-nc-nd/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-nd/4.0/'}","[{'name': 'Louis Rosenberg, Hans Schumann, Christopher Dishop, Gregg Willcox, Anita Woolley, Ganesh Mani'}]","Louis Rosenberg, Hans Schumann, Christopher Dishop, Gregg Willcox, Anita Woolley, Ganesh Mani","{'name': 'Louis Rosenberg, Hans Schumann, Christopher Dishop, Gregg Willcox, Anita Woolley, Ganesh Mani'}",,
24,Design of an AI-Enhanced Digital Stethoscope: Advancing Cardiovascular Diagnostics Through Smart Auscultation,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Design of an AI-Enhanced Digital Stethoscope: Advancing Cardiovascular Diagnostics Through Smart Auscultation'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14206'}]",https://arxiv.org/abs/2412.14206,"arXiv:2412.14206v1 Announce Type: new 
Abstract: In the ever-evolving landscape of medical diagnostics, this study details the systematic design process and concept selection methodology for developing an advanced digital stethoscope, demonstrating the evolution from traditional acoustic models to AI-enhanced digital solutions. The device integrates cutting-edge AI technology with traditional auscultation methods to create a more accurate, efficient, and user-friendly diagnostic tool. Through systematic product planning, customer need analysis, and rigorous specification development, we identified key opportunities to enhance conventional stethoscope functionality. The proposed system features real-time sound analysis, automated classification of heart sounds, wireless connectivity for remote consultations, and an intuitive user interface accessible via smartphone integration. The design process employed a methodical approach incorporating customer feedback, competitive benchmarking, and systematic concept generation and selection. Through a structured evaluation framework, we analyzed portability, frequency response sensitivity, transmission quality, maintenance ease, user interface simplicity, output signal quality, power efficiency, and cost-effectiveness. The final design prioritizes biocompatibility, reliability, and cost-effectiveness while addressing the growing demand for telemedicine capabilities in cardiovascular care. The project emphasizes the transition from conventional design to advanced digital solutions while maintaining a focus on practical clinical applications. Each concept was modelled using SOLIDWORKS software, enabling detailed visualization and engineering analysis. This systematic approach to concept screening and selection ensures the final design meets both current healthcare needs and future technological adaptability.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14206v1 Announce Type: new \nAbstract: In the ever-evolving landscape of medical diagnostics, this study details the systematic design process and concept selection methodology for developing an advanced digital stethoscope, demonstrating the evolution from traditional acoustic models to AI-enhanced digital solutions. The device integrates cutting-edge AI technology with traditional auscultation methods to create a more accurate, efficient, and user-friendly diagnostic tool. Through systematic product planning, customer need analysis, and rigorous specification development, we identified key opportunities to enhance conventional stethoscope functionality. The proposed system features real-time sound analysis, automated classification of heart sounds, wireless connectivity for remote consultations, and an intuitive user interface accessible via smartphone integration. The design process employed a methodical approach incorporating customer feedback, competitive benchmarking, and systematic concept generation and selection. Through a structured evaluation framework, we analyzed portability, frequency response sensitivity, transmission quality, maintenance ease, user interface simplicity, output signal quality, power efficiency, and cost-effectiveness. The final design prioritizes biocompatibility, reliability, and cost-effectiveness while addressing the growing demand for telemedicine capabilities in cardiovascular care. The project emphasizes the transition from conventional design to advanced digital solutions while maintaining a focus on practical clinical applications. Each concept was modelled using SOLIDWORKS software, enabling detailed visualization and engineering analysis. This systematic approach to concept screening and selection ensures the final design meets both current healthcare needs and future technological adaptability.'}",oai:arXiv.org:2412.14206v1,False,"[{'term': 'cs.HC', 'scheme': None, 'label': None}, {'term': 'cs.AR', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by-sa/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-sa/4.0/'}","[{'name': 'Abraham G. Taye, Sador Yemane, Eshetu Negash, Yared Minwuyelet, Nebiha Tofik'}]","Abraham G. Taye, Sador Yemane, Eshetu Negash, Yared Minwuyelet, Nebiha Tofik","{'name': 'Abraham G. Taye, Sador Yemane, Eshetu Negash, Yared Minwuyelet, Nebiha Tofik'}",,https://www.researchgate.net/publication/369693979_Design_for_Advanced_Digital_Stethoscope (2021)
25,A Comprehensive Review on Traffic Datasets and Simulators for Autonomous Vehicles,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'A Comprehensive Review on Traffic Datasets and Simulators for Autonomous Vehicles'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14207'}]",https://arxiv.org/abs/2412.14207,"arXiv:2412.14207v1 Announce Type: new 
Abstract: Autonomous driving has rapidly developed and shown promising performance due to recent advances in hardware and deep learning techniques. High-quality datasets are fundamental for developing reliable autonomous driving algorithms. Previous dataset surveys either focused on a limited number or lacked detailed investigation of dataset characteristics. Besides, we analyze the annotation processes, existing labeling tools, and the annotation quality of datasets, showing the importance of establishing a standard annotation pipeline. On the other hand, we thoroughly analyze the impact of geographical and adversarial environmental conditions on the performance of autonomous driving systems. Moreover, we exhibit the data distribution of several vital datasets and discuss their pros and cons accordingly.
  Additionally, this paper provides a comprehensive analysis of publicly available traffic simulators. In addition to informing about traffic datasets, it is also the goal of this paper to provide context and information on the current capabilities of traffic simulators for their specific contributions to autonomous vehicle simulation and development. Furthermore, this paper discusses future directions and the increasing importance of synthetic data generation in simulators to enhance the training and creation of effective simulations. Finally, we discuss the current challenges and the development trend of future autonomous driving datasets.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14207v1 Announce Type: new \nAbstract: Autonomous driving has rapidly developed and shown promising performance due to recent advances in hardware and deep learning techniques. High-quality datasets are fundamental for developing reliable autonomous driving algorithms. Previous dataset surveys either focused on a limited number or lacked detailed investigation of dataset characteristics. Besides, we analyze the annotation processes, existing labeling tools, and the annotation quality of datasets, showing the importance of establishing a standard annotation pipeline. On the other hand, we thoroughly analyze the impact of geographical and adversarial environmental conditions on the performance of autonomous driving systems. Moreover, we exhibit the data distribution of several vital datasets and discuss their pros and cons accordingly.\n  Additionally, this paper provides a comprehensive analysis of publicly available traffic simulators. In addition to informing about traffic datasets, it is also the goal of this paper to provide context and information on the current capabilities of traffic simulators for their specific contributions to autonomous vehicle simulation and development. Furthermore, this paper discusses future directions and the increasing importance of synthetic data generation in simulators to enhance the training and creation of effective simulations. Finally, we discuss the current challenges and the development trend of future autonomous driving datasets.'}",oai:arXiv.org:2412.14207v1,False,"[{'term': 'cs.RO', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Supriya Sarker, Brent Maples, Weizi Li'}]","Supriya Sarker, Brent Maples, Weizi Li","{'name': 'Supriya Sarker, Brent Maples, Weizi Li'}",,
26,Beacon: A Naturalistic Driving Dataset During Blackouts for Benchmarking Traffic Reconstruction and Control,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Beacon: A Naturalistic Driving Dataset During Blackouts for Benchmarking Traffic Reconstruction and Control'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14208'}]",https://arxiv.org/abs/2412.14208,"arXiv:2412.14208v1 Announce Type: new 
Abstract: Extreme weather events and other vulnerabilities are causing blackouts with increasing frequency, disrupting traffic control systems and posing significant challenges to urban mobility. To address this growing concern, we introduce \model{}, a naturalistic driving dataset collected during blackouts at complex intersections. Beacon provides detailed traffic data from two unsignalized intersections in Memphis, TN, including timesteps, origin, and destination lanes for each vehicle over four hours. We analyze traffic demand, vehicle trajectories, and density across different scenarios. We also use the dataset to reconstruct unsignalized, signalized and mixed traffic conditions, demonstrating its utility for benchmarking traffic reconstruction techniques and control methods. To the best of our knowledge, Beacon could be the first public available traffic dataset that captures naturalistic driving behaviors at complex intersections.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14208v1 Announce Type: new \nAbstract: Extreme weather events and other vulnerabilities are causing blackouts with increasing frequency, disrupting traffic control systems and posing significant challenges to urban mobility. To address this growing concern, we introduce \\model{}, a naturalistic driving dataset collected during blackouts at complex intersections. Beacon provides detailed traffic data from two unsignalized intersections in Memphis, TN, including timesteps, origin, and destination lanes for each vehicle over four hours. We analyze traffic demand, vehicle trajectories, and density across different scenarios. We also use the dataset to reconstruct unsignalized, signalized and mixed traffic conditions, demonstrating its utility for benchmarking traffic reconstruction techniques and control methods. To the best of our knowledge, Beacon could be the first public available traffic dataset that captures naturalistic driving behaviors at complex intersections.'}",oai:arXiv.org:2412.14208v1,False,"[{'term': 'cs.RO', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Supriya Sarker, Iftekharul Islam, Bibek Poudel, Weizi Li'}]","Supriya Sarker, Iftekharul Islam, Bibek Poudel, Weizi Li","{'name': 'Supriya Sarker, Iftekharul Islam, Bibek Poudel, Weizi Li'}",,
27,Integrating Evidence into the Design of XAI and AI-based Decision Support Systems: A Means-End Framework for End-users in Construction,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Integrating Evidence into the Design of XAI and AI-based Decision Support Systems: A Means-End Framework for End-users in Construction'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14209'}]",https://arxiv.org/abs/2412.14209,"arXiv:2412.14209v1 Announce Type: new 
Abstract: A narrative review is used to develop a theoretical evidence-based means-end framework to build an epistemic foundation to uphold explainable artificial intelligence instruments so that the reliability of outcomes generated from decision support systems can be assured and better explained to end-users. The implications of adopting an evidence-based approach to designing decision support systems in construction are discussed with emphasis placed on evaluating the strength, value, and utility of evidence needed to develop meaningful human explanations for end-users. While the developed means-end framework is focused on end-users, stakeholders can also utilize it to create meaningful human explanations. However, they will vary due to their different epistemic goals. Including evidence in the design and development of explainable artificial intelligence and decision support systems will improve decision-making effectiveness, enabling end-users' epistemic goals to be achieved. The proposed means-end framework is developed from a broad spectrum of literature. Thus, it is suggested that it can be used in construction and other engineering domains where there is a need to integrate evidence into the design of explainable artificial intelligence and decision support systems.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14209v1 Announce Type: new \nAbstract: A narrative review is used to develop a theoretical evidence-based means-end framework to build an epistemic foundation to uphold explainable artificial intelligence instruments so that the reliability of outcomes generated from decision support systems can be assured and better explained to end-users. The implications of adopting an evidence-based approach to designing decision support systems in construction are discussed with emphasis placed on evaluating the strength, value, and utility of evidence needed to develop meaningful human explanations for end-users. While the developed means-end framework is focused on end-users, stakeholders can also utilize it to create meaningful human explanations. However, they will vary due to their different epistemic goals. Including evidence in the design and development of explainable artificial intelligence and decision support systems will improve decision-making effectiveness, enabling end-users' epistemic goals to be achieved. The proposed means-end framework is developed from a broad spectrum of literature. Thus, it is suggested that it can be used in construction and other engineering domains where there is a need to integrate evidence into the design of explainable artificial intelligence and decision support systems.""}",oai:arXiv.org:2412.14209v1,False,"[{'term': 'cs.HC', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Peter . E. D. Love, Jane Matthews, Weili Fang, Hadi Mahamivanan'}]","Peter . E. D. Love, Jane Matthews, Weili Fang, Hadi Mahamivanan","{'name': 'Peter . E. D. Love, Jane Matthews, Weili Fang, Hadi Mahamivanan'}",,
28,Mobilizing Waldo: Evaluating Multimodal AI for Public Mobilization,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Mobilizing Waldo: Evaluating Multimodal AI for Public Mobilization'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14210'}]",https://arxiv.org/abs/2412.14210,"arXiv:2412.14210v1 Announce Type: new 
Abstract: Advancements in multimodal Large Language Models (LLMs), such as OpenAI's GPT-4o, offer significant potential for mediating human interactions across various contexts. However, their use in areas such as persuasion, influence, and recruitment raises ethical and security concerns. To evaluate these models ethically in public influence and persuasion scenarios, we developed a prompting strategy using ""Where's Waldo?"" images as proxies for complex, crowded gatherings. This approach provides a controlled, replicable environment to assess the model's ability to process intricate visual information, interpret social dynamics, and propose engagement strategies while avoiding privacy concerns. By positioning Waldo as a hypothetical agent tasked with face-to-face mobilization, we analyzed the model's performance in identifying key individuals and formulating mobilization tactics. Our results show that while the model generates vivid descriptions and creative strategies, it cannot accurately identify individuals or reliably assess social dynamics in these scenarios. Nevertheless, this methodology provides a valuable framework for testing and benchmarking the evolving capabilities of multimodal LLMs in social contexts.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14210v1 Announce Type: new \nAbstract: Advancements in multimodal Large Language Models (LLMs), such as OpenAI\'s GPT-4o, offer significant potential for mediating human interactions across various contexts. However, their use in areas such as persuasion, influence, and recruitment raises ethical and security concerns. To evaluate these models ethically in public influence and persuasion scenarios, we developed a prompting strategy using ""Where\'s Waldo?"" images as proxies for complex, crowded gatherings. This approach provides a controlled, replicable environment to assess the model\'s ability to process intricate visual information, interpret social dynamics, and propose engagement strategies while avoiding privacy concerns. By positioning Waldo as a hypothetical agent tasked with face-to-face mobilization, we analyzed the model\'s performance in identifying key individuals and formulating mobilization tactics. Our results show that while the model generates vivid descriptions and creative strategies, it cannot accurately identify individuals or reliably assess social dynamics in these scenarios. Nevertheless, this methodology provides a valuable framework for testing and benchmarking the evolving capabilities of multimodal LLMs in social contexts.'}",oai:arXiv.org:2412.14210v1,False,"[{'term': 'cs.HC', 'scheme': None, 'label': None}, {'term': 'cs.CY', 'scheme': None, 'label': None}, {'term': 'cs.SI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Manuel Cebrian, Petter Holme, Niccolo Pescetelli'}]","Manuel Cebrian, Petter Holme, Niccolo Pescetelli","{'name': 'Manuel Cebrian, Petter Holme, Niccolo Pescetelli'}",,
29,Improving Generalization Performance of YOLOv8 for Camera Trap Object Detection,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Improving Generalization Performance of YOLOv8 for Camera Trap Object Detection'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14211'}]",https://arxiv.org/abs/2412.14211,"arXiv:2412.14211v1 Announce Type: new 
Abstract: Camera traps have become integral tools in wildlife conservation, providing non-intrusive means to monitor and study wildlife in their natural habitats. The utilization of object detection algorithms to automate species identification from Camera Trap images is of huge importance for research and conservation purposes. However, the generalization issue, where the trained model is unable to apply its learnings to a never-before-seen dataset, is prevalent. This thesis explores the enhancements made to the YOLOv8 object detection algorithm to address the problem of generalization. The study delves into the limitations of the baseline YOLOv8 model, emphasizing its struggles with generalization in real-world environments. To overcome these limitations, enhancements are proposed, including the incorporation of a Global Attention Mechanism (GAM) module, modified multi-scale feature fusion, and Wise Intersection over Union (WIoUv3) as a bounding box regression loss function. A thorough evaluation and ablation experiments reveal the improved model's ability to suppress the background noise, focus on object properties, and exhibit robust generalization in novel environments. The proposed enhancements not only address the challenges inherent in camera trap datasets but also pave the way for broader applicability in real-world conservation scenarios, ultimately aiding in the effective management of wildlife populations and habitats.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14211v1 Announce Type: new \nAbstract: Camera traps have become integral tools in wildlife conservation, providing non-intrusive means to monitor and study wildlife in their natural habitats. The utilization of object detection algorithms to automate species identification from Camera Trap images is of huge importance for research and conservation purposes. However, the generalization issue, where the trained model is unable to apply its learnings to a never-before-seen dataset, is prevalent. This thesis explores the enhancements made to the YOLOv8 object detection algorithm to address the problem of generalization. The study delves into the limitations of the baseline YOLOv8 model, emphasizing its struggles with generalization in real-world environments. To overcome these limitations, enhancements are proposed, including the incorporation of a Global Attention Mechanism (GAM) module, modified multi-scale feature fusion, and Wise Intersection over Union (WIoUv3) as a bounding box regression loss function. A thorough evaluation and ablation experiments reveal the improved model's ability to suppress the background noise, focus on object properties, and exhibit robust generalization in novel environments. The proposed enhancements not only address the challenges inherent in camera trap datasets but also pave the way for broader applicability in real-world conservation scenarios, ultimately aiding in the effective management of wildlife populations and habitats.""}",oai:arXiv.org:2412.14211v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}",[{'name': 'Aroj Subedi'}],Aroj Subedi,{'name': 'Aroj Subedi'},,
30,Tree-of-Code: A Hybrid Approach for Robust Complex Task Planning and Execution,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Tree-of-Code: A Hybrid Approach for Robust Complex Task Planning and Execution'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14212'}]",https://arxiv.org/abs/2412.14212,"arXiv:2412.14212v1 Announce Type: new 
Abstract: The exceptional capabilities of large language models (LLMs) have substantially accelerated the rapid rise and widespread adoption of agents. Recent studies have demonstrated that generating Python code to consolidate LLM-based agents' actions into a unified action space (CodeAct) is a promising approach for developing real-world LLM agents. However, this step-by-step code generation approach often lacks consistency and robustness, leading to instability in agent applications, particularly for complex reasoning and out-of-domain tasks. In this paper, we propose a novel approach called Tree-of-Code (ToC) to tackle the challenges of complex problem planning and execution with an end-to-end mechanism. By integrating key ideas from both Tree-of-Thought and CodeAct, ToC combines their strengths to enhance solution exploration. In our framework, each final code execution result is treated as a node in the decision tree, with a breadth-first search strategy employed to explore potential solutions. The final outcome is determined through a voting mechanism based on the outputs of the nodes.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14212v1 Announce Type: new \nAbstract: The exceptional capabilities of large language models (LLMs) have substantially accelerated the rapid rise and widespread adoption of agents. Recent studies have demonstrated that generating Python code to consolidate LLM-based agents' actions into a unified action space (CodeAct) is a promising approach for developing real-world LLM agents. However, this step-by-step code generation approach often lacks consistency and robustness, leading to instability in agent applications, particularly for complex reasoning and out-of-domain tasks. In this paper, we propose a novel approach called Tree-of-Code (ToC) to tackle the challenges of complex problem planning and execution with an end-to-end mechanism. By integrating key ideas from both Tree-of-Thought and CodeAct, ToC combines their strengths to enhance solution exploration. In our framework, each final code execution result is treated as a node in the decision tree, with a breadth-first search strategy employed to explore potential solutions. The final outcome is determined through a voting mechanism based on the outputs of the nodes.""}",oai:arXiv.org:2412.14212v1,False,"[{'term': 'cs.SE', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Ziyi Ni, Yifan Li, Daxiang Dong'}]","Ziyi Ni, Yifan Li, Daxiang Dong","{'name': 'Ziyi Ni, Yifan Li, Daxiang Dong'}",,
31,GraphicsDreamer: Image to 3D Generation with Physical Consistency,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'GraphicsDreamer: Image to 3D Generation with Physical Consistency'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14214'}]",https://arxiv.org/abs/2412.14214,"arXiv:2412.14214v1 Announce Type: new 
Abstract: Recently, the surge of efficient and automated 3D AI-generated content (AIGC) methods has increasingly illuminated the path of transforming human imagination into complex 3D structures. However, the automated generation of 3D content is still significantly lags in industrial application. This gap exists because 3D modeling demands high-quality assets with sharp geometry, exquisite topology, and physically based rendering (PBR), among other criteria. To narrow the disparity between generated results and artists' expectations, we introduce GraphicsDreamer, a method for creating highly usable 3D meshes from single images. To better capture the geometry and material details, we integrate the PBR lighting equation into our cross-domain diffusion model, concurrently predicting multi-view color, normal, depth images, and PBR materials. In the geometry fusion stage, we continue to enforce the PBR constraints, ensuring that the generated 3D objects possess reliable texture details, supporting realistic relighting. Furthermore, our method incorporates topology optimization and fast UV unwrapping capabilities, allowing the 3D products to be seamlessly imported into graphics engines. Extensive experiments demonstrate that our model can produce high quality 3D assets in a reasonable time cost compared to previous methods.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14214v1 Announce Type: new \nAbstract: Recently, the surge of efficient and automated 3D AI-generated content (AIGC) methods has increasingly illuminated the path of transforming human imagination into complex 3D structures. However, the automated generation of 3D content is still significantly lags in industrial application. This gap exists because 3D modeling demands high-quality assets with sharp geometry, exquisite topology, and physically based rendering (PBR), among other criteria. To narrow the disparity between generated results and artists' expectations, we introduce GraphicsDreamer, a method for creating highly usable 3D meshes from single images. To better capture the geometry and material details, we integrate the PBR lighting equation into our cross-domain diffusion model, concurrently predicting multi-view color, normal, depth images, and PBR materials. In the geometry fusion stage, we continue to enforce the PBR constraints, ensuring that the generated 3D objects possess reliable texture details, supporting realistic relighting. Furthermore, our method incorporates topology optimization and fast UV unwrapping capabilities, allowing the 3D products to be seamlessly imported into graphics engines. Extensive experiments demonstrate that our model can produce high quality 3D assets in a reasonable time cost compared to previous methods.""}",oai:arXiv.org:2412.14214v1,False,"[{'term': 'cs.GR', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by-nc-nd/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-nd/4.0/'}","[{'name': 'Pei Chen, Fudong Wang, Yixuan Tong, Jingdong Chen, Ming Yang, Minghui Yang'}]","Pei Chen, Fudong Wang, Yixuan Tong, Jingdong Chen, Ming Yang, Minghui Yang","{'name': 'Pei Chen, Fudong Wang, Yixuan Tong, Jingdong Chen, Ming Yang, Minghui Yang'}",,
32,Generative AI Toolkit -- a framework for increasing the quality of LLM-based applications over their whole life cycle,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Generative AI Toolkit -- a framework for increasing the quality of LLM-based applications over their whole life cycle'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14215'}]",https://arxiv.org/abs/2412.14215,"arXiv:2412.14215v1 Announce Type: new 
Abstract: As LLM-based applications reach millions of customers, ensuring their scalability and continuous quality improvement is critical for success. However, the current workflows for developing, maintaining, and operating (DevOps) these applications are predominantly manual, slow, and based on trial-and-error. With this paper we introduce the Generative AI Toolkit, which automates essential workflows over the whole life cycle of LLM-based applications. The toolkit helps to configure, test, continuously monitor and optimize Generative AI applications such as agents, thus significantly improving quality while shortening release cycles. We showcase the effectiveness of our toolkit on representative use cases, share best practices, and outline future enhancements. Since we are convinced that our Generative AI Toolkit is helpful for other teams, we are open sourcing it on and hope that others will use, forward, adapt and improve","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14215v1 Announce Type: new \nAbstract: As LLM-based applications reach millions of customers, ensuring their scalability and continuous quality improvement is critical for success. However, the current workflows for developing, maintaining, and operating (DevOps) these applications are predominantly manual, slow, and based on trial-and-error. With this paper we introduce the Generative AI Toolkit, which automates essential workflows over the whole life cycle of LLM-based applications. The toolkit helps to configure, test, continuously monitor and optimize Generative AI applications such as agents, thus significantly improving quality while shortening release cycles. We showcase the effectiveness of our toolkit on representative use cases, share best practices, and outline future enhancements. Since we are convinced that our Generative AI Toolkit is helpful for other teams, we are open sourcing it on and hope that others will use, forward, adapt and improve'}",oai:arXiv.org:2412.14215v1,False,"[{'term': 'cs.SE', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Jens Kohl, Luisa Gloger, Rui Costa, Otto Kruse, Manuel P. Luitz, David Katz, Gonzalo Barbeito, Markus Schweier, Ryan French, Jonas Schroeder, Thomas Riedl, Raphael Perri, Youssef Mostafa'}]","Jens Kohl, Luisa Gloger, Rui Costa, Otto Kruse, Manuel P. Luitz, David Katz, Gonzalo Barbeito, Markus Schweier, Ryan French, Jonas Schroeder, Thomas Riedl, Raphael Perri, Youssef Mostafa","{'name': 'Jens Kohl, Luisa Gloger, Rui Costa, Otto Kruse, Manuel P. Luitz, David Katz, Gonzalo Barbeito, Markus Schweier, Ryan French, Jonas Schroeder, Thomas Riedl, Raphael Perri, Youssef Mostafa'}",,
33,Development and Adoption of SATD Detection Tools: A State-of-practice Report,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Development and Adoption of SATD Detection Tools: A State-of-practice Report'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14217'}]",https://arxiv.org/abs/2412.14217,"arXiv:2412.14217v1 Announce Type: new 
Abstract: Self-Admitted Technical Debt (SATD) refers to instances where developers knowingly introduce suboptimal solutions into code and document them, often through textual artifacts. This paper provides a comprehensive state-of-practice report on the development and adoption of SATD detection tools. Through a systematic review of the available literature and tools, we examined their overall accessibility. Our findings reveal that, although SATD detection tools are crucial for maintaining software quality, many face challenges such as technological obsolescence, poor maintenance, and limited platform compatibility. Only a small number of tools are actively maintained, hindering their widespread adoption. This report discusses common anti-patterns in tool development, proposes corrections, and highlights the need for implementing Findable, Accessible, Interoperable, and Reusable (FAIR) principles and fostering greater collaboration between academia and industry to ensure the sustainability and efficacy of these tools. The insights presented here aim to drive more robust management of technical debt and enhance the reliability of SATD tools.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14217v1 Announce Type: new \nAbstract: Self-Admitted Technical Debt (SATD) refers to instances where developers knowingly introduce suboptimal solutions into code and document them, often through textual artifacts. This paper provides a comprehensive state-of-practice report on the development and adoption of SATD detection tools. Through a systematic review of the available literature and tools, we examined their overall accessibility. Our findings reveal that, although SATD detection tools are crucial for maintaining software quality, many face challenges such as technological obsolescence, poor maintenance, and limited platform compatibility. Only a small number of tools are actively maintained, hindering their widespread adoption. This report discusses common anti-patterns in tool development, proposes corrections, and highlights the need for implementing Findable, Accessible, Interoperable, and Reusable (FAIR) principles and fostering greater collaboration between academia and industry to ensure the sustainability and efficacy of these tools. The insights presented here aim to drive more robust management of technical debt and enhance the reliability of SATD tools.'}",oai:arXiv.org:2412.14217v1,False,"[{'term': 'cs.SE', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Edi Sutoyo, Andrea Capiluppi'}]","Edi Sutoyo, Andrea Capiluppi","{'name': 'Edi Sutoyo, Andrea Capiluppi'}",,
34,Heterogeneous Multi-Agent Reinforcement Learning for Distributed Channel Access in WLANs,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Heterogeneous Multi-Agent Reinforcement Learning for Distributed Channel Access in WLANs'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14218'}]",https://arxiv.org/abs/2412.14218,"arXiv:2412.14218v1 Announce Type: new 
Abstract: This paper investigates the use of multi-agent reinforcement learning (MARL) to address distributed channel access in wireless local area networks. In particular, we consider the challenging yet more practical case where the agents heterogeneously adopt value-based or policy-based reinforcement learning algorithms to train the model. We propose a heterogeneous MARL training framework, named QPMIX, which adopts a centralized training with distributed execution paradigm to enable heterogeneous agents to collaborate. Moreover, we theoretically prove the convergence of the proposed heterogeneous MARL method when using the linear value function approximation. Our method maximizes the network throughput and ensures fairness among stations, therefore, enhancing the overall network performance. Simulation results demonstrate that the proposed QPMIX algorithm improves throughput, mean delay, delay jitter, and collision rates compared with conventional carrier-sense multiple access with collision avoidance in the saturated traffic scenario. Furthermore, the QPMIX is shown to be robust in unsaturated and delay-sensitive traffic scenarios, and promotes cooperation among heterogeneous agents.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14218v1 Announce Type: new \nAbstract: This paper investigates the use of multi-agent reinforcement learning (MARL) to address distributed channel access in wireless local area networks. In particular, we consider the challenging yet more practical case where the agents heterogeneously adopt value-based or policy-based reinforcement learning algorithms to train the model. We propose a heterogeneous MARL training framework, named QPMIX, which adopts a centralized training with distributed execution paradigm to enable heterogeneous agents to collaborate. Moreover, we theoretically prove the convergence of the proposed heterogeneous MARL method when using the linear value function approximation. Our method maximizes the network throughput and ensures fairness among stations, therefore, enhancing the overall network performance. Simulation results demonstrate that the proposed QPMIX algorithm improves throughput, mean delay, delay jitter, and collision rates compared with conventional carrier-sense multiple access with collision avoidance in the saturated traffic scenario. Furthermore, the QPMIX is shown to be robust in unsaturated and delay-sensitive traffic scenarios, and promotes cooperation among heterogeneous agents.'}",oai:arXiv.org:2412.14218v1,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.NI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Jiaming Yu, Le Liang, Chongtao Guo, Ziyang Guo, Shi Jin, Geoffrey Ye Li'}]","Jiaming Yu, Le Liang, Chongtao Guo, Ziyang Guo, Shi Jin, Geoffrey Ye Li","{'name': 'Jiaming Yu, Le Liang, Chongtao Guo, Ziyang Guo, Shi Jin, Geoffrey Ye Li'}",,
35,A Survey on Inference Optimization Techniques for Mixture of Experts Models,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'A Survey on Inference Optimization Techniques for Mixture of Experts Models'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14219'}]",https://arxiv.org/abs/2412.14219,"arXiv:2412.14219v1 Announce Type: new 
Abstract: The emergence of large-scale Mixture of Experts (MoE) models has marked a significant advancement in artificial intelligence, offering enhanced model capacity and computational efficiency through conditional computation. However, the deployment and inference of these models present substantial challenges in terms of computational resources, latency, and energy efficiency. This comprehensive survey systematically analyzes the current landscape of inference optimization techniques for MoE models across the entire system stack. We first establish a taxonomical framework that categorizes optimization approaches into model-level, system-level, and hardware-level optimizations. At the model level, we examine architectural innovations including efficient expert design, attention mechanisms, various compression techniques such as pruning, quantization, and knowledge distillation, as well as algorithm improvement including dynamic routing strategies and expert merging methods. At the system level, we investigate distributed computing approaches, load balancing mechanisms, and efficient scheduling algorithms that enable scalable deployment. Furthermore, we delve into hardware-specific optimizations and co-design strategies that maximize throughput and energy efficiency. This survey not only provides a structured overview of existing solutions but also identifies key challenges and promising research directions in MoE inference optimization. Our comprehensive analysis serves as a valuable resource for researchers and practitioners working on large-scale deployment of MoE models in resource-constrained environments. To facilitate ongoing updates and the sharing of cutting-edge advances in MoE inference optimization research, we have established a repository accessible at \url{https://github.com/MoE-Inf/awesome-moe-inference/}.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14219v1 Announce Type: new \nAbstract: The emergence of large-scale Mixture of Experts (MoE) models has marked a significant advancement in artificial intelligence, offering enhanced model capacity and computational efficiency through conditional computation. However, the deployment and inference of these models present substantial challenges in terms of computational resources, latency, and energy efficiency. This comprehensive survey systematically analyzes the current landscape of inference optimization techniques for MoE models across the entire system stack. We first establish a taxonomical framework that categorizes optimization approaches into model-level, system-level, and hardware-level optimizations. At the model level, we examine architectural innovations including efficient expert design, attention mechanisms, various compression techniques such as pruning, quantization, and knowledge distillation, as well as algorithm improvement including dynamic routing strategies and expert merging methods. At the system level, we investigate distributed computing approaches, load balancing mechanisms, and efficient scheduling algorithms that enable scalable deployment. Furthermore, we delve into hardware-specific optimizations and co-design strategies that maximize throughput and energy efficiency. This survey not only provides a structured overview of existing solutions but also identifies key challenges and promising research directions in MoE inference optimization. Our comprehensive analysis serves as a valuable resource for researchers and practitioners working on large-scale deployment of MoE models in resource-constrained environments. To facilitate ongoing updates and the sharing of cutting-edge advances in MoE inference optimization research, we have established a repository accessible at \\url{https://github.com/MoE-Inf/awesome-moe-inference/}.'}",oai:arXiv.org:2412.14219v1,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.DC', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Jiacheng Liu, Peng Tang, Wenfeng Wang, Yuhang Ren, Xiaofeng Hou, Pheng-Ann Heng, Minyi Guo, Chao Li'}]","Jiacheng Liu, Peng Tang, Wenfeng Wang, Yuhang Ren, Xiaofeng Hou, Pheng-Ann Heng, Minyi Guo, Chao Li","{'name': 'Jiacheng Liu, Peng Tang, Wenfeng Wang, Yuhang Ren, Xiaofeng Hou, Pheng-Ann Heng, Minyi Guo, Chao Li'}",,
36,Distilled Pooling Transformer Encoder for Efficient Realistic Image Dehazing,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Distilled Pooling Transformer Encoder for Efficient Realistic Image Dehazing'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14220'}]",https://arxiv.org/abs/2412.14220,"arXiv:2412.14220v1 Announce Type: new 
Abstract: This paper proposes a lightweight neural network designed for realistic image dehazing, utilizing a Distilled Pooling Transformer Encoder, named DPTE-Net. Recently, while vision transformers (ViTs) have achieved great success in various vision tasks, their self-attention (SA) module's complexity scales quadratically with image resolution, hindering their applicability on resource-constrained devices. To overcome this, the proposed DPTE-Net substitutes traditional SA modules with efficient pooling mechanisms, significantly reducing computational demands while preserving ViTs' learning capabilities. To further enhance semantic feature learning, a distillation-based training process is implemented which transfers rich knowledge from a larger teacher network to DPTE-Net. Additionally, DPTE-Net is trained within a generative adversarial network (GAN) framework, leveraging the strong generalization of GAN in image restoration, and employs a transmission-aware loss function to dynamically adapt to varying haze densities. Experimental results on various benchmark datasets have shown that the proposed DPTE-Net can achieve competitive dehazing performance when compared to state-of-the-art methods while maintaining low computational complexity, making it a promising solution for resource-limited applications. The code of this work is available at https://github.com/tranleanh/dpte-net.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14220v1 Announce Type: new \nAbstract: This paper proposes a lightweight neural network designed for realistic image dehazing, utilizing a Distilled Pooling Transformer Encoder, named DPTE-Net. Recently, while vision transformers (ViTs) have achieved great success in various vision tasks, their self-attention (SA) module's complexity scales quadratically with image resolution, hindering their applicability on resource-constrained devices. To overcome this, the proposed DPTE-Net substitutes traditional SA modules with efficient pooling mechanisms, significantly reducing computational demands while preserving ViTs' learning capabilities. To further enhance semantic feature learning, a distillation-based training process is implemented which transfers rich knowledge from a larger teacher network to DPTE-Net. Additionally, DPTE-Net is trained within a generative adversarial network (GAN) framework, leveraging the strong generalization of GAN in image restoration, and employs a transmission-aware loss function to dynamically adapt to varying haze densities. Experimental results on various benchmark datasets have shown that the proposed DPTE-Net can achieve competitive dehazing performance when compared to state-of-the-art methods while maintaining low computational complexity, making it a promising solution for resource-limited applications. The code of this work is available at https://github.com/tranleanh/dpte-net.""}",oai:arXiv.org:2412.14220v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Le-Anh Tran, Dong-Chul Park'}]","Le-Anh Tran, Dong-Chul Park","{'name': 'Le-Anh Tran, Dong-Chul Park'}",,
37,A Survey on Large Language Model-based Agents for Statistics and Data Science,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'A Survey on Large Language Model-based Agents for Statistics and Data Science'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14222'}]",https://arxiv.org/abs/2412.14222,"arXiv:2412.14222v1 Announce Type: new 
Abstract: In recent years, data science agents powered by Large Language Models (LLMs), known as ""data agents,"" have shown significant potential to transform the traditional data analysis paradigm. This survey provides an overview of the evolution, capabilities, and applications of LLM-based data agents, highlighting their role in simplifying complex data tasks and lowering the entry barrier for users without related expertise. We explore current trends in the design of LLM-based frameworks, detailing essential features such as planning, reasoning, reflection, multi-agent collaboration, user interface, knowledge integration, and system design, which enable agents to address data-centric problems with minimal human intervention. Furthermore, we analyze several case studies to demonstrate the practical applications of various data agents in real-world scenarios. Finally, we identify key challenges and propose future research directions to advance the development of data agents into intelligent statistical analysis software.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14222v1 Announce Type: new \nAbstract: In recent years, data science agents powered by Large Language Models (LLMs), known as ""data agents,"" have shown significant potential to transform the traditional data analysis paradigm. This survey provides an overview of the evolution, capabilities, and applications of LLM-based data agents, highlighting their role in simplifying complex data tasks and lowering the entry barrier for users without related expertise. We explore current trends in the design of LLM-based frameworks, detailing essential features such as planning, reasoning, reflection, multi-agent collaboration, user interface, knowledge integration, and system design, which enable agents to address data-centric problems with minimal human intervention. Furthermore, we analyze several case studies to demonstrate the practical applications of various data agents in real-world scenarios. Finally, we identify key challenges and propose future research directions to advance the development of data agents into intelligent statistical analysis software.'}",oai:arXiv.org:2412.14222v1,False,"[{'term': 'cs.HC', 'scheme': None, 'label': None}, {'term': 'cs.MA', 'scheme': None, 'label': None}, {'term': 'cs.SE', 'scheme': None, 'label': None}, {'term': 'stat.AP', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Maojun Sun, Ruijian Han, Binyan Jiang, Houduo Qi, Defeng Sun, Yancheng Yuan, Jian Huang'}]","Maojun Sun, Ruijian Han, Binyan Jiang, Houduo Qi, Defeng Sun, Yancheng Yuan, Jian Huang","{'name': 'Maojun Sun, Ruijian Han, Binyan Jiang, Houduo Qi, Defeng Sun, Yancheng Yuan, Jian Huang'}",,
38,Towards Precise Prediction Uncertainty in GNNs: Refining GNNs with Topology-grouping Strategy,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Towards Precise Prediction Uncertainty in GNNs: Refining GNNs with Topology-grouping Strategy'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14223'}]",https://arxiv.org/abs/2412.14223,"arXiv:2412.14223v1 Announce Type: new 
Abstract: Recent advancements in graph neural networks (GNNs) have highlighted the critical need of calibrating model predictions, with neighborhood prediction similarity recognized as a pivotal component. Existing studies suggest that nodes with analogous neighborhood prediction similarity often exhibit similar calibration characteristics. Building on this insight, recent approaches incorporate neighborhood similarity into node-wise temperature scaling techniques. However, our analysis reveals that this assumption does not hold universally. Calibration errors can differ significantly even among nodes with comparable neighborhood similarity, depending on their confidence levels. This necessitates a re-evaluation of existing GNN calibration methods, as a single, unified approach may lead to sub-optimal calibration. In response, we introduce **Simi-Mailbox**, a novel approach that categorizes nodes by both neighborhood similarity and their own confidence, irrespective of proximity or connectivity. Our method allows fine-grained calibration by employing *group-specific* temperature scaling, with each temperature tailored to address the specific miscalibration level of affiliated nodes, rather than adhering to a uniform trend based on neighborhood similarity. Extensive experiments demonstrate the effectiveness of our **Simi-Mailbox** across diverse datasets on different GNN architectures, achieving up to 13.79\% error reduction compared to uncalibrated GNN predictions.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14223v1 Announce Type: new \nAbstract: Recent advancements in graph neural networks (GNNs) have highlighted the critical need of calibrating model predictions, with neighborhood prediction similarity recognized as a pivotal component. Existing studies suggest that nodes with analogous neighborhood prediction similarity often exhibit similar calibration characteristics. Building on this insight, recent approaches incorporate neighborhood similarity into node-wise temperature scaling techniques. However, our analysis reveals that this assumption does not hold universally. Calibration errors can differ significantly even among nodes with comparable neighborhood similarity, depending on their confidence levels. This necessitates a re-evaluation of existing GNN calibration methods, as a single, unified approach may lead to sub-optimal calibration. In response, we introduce **Simi-Mailbox**, a novel approach that categorizes nodes by both neighborhood similarity and their own confidence, irrespective of proximity or connectivity. Our method allows fine-grained calibration by employing *group-specific* temperature scaling, with each temperature tailored to address the specific miscalibration level of affiliated nodes, rather than adhering to a uniform trend based on neighborhood similarity. Extensive experiments demonstrate the effectiveness of our **Simi-Mailbox** across diverse datasets on different GNN architectures, achieving up to 13.79\\% error reduction compared to uncalibrated GNN predictions.'}",oai:arXiv.org:2412.14223v1,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by-nc-sa/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-sa/4.0/'}","[{'name': 'Hyunjin Seo, Kyusung Seo, Joonhyung Park, Eunho Yang'}]","Hyunjin Seo, Kyusung Seo, Joonhyung Park, Eunho Yang","{'name': 'Hyunjin Seo, Kyusung Seo, Joonhyung Park, Eunho Yang'}",,
39,The Effect of Age Introduced to Virtual Reality on Susceptibility to Motion Sickness,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'The Effect of Age Introduced to Virtual Reality on Susceptibility to Motion Sickness'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14225'}]",https://arxiv.org/abs/2412.14225,"arXiv:2412.14225v1 Announce Type: new 
Abstract: Every human with a functioning vestibular system is capable of feeling motion sickness, but some are more vulnerable than others. Based on the leading theories explaining this condition, vulnerability should be predicted by a person's years of real-life experience before using a VR device and years of VR experience after. A questionnaire was filled out on susceptibility to motion sickness in VR by people on VR-related forums. Results from the survey show that the condition has a significant relationship with age or experience outside the environment.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14225v1 Announce Type: new \nAbstract: Every human with a functioning vestibular system is capable of feeling motion sickness, but some are more vulnerable than others. Based on the leading theories explaining this condition, vulnerability should be predicted by a person's years of real-life experience before using a VR device and years of VR experience after. A questionnaire was filled out on susceptibility to motion sickness in VR by people on VR-related forums. Results from the survey show that the condition has a significant relationship with age or experience outside the environment.""}",oai:arXiv.org:2412.14225v1,False,"[{'term': 'cs.HC', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}",[{'name': 'Michael Abia'}],Michael Abia,{'name': 'Michael Abia'},,
40,FedSTaS: Client Stratification and Client Level Sampling for Efficient Federated Learning,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'FedSTaS: Client Stratification and Client Level Sampling for Efficient Federated Learning'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14226'}]",https://arxiv.org/abs/2412.14226,"arXiv:2412.14226v1 Announce Type: new 
Abstract: Federated learning (FL) is a machine learning methodology that involves the collaborative training of a global model across multiple decentralized clients in a privacy-preserving way. Several FL methods are introduced to tackle communication inefficiencies but do not address how to sample participating clients in each round effectively and in a privacy-preserving manner. In this paper, we propose \textit{FedSTaS}, a client and data-level sampling method inspired by \textit{FedSTS} and \textit{FedSampling}. In each federated learning round, \textit{FedSTaS} stratifies clients based on their compressed gradients, re-allocate the number of clients to sample using an optimal Neyman allocation, and sample local data from each participating clients using a data uniform sampling strategy. Experiments on three datasets show that \textit{FedSTaS} can achieve higher accuracy scores than those of \textit{FedSTS} within a fixed number of training rounds.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14226v1 Announce Type: new \nAbstract: Federated learning (FL) is a machine learning methodology that involves the collaborative training of a global model across multiple decentralized clients in a privacy-preserving way. Several FL methods are introduced to tackle communication inefficiencies but do not address how to sample participating clients in each round effectively and in a privacy-preserving manner. In this paper, we propose \\textit{FedSTaS}, a client and data-level sampling method inspired by \\textit{FedSTS} and \\textit{FedSampling}. In each federated learning round, \\textit{FedSTaS} stratifies clients based on their compressed gradients, re-allocate the number of clients to sample using an optimal Neyman allocation, and sample local data from each participating clients using a data uniform sampling strategy. Experiments on three datasets show that \\textit{FedSTaS} can achieve higher accuracy scores than those of \\textit{FedSTS} within a fixed number of training rounds.'}",oai:arXiv.org:2412.14226v1,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'stat.ML', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Jordan Slessor, Dezheng Kong, Xiaofen Tang, Zheng En Than, Linglong Kong'}]","Jordan Slessor, Dezheng Kong, Xiaofen Tang, Zheng En Than, Linglong Kong","{'name': 'Jordan Slessor, Dezheng Kong, Xiaofen Tang, Zheng En Than, Linglong Kong'}",,
41,Transversal PACS Browser API: Addressing Interoperability Challenges in Medical Imaging Systems,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Transversal PACS Browser API: Addressing Interoperability Challenges in Medical Imaging Systems'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14229'}]",https://arxiv.org/abs/2412.14229,"arXiv:2412.14229v1 Announce Type: new 
Abstract: Advances in imaging technologies have revolutionised the medical imaging and healthcare sectors, leading to the widespread adoption of PACS for the storage, retrieval, and communication of medical images. Although these systems have improved operational efficiency, significant challenges remain in effectively retrieving DICOM images, which are essential for diagnosis and overall patient care. Moreover, issues such as fragmented systems, interoperability barriers, and complex user interfaces can often prevent healthcare professionals from efficiently accessing medical images. Addressing these challenges, the Transversal PACS Browser API is a robust and user-friendly solution designed to enhance the process of querying and retrieving DICOM images. It offers advanced filtering capabilities through a variety of filter options as well as a custom field search, that allows users to easily navigate through large medical image collections with ease. Additionally, the application provides a unified interface for querying and retrieving from multiple PACS stations, addressing the challenges of fragmentation and complexity associated with accessing medical images. Other key features include the ability to pre-view images directly within the application. All of this contributes to the transversal nature of the API, serving not only healthcare providers, but anyone who relies on efficient access to these resources. To validate the performance and usability of the application, comprehensive testing was carried out with stakeholders of the field, the results of which showed general satisfaction, highlighting the API's clean design, ease of use, and effective search capabilities of the API, as well as the usefulness of previewing images within the application.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14229v1 Announce Type: new \nAbstract: Advances in imaging technologies have revolutionised the medical imaging and healthcare sectors, leading to the widespread adoption of PACS for the storage, retrieval, and communication of medical images. Although these systems have improved operational efficiency, significant challenges remain in effectively retrieving DICOM images, which are essential for diagnosis and overall patient care. Moreover, issues such as fragmented systems, interoperability barriers, and complex user interfaces can often prevent healthcare professionals from efficiently accessing medical images. Addressing these challenges, the Transversal PACS Browser API is a robust and user-friendly solution designed to enhance the process of querying and retrieving DICOM images. It offers advanced filtering capabilities through a variety of filter options as well as a custom field search, that allows users to easily navigate through large medical image collections with ease. Additionally, the application provides a unified interface for querying and retrieving from multiple PACS stations, addressing the challenges of fragmentation and complexity associated with accessing medical images. Other key features include the ability to pre-view images directly within the application. All of this contributes to the transversal nature of the API, serving not only healthcare providers, but anyone who relies on efficient access to these resources. To validate the performance and usability of the application, comprehensive testing was carried out with stakeholders of the field, the results of which showed general satisfaction, highlighting the API's clean design, ease of use, and effective search capabilities of the API, as well as the usefulness of previewing images within the application.""}",oai:arXiv.org:2412.14229v1,False,"[{'term': 'cs.HC', 'scheme': None, 'label': None}, {'term': 'cs.CE', 'scheme': None, 'label': None}, {'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.IR', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Diogo Lameira, Filipa Ferraz'}]","Diogo Lameira, Filipa Ferraz","{'name': 'Diogo Lameira, Filipa Ferraz'}",,
42,ViTmiX: Vision Transformer Explainability Augmented by Mixed Visualization Methods,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'ViTmiX: Vision Transformer Explainability Augmented by Mixed Visualization Methods'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14231'}]",https://arxiv.org/abs/2412.14231,"arXiv:2412.14231v1 Announce Type: new 
Abstract: Recent advancements in Vision Transformers (ViT) have demonstrated exceptional results in various visual recognition tasks, owing to their ability to capture long-range dependencies in images through self-attention mechanisms. However, the complex nature of ViT models requires robust explainability methods to unveil their decision-making processes. Explainable Artificial Intelligence (XAI) plays a crucial role in improving model transparency and trustworthiness by providing insights into model predictions. Current approaches to ViT explainability, based on visualization techniques such as Layer-wise Relevance Propagation (LRP) and gradient-based methods, have shown promising but sometimes limited results. In this study, we explore a hybrid approach that mixes multiple explainability techniques to overcome these limitations and enhance the interpretability of ViT models. Our experiments reveal that this hybrid approach significantly improves the interpretability of ViT models compared to individual methods. We also introduce modifications to existing techniques, such as using geometric mean for mixing, which demonstrates notable results in object segmentation tasks. To quantify the explainability gain, we introduced a novel post-hoc explainability measure by applying the Pigeonhole principle. These findings underscore the importance of refining and optimizing explainability methods for ViT models, paving the way to reliable XAI-based segmentations.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14231v1 Announce Type: new \nAbstract: Recent advancements in Vision Transformers (ViT) have demonstrated exceptional results in various visual recognition tasks, owing to their ability to capture long-range dependencies in images through self-attention mechanisms. However, the complex nature of ViT models requires robust explainability methods to unveil their decision-making processes. Explainable Artificial Intelligence (XAI) plays a crucial role in improving model transparency and trustworthiness by providing insights into model predictions. Current approaches to ViT explainability, based on visualization techniques such as Layer-wise Relevance Propagation (LRP) and gradient-based methods, have shown promising but sometimes limited results. In this study, we explore a hybrid approach that mixes multiple explainability techniques to overcome these limitations and enhance the interpretability of ViT models. Our experiments reveal that this hybrid approach significantly improves the interpretability of ViT models compared to individual methods. We also introduce modifications to existing techniques, such as using geometric mean for mixing, which demonstrates notable results in object segmentation tasks. To quantify the explainability gain, we introduced a novel post-hoc explainability measure by applying the Pigeonhole principle. These findings underscore the importance of refining and optimizing explainability methods for ViT models, paving the way to reliable XAI-based segmentations.'}",oai:arXiv.org:2412.14231v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Eduard Hogea, Darian M. Onchis, Ana Coporan, Adina Magda Florea, Codruta Istin'}]","Eduard Hogea, Darian M. Onchis, Ana Coporan, Adina Magda Florea, Codruta Istin","{'name': 'Eduard Hogea, Darian M. Onchis, Ana Coporan, Adina Magda Florea, Codruta Istin'}",,
43,Human-in-the-loop or AI-in-the-loop? Automate or Collaborate?,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Human-in-the-loop or AI-in-the-loop? Automate or Collaborate?'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14232'}]",https://arxiv.org/abs/2412.14232,"arXiv:2412.14232v1 Announce Type: new 
Abstract: Human-in-the-loop (HIL) systems have emerged as a promising approach for combining the strengths of data-driven machine learning models with the contextual understanding of human experts. However, a deeper look into several of these systems reveals that calling them HIL would be a misnomer, as they are quite the opposite, namely AI-in-the-loop ($AI^2L$) systems, where the human is in control of the system, while the AI is there to support the human. We argue that existing evaluation methods often overemphasize the machine (learning) component's performance, neglecting the human expert's critical role. Consequently, we propose an $AI^2L$ perspective, which recognizes that the human expert is an active participant in the system, significantly influencing its overall performance. By adopting an $AI^2L$ approach, we can develop more comprehensive systems that faithfully model the intricate interplay between the human and machine components, leading to more effective and robust AI systems.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14232v1 Announce Type: new \nAbstract: Human-in-the-loop (HIL) systems have emerged as a promising approach for combining the strengths of data-driven machine learning models with the contextual understanding of human experts. However, a deeper look into several of these systems reveals that calling them HIL would be a misnomer, as they are quite the opposite, namely AI-in-the-loop ($AI^2L$) systems, where the human is in control of the system, while the AI is there to support the human. We argue that existing evaluation methods often overemphasize the machine (learning) component's performance, neglecting the human expert's critical role. Consequently, we propose an $AI^2L$ perspective, which recognizes that the human expert is an active participant in the system, significantly influencing its overall performance. By adopting an $AI^2L$ approach, we can develop more comprehensive systems that faithfully model the intricate interplay between the human and machine components, leading to more effective and robust AI systems.""}",oai:arXiv.org:2412.14232v1,False,"[{'term': 'cs.HC', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Sriraam Natarajan, Saurabh Mathur, Sahil Sidheekh, Wolfgang Stammer, Kristian Kersting'}]","Sriraam Natarajan, Saurabh Mathur, Sahil Sidheekh, Wolfgang Stammer, Kristian Kersting","{'name': 'Sriraam Natarajan, Saurabh Mathur, Sahil Sidheekh, Wolfgang Stammer, Kristian Kersting'}",,
44,Descriptive Caption Enhancement with Visual Specialists for Multimodal Perception,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Descriptive Caption Enhancement with Visual Specialists for Multimodal Perception'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14233'}]",https://arxiv.org/abs/2412.14233,"arXiv:2412.14233v1 Announce Type: new 
Abstract: Training Large Multimodality Models (LMMs) relies on descriptive image caption that connects image and language. Existing methods either distill the caption from the LMM models or construct the captions from the internet images or by human. We propose to leverage off-the-shelf visual specialists, which were trained from annotated images initially not for image captioning, for enhancing the image caption.
  Our approach, named DCE, explores object low-level and fine-grained attributes (e.g., depth, emotion and fine-grained categories) and object relations (e.g., relative location and human-object-interaction (HOI)), and combine the attributes into the descriptive caption. Experiments demonstrate that such visual specialists are able to improve the performance for visual understanding tasks as well as reasoning that benefits from more accurate visual understanding. We will release the source code and the pipeline so that other visual specialists are easily combined into the pipeline. The complete source code of DCE pipeline and datasets will be available at \url{https://github.com/syp2ysy/DCE}.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14233v1 Announce Type: new \nAbstract: Training Large Multimodality Models (LMMs) relies on descriptive image caption that connects image and language. Existing methods either distill the caption from the LMM models or construct the captions from the internet images or by human. We propose to leverage off-the-shelf visual specialists, which were trained from annotated images initially not for image captioning, for enhancing the image caption.\n  Our approach, named DCE, explores object low-level and fine-grained attributes (e.g., depth, emotion and fine-grained categories) and object relations (e.g., relative location and human-object-interaction (HOI)), and combine the attributes into the descriptive caption. Experiments demonstrate that such visual specialists are able to improve the performance for visual understanding tasks as well as reasoning that benefits from more accurate visual understanding. We will release the source code and the pipeline so that other visual specialists are easily combined into the pipeline. The complete source code of DCE pipeline and datasets will be available at \\url{https://github.com/syp2ysy/DCE}.'}",oai:arXiv.org:2412.14233v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Yanpeng Sun, Jing Hao, Ke Zhu, Jiang-Jiang Liu, Yuxiang Zhao, Xiaofan Li, Gang Zhang, Zechao Li, Jingdong Wang'}]","Yanpeng Sun, Jing Hao, Ke Zhu, Jiang-Jiang Liu, Yuxiang Zhao, Xiaofan Li, Gang Zhang, Zechao Li, Jingdong Wang","{'name': 'Yanpeng Sun, Jing Hao, Ke Zhu, Jiang-Jiang Liu, Yuxiang Zhao, Xiaofan Li, Gang Zhang, Zechao Li, Jingdong Wang'}",,
45,Syzygy: Dual Code-Test C to (safe) Rust Translation using LLMs and Dynamic Analysis,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Syzygy: Dual Code-Test C to (safe) Rust Translation using LLMs and Dynamic Analysis'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14234'}]",https://arxiv.org/abs/2412.14234,"arXiv:2412.14234v1 Announce Type: new 
Abstract: Despite extensive usage in high-performance, low-level systems programming applications, C is susceptible to vulnerabilities due to manual memory management and unsafe pointer operations. Rust, a modern systems programming language, offers a compelling alternative. Its unique ownership model and type system ensure memory safety without sacrificing performance.
  In this paper, we present Syzygy, an automated approach to translate C to safe Rust. Our technique uses a synergistic combination of LLM-driven code and test translation guided by dynamic-analysis-generated execution information. This paired translation runs incrementally in a loop over the program in dependency order of the code elements while maintaining per-step correctness. Our approach exposes novel insights on combining the strengths of LLMs and dynamic analysis in the context of scaling and combining code generation with testing. We apply our approach to successfully translate Zopfli, a high-performance compression library with ~3000 lines of code and 98 functions. We validate the translation by testing equivalence with the source C program on a set of inputs. To our knowledge, this is the largest automated and test-validated C to safe Rust code translation achieved so far.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14234v1 Announce Type: new \nAbstract: Despite extensive usage in high-performance, low-level systems programming applications, C is susceptible to vulnerabilities due to manual memory management and unsafe pointer operations. Rust, a modern systems programming language, offers a compelling alternative. Its unique ownership model and type system ensure memory safety without sacrificing performance.\n  In this paper, we present Syzygy, an automated approach to translate C to safe Rust. Our technique uses a synergistic combination of LLM-driven code and test translation guided by dynamic-analysis-generated execution information. This paired translation runs incrementally in a loop over the program in dependency order of the code elements while maintaining per-step correctness. Our approach exposes novel insights on combining the strengths of LLMs and dynamic analysis in the context of scaling and combining code generation with testing. We apply our approach to successfully translate Zopfli, a high-performance compression library with ~3000 lines of code and 98 functions. We validate the translation by testing equivalence with the source C program on a set of inputs. To our knowledge, this is the largest automated and test-validated C to safe Rust code translation achieved so far.'}",oai:arXiv.org:2412.14234v1,False,"[{'term': 'cs.SE', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'cs.PL', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by-sa/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-sa/4.0/'}","[{'name': 'Manish Shetty, Naman Jain, Adwait Godbole, Sanjit A. Seshia, Koushik Sen'}]","Manish Shetty, Naman Jain, Adwait Godbole, Sanjit A. Seshia, Koushik Sen","{'name': 'Manish Shetty, Naman Jain, Adwait Godbole, Sanjit A. Seshia, Koushik Sen'}",,
46,Split Learning in Computer Vision for Semantic Segmentation Delay Minimization,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Split Learning in Computer Vision for Semantic Segmentation Delay Minimization'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14272'}]",https://arxiv.org/abs/2412.14272,"arXiv:2412.14272v1 Announce Type: new 
Abstract: In this paper, we propose a novel approach to minimize the inference delay in semantic segmentation using split learning (SL), tailored to the needs of real-time computer vision (CV) applications for resource-constrained devices. Semantic segmentation is essential for applications such as autonomous vehicles and smart city infrastructure, but faces significant latency challenges due to high computational and communication loads. Traditional centralized processing methods are inefficient for such scenarios, often resulting in unacceptable inference delays. SL offers a promising alternative by partitioning deep neural networks (DNNs) between edge devices and a central server, enabling localized data processing and reducing the amount of data required for transmission. Our contribution includes the joint optimization of bandwidth allocation, cut layer selection of the edge devices' DNN, and the central server's processing resource allocation. We investigate both parallel and serial data processing scenarios and propose low-complexity heuristic solutions that maintain near-optimal performance while reducing computational requirements. Numerical results show that our approach effectively reduces inference delay, demonstrating the potential of SL for improving real-time CV applications in dynamic, resource-constrained environments.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14272v1 Announce Type: new \nAbstract: In this paper, we propose a novel approach to minimize the inference delay in semantic segmentation using split learning (SL), tailored to the needs of real-time computer vision (CV) applications for resource-constrained devices. Semantic segmentation is essential for applications such as autonomous vehicles and smart city infrastructure, but faces significant latency challenges due to high computational and communication loads. Traditional centralized processing methods are inefficient for such scenarios, often resulting in unacceptable inference delays. SL offers a promising alternative by partitioning deep neural networks (DNNs) between edge devices and a central server, enabling localized data processing and reducing the amount of data required for transmission. Our contribution includes the joint optimization of bandwidth allocation, cut layer selection of the edge devices' DNN, and the central server's processing resource allocation. We investigate both parallel and serial data processing scenarios and propose low-complexity heuristic solutions that maintain near-optimal performance while reducing computational requirements. Numerical results show that our approach effectively reduces inference delay, demonstrating the potential of SL for improving real-time CV applications in dynamic, resource-constrained environments.""}",oai:arXiv.org:2412.14272v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.DC', 'scheme': None, 'label': None}, {'term': 'cs.IT', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'math.IT', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Nikos G. Evgenidis, Nikos A. Mitsiou, Sotiris A. Tegos, Panagiotis D. Diamantoulakis, George K. Karagiannidis'}]","Nikos G. Evgenidis, Nikos A. Mitsiou, Sotiris A. Tegos, Panagiotis D. Diamantoulakis, George K. Karagiannidis","{'name': 'Nikos G. Evgenidis, Nikos A. Mitsiou, Sotiris A. Tegos, Panagiotis D. Diamantoulakis, George K. Karagiannidis'}",,
47,Approximation Schemes for Age of Information Minimization in UAV Grid Patrols,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Approximation Schemes for Age of Information Minimization in UAV Grid Patrols'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14273'}]",https://arxiv.org/abs/2412.14273,"arXiv:2412.14273v1 Announce Type: new 
Abstract: Motivated by the critical need for unmanned aerial vehicles (UAVs) to patrol grid systems in hazardous and dynamically changing environments, this study addresses a routing problem aimed at minimizing the time-average Age of Information (AoI) for edges in general graphs. We establish a lower bound for all feasible patrol policies and demonstrate that this bound is tight when the graph contains an Eulerian cycle. For graphs without Eulerian cycles, it becomes challenging to identify the optimal patrol strategy due to the extensive range of feasible options. Our analysis shows that restricting the strategy to periodic sequences still results in an exponentially large number of possible strategies. To address this complexity, we introduce two polynomial-time approximation schemes, each involving a two-step process: constructing multigraphs first and then embedding Eulerian cycles within these multigraphs. We prove that both schemes achieve an approximation ratio of 2. Further, both analytical and numerical results suggest that evenly and sparsely distributing edge visits within a periodic route significantly reduces the average AoI compared to strategies that merely minimize the route travel distance. Building on this insight, we propose a heuristic method that not only maintains the approximation ratio of 2 but also ensures robust performance across varying random graphs.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14273v1 Announce Type: new \nAbstract: Motivated by the critical need for unmanned aerial vehicles (UAVs) to patrol grid systems in hazardous and dynamically changing environments, this study addresses a routing problem aimed at minimizing the time-average Age of Information (AoI) for edges in general graphs. We establish a lower bound for all feasible patrol policies and demonstrate that this bound is tight when the graph contains an Eulerian cycle. For graphs without Eulerian cycles, it becomes challenging to identify the optimal patrol strategy due to the extensive range of feasible options. Our analysis shows that restricting the strategy to periodic sequences still results in an exponentially large number of possible strategies. To address this complexity, we introduce two polynomial-time approximation schemes, each involving a two-step process: constructing multigraphs first and then embedding Eulerian cycles within these multigraphs. We prove that both schemes achieve an approximation ratio of 2. Further, both analytical and numerical results suggest that evenly and sparsely distributing edge visits within a periodic route significantly reduces the average AoI compared to strategies that merely minimize the route travel distance. Building on this insight, we propose a heuristic method that not only maintains the approximation ratio of 2 but also ensures robust performance across varying random graphs.'}",oai:arXiv.org:2412.14273v1,False,"[{'term': 'cs.IT', 'scheme': None, 'label': None}, {'term': 'math.IT', 'scheme': None, 'label': None}, {'term': 'math.OC', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by-nc-nd/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-nd/4.0/'}","[{'name': 'Weiqi Wang, Jin Xu'}]","Weiqi Wang, Jin Xu","{'name': 'Weiqi Wang, Jin Xu'}",10.1016/j.adhoc.2024.103686,"Ad Hoc Networks Volume 166, 1 January 2025, 103686"
48,Fake News Detection: Comparative Evaluation of BERT-like Models and Large Language Models with Generative AI-Annotated Data,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Fake News Detection: Comparative Evaluation of BERT-like Models and Large Language Models with Generative AI-Annotated Data'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14276'}]",https://arxiv.org/abs/2412.14276,"arXiv:2412.14276v1 Announce Type: new 
Abstract: Fake news poses a significant threat to public opinion and social stability in modern society. This study presents a comparative evaluation of BERT-like encoder-only models and autoregressive decoder-only large language models (LLMs) for fake news detection. We introduce a dataset of news articles labeled with GPT-4 assistance (an AI-labeling method) and verified by human experts to ensure reliability. Both BERT-like encoder-only models and LLMs were fine-tuned on this dataset. Additionally, we developed an instruction-tuned LLM approach with majority voting during inference for label generation. Our analysis reveals that BERT-like models generally outperform LLMs in classification tasks, while LLMs demonstrate superior robustness against text perturbations. Compared to weak labels (distant supervision) data, the results show that AI labels with human supervision achieve better classification results. This study highlights the effectiveness of combining AI-based annotation with human oversight and demonstrates the performance of different families of machine learning models for fake news detection","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14276v1 Announce Type: new \nAbstract: Fake news poses a significant threat to public opinion and social stability in modern society. This study presents a comparative evaluation of BERT-like encoder-only models and autoregressive decoder-only large language models (LLMs) for fake news detection. We introduce a dataset of news articles labeled with GPT-4 assistance (an AI-labeling method) and verified by human experts to ensure reliability. Both BERT-like encoder-only models and LLMs were fine-tuned on this dataset. Additionally, we developed an instruction-tuned LLM approach with majority voting during inference for label generation. Our analysis reveals that BERT-like models generally outperform LLMs in classification tasks, while LLMs demonstrate superior robustness against text perturbations. Compared to weak labels (distant supervision) data, the results show that AI labels with human supervision achieve better classification results. This study highlights the effectiveness of combining AI-based annotation with human oversight and demonstrates the performance of different families of machine learning models for fake news detection'}",oai:arXiv.org:2412.14276v1,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'haina Raza, Drai Paulen-Patterson, Chen Ding'}]","haina Raza, Drai Paulen-Patterson, Chen Ding","{'name': 'haina Raza, Drai Paulen-Patterson, Chen Ding'}",,
49,PixelMan: Consistent Object Editing with Diffusion Models via Pixel Manipulation and Generation,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'PixelMan: Consistent Object Editing with Diffusion Models via Pixel Manipulation and Generation'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14283'}]",https://arxiv.org/abs/2412.14283,"arXiv:2412.14283v1 Announce Type: new 
Abstract: Recent research explores the potential of Diffusion Models (DMs) for consistent object editing, which aims to modify object position, size, and composition, etc., while preserving the consistency of objects and background without changing their texture and attributes. Current inference-time methods often rely on DDIM inversion, which inherently compromises efficiency and the achievable consistency of edited images. Recent methods also utilize energy guidance which iteratively updates the predicted noise and can drive the latents away from the original image, resulting in distortions. In this paper, we propose PixelMan, an inversion-free and training-free method for achieving consistent object editing via Pixel Manipulation and generation, where we directly create a duplicate copy of the source object at target location in the pixel space, and introduce an efficient sampling approach to iteratively harmonize the manipulated object into the target location and inpaint its original location, while ensuring image consistency by anchoring the edited image to be generated to the pixel-manipulated image as well as by introducing various consistency-preserving optimization techniques during inference. Experimental evaluations based on benchmark datasets as well as extensive visual comparisons show that in as few as 16 inference steps, PixelMan outperforms a range of state-of-the-art training-based and training-free methods (usually requiring 50 steps) on multiple consistent object editing tasks.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14283v1 Announce Type: new \nAbstract: Recent research explores the potential of Diffusion Models (DMs) for consistent object editing, which aims to modify object position, size, and composition, etc., while preserving the consistency of objects and background without changing their texture and attributes. Current inference-time methods often rely on DDIM inversion, which inherently compromises efficiency and the achievable consistency of edited images. Recent methods also utilize energy guidance which iteratively updates the predicted noise and can drive the latents away from the original image, resulting in distortions. In this paper, we propose PixelMan, an inversion-free and training-free method for achieving consistent object editing via Pixel Manipulation and generation, where we directly create a duplicate copy of the source object at target location in the pixel space, and introduce an efficient sampling approach to iteratively harmonize the manipulated object into the target location and inpaint its original location, while ensuring image consistency by anchoring the edited image to be generated to the pixel-manipulated image as well as by introducing various consistency-preserving optimization techniques during inference. Experimental evaluations based on benchmark datasets as well as extensive visual comparisons show that in as few as 16 inference steps, PixelMan outperforms a range of state-of-the-art training-based and training-free methods (usually requiring 50 steps) on multiple consistent object editing tasks.'}",oai:arXiv.org:2412.14283v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.GR', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Liyao Jiang, Negar Hassanpour, Mohammad Salameh, Mohammadreza Samadi, Jiao He, Fengyu Sun, Di Niu'}]","Liyao Jiang, Negar Hassanpour, Mohammad Salameh, Mohammadreza Samadi, Jiao He, Fengyu Sun, Di Niu","{'name': 'Liyao Jiang, Negar Hassanpour, Mohammad Salameh, Mohammadreza Samadi, Jiao He, Fengyu Sun, Di Niu'}",,
50,Measuring DNS Censorship of Generative AI Platforms,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Measuring DNS Censorship of Generative AI Platforms'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14286'}]",https://arxiv.org/abs/2412.14286,"arXiv:2412.14286v1 Announce Type: new 
Abstract: Generative AI is an invaluable tool, however, in some parts of the world, this technology is censored due to political or societal issues. In this work, we monitor Generative AI censorship through the DNS protocol. We find China to be a leading country of Generative AI censorship. Interestingly, China does not censor all AI domain names. We also report censorship in Russia and find inconsistencies in their process. We compare our results to other measurement platforms (OONI, Censored Planet, GFWatch), and present their lack of data on Generative AI domains.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14286v1 Announce Type: new \nAbstract: Generative AI is an invaluable tool, however, in some parts of the world, this technology is censored due to political or societal issues. In this work, we monitor Generative AI censorship through the DNS protocol. We find China to be a leading country of Generative AI censorship. Interestingly, China does not censor all AI domain names. We also report censorship in Russia and find inconsistencies in their process. We compare our results to other measurement platforms (OONI, Censored Planet, GFWatch), and present their lack of data on Generative AI domains.'}",oai:arXiv.org:2412.14286v1,False,"[{'term': 'cs.CY', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Harel Berger, Yuval Shavitt'}]","Harel Berger, Yuval Shavitt","{'name': 'Harel Berger, Yuval Shavitt'}",,
51,TRecViT: A Recurrent Video Transformer,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'TRecViT: A Recurrent Video Transformer'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14294'}]",https://arxiv.org/abs/2412.14294,"arXiv:2412.14294v1 Announce Type: new 
Abstract: We propose a novel block for video modelling. It relies on a time-space-channel factorisation with dedicated blocks for each dimension: gated linear recurrent units (LRUs) perform information mixing over time, self-attention layers perform mixing over space, and MLPs over channels. The resulting architecture TRecViT performs well on sparse and dense tasks, trained in supervised or self-supervised regimes. Notably, our model is causal and outperforms or is on par with a pure attention model ViViT-L on large scale video datasets (SSv2, Kinetics400), while having $3\times$ less parameters, $12\times$ smaller memory footprint, and $5\times$ lower FLOPs count. Code and checkpoints will be made available online at https://github.com/google-deepmind/trecvit.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14294v1 Announce Type: new \nAbstract: We propose a novel block for video modelling. It relies on a time-space-channel factorisation with dedicated blocks for each dimension: gated linear recurrent units (LRUs) perform information mixing over time, self-attention layers perform mixing over space, and MLPs over channels. The resulting architecture TRecViT performs well on sparse and dense tasks, trained in supervised or self-supervised regimes. Notably, our model is causal and outperforms or is on par with a pure attention model ViViT-L on large scale video datasets (SSv2, Kinetics400), while having $3\\times$ less parameters, $12\\times$ smaller memory footprint, and $5\\times$ lower FLOPs count. Code and checkpoints will be made available online at https://github.com/google-deepmind/trecvit.'}",oai:arXiv.org:2412.14294v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Viorica P\\u{a}tr\\u{a}ucean, Xu Owen He, Joseph Heyward, Chuhan Zhang, Mehdi S. M. Sajjadi, George-Cristian Muraru, Artem Zholus, Mahdi Karami, Ross Goroshin, Yutian Chen, Simon Osindero, Jo\\~ao Carreira, Razvan Pascanu'}]","Viorica P\u{a}tr\u{a}ucean, Xu Owen He, Joseph Heyward, Chuhan Zhang, Mehdi S. M. Sajjadi, George-Cristian Muraru, Artem Zholus, Mahdi Karami, Ross Goroshin, Yutian Chen, Simon Osindero, Jo\~ao Carreira, Razvan Pascanu","{'name': 'Viorica P\\u{a}tr\\u{a}ucean, Xu Owen He, Joseph Heyward, Chuhan Zhang, Mehdi S. M. Sajjadi, George-Cristian Muraru, Artem Zholus, Mahdi Karami, Ross Goroshin, Yutian Chen, Simon Osindero, Jo\\~ao Carreira, Razvan Pascanu'}",,
52,Temporally Consistent Object-Centric Learning by Contrasting Slots,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Temporally Consistent Object-Centric Learning by Contrasting Slots'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14295'}]",https://arxiv.org/abs/2412.14295,"arXiv:2412.14295v1 Announce Type: new 
Abstract: Unsupervised object-centric learning from videos is a promising approach to extract structured representations from large, unlabeled collections of videos. To support downstream tasks like autonomous control, these representations must be both compositional and temporally consistent. Existing approaches based on recurrent processing often lack long-term stability across frames because their training objective does not enforce temporal consistency. In this work, we introduce a novel object-level temporal contrastive loss for video object-centric models that explicitly promotes temporal consistency. Our method significantly improves the temporal consistency of the learned object-centric representations, yielding more reliable video decompositions that facilitate challenging downstream tasks such as unsupervised object dynamics prediction. Furthermore, the inductive bias added by our loss strongly improves object discovery, leading to state-of-the-art results on both synthetic and real-world datasets, outperforming even weakly-supervised methods that leverage motion masks as additional cues.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14295v1 Announce Type: new \nAbstract: Unsupervised object-centric learning from videos is a promising approach to extract structured representations from large, unlabeled collections of videos. To support downstream tasks like autonomous control, these representations must be both compositional and temporally consistent. Existing approaches based on recurrent processing often lack long-term stability across frames because their training objective does not enforce temporal consistency. In this work, we introduce a novel object-level temporal contrastive loss for video object-centric models that explicitly promotes temporal consistency. Our method significantly improves the temporal consistency of the learned object-centric representations, yielding more reliable video decompositions that facilitate challenging downstream tasks such as unsupervised object dynamics prediction. Furthermore, the inductive bias added by our loss strongly improves object discovery, leading to state-of-the-art results on both synthetic and real-world datasets, outperforming even weakly-supervised methods that leverage motion masks as additional cues.'}",oai:arXiv.org:2412.14295v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'cs.RO', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Anna Manasyan, Maximilian Seitzer, Filip Radovic, Georg Martius, Andrii Zadaianchuk'}]","Anna Manasyan, Maximilian Seitzer, Filip Radovic, Georg Martius, Andrii Zadaianchuk","{'name': 'Anna Manasyan, Maximilian Seitzer, Filip Radovic, Georg Martius, Andrii Zadaianchuk'}",,
53,Distributionally Robust Policy Learning under Concept Drifts,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Distributionally Robust Policy Learning under Concept Drifts'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14297'}]",https://arxiv.org/abs/2412.14297,"arXiv:2412.14297v1 Announce Type: new 
Abstract: Distributionally robust policy learning aims to find a policy that performs well under the worst-case distributional shift, and yet most existing methods for robust policy learning consider the worst-case joint distribution of the covariate and the outcome. The joint-modeling strategy can be unnecessarily conservative when we have more information on the source of distributional shifts. This paper studiesa more nuanced problem -- robust policy learning under the concept drift, when only the conditional relationship between the outcome and the covariate changes. To this end, we first provide a doubly-robust estimator for evaluating the worst-case average reward of a given policy under a set of perturbed conditional distributions. We show that the policy value estimator enjoys asymptotic normality even if the nuisance parameters are estimated with a slower-than-root-$n$ rate. We then propose a learning algorithm that outputs the policy maximizing the estimated policy value within a given policy class $\Pi$, and show that the sub-optimality gap of the proposed algorithm is of the order $\kappa(\Pi)n^{-1/2}$, with $\kappa(\Pi)$ is the entropy integral of $\Pi$ under the Hamming distance and $n$ is the sample size. A matching lower bound is provided to show the optimality of the rate. The proposed methods are implemented and evaluated in numerical studies, demonstrating substantial improvement compared with existing benchmarks.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14297v1 Announce Type: new \nAbstract: Distributionally robust policy learning aims to find a policy that performs well under the worst-case distributional shift, and yet most existing methods for robust policy learning consider the worst-case joint distribution of the covariate and the outcome. The joint-modeling strategy can be unnecessarily conservative when we have more information on the source of distributional shifts. This paper studiesa more nuanced problem -- robust policy learning under the concept drift, when only the conditional relationship between the outcome and the covariate changes. To this end, we first provide a doubly-robust estimator for evaluating the worst-case average reward of a given policy under a set of perturbed conditional distributions. We show that the policy value estimator enjoys asymptotic normality even if the nuisance parameters are estimated with a slower-than-root-$n$ rate. We then propose a learning algorithm that outputs the policy maximizing the estimated policy value within a given policy class $\\Pi$, and show that the sub-optimality gap of the proposed algorithm is of the order $\\kappa(\\Pi)n^{-1/2}$, with $\\kappa(\\Pi)$ is the entropy integral of $\\Pi$ under the Hamming distance and $n$ is the sample size. A matching lower bound is provided to show the optimality of the rate. The proposed methods are implemented and evaluated in numerical studies, demonstrating substantial improvement compared with existing benchmarks.'}",oai:arXiv.org:2412.14297v1,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'stat.ML', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Jingyuan Wang, Zhimei Ren, Ruohan Zhan, Zhengyuan Zhou'}]","Jingyuan Wang, Zhimei Ren, Ruohan Zhan, Zhengyuan Zhou","{'name': 'Jingyuan Wang, Zhimei Ren, Ruohan Zhan, Zhengyuan Zhou'}",,
54,"The Multiplex Classification Framework: optimizing multi-label classifiers through problem transformation, ontology engineering, and model ensembling","{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'The Multiplex Classification Framework: optimizing multi-label classifiers through problem transformation, ontology engineering, and model ensembling'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14299'}]",https://arxiv.org/abs/2412.14299,"arXiv:2412.14299v1 Announce Type: new 
Abstract: Classification is a fundamental task in machine learning. While conventional methods-such as binary, multiclass, and multi-label classification-are effective for simpler problems, they may not adequately address the complexities of some real-world scenarios. This paper introduces the Multiplex Classification Framework, a novel approach developed to tackle these and similar challenges through the integration of problem transformation, ontology engineering, and model ensembling. The framework offers several advantages, including adaptability to any number of classes and logical constraints, an innovative method for managing class imbalance, the elimination of confidence threshold selection, and a modular structure. Two experiments were conducted to compare the performance of conventional classification models with the Multiplex approach. Our results demonstrate that the Multiplex approach can improve classification performance significantly (up to 10% gain in overall F1 score), particularly in classification problems with a large number of classes and pronounced class imbalances. However, it also has limitations, as it requires a thorough understanding of the problem domain and some experience with ontology engineering, and it involves training multiple models, which can make the whole process more intricate. Overall, this methodology provides a valuable tool for researchers and practitioners dealing with complex classification problems in machine learning.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14299v1 Announce Type: new \nAbstract: Classification is a fundamental task in machine learning. While conventional methods-such as binary, multiclass, and multi-label classification-are effective for simpler problems, they may not adequately address the complexities of some real-world scenarios. This paper introduces the Multiplex Classification Framework, a novel approach developed to tackle these and similar challenges through the integration of problem transformation, ontology engineering, and model ensembling. The framework offers several advantages, including adaptability to any number of classes and logical constraints, an innovative method for managing class imbalance, the elimination of confidence threshold selection, and a modular structure. Two experiments were conducted to compare the performance of conventional classification models with the Multiplex approach. Our results demonstrate that the Multiplex approach can improve classification performance significantly (up to 10% gain in overall F1 score), particularly in classification problems with a large number of classes and pronounced class imbalances. However, it also has limitations, as it requires a thorough understanding of the problem domain and some experience with ontology engineering, and it involves training multiple models, which can make the whole process more intricate. Overall, this methodology provides a valuable tool for researchers and practitioners dealing with complex classification problems in machine learning.'}",oai:arXiv.org:2412.14299v1,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by-nc-sa/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-sa/4.0/'}","[{'name': 'Mauro Nievas Offidani, Facundo Roffet, Claudio Augusto Delrieux, Maria Carolina Gonzalez Galtier, Marcos Zarate'}]","Mauro Nievas Offidani, Facundo Roffet, Claudio Augusto Delrieux, Maria Carolina Gonzalez Galtier, Marcos Zarate","{'name': 'Mauro Nievas Offidani, Facundo Roffet, Claudio Augusto Delrieux, Maria Carolina Gonzalez Galtier, Marcos Zarate'}",,
55,What Has Been Overlooked in Contrastive Source-Free Domain Adaptation: Leveraging Source-Informed Latent Augmentation within Neighborhood Context,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'What Has Been Overlooked in Contrastive Source-Free Domain Adaptation: Leveraging Source-Informed Latent Augmentation within Neighborhood Context'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14301'}]",https://arxiv.org/abs/2412.14301,"arXiv:2412.14301v1 Announce Type: new 
Abstract: Source-free domain adaptation (SFDA) involves adapting a model originally trained using a labeled dataset ({\em source domain}) to perform effectively on an unlabeled dataset ({\em target domain}) without relying on any source data during adaptation. This adaptation is especially crucial when significant disparities in data distributions exist between the two domains and when there are privacy concerns regarding the source model's training data. The absence of access to source data during adaptation makes it challenging to analytically estimate the domain gap. To tackle this issue, various techniques have been proposed, such as unsupervised clustering, contrastive learning, and continual learning. In this paper, we first conduct an extensive theoretical analysis of SFDA based on contrastive learning, primarily because it has demonstrated superior performance compared to other techniques. Motivated by the obtained insights, we then introduce a straightforward yet highly effective latent augmentation method tailored for contrastive SFDA. This augmentation method leverages the dispersion of latent features within the neighborhood of the query sample, guided by the source pre-trained model, to enhance the informativeness of positive keys. Our approach, based on a single InfoNCE-based contrastive loss, outperforms state-of-the-art SFDA methods on widely recognized benchmark datasets.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14301v1 Announce Type: new \nAbstract: Source-free domain adaptation (SFDA) involves adapting a model originally trained using a labeled dataset ({\\em source domain}) to perform effectively on an unlabeled dataset ({\\em target domain}) without relying on any source data during adaptation. This adaptation is especially crucial when significant disparities in data distributions exist between the two domains and when there are privacy concerns regarding the source model's training data. The absence of access to source data during adaptation makes it challenging to analytically estimate the domain gap. To tackle this issue, various techniques have been proposed, such as unsupervised clustering, contrastive learning, and continual learning. In this paper, we first conduct an extensive theoretical analysis of SFDA based on contrastive learning, primarily because it has demonstrated superior performance compared to other techniques. Motivated by the obtained insights, we then introduce a straightforward yet highly effective latent augmentation method tailored for contrastive SFDA. This augmentation method leverages the dispersion of latent features within the neighborhood of the query sample, guided by the source pre-trained model, to enhance the informativeness of positive keys. Our approach, based on a single InfoNCE-based contrastive loss, outperforms state-of-the-art SFDA methods on widely recognized benchmark datasets.""}",oai:arXiv.org:2412.14301v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Jing Wang, Wonho Bae, Jiahong Chen, Kuangen Zhang, Leonid Sigal, Clarence W. de Silva'}]","Jing Wang, Wonho Bae, Jiahong Chen, Kuangen Zhang, Leonid Sigal, Clarence W. de Silva","{'name': 'Jing Wang, Wonho Bae, Jiahong Chen, Kuangen Zhang, Leonid Sigal, Clarence W. de Silva'}",,
56,SAFERec: Self-Attention and Frequency Enriched Model for Next Basket Recommendation,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'SAFERec: Self-Attention and Frequency Enriched Model for Next Basket Recommendation'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14302'}]",https://arxiv.org/abs/2412.14302,"arXiv:2412.14302v1 Announce Type: new 
Abstract: Transformer-based approaches such as BERT4Rec and SASRec demonstrate strong performance in Next Item Recommendation (NIR) tasks. However, applying these architectures to Next-Basket Recommendation (NBR) tasks, which often involve highly repetitive interactions, is challenging due to the vast number of possible item combinations in a basket. Moreover, frequency-based methods such as TIFU-KNN and UP-CF still demonstrate strong performance in NBR tasks, frequently outperforming deep-learning approaches. This paper introduces SAFERec, a novel algorithm for NBR that enhances transformer-based architectures from NIR by incorporating item frequency information, consequently improving their applicability to NBR tasks. Extensive experiments on multiple datasets show that SAFERec outperforms all other baselines, specifically achieving an 8\% improvement in Recall@10.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14302v1 Announce Type: new \nAbstract: Transformer-based approaches such as BERT4Rec and SASRec demonstrate strong performance in Next Item Recommendation (NIR) tasks. However, applying these architectures to Next-Basket Recommendation (NBR) tasks, which often involve highly repetitive interactions, is challenging due to the vast number of possible item combinations in a basket. Moreover, frequency-based methods such as TIFU-KNN and UP-CF still demonstrate strong performance in NBR tasks, frequently outperforming deep-learning approaches. This paper introduces SAFERec, a novel algorithm for NBR that enhances transformer-based architectures from NIR by incorporating item frequency information, consequently improving their applicability to NBR tasks. Extensive experiments on multiple datasets show that SAFERec outperforms all other baselines, specifically achieving an 8\\% improvement in Recall@10.'}",oai:arXiv.org:2412.14302v1,False,"[{'term': 'cs.IR', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Oleg Lashinin, Denis Krasilnikov, Aleksandr Milogradskii, Marina Ananyeva'}]","Oleg Lashinin, Denis Krasilnikov, Aleksandr Milogradskii, Marina Ananyeva","{'name': 'Oleg Lashinin, Denis Krasilnikov, Aleksandr Milogradskii, Marina Ananyeva'}",,
57,Multi-OphthaLingua: A Multilingual Benchmark for Assessing and Debiasing LLM Ophthalmological QA in LMICs,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Multi-OphthaLingua: A Multilingual Benchmark for Assessing and Debiasing LLM Ophthalmological QA in LMICs'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14304'}]",https://arxiv.org/abs/2412.14304,"arXiv:2412.14304v1 Announce Type: new 
Abstract: Current ophthalmology clinical workflows are plagued by over-referrals, long waits, and complex and heterogeneous medical records. Large language models (LLMs) present a promising solution to automate various procedures such as triaging, preliminary tests like visual acuity assessment, and report summaries. However, LLMs have demonstrated significantly varied performance across different languages in natural language question-answering tasks, potentially exacerbating healthcare disparities in Low and Middle-Income Countries (LMICs). This study introduces the first multilingual ophthalmological question-answering benchmark with manually curated questions parallel across languages, allowing for direct cross-lingual comparisons. Our evaluation of 6 popular LLMs across 7 different languages reveals substantial bias across different languages, highlighting risks for clinical deployment of LLMs in LMICs. Existing debiasing methods such as Translation Chain-of-Thought or Retrieval-augmented generation (RAG) by themselves fall short of closing this performance gap, often failing to improve performance across all languages and lacking specificity for the medical domain. To address this issue, We propose CLARA (Cross-Lingual Reflective Agentic system), a novel inference time de-biasing method leveraging retrieval augmented generation and self-verification. Our approach not only improves performance across all languages but also significantly reduces the multilingual bias gap, facilitating equitable LLM application across the globe.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14304v1 Announce Type: new \nAbstract: Current ophthalmology clinical workflows are plagued by over-referrals, long waits, and complex and heterogeneous medical records. Large language models (LLMs) present a promising solution to automate various procedures such as triaging, preliminary tests like visual acuity assessment, and report summaries. However, LLMs have demonstrated significantly varied performance across different languages in natural language question-answering tasks, potentially exacerbating healthcare disparities in Low and Middle-Income Countries (LMICs). This study introduces the first multilingual ophthalmological question-answering benchmark with manually curated questions parallel across languages, allowing for direct cross-lingual comparisons. Our evaluation of 6 popular LLMs across 7 different languages reveals substantial bias across different languages, highlighting risks for clinical deployment of LLMs in LMICs. Existing debiasing methods such as Translation Chain-of-Thought or Retrieval-augmented generation (RAG) by themselves fall short of closing this performance gap, often failing to improve performance across all languages and lacking specificity for the medical domain. To address this issue, We propose CLARA (Cross-Lingual Reflective Agentic system), a novel inference time de-biasing method leveraging retrieval augmented generation and self-verification. Our approach not only improves performance across all languages but also significantly reduces the multilingual bias gap, facilitating equitable LLM application across the globe.'}",oai:arXiv.org:2412.14304v1,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': ""David Restrepo, Chenwei Wu, Zhengxu Tang, Zitao Shuai, Thao Nguyen Minh Phan, Jun-En Ding, Cong-Tinh Dao, Jack Gallifant, Robyn Gayle Dychiao, Jose Carlo Artiaga, Andr\\'e Hiroshi Bando, Carolina Pelegrini Barbosa Gracitelli, Vincenz Ferrer, Leo Anthony Celi, Danielle Bitterman, Michael G Morley, Luis Filipe Nakayama""}]","David Restrepo, Chenwei Wu, Zhengxu Tang, Zitao Shuai, Thao Nguyen Minh Phan, Jun-En Ding, Cong-Tinh Dao, Jack Gallifant, Robyn Gayle Dychiao, Jose Carlo Artiaga, Andr\'e Hiroshi Bando, Carolina Pelegrini Barbosa Gracitelli, Vincenz Ferrer, Leo Anthony Celi, Danielle Bitterman, Michael G Morley, Luis Filipe Nakayama","{'name': ""David Restrepo, Chenwei Wu, Zhengxu Tang, Zitao Shuai, Thao Nguyen Minh Phan, Jun-En Ding, Cong-Tinh Dao, Jack Gallifant, Robyn Gayle Dychiao, Jose Carlo Artiaga, Andr\\'e Hiroshi Bando, Carolina Pelegrini Barbosa Gracitelli, Vincenz Ferrer, Leo Anthony Celi, Danielle Bitterman, Michael G Morley, Luis Filipe Nakayama""}",,
58,Closing the Gap: A User Study on the Real-world Usefulness of AI-powered Vulnerability Detection & Repair in the IDE,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Closing the Gap: A User Study on the Real-world Usefulness of AI-powered Vulnerability Detection & Repair in the IDE'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14306'}]",https://arxiv.org/abs/2412.14306,"arXiv:2412.14306v1 Announce Type: new 
Abstract: This paper presents the first empirical study of a vulnerability detection and fix tool with professional software developers on real projects that they own. We implemented DeepVulGuard, an IDE-integrated tool based on state-of-the-art detection and fix models, and show that it has promising performance on benchmarks of historic vulnerability data. DeepVulGuard scans code for vulnerabilities (including identifying the vulnerability type and vulnerable region of code), suggests fixes, provides natural-language explanations for alerts and fixes, leveraging chat interfaces. We recruited 17 professional software developers at Microsoft, observed their usage of the tool on their code, and conducted interviews to assess the tool's usefulness, speed, trust, relevance, and workflow integration. We also gathered detailed qualitative feedback on users' perceptions and their desired features. Study participants scanned a total of 24 projects, 6.9k files, and over 1.7 million lines of source code, and generated 170 alerts and 50 fix suggestions. We find that although state-of-the-art AI-powered detection and fix tools show promise, they are not yet practical for real-world use due to a high rate of false positives and non-applicable fixes. User feedback reveals several actionable pain points, ranging from incomplete context to lack of customization for the user's codebase. Additionally, we explore how AI features, including confidence scores, explanations, and chat interaction, can apply to vulnerability detection and fixing. Based on these insights, we offer practical recommendations for evaluating and deploying AI detection and fix models. Our code and data are available at https://doi.org/10.6084/m9.figshare.26367139.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14306v1 Announce Type: new \nAbstract: This paper presents the first empirical study of a vulnerability detection and fix tool with professional software developers on real projects that they own. We implemented DeepVulGuard, an IDE-integrated tool based on state-of-the-art detection and fix models, and show that it has promising performance on benchmarks of historic vulnerability data. DeepVulGuard scans code for vulnerabilities (including identifying the vulnerability type and vulnerable region of code), suggests fixes, provides natural-language explanations for alerts and fixes, leveraging chat interfaces. We recruited 17 professional software developers at Microsoft, observed their usage of the tool on their code, and conducted interviews to assess the tool's usefulness, speed, trust, relevance, and workflow integration. We also gathered detailed qualitative feedback on users' perceptions and their desired features. Study participants scanned a total of 24 projects, 6.9k files, and over 1.7 million lines of source code, and generated 170 alerts and 50 fix suggestions. We find that although state-of-the-art AI-powered detection and fix tools show promise, they are not yet practical for real-world use due to a high rate of false positives and non-applicable fixes. User feedback reveals several actionable pain points, ranging from incomplete context to lack of customization for the user's codebase. Additionally, we explore how AI features, including confidence scores, explanations, and chat interaction, can apply to vulnerability detection and fixing. Based on these insights, we offer practical recommendations for evaluating and deploying AI detection and fix models. Our code and data are available at https://doi.org/10.6084/m9.figshare.26367139.""}",oai:arXiv.org:2412.14306v1,False,"[{'term': 'cs.SE', 'scheme': None, 'label': None}, {'term': 'cs.CR', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Benjamin Steenhoek, Kalpathy Sivaraman, Renata Saldivar Gonzalez, Yevhen Mohylevskyy, Roshanak Zilouchian Moghaddam, Wei Le'}]","Benjamin Steenhoek, Kalpathy Sivaraman, Renata Saldivar Gonzalez, Yevhen Mohylevskyy, Roshanak Zilouchian Moghaddam, Wei Le","{'name': 'Benjamin Steenhoek, Kalpathy Sivaraman, Renata Saldivar Gonzalez, Yevhen Mohylevskyy, Roshanak Zilouchian Moghaddam, Wei Le'}",,
59,Race Discrimination in Internet Advertising: Evidence From a Field Experiment,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Race Discrimination in Internet Advertising: Evidence From a Field Experiment'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14307'}]",https://arxiv.org/abs/2412.14307,"arXiv:2412.14307v1 Announce Type: new 
Abstract: We present the results of an experiment documenting racial bias on Meta's Advertising Platform in Brazil and the United States. We find that darker skin complexions are penalized, leading to real economic consequences. For every \$1,000 an advertiser spends on ads with models with light-skin complexions, that advertiser would have to spend \$1,159 to achieve the same level of engagement using photos of darker skin complexion models. Meta's budget optimization tool reinforces these viewer biases. When pictures of models with light and dark complexions are allocated a shared budget, Meta funnels roughly 64\% of the budget towards photos featuring lighter skin complexions.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14307v1 Announce Type: new \nAbstract: We present the results of an experiment documenting racial bias on Meta's Advertising Platform in Brazil and the United States. We find that darker skin complexions are penalized, leading to real economic consequences. For every \\$1,000 an advertiser spends on ads with models with light-skin complexions, that advertiser would have to spend \\$1,159 to achieve the same level of engagement using photos of darker skin complexion models. Meta's budget optimization tool reinforces these viewer biases. When pictures of models with light and dark complexions are allocated a shared budget, Meta funnels roughly 64\\% of the budget towards photos featuring lighter skin complexions.""}",oai:arXiv.org:2412.14307v1,False,"[{'term': 'cs.CY', 'scheme': None, 'label': None}, {'term': 'cs.HC', 'scheme': None, 'label': None}, {'term': 'cs.SI', 'scheme': None, 'label': None}, {'term': 'econ.GN', 'scheme': None, 'label': None}, {'term': 'q-fin.EC', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Neil K. R. Sehgal, Dan Svirsky'}]","Neil K. R. Sehgal, Dan Svirsky","{'name': 'Neil K. R. Sehgal, Dan Svirsky'}",,
60,Reinforcement Learning from Automatic Feedback for High-Quality Unit Test Generation,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Reinforcement Learning from Automatic Feedback for High-Quality Unit Test Generation'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14308'}]",https://arxiv.org/abs/2412.14308,"arXiv:2412.14308v1 Announce Type: new 
Abstract: Software testing is a crucial but time-consuming aspect of software development, and recently, Large Language Models (LLMs) have gained popularity for automated test case generation. However, because LLMs are trained on vast amounts of open-source code, they often generate test cases that do not adhere to best practices and may even contain test smells (anti-patterns). To address this issue, we propose Reinforcement Learning from Static Quality Metrics (RLSQM), wherein we utilize Reinforcement Learning to generate high-quality unit tests based on static analysis-based quality metrics. First, we analyzed LLM-generated tests and show that LLMs frequently do generate undesirable test smells -- up to 37% of the time. Then, we implemented lightweight static analysis-based reward model and trained LLMs using this reward model to optimize for five code quality metrics. Our experimental results demonstrate that the RL-optimized Codex model consistently generated higher-quality test cases than the base LLM, improving quality metrics by up to 23%, and generated nearly 100% syntactically-correct code. RLSQM also outperformed GPT-4 on all code quality metrics, in spite of training a substantially cheaper Codex model. We provide insights into how reliably utilize RL to improve test generation quality and show that RLSQM is a significant step towards enhancing the overall efficiency and reliability of automated software testing. Our data are available at https://doi.org/10.6084/m9.figshare.25983166.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14308v1 Announce Type: new \nAbstract: Software testing is a crucial but time-consuming aspect of software development, and recently, Large Language Models (LLMs) have gained popularity for automated test case generation. However, because LLMs are trained on vast amounts of open-source code, they often generate test cases that do not adhere to best practices and may even contain test smells (anti-patterns). To address this issue, we propose Reinforcement Learning from Static Quality Metrics (RLSQM), wherein we utilize Reinforcement Learning to generate high-quality unit tests based on static analysis-based quality metrics. First, we analyzed LLM-generated tests and show that LLMs frequently do generate undesirable test smells -- up to 37% of the time. Then, we implemented lightweight static analysis-based reward model and trained LLMs using this reward model to optimize for five code quality metrics. Our experimental results demonstrate that the RL-optimized Codex model consistently generated higher-quality test cases than the base LLM, improving quality metrics by up to 23%, and generated nearly 100% syntactically-correct code. RLSQM also outperformed GPT-4 on all code quality metrics, in spite of training a substantially cheaper Codex model. We provide insights into how reliably utilize RL to improve test generation quality and show that RLSQM is a significant step towards enhancing the overall efficiency and reliability of automated software testing. Our data are available at https://doi.org/10.6084/m9.figshare.25983166.'}",oai:arXiv.org:2412.14308v1,False,"[{'term': 'cs.SE', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Benjamin Steenhoek, Michele Tufano, Neel Sundaresan, Alexey Svyatkovskiy'}]","Benjamin Steenhoek, Michele Tufano, Neel Sundaresan, Alexey Svyatkovskiy","{'name': 'Benjamin Steenhoek, Michele Tufano, Neel Sundaresan, Alexey Svyatkovskiy'}",,
61,Consistency Matters: Defining Demonstration Data Quality Metrics in Robot Learning from Demonstration,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Consistency Matters: Defining Demonstration Data Quality Metrics in Robot Learning from Demonstration'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14309'}]",https://arxiv.org/abs/2412.14309,"arXiv:2412.14309v1 Announce Type: new 
Abstract: Learning from Demonstration (LfD) empowers robots to acquire new skills through human demonstrations, making it feasible for everyday users to teach robots. However, the success of learning and generalization heavily depends on the quality of these demonstrations. Consistency is often used to indicate quality in LfD, yet the factors that define this consistency remain underexplored. In this paper, we evaluate a comprehensive set of motion data characteristics to determine which consistency measures best predict learning performance. By ensuring demonstration consistency prior to training, we enhance models' predictive accuracy and generalization to novel scenarios. We validate our approach with two user studies involving participants with diverse levels of robotics expertise. In the first study (N = 24), users taught a PR2 robot to perform a button-pressing task in a constrained environment, while in the second study (N = 30), participants trained a UR5 robot on a pick-and-place task. Results show that demonstration consistency significantly impacts success rates in both learning and generalization, with 70% and 89% of task success rates in the two studies predicted using our consistency metrics. Moreover, our metrics estimate generalized performance success rates with 76% and 91% accuracy. These findings suggest that our proposed measures provide an intuitive, practical way to assess demonstration data quality before training, without requiring expert data or algorithm-specific modifications. Our approach offers a systematic way to evaluate demonstration quality, addressing a critical gap in LfD by formalizing consistency metrics that enhance the reliability of robot learning from human demonstrations.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14309v1 Announce Type: new \nAbstract: Learning from Demonstration (LfD) empowers robots to acquire new skills through human demonstrations, making it feasible for everyday users to teach robots. However, the success of learning and generalization heavily depends on the quality of these demonstrations. Consistency is often used to indicate quality in LfD, yet the factors that define this consistency remain underexplored. In this paper, we evaluate a comprehensive set of motion data characteristics to determine which consistency measures best predict learning performance. By ensuring demonstration consistency prior to training, we enhance models' predictive accuracy and generalization to novel scenarios. We validate our approach with two user studies involving participants with diverse levels of robotics expertise. In the first study (N = 24), users taught a PR2 robot to perform a button-pressing task in a constrained environment, while in the second study (N = 30), participants trained a UR5 robot on a pick-and-place task. Results show that demonstration consistency significantly impacts success rates in both learning and generalization, with 70% and 89% of task success rates in the two studies predicted using our consistency metrics. Moreover, our metrics estimate generalized performance success rates with 76% and 91% accuracy. These findings suggest that our proposed measures provide an intuitive, practical way to assess demonstration data quality before training, without requiring expert data or algorithm-specific modifications. Our approach offers a systematic way to evaluate demonstration quality, addressing a critical gap in LfD by formalizing consistency metrics that enhance the reliability of robot learning from human demonstrations.""}",oai:arXiv.org:2412.14309v1,False,"[{'term': 'cs.RO', 'scheme': None, 'label': None}, {'term': 'cs.HC', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by-nc-sa/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-sa/4.0/'}","[{'name': 'Maram Sakr, H. F. Machiel Van der Loos, Dana Kulic, Elizabeth Croft'}]","Maram Sakr, H. F. Machiel Van der Loos, Dana Kulic, Elizabeth Croft","{'name': 'Maram Sakr, H. F. Machiel Van der Loos, Dana Kulic, Elizabeth Croft'}",,
62,Stealing That Free Lunch: Exposing the Limits of Dyna-Style Reinforcement Learning,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Stealing That Free Lunch: Exposing the Limits of Dyna-Style Reinforcement Learning'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14312'}]",https://arxiv.org/abs/2412.14312,"arXiv:2412.14312v1 Announce Type: new 
Abstract: Dyna-style off-policy model-based reinforcement learning (DMBRL) algorithms are a family of techniques for generating synthetic state transition data and thereby enhancing the sample efficiency of off-policy RL algorithms. This paper identifies and investigates a surprising performance gap observed when applying DMBRL algorithms across different benchmark environments with proprioceptive observations. We show that, while DMBRL algorithms perform well in OpenAI Gym, their performance can drop significantly in DeepMind Control Suite (DMC), even though these settings offer similar tasks and identical physics backends. Modern techniques designed to address several key issues that arise in these settings do not provide a consistent improvement across all environments, and overall our results show that adding synthetic rollouts to the training process -- the backbone of Dyna-style algorithms -- significantly degrades performance across most DMC environments. Our findings contribute to a deeper understanding of several fundamental challenges in model-based RL and show that, like many optimization fields, there is no free lunch when evaluating performance across diverse benchmarks in RL.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14312v1 Announce Type: new \nAbstract: Dyna-style off-policy model-based reinforcement learning (DMBRL) algorithms are a family of techniques for generating synthetic state transition data and thereby enhancing the sample efficiency of off-policy RL algorithms. This paper identifies and investigates a surprising performance gap observed when applying DMBRL algorithms across different benchmark environments with proprioceptive observations. We show that, while DMBRL algorithms perform well in OpenAI Gym, their performance can drop significantly in DeepMind Control Suite (DMC), even though these settings offer similar tasks and identical physics backends. Modern techniques designed to address several key issues that arise in these settings do not provide a consistent improvement across all environments, and overall our results show that adding synthetic rollouts to the training process -- the backbone of Dyna-style algorithms -- significantly degrades performance across most DMC environments. Our findings contribute to a deeper understanding of several fundamental challenges in model-based RL and show that, like many optimization fields, there is no free lunch when evaluating performance across diverse benchmarks in RL.'}",oai:arXiv.org:2412.14312v1,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Brett Barkley, David Fridovich-Keil'}]","Brett Barkley, David Fridovich-Keil","{'name': 'Brett Barkley, David Fridovich-Keil'}",,
63,Reaching the equilibrium: Long-term stable approximations for stochastic non-Newtonian Stokes equations with transport noise,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Reaching the equilibrium: Long-term stable approximations for stochastic non-Newtonian Stokes equations with transport noise'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14316'}]",https://arxiv.org/abs/2412.14316,"arXiv:2412.14316v1 Announce Type: new 
Abstract: We propose and analyse a novel, fully discrete numerical algorithm for the approximation of the generalised Stokes system forced by transport noise -- a prototype model for non-Newtonian fluids including turbulence. Utilising the Gradient Discretisation Method, we show that the algorithm is long-term stable for a broad class of particular Gradient Discretisations. Building on the long-term stability and the derived continuity of the algorithm's solution operator, we construct two sequences of approximate invariant measures. At the moment, each sequence lacks one important feature: either the existence of a limit measure, or the invariance with respect to the discrete semigroup. We derive an abstract condition that merges both properties, recovering the existence of an invariant measure. We provide an example for which invariance and existence hold simultaneously, and characterise the invariant measure completely. We close the article by conducting two numerical experiments that show the influence of transport noise on the dynamics of power-law fluids; in particular, we find that transport noise enhances the dissipation of kinetic energy, the mixing of particles, as well as the size of vortices.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14316v1 Announce Type: new \nAbstract: We propose and analyse a novel, fully discrete numerical algorithm for the approximation of the generalised Stokes system forced by transport noise -- a prototype model for non-Newtonian fluids including turbulence. Utilising the Gradient Discretisation Method, we show that the algorithm is long-term stable for a broad class of particular Gradient Discretisations. Building on the long-term stability and the derived continuity of the algorithm's solution operator, we construct two sequences of approximate invariant measures. At the moment, each sequence lacks one important feature: either the existence of a limit measure, or the invariance with respect to the discrete semigroup. We derive an abstract condition that merges both properties, recovering the existence of an invariant measure. We provide an example for which invariance and existence hold simultaneously, and characterise the invariant measure completely. We close the article by conducting two numerical experiments that show the influence of transport noise on the dynamics of power-law fluids; in particular, we find that transport noise enhances the dissipation of kinetic energy, the mixing of particles, as well as the size of vortices.""}",oai:arXiv.org:2412.14316v1,False,"[{'term': 'math.NA', 'scheme': None, 'label': None}, {'term': 'cs.NA', 'scheme': None, 'label': None}, {'term': 'math.PR', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Jerome Droniou, Kim-Ngan Le, J\\""orn Wichmann'}]","Jerome Droniou, Kim-Ngan Le, J\""orn Wichmann","{'name': 'Jerome Droniou, Kim-Ngan Le, J\\""orn Wichmann'}",,
64,The Role of Handling Attributive Nouns in Improving Chinese-To-English Machine Translation,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'The Role of Handling Attributive Nouns in Improving Chinese-To-English Machine Translation'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14323'}]",https://arxiv.org/abs/2412.14323,"arXiv:2412.14323v1 Announce Type: new 
Abstract: Translating between languages with drastically different grammatical conventions poses challenges, not just for human interpreters but also for machine translation systems. In this work, we specifically target the translation challenges posed by attributive nouns in Chinese, which frequently cause ambiguities in English translation. By manually inserting the omitted particle X ('DE'). In news article titles from the Penn Chinese Discourse Treebank, we developed a targeted dataset to fine-tune Hugging Face Chinese to English translation models, specifically improving how this critical function word is handled. This focused approach not only complements the broader strategies suggested by previous studies but also offers a practical enhancement by specifically addressing a common error type in Chinese-English translation.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14323v1 Announce Type: new \nAbstract: Translating between languages with drastically different grammatical conventions poses challenges, not just for human interpreters but also for machine translation systems. In this work, we specifically target the translation challenges posed by attributive nouns in Chinese, which frequently cause ambiguities in English translation. By manually inserting the omitted particle X ('DE'). In news article titles from the Penn Chinese Discourse Treebank, we developed a targeted dataset to fine-tune Hugging Face Chinese to English translation models, specifically improving how this critical function word is handled. This focused approach not only complements the broader strategies suggested by previous studies but also offers a practical enhancement by specifically addressing a common error type in Chinese-English translation.""}",oai:arXiv.org:2412.14323v1,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Haohao (Lisa),  Wang, Adam Meyers, John E. Ortega, Rodolfo Zevallos'}]","Haohao (Lisa),  Wang, Adam Meyers, John E. Ortega, Rodolfo Zevallos","{'name': 'Haohao (Lisa),  Wang, Adam Meyers, John E. Ortega, Rodolfo Zevallos'}",,
65,Covariances for Free: Exploiting Mean Distributions for Federated Learning with Pre-Trained Models,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Covariances for Free: Exploiting Mean Distributions for Federated Learning with Pre-Trained Models'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14326'}]",https://arxiv.org/abs/2412.14326,"arXiv:2412.14326v1 Announce Type: new 
Abstract: Using pre-trained models has been found to reduce the effect of data heterogeneity and speed up federated learning algorithms. Recent works have investigated the use of first-order statistics and second-order statistics to aggregate local client data distributions at the server and achieve very high performance without any training. In this work we propose a training-free method based on an unbiased estimator of class covariance matrices. Our method, which only uses first-order statistics in the form of class means communicated by clients to the server, incurs only a fraction of the communication costs required by methods based on communicating second-order statistics. We show how these estimated class covariances can be used to initialize a linear classifier, thus exploiting the covariances without actually sharing them. When compared to state-of-the-art methods which also share only class means, our approach improves performance in the range of 4-26\% with exactly the same communication cost. Moreover, our method achieves performance competitive or superior to sharing second-order statistics with dramatically less communication overhead. Finally, using our method to initialize classifiers and then performing federated fine-tuning yields better and faster convergence. Code is available at https://github.com/dipamgoswami/FedCOF.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14326v1 Announce Type: new \nAbstract: Using pre-trained models has been found to reduce the effect of data heterogeneity and speed up federated learning algorithms. Recent works have investigated the use of first-order statistics and second-order statistics to aggregate local client data distributions at the server and achieve very high performance without any training. In this work we propose a training-free method based on an unbiased estimator of class covariance matrices. Our method, which only uses first-order statistics in the form of class means communicated by clients to the server, incurs only a fraction of the communication costs required by methods based on communicating second-order statistics. We show how these estimated class covariances can be used to initialize a linear classifier, thus exploiting the covariances without actually sharing them. When compared to state-of-the-art methods which also share only class means, our approach improves performance in the range of 4-26\\% with exactly the same communication cost. Moreover, our method achieves performance competitive or superior to sharing second-order statistics with dramatically less communication overhead. Finally, using our method to initialize classifiers and then performing federated fine-tuning yields better and faster convergence. Code is available at https://github.com/dipamgoswami/FedCOF.'}",oai:arXiv.org:2412.14326v1,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Dipam Goswami, Simone Magistri, Kai Wang, Bart{\\l}omiej Twardowski, Andrew D. Bagdanov, Joost van de Weijer'}]","Dipam Goswami, Simone Magistri, Kai Wang, Bart{\l}omiej Twardowski, Andrew D. Bagdanov, Joost van de Weijer","{'name': 'Dipam Goswami, Simone Magistri, Kai Wang, Bart{\\l}omiej Twardowski, Andrew D. Bagdanov, Joost van de Weijer'}",,
66,Personalized Generative Low-light Image Denoising and Enhancement,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Personalized Generative Low-light Image Denoising and Enhancement'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14327'}]",https://arxiv.org/abs/2412.14327,"arXiv:2412.14327v1 Announce Type: new 
Abstract: While smartphone cameras today can produce astonishingly good photos, their performance in low light is still not completely satisfactory because of the fundamental limits in photon shot noise and sensor read noise. Generative image restoration methods have demonstrated promising results compared to traditional methods, but they suffer from hallucinatory content generation when the signal-to-noise ratio (SNR) is low. Recognizing the availability of personalized photo galleries on users' smartphones, we propose Personalized Generative Denoising (PGD) by building a diffusion model customized for different users. Our core innovation is an identity-consistent physical buffer that extracts the physical attributes of the person from the gallery. This ID-consistent physical buffer provides a strong prior that can be integrated with the diffusion model to restore the degraded images, without the need of fine-tuning. Over a wide range of low-light testing scenarios, we show that PGD achieves superior image denoising and enhancement performance compared to existing diffusion-based denoising approaches.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14327v1 Announce Type: new \nAbstract: While smartphone cameras today can produce astonishingly good photos, their performance in low light is still not completely satisfactory because of the fundamental limits in photon shot noise and sensor read noise. Generative image restoration methods have demonstrated promising results compared to traditional methods, but they suffer from hallucinatory content generation when the signal-to-noise ratio (SNR) is low. Recognizing the availability of personalized photo galleries on users' smartphones, we propose Personalized Generative Denoising (PGD) by building a diffusion model customized for different users. Our core innovation is an identity-consistent physical buffer that extracts the physical attributes of the person from the gallery. This ID-consistent physical buffer provides a strong prior that can be integrated with the diffusion model to restore the degraded images, without the need of fine-tuning. Over a wide range of low-light testing scenarios, we show that PGD achieves superior image denoising and enhancement performance compared to existing diffusion-based denoising approaches.""}",oai:arXiv.org:2412.14327v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Xijun Wang, Prateek Chennuri, Yu Yuan, Bole Ma, Xingguang Zhang, Stanley Chan'}]","Xijun Wang, Prateek Chennuri, Yu Yuan, Bole Ma, Xingguang Zhang, Stanley Chan","{'name': 'Xijun Wang, Prateek Chennuri, Yu Yuan, Bole Ma, Xingguang Zhang, Stanley Chan'}",,
67,Semantic Role Labeling of NomBank Partitives,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Semantic Role Labeling of NomBank Partitives'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14328'}]",https://arxiv.org/abs/2412.14328,"arXiv:2412.14328v1 Announce Type: new 
Abstract: This article is about Semantic Role Labeling for English partitive nouns (5%/REL of the price/ARG1; The price/ARG1 rose 5 percent/REL) in the NomBank annotated corpus. Several systems are described using traditional and transformer-based machine learning, as well as ensembling. Our highest scoring system achieves an F1 of 91.74% using ""gold"" parses from the Penn Treebank and 91.12% when using the Berkeley Neural parser. This research includes both classroom and experimental settings for system development.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14328v1 Announce Type: new \nAbstract: This article is about Semantic Role Labeling for English partitive nouns (5%/REL of the price/ARG1; The price/ARG1 rose 5 percent/REL) in the NomBank annotated corpus. Several systems are described using traditional and transformer-based machine learning, as well as ensembling. Our highest scoring system achieves an F1 of 91.74% using ""gold"" parses from the Penn Treebank and 91.12% when using the Berkeley Neural parser. This research includes both classroom and experimental settings for system development.'}",oai:arXiv.org:2412.14328v1,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Adam Meyers, Advait Pravin Savant, John E. Ortega'}]","Adam Meyers, Advait Pravin Savant, John E. Ortega","{'name': 'Adam Meyers, Advait Pravin Savant, John E. Ortega'}",,
68,Embedding Cultural Diversity in Prototype-based Recommender Systems,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Embedding Cultural Diversity in Prototype-based Recommender Systems'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14329'}]",https://arxiv.org/abs/2412.14329,"arXiv:2412.14329v1 Announce Type: new 
Abstract: Popularity bias in recommender systems can increase cultural overrepresentation by favoring norms from dominant cultures and marginalizing underrepresented groups. This issue is critical for platforms offering cultural products, as they influence consumption patterns and human perceptions. In this work, we address popularity bias by identifying demographic biases within prototype-based matrix factorization methods. Using the country of origin as a proxy for cultural identity, we link this demographic attribute to popularity bias by refining the embedding space learning process. First, we propose filtering out irrelevant prototypes to improve representativity. Second, we introduce a regularization technique to enforce a uniform distribution of prototypes within the embedding space. Across four datasets, our results demonstrate a 27\% reduction in the average rank of long-tail items and a 2\% reduction in the average rank of items from underrepresented countries. Additionally, our model achieves a 2\% improvement in HitRatio@10 compared to the state-of-the-art, highlighting that fairness is enhanced without compromising recommendation quality. Moreover, the distribution of prototypes leads to more inclusive explanations by better aligning items with diverse prototypes.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14329v1 Announce Type: new \nAbstract: Popularity bias in recommender systems can increase cultural overrepresentation by favoring norms from dominant cultures and marginalizing underrepresented groups. This issue is critical for platforms offering cultural products, as they influence consumption patterns and human perceptions. In this work, we address popularity bias by identifying demographic biases within prototype-based matrix factorization methods. Using the country of origin as a proxy for cultural identity, we link this demographic attribute to popularity bias by refining the embedding space learning process. First, we propose filtering out irrelevant prototypes to improve representativity. Second, we introduce a regularization technique to enforce a uniform distribution of prototypes within the embedding space. Across four datasets, our results demonstrate a 27\\% reduction in the average rank of long-tail items and a 2\\% reduction in the average rank of items from underrepresented countries. Additionally, our model achieves a 2\\% improvement in HitRatio@10 compared to the state-of-the-art, highlighting that fairness is enhanced without compromising recommendation quality. Moreover, the distribution of prototypes leads to more inclusive explanations by better aligning items with diverse prototypes.'}",oai:arXiv.org:2412.14329v1,False,"[{'term': 'cs.IR', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.CY', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Armin Moradi, Nicola Neophytou, Florian Carichon, Golnoosh Farnadi'}]","Armin Moradi, Nicola Neophytou, Florian Carichon, Golnoosh Farnadi","{'name': 'Armin Moradi, Nicola Neophytou, Florian Carichon, Golnoosh Farnadi'}",,
69,Joint Co-Speech Gesture and Expressive Talking Face Generation using Diffusion with Adapters,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Joint Co-Speech Gesture and Expressive Talking Face Generation using Diffusion with Adapters'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14333'}]",https://arxiv.org/abs/2412.14333,"arXiv:2412.14333v1 Announce Type: new 
Abstract: Recent advances in co-speech gesture and talking head generation have been impressive, yet most methods focus on only one of the two tasks. Those that attempt to generate both often rely on separate models or network modules, increasing training complexity and ignoring the inherent relationship between face and body movements. To address the challenges, in this paper, we propose a novel model architecture that jointly generates face and body motions within a single network. This approach leverages shared weights between modalities, facilitated by adapters that enable adaptation to a common latent space. Our experiments demonstrate that the proposed framework not only maintains state-of-the-art co-speech gesture and talking head generation performance but also significantly reduces the number of parameters required.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14333v1 Announce Type: new \nAbstract: Recent advances in co-speech gesture and talking head generation have been impressive, yet most methods focus on only one of the two tasks. Those that attempt to generate both often rely on separate models or network modules, increasing training complexity and ignoring the inherent relationship between face and body movements. To address the challenges, in this paper, we propose a novel model architecture that jointly generates face and body motions within a single network. This approach leverages shared weights between modalities, facilitated by adapters that enable adaptation to a common latent space. Our experiments demonstrate that the proposed framework not only maintains state-of-the-art co-speech gesture and talking head generation performance but also significantly reduces the number of parameters required.'}",oai:arXiv.org:2412.14333v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Steven Hogue, Chenxu Zhang, Yapeng Tian, Xiaohu Guo'}]","Steven Hogue, Chenxu Zhang, Yapeng Tian, Xiaohu Guo","{'name': 'Steven Hogue, Chenxu Zhang, Yapeng Tian, Xiaohu Guo'}",,
70,Optimizing ML Concurrent Computation and Communication with GPU DMA Engines,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Optimizing ML Concurrent Computation and Communication with GPU DMA Engines'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14335'}]",https://arxiv.org/abs/2412.14335,"arXiv:2412.14335v1 Announce Type: new 
Abstract: Concurrent computation and communication (C3) is a pervasive paradigm in ML and other domains, making its performance optimization crucial. In this paper, we carefully characterize C3 in ML on GPUs, which are most widely deployed for ML training and inference. We observe that while C3 leads to performance uplifts, the uplifts are far lower than ideal speedups (serial computation and communication versus maximum of computation or communication; all times from isolated executions). C3 on average achieves only 21% of ideal speedup, this is due to known challenges of compute and memory interference between concurrent GPU kernels (that is, sharing of GPU's compute units, caches and HBM).
  To attain better performance for C3, first, we evaluate dual strategies of schedule prioritization and careful resource partitioning of compute units on GPUs to push performance attained with C3 (on average 42% of ideal speedup). We also provide heuristics that can guide a runtime while employing these strategies. To further enhance C3 performance, we propose to mitigate C3 interference by offloading communication tasks to the GPU's DMA engines. To this end, we build Concurrent Communication CoLlectives (ConCCL) proof-of-concepts that harness DMA engines for communication. We show how ConCCL considerably closes the gap between realized and ideal speedup for C3 (on average 72% of ideal speedup is realized, up to 1.67x speedup). Overall, our work makes a strong case for GPU DMA engine advancements to better support C3 on GPUs.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14335v1 Announce Type: new \nAbstract: Concurrent computation and communication (C3) is a pervasive paradigm in ML and other domains, making its performance optimization crucial. In this paper, we carefully characterize C3 in ML on GPUs, which are most widely deployed for ML training and inference. We observe that while C3 leads to performance uplifts, the uplifts are far lower than ideal speedups (serial computation and communication versus maximum of computation or communication; all times from isolated executions). C3 on average achieves only 21% of ideal speedup, this is due to known challenges of compute and memory interference between concurrent GPU kernels (that is, sharing of GPU's compute units, caches and HBM).\n  To attain better performance for C3, first, we evaluate dual strategies of schedule prioritization and careful resource partitioning of compute units on GPUs to push performance attained with C3 (on average 42% of ideal speedup). We also provide heuristics that can guide a runtime while employing these strategies. To further enhance C3 performance, we propose to mitigate C3 interference by offloading communication tasks to the GPU's DMA engines. To this end, we build Concurrent Communication CoLlectives (ConCCL) proof-of-concepts that harness DMA engines for communication. We show how ConCCL considerably closes the gap between realized and ideal speedup for C3 (on average 72% of ideal speedup is realized, up to 1.67x speedup). Overall, our work makes a strong case for GPU DMA engine advancements to better support C3 on GPUs.""}",oai:arXiv.org:2412.14335v1,False,"[{'term': 'cs.AR', 'scheme': None, 'label': None}, {'term': 'cs.DC', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Anirudha Agrawal, Shaizeen Aga, Suchita Pati, Mahzabeen Islam'}]","Anirudha Agrawal, Shaizeen Aga, Suchita Pati, Mahzabeen Islam","{'name': 'Anirudha Agrawal, Shaizeen Aga, Suchita Pati, Mahzabeen Islam'}",,
71,Commitment to Sparse Strategies in Two-Player Games,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Commitment to Sparse Strategies in Two-Player Games'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14337'}]",https://arxiv.org/abs/2412.14337,"arXiv:2412.14337v1 Announce Type: new 
Abstract: While Nash equilibria are guaranteed to exist, they may exhibit dense support, making them difficult to understand and execute in some applications. In this paper, we study $k$-sparse commitments in games where one player is restricted to mixed strategies with support size at most $k$. Finding $k$-sparse commitments is known to be computationally hard. We start by showing several structural properties of $k$-sparse solutions, including that the optimal support may vary dramatically as $k$ increases. These results suggest that naive greedy or double-oracle-based approaches are unlikely to yield practical algorithms. We then develop a simple approach based on mixed integer linear programs (MILPs) for zero-sum games, general-sum Stackelberg games, and various forms of structured sparsity. We also propose practical algorithms for cases where one or both players have large (i.e., practically innumerable) action sets, utilizing a combination of MILPs and incremental strategy generation. We evaluate our methods on synthetic and real-world scenarios based on security applications. In both settings, we observe that even for small support sizes, we can obtain more than $90\%$ of the true Nash value while maintaining a reasonable runtime, demonstrating the significance of our formulation and algorithms.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14337v1 Announce Type: new \nAbstract: While Nash equilibria are guaranteed to exist, they may exhibit dense support, making them difficult to understand and execute in some applications. In this paper, we study $k$-sparse commitments in games where one player is restricted to mixed strategies with support size at most $k$. Finding $k$-sparse commitments is known to be computationally hard. We start by showing several structural properties of $k$-sparse solutions, including that the optimal support may vary dramatically as $k$ increases. These results suggest that naive greedy or double-oracle-based approaches are unlikely to yield practical algorithms. We then develop a simple approach based on mixed integer linear programs (MILPs) for zero-sum games, general-sum Stackelberg games, and various forms of structured sparsity. We also propose practical algorithms for cases where one or both players have large (i.e., practically innumerable) action sets, utilizing a combination of MILPs and incremental strategy generation. We evaluate our methods on synthetic and real-world scenarios based on security applications. In both settings, we observe that even for small support sizes, we can obtain more than $90\\%$ of the true Nash value while maintaining a reasonable runtime, demonstrating the significance of our formulation and algorithms.'}",oai:arXiv.org:2412.14337v1,False,"[{'term': 'cs.GT', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': ""Salam Afiouni, Jakub \\v{C}ern\\'y, Chun Kai Ling, Christian Kroer""}]","Salam Afiouni, Jakub \v{C}ern\'y, Chun Kai Ling, Christian Kroer","{'name': ""Salam Afiouni, Jakub \\v{C}ern\\'y, Chun Kai Ling, Christian Kroer""}",,
72,A Unifying Information-theoretic Perspective on Evaluating Generative Models,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'A Unifying Information-theoretic Perspective on Evaluating Generative Models'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14340'}]",https://arxiv.org/abs/2412.14340,"arXiv:2412.14340v1 Announce Type: new 
Abstract: Considering the difficulty of interpreting generative model output, there is significant current research focused on determining meaningful evaluation metrics. Several recent approaches utilize ""precision"" and ""recall,"" borrowed from the classification domain, to individually quantify the output fidelity (realism) and output diversity (representation of the real data variation), respectively. With the increase in metric proposals, there is a need for a unifying perspective, allowing for easier comparison and clearer explanation of their benefits and drawbacks. To this end, we unify a class of kth-nearest-neighbors (kNN)-based metrics under an information-theoretic lens using approaches from kNN density estimation. Additionally, we propose a tri-dimensional metric composed of Precision Cross-Entropy (PCE), Recall Cross-Entropy (RCE), and Recall Entropy (RE), which separately measure fidelity and two distinct aspects of diversity, inter- and intra-class. Our domain-agnostic metric, derived from the information-theoretic concepts of entropy and cross-entropy, can be dissected for both sample- and mode-level analysis. Our detailed experimental results demonstrate the sensitivity of our metric components to their respective qualities and reveal undesirable behaviors of other metrics.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14340v1 Announce Type: new \nAbstract: Considering the difficulty of interpreting generative model output, there is significant current research focused on determining meaningful evaluation metrics. Several recent approaches utilize ""precision"" and ""recall,"" borrowed from the classification domain, to individually quantify the output fidelity (realism) and output diversity (representation of the real data variation), respectively. With the increase in metric proposals, there is a need for a unifying perspective, allowing for easier comparison and clearer explanation of their benefits and drawbacks. To this end, we unify a class of kth-nearest-neighbors (kNN)-based metrics under an information-theoretic lens using approaches from kNN density estimation. Additionally, we propose a tri-dimensional metric composed of Precision Cross-Entropy (PCE), Recall Cross-Entropy (RCE), and Recall Entropy (RE), which separately measure fidelity and two distinct aspects of diversity, inter- and intra-class. Our domain-agnostic metric, derived from the information-theoretic concepts of entropy and cross-entropy, can be dissected for both sample- and mode-level analysis. Our detailed experimental results demonstrate the sensitivity of our metric components to their respective qualities and reveal undesirable behaviors of other metrics.'}",oai:arXiv.org:2412.14340v1,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Alexis Fox, Samarth Swarup, Abhijin Adiga'}]","Alexis Fox, Samarth Swarup, Abhijin Adiga","{'name': 'Alexis Fox, Samarth Swarup, Abhijin Adiga'}",,
73,Gaussian-convolution-invariant shell approximation to spherically-symmetric functions,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Gaussian-convolution-invariant shell approximation to spherically-symmetric functions'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14350'}]",https://arxiv.org/abs/2412.14350,"arXiv:2412.14350v1 Announce Type: new 
Abstract: We develop a class of functions Omega_N(x; mu, nu) in N-dimensional space concentrated around a spherical shell of the radius mu and such that, being convoluted with an isotropic Gaussian function, these functions do not change their expression but only a value of its 'width' parameter, nu. Isotropic Gaussian functions are a particular case of Omega_N(x; mu, nu) corresponding to mu = 0. Due to their features, these functions are an efficient tool to build approximations to smooth and continuous spherically-symmetric functions including oscillating ones. Atomic images in limited-resolution maps of the electron density, electrostatic scattering potential and other scalar fields studied in physics, chemistry, biology, and other natural sciences are examples of such functions. We give simple analytic expressions of Omega_N(x; mu, nu) for N = 1, 2, 3 and analyze properties of these functions. Representation of oscillating functions by a sum of Omega_N(x; mu, nu) allows calculating distorted maps for the same cost as the respective theoretical fields. We give practical examples of such representation for the interference functions of the uniform unit spheres for N = 1, 2, 3 that define the resolution of the respective images. Using the chain rule and analytic expressions of the Omega_N(x; mu, nu) derivatives makes simple refinement of parameters of the models which describe these fields.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14350v1 Announce Type: new \nAbstract: We develop a class of functions Omega_N(x; mu, nu) in N-dimensional space concentrated around a spherical shell of the radius mu and such that, being convoluted with an isotropic Gaussian function, these functions do not change their expression but only a value of its 'width' parameter, nu. Isotropic Gaussian functions are a particular case of Omega_N(x; mu, nu) corresponding to mu = 0. Due to their features, these functions are an efficient tool to build approximations to smooth and continuous spherically-symmetric functions including oscillating ones. Atomic images in limited-resolution maps of the electron density, electrostatic scattering potential and other scalar fields studied in physics, chemistry, biology, and other natural sciences are examples of such functions. We give simple analytic expressions of Omega_N(x; mu, nu) for N = 1, 2, 3 and analyze properties of these functions. Representation of oscillating functions by a sum of Omega_N(x; mu, nu) allows calculating distorted maps for the same cost as the respective theoretical fields. We give practical examples of such representation for the interference functions of the uniform unit spheres for N = 1, 2, 3 that define the resolution of the respective images. Using the chain rule and analytic expressions of the Omega_N(x; mu, nu) derivatives makes simple refinement of parameters of the models which describe these fields.""}",oai:arXiv.org:2412.14350v1,False,"[{'term': 'math.NA', 'scheme': None, 'label': None}, {'term': 'cs.CE', 'scheme': None, 'label': None}, {'term': 'cs.NA', 'scheme': None, 'label': None}, {'term': 'math-ph', 'scheme': None, 'label': None}, {'term': 'math.MP', 'scheme': None, 'label': None}, {'term': 'q-bio.BM', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Alexandre G. Urzhumtsev, Vladimir Y. Lunin'}]","Alexandre G. Urzhumtsev, Vladimir Y. Lunin","{'name': 'Alexandre G. Urzhumtsev, Vladimir Y. Lunin'}",,
74,Is Peer-Reviewing Worth the Effort?,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Is Peer-Reviewing Worth the Effort?'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14351'}]",https://arxiv.org/abs/2412.14351,"arXiv:2412.14351v1 Announce Type: new 
Abstract: How effective is peer-reviewing in identifying important papers? We treat this question as a forecasting task. Can we predict which papers will be highly cited in the future based on venue and ""early returns"" (citations soon after publication)? We show early returns are more predictive than venue. Finally, we end with constructive suggestions to address scaling challenges: (a) too many submissions and (b) too few qualified reviewers.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14351v1 Announce Type: new \nAbstract: How effective is peer-reviewing in identifying important papers? We treat this question as a forecasting task. Can we predict which papers will be highly cited in the future based on venue and ""early returns"" (citations soon after publication)? We show early returns are more predictive than venue. Finally, we end with constructive suggestions to address scaling challenges: (a) too many submissions and (b) too few qualified reviewers.'}",oai:arXiv.org:2412.14351v1,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Kenneth Church, Raman Chandrasekar, John E. Ortega, Ibrahim Said Ahmad'}]","Kenneth Church, Raman Chandrasekar, John E. Ortega, Ibrahim Said Ahmad","{'name': 'Kenneth Church, Raman Chandrasekar, John E. Ortega, Ibrahim Said Ahmad'}",,
75,A Survey on LLM Inference-Time Self-Improvement,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'A Survey on LLM Inference-Time Self-Improvement'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14352'}]",https://arxiv.org/abs/2412.14352,"arXiv:2412.14352v1 Announce Type: new 
Abstract: Techniques that enhance inference through increased computation at test-time have recently gained attention. In this survey, we investigate the current state of LLM Inference-Time Self-Improvement from three different perspectives: Independent Self-improvement, focusing on enhancements via decoding or sampling methods; Context-Aware Self-Improvement, leveraging additional context or datastore; and Model-Aided Self-Improvement, achieving improvement through model collaboration. We provide a comprehensive review of recent relevant studies, contribute an in-depth taxonomy, and discuss challenges and limitations, offering insights for future research.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14352v1 Announce Type: new \nAbstract: Techniques that enhance inference through increased computation at test-time have recently gained attention. In this survey, we investigate the current state of LLM Inference-Time Self-Improvement from three different perspectives: Independent Self-improvement, focusing on enhancements via decoding or sampling methods; Context-Aware Self-Improvement, leveraging additional context or datastore; and Model-Aided Self-Improvement, achieving improvement through model collaboration. We provide a comprehensive review of recent relevant studies, contribute an in-depth taxonomy, and discuss challenges and limitations, offering insights for future research.'}",oai:arXiv.org:2412.14352v1,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by-nc-sa/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-sa/4.0/'}","[{'name': 'Xiangjue Dong, Maria Teleki, James Caverlee'}]","Xiangjue Dong, Maria Teleki, James Caverlee","{'name': 'Xiangjue Dong, Maria Teleki, James Caverlee'}",,
76,State Space Models are Strong Text Rerankers,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'State Space Models are Strong Text Rerankers'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14354'}]",https://arxiv.org/abs/2412.14354,"arXiv:2412.14354v1 Announce Type: new 
Abstract: Transformers dominate NLP and IR; but their inference inefficiencies and challenges in extrapolating to longer contexts have sparked interest in alternative model architectures. Among these, state space models (SSMs) like Mamba offer promising advantages, particularly $O(1)$ time complexity in inference. Despite their potential, SSMs' effectiveness at text reranking -- a task requiring fine-grained query-document interaction and long-context understanding -- remains underexplored.
  This study benchmarks SSM-based architectures (specifically, Mamba-1 and Mamba-2) against transformer-based models across various scales, architectures, and pre-training objectives, focusing on performance and efficiency in text reranking tasks. We find that (1) Mamba architectures achieve competitive text ranking performance, comparable to transformer-based models of similar size; (2) they are less efficient in training and inference compared to transformers with flash attention; and (3) Mamba-2 outperforms Mamba-1 in both performance and efficiency. These results underscore the potential of state space models as a transformer alternative and highlight areas for improvement in future IR applications.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14354v1 Announce Type: new \nAbstract: Transformers dominate NLP and IR; but their inference inefficiencies and challenges in extrapolating to longer contexts have sparked interest in alternative model architectures. Among these, state space models (SSMs) like Mamba offer promising advantages, particularly $O(1)$ time complexity in inference. Despite their potential, SSMs' effectiveness at text reranking -- a task requiring fine-grained query-document interaction and long-context understanding -- remains underexplored.\n  This study benchmarks SSM-based architectures (specifically, Mamba-1 and Mamba-2) against transformer-based models across various scales, architectures, and pre-training objectives, focusing on performance and efficiency in text reranking tasks. We find that (1) Mamba architectures achieve competitive text ranking performance, comparable to transformer-based models of similar size; (2) they are less efficient in training and inference compared to transformers with flash attention; and (3) Mamba-2 outperforms Mamba-1 in both performance and efficiency. These results underscore the potential of state space models as a transformer alternative and highlight areas for improvement in future IR applications.""}",oai:arXiv.org:2412.14354v1,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}, {'term': 'cs.IR', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Zhichao Xu, Jinghua Yan, Ashim Gupta, Vivek Srikumar'}]","Zhichao Xu, Jinghua Yan, Ashim Gupta, Vivek Srikumar","{'name': 'Zhichao Xu, Jinghua Yan, Ashim Gupta, Vivek Srikumar'}",,
77,Enabling Realtime Reinforcement Learning at Scale with Staggered Asynchronous Inference,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Enabling Realtime Reinforcement Learning at Scale with Staggered Asynchronous Inference'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14355'}]",https://arxiv.org/abs/2412.14355,"arXiv:2412.14355v1 Announce Type: new 
Abstract: Realtime environments change even as agents perform action inference and learning, thus requiring high interaction frequencies to effectively minimize regret. However, recent advances in machine learning involve larger neural networks with longer inference times, raising questions about their applicability in realtime systems where reaction time is crucial. We present an analysis of lower bounds on regret in realtime reinforcement learning (RL) environments to show that minimizing long-term regret is generally impossible within the typical sequential interaction and learning paradigm, but often becomes possible when sufficient asynchronous compute is available. We propose novel algorithms for staggering asynchronous inference processes to ensure that actions are taken at consistent time intervals, and demonstrate that use of models with high action inference times is only constrained by the environment's effective stochasticity over the inference horizon, and not by action frequency. Our analysis shows that the number of inference processes needed scales linearly with increasing inference times while enabling use of models that are multiple orders of magnitude larger than existing approaches when learning from a realtime simulation of Game Boy games such as Pok\'emon and Tetris.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14355v1 Announce Type: new \nAbstract: Realtime environments change even as agents perform action inference and learning, thus requiring high interaction frequencies to effectively minimize regret. However, recent advances in machine learning involve larger neural networks with longer inference times, raising questions about their applicability in realtime systems where reaction time is crucial. We present an analysis of lower bounds on regret in realtime reinforcement learning (RL) environments to show that minimizing long-term regret is generally impossible within the typical sequential interaction and learning paradigm, but often becomes possible when sufficient asynchronous compute is available. We propose novel algorithms for staggering asynchronous inference processes to ensure that actions are taken at consistent time intervals, and demonstrate that use of models with high action inference times is only constrained by the environment's effective stochasticity over the inference horizon, and not by action frequency. Our analysis shows that the number of inference processes needed scales linearly with increasing inference times while enabling use of models that are multiple orders of magnitude larger than existing approaches when learning from a realtime simulation of Game Boy games such as Pok\\'emon and Tetris.""}",oai:arXiv.org:2412.14355v1,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Matthew Riemer, Gopeshh Subbaraj, Glen Berseth, Irina Rish'}]","Matthew Riemer, Gopeshh Subbaraj, Glen Berseth, Irina Rish","{'name': 'Matthew Riemer, Gopeshh Subbaraj, Glen Berseth, Irina Rish'}",,
78,Dynamic semantic VSLAM with known and unknown objects,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Dynamic semantic VSLAM with known and unknown objects'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14359'}]",https://arxiv.org/abs/2412.14359,"arXiv:2412.14359v1 Announce Type: new 
Abstract: Traditional Visual Simultaneous Localization and Mapping (VSLAM) systems assume a static environment, which makes them ineffective in highly dynamic settings. To overcome this, many approaches integrate semantic information from deep learning models to identify dynamic regions within images. However, these methods face a significant limitation as a supervised model cannot recognize objects not included in the training datasets. This paper introduces a novel feature-based Semantic VSLAM capable of detecting dynamic features in the presence of both known and unknown objects. By employing an unsupervised segmentation network, we achieve unlabeled segmentation, and next utilize an objector detector to identify any of the known classes among those. We then pair this with the computed high-gradient optical-flow information to next identify the static versus dynamic segmentations for both known and unknown object classes. A consistency check module is also introduced for further refinement and final classification into static versus dynamic features. Evaluations using public datasets demonstrate that our method offers superior performance than traditional VSLAM when unknown objects are present in the images while still matching the performance of the leading semantic VSLAM techniques when the images contain only the known objects","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14359v1 Announce Type: new \nAbstract: Traditional Visual Simultaneous Localization and Mapping (VSLAM) systems assume a static environment, which makes them ineffective in highly dynamic settings. To overcome this, many approaches integrate semantic information from deep learning models to identify dynamic regions within images. However, these methods face a significant limitation as a supervised model cannot recognize objects not included in the training datasets. This paper introduces a novel feature-based Semantic VSLAM capable of detecting dynamic features in the presence of both known and unknown objects. By employing an unsupervised segmentation network, we achieve unlabeled segmentation, and next utilize an objector detector to identify any of the known classes among those. We then pair this with the computed high-gradient optical-flow information to next identify the static versus dynamic segmentations for both known and unknown object classes. A consistency check module is also introduced for further refinement and final classification into static versus dynamic features. Evaluations using public datasets demonstrate that our method offers superior performance than traditional VSLAM when unknown objects are present in the images while still matching the performance of the leading semantic VSLAM techniques when the images contain only the known objects'}",oai:arXiv.org:2412.14359v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Sanghyoup Gu, Ratnesh Kumar'}]","Sanghyoup Gu, Ratnesh Kumar","{'name': 'Sanghyoup Gu, Ratnesh Kumar'}",,
79,A Fully Adaptive Radau Method for the Efficient Solution of Stiff Ordinary Differential Equations at Low Tolerances,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'A Fully Adaptive Radau Method for the Efficient Solution of Stiff Ordinary Differential Equations at Low Tolerances'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14362'}]",https://arxiv.org/abs/2412.14362,"arXiv:2412.14362v1 Announce Type: new 
Abstract: Radau IIA methods, specifically the adaptive order radau method in Fortran due to Hairer, are known to be state-of-the-art for the high-accuracy solution of highly stiff ordinary differential equations (ODEs). However, the traditional implementation was specialized to a specific range of tolerance, in particular only supporting 5th, 9th, and 13th order versions of the tableau and only derived in double precision floating point, thus limiting the ability to be truly general purpose for highly accurate scenarios. To alleviate these constraints, we implement an adaptive-time adaptive-order Radau method which can derive the coefficients for the Radau IIA embedded tableau to any order on the fly to any precision. Additionally, our Julia-based implementation includes many modernizations to improve performance, including improvements to the order adaptation scheme and improved linear algebra integrations. In a head-to-head benchmark against the classic Fortran implementation, we demonstrate our implementation is approximately 2x across a range of stiff ODEs. We benchmark our algorithm against several well-reputed numerical integrators for stiff ODEs and find state-of-the-art performance on several test problems, with a 1.5-times speed-up over common numerical integrators for stiff ODEs when low error tolerance is required. The newly implemented method is distributed in open source software for free usage on stiff ODEs.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14362v1 Announce Type: new \nAbstract: Radau IIA methods, specifically the adaptive order radau method in Fortran due to Hairer, are known to be state-of-the-art for the high-accuracy solution of highly stiff ordinary differential equations (ODEs). However, the traditional implementation was specialized to a specific range of tolerance, in particular only supporting 5th, 9th, and 13th order versions of the tableau and only derived in double precision floating point, thus limiting the ability to be truly general purpose for highly accurate scenarios. To alleviate these constraints, we implement an adaptive-time adaptive-order Radau method which can derive the coefficients for the Radau IIA embedded tableau to any order on the fly to any precision. Additionally, our Julia-based implementation includes many modernizations to improve performance, including improvements to the order adaptation scheme and improved linear algebra integrations. In a head-to-head benchmark against the classic Fortran implementation, we demonstrate our implementation is approximately 2x across a range of stiff ODEs. We benchmark our algorithm against several well-reputed numerical integrators for stiff ODEs and find state-of-the-art performance on several test problems, with a 1.5-times speed-up over common numerical integrators for stiff ODEs when low error tolerance is required. The newly implemented method is distributed in open source software for free usage on stiff ODEs.'}",oai:arXiv.org:2412.14362v1,False,"[{'term': 'math.NA', 'scheme': None, 'label': None}, {'term': 'cs.MS', 'scheme': None, 'label': None}, {'term': 'cs.NA', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by-nc-sa/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-sa/4.0/'}","[{'name': 'Shreyas Ekanathan, Oscar Smith, Christopher Rackauckas'}]","Shreyas Ekanathan, Oscar Smith, Christopher Rackauckas","{'name': 'Shreyas Ekanathan, Oscar Smith, Christopher Rackauckas'}",,
80,ResQ: Mixed-Precision Quantization of Large Language Models with Low-Rank Residuals,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'ResQ: Mixed-Precision Quantization of Large Language Models with Low-Rank Residuals'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14363'}]",https://arxiv.org/abs/2412.14363,"arXiv:2412.14363v1 Announce Type: new 
Abstract: Post-training quantization (PTQ) of large language models (LLMs) holds the promise in reducing the prohibitive computational cost at inference time. Quantization of all weight, activation and key-value (KV) cache tensors to 4-bit without significantly degrading generalizability is challenging, due to the high quantization error caused by extreme outliers in activations. To tackle this problem, we propose ResQ, a PTQ method that pushes further the state-of-the-art. By means of principal component analysis (PCA), it identifies a low-rank subspace (in practice 1/8 of the hidden dimension) in which activation variances are highest, and keep the coefficients within this subspace in high precision, e.g. 8-bit, while quantizing the rest to 4-bit. Within each subspace, invariant random rotation is applied to further suppress outliers. We show that this is a provably optimal mixed precision quantization scheme that minimizes error. With the Llama families of models, we demonstrate that ResQ outperforms recent uniform and mixed precision PTQ methods on a variety of benchmarks, achieving up to 33% lower perplexity on Wikitext than the next best method SpinQuant, and a 2.4x speedup over 16-bit baseline. Code is available at https://github.com/utkarsh-dmx/project-resq.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14363v1 Announce Type: new \nAbstract: Post-training quantization (PTQ) of large language models (LLMs) holds the promise in reducing the prohibitive computational cost at inference time. Quantization of all weight, activation and key-value (KV) cache tensors to 4-bit without significantly degrading generalizability is challenging, due to the high quantization error caused by extreme outliers in activations. To tackle this problem, we propose ResQ, a PTQ method that pushes further the state-of-the-art. By means of principal component analysis (PCA), it identifies a low-rank subspace (in practice 1/8 of the hidden dimension) in which activation variances are highest, and keep the coefficients within this subspace in high precision, e.g. 8-bit, while quantizing the rest to 4-bit. Within each subspace, invariant random rotation is applied to further suppress outliers. We show that this is a provably optimal mixed precision quantization scheme that minimizes error. With the Llama families of models, we demonstrate that ResQ outperforms recent uniform and mixed precision PTQ methods on a variety of benchmarks, achieving up to 33% lower perplexity on Wikitext than the next best method SpinQuant, and a 2.4x speedup over 16-bit baseline. Code is available at https://github.com/utkarsh-dmx/project-resq.'}",oai:arXiv.org:2412.14363v1,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'cs.CL', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Utkarsh Saxena, Sayeh Sharify, Kaushik Roy, Xin Wang'}]","Utkarsh Saxena, Sayeh Sharify, Kaushik Roy, Xin Wang","{'name': 'Utkarsh Saxena, Sayeh Sharify, Kaushik Roy, Xin Wang'}",,
81,Surrealistic-like Image Generation with Vision-Language Models,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Surrealistic-like Image Generation with Vision-Language Models'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14366'}]",https://arxiv.org/abs/2412.14366,"arXiv:2412.14366v1 Announce Type: new 
Abstract: Recent advances in generative AI make it convenient to create different types of content, including text, images, and code. In this paper, we explore the generation of images in the style of paintings in the surrealism movement using vision-language generative models, including DALL-E, Deep Dream Generator, and DreamStudio. Our investigation starts with the generation of images under various image generation settings and different models. The primary objective is to identify the most suitable model and settings for producing such images. Additionally, we aim to understand the impact of using edited base images on the generated resulting images. Through these experiments, we evaluate the performance of selected models and gain valuable insights into their capabilities in generating such images. Our analysis shows that Dall-E 2 performs the best when using the generated prompt by ChatGPT.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14366v1 Announce Type: new \nAbstract: Recent advances in generative AI make it convenient to create different types of content, including text, images, and code. In this paper, we explore the generation of images in the style of paintings in the surrealism movement using vision-language generative models, including DALL-E, Deep Dream Generator, and DreamStudio. Our investigation starts with the generation of images under various image generation settings and different models. The primary objective is to identify the most suitable model and settings for producing such images. Additionally, we aim to understand the impact of using edited base images on the generated resulting images. Through these experiments, we evaluate the performance of selected models and gain valuable insights into their capabilities in generating such images. Our analysis shows that Dall-E 2 performs the best when using the generated prompt by ChatGPT.'}",oai:arXiv.org:2412.14366v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Elif Ayten, Shuai Wang, Hjalmar Snoep'}]","Elif Ayten, Shuai Wang, Hjalmar Snoep","{'name': 'Elif Ayten, Shuai Wang, Hjalmar Snoep'}",,
82,Implementing TD3 to train a Neural Network to fly a Quadcopter through an FPV Gate,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Implementing TD3 to train a Neural Network to fly a Quadcopter through an FPV Gate'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14367'}]",https://arxiv.org/abs/2412.14367,"arXiv:2412.14367v1 Announce Type: new 
Abstract: Deep Reinforcement learning has shown to be a powerful tool for developing policies in environments where an optimal solution is unclear. In this paper, we attempt to apply Twin Delayed Deep Deterministic Policy Gradients to train a neural network to act as a velocity controller for a quadcopter. The quadcopter's objective is to quickly fly through a gate while avoiding crashing into the gate. We transfer our trained policy to the real world by deploying it on a quadcopter in a laboratory environment. Finally, we demonstrate that the trained policy is able to navigate the drone to the gate in the real world.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14367v1 Announce Type: new \nAbstract: Deep Reinforcement learning has shown to be a powerful tool for developing policies in environments where an optimal solution is unclear. In this paper, we attempt to apply Twin Delayed Deep Deterministic Policy Gradients to train a neural network to act as a velocity controller for a quadcopter. The quadcopter's objective is to quickly fly through a gate while avoiding crashing into the gate. We transfer our trained policy to the real world by deploying it on a quadcopter in a laboratory environment. Finally, we demonstrate that the trained policy is able to navigate the drone to the gate in the real world.""}",oai:arXiv.org:2412.14367v1,False,"[{'term': 'cs.RO', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Patrick Thomas, Kevin Schroeder, Jonathan Black'}]","Patrick Thomas, Kevin Schroeder, Jonathan Black","{'name': 'Patrick Thomas, Kevin Schroeder, Jonathan Black'}",,
83,Memorization Over Reasoning? Exposing and Mitigating Verbatim Memorization in Large Language Models' Character Understanding Evaluation,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""Memorization Over Reasoning? Exposing and Mitigating Verbatim Memorization in Large Language Models' Character Understanding Evaluation""}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14368'}]",https://arxiv.org/abs/2412.14368,"arXiv:2412.14368v1 Announce Type: new 
Abstract: Recently, Large Language Models (LLMs) have shown impressive performance in character understanding tasks, such as analyzing the roles, personalities, and relationships of fictional characters. However, the extensive pre-training corpora used by LLMs raise concerns that they may rely on memorizing popular fictional works rather than genuinely understanding and reasoning about them. In this work, we argue that 'gist memory'-capturing essential meaning - should be the primary mechanism for character understanding tasks, as opposed to 'verbatim memory' - exact match of a string. We introduce a simple yet effective method to mitigate mechanized memorization in character understanding evaluations while preserving the essential implicit cues needed for comprehension and reasoning. Our approach reduces memorization-driven performance on popular fictional works from 96% accuracy to 72% and results in up to an 18% drop in accuracy across various character understanding tasks. These findings underscore the issue of data contamination in existing benchmarks, which often measure memorization rather than true character understanding.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14368v1 Announce Type: new \nAbstract: Recently, Large Language Models (LLMs) have shown impressive performance in character understanding tasks, such as analyzing the roles, personalities, and relationships of fictional characters. However, the extensive pre-training corpora used by LLMs raise concerns that they may rely on memorizing popular fictional works rather than genuinely understanding and reasoning about them. In this work, we argue that 'gist memory'-capturing essential meaning - should be the primary mechanism for character understanding tasks, as opposed to 'verbatim memory' - exact match of a string. We introduce a simple yet effective method to mitigate mechanized memorization in character understanding evaluations while preserving the essential implicit cues needed for comprehension and reasoning. Our approach reduces memorization-driven performance on popular fictional works from 96% accuracy to 72% and results in up to an 18% drop in accuracy across various character understanding tasks. These findings underscore the issue of data contamination in existing benchmarks, which often measure memorization rather than true character understanding.""}",oai:arXiv.org:2412.14368v1,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Yuxuan Jiang, Francis Ferraro'}]","Yuxuan Jiang, Francis Ferraro","{'name': 'Yuxuan Jiang, Francis Ferraro'}",,
84,SEREP: Semantic Facial Expression Representation for Robust In-the-Wild Capture and Retargeting,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'SEREP: Semantic Facial Expression Representation for Robust In-the-Wild Capture and Retargeting'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14371'}]",https://arxiv.org/abs/2412.14371,"arXiv:2412.14371v1 Announce Type: new 
Abstract: Monocular facial performance capture in-the-wild is challenging due to varied capture conditions, face shapes, and expressions. Most current methods rely on linear 3D Morphable Models, which represent facial expressions independently of identity at the vertex displacement level. We propose SEREP (Semantic Expression Representation), a model that disentangles expression from identity at the semantic level. It first learns an expression representation from unpaired 3D facial expressions using a cycle consistency loss. Then we train a model to predict expression from monocular images using a novel semi-supervised scheme that relies on domain adaptation. In addition, we introduce MultiREX, a benchmark addressing the lack of evaluation resources for the expression capture task. Our experiments show that SEREP outperforms state-of-the-art methods, capturing challenging expressions and transferring them to novel identities.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14371v1 Announce Type: new \nAbstract: Monocular facial performance capture in-the-wild is challenging due to varied capture conditions, face shapes, and expressions. Most current methods rely on linear 3D Morphable Models, which represent facial expressions independently of identity at the vertex displacement level. We propose SEREP (Semantic Expression Representation), a model that disentangles expression from identity at the semantic level. It first learns an expression representation from unpaired 3D facial expressions using a cycle consistency loss. Then we train a model to predict expression from monocular images using a novel semi-supervised scheme that relies on domain adaptation. In addition, we introduce MultiREX, a benchmark addressing the lack of evaluation resources for the expression capture task. Our experiments show that SEREP outperforms state-of-the-art methods, capturing challenging expressions and transferring them to novel identities.'}",oai:arXiv.org:2412.14371v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.GR', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Arthur Josi, Luiz Gustavo Hafemann, Abdallah Dib, Emeline Got, Rafael M. O. Cruz, Marc-Andre Carbonneau'}]","Arthur Josi, Luiz Gustavo Hafemann, Abdallah Dib, Emeline Got, Rafael M. O. Cruz, Marc-Andre Carbonneau","{'name': 'Arthur Josi, Luiz Gustavo Hafemann, Abdallah Dib, Emeline Got, Rafael M. O. Cruz, Marc-Andre Carbonneau'}",,
85,Python Agent in Ludii,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Python Agent in Ludii'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14372'}]",https://arxiv.org/abs/2412.14372,"arXiv:2412.14372v1 Announce Type: new 
Abstract: Ludii is a Java general game system with a considerable number of board games, with an API for developing new agents and a game description language to create new games. To improve versatility and ease development, we provide Python interfaces for agent programming. This allows the use of Python modules to implement general game playing agents.
  As a means of enabling Python for creating Ludii agents, the interfaces are implemented using different Java libraries: jpy and Py4J. The main goal of this work is to determine which version is faster. To do so, we conducted a performance analysis of two different GGP algorithms, Minimax adapted to GGP and MCTS. The analysis was performed across several combinatorial games with varying depth, branching factor, and ply time. For reproducibility, we provide tutorials and repositories.
  Our analysis includes predictive models using regression, which suggest that jpy is faster than Py4J, however slower than a native Java Ludii agent, as expected.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14372v1 Announce Type: new \nAbstract: Ludii is a Java general game system with a considerable number of board games, with an API for developing new agents and a game description language to create new games. To improve versatility and ease development, we provide Python interfaces for agent programming. This allows the use of Python modules to implement general game playing agents.\n  As a means of enabling Python for creating Ludii agents, the interfaces are implemented using different Java libraries: jpy and Py4J. The main goal of this work is to determine which version is faster. To do so, we conducted a performance analysis of two different GGP algorithms, Minimax adapted to GGP and MCTS. The analysis was performed across several combinatorial games with varying depth, branching factor, and ply time. For reproducibility, we provide tutorials and repositories.\n  Our analysis includes predictive models using regression, which suggest that jpy is faster than Py4J, however slower than a native Java Ludii agent, as expected.'}",oai:arXiv.org:2412.14372v1,False,"[{'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': ""Izaias S. de Lima Neto (Instituto de Inform\\'atica, Universidade Federal do Rio Grande do Sul), Marco A. A. de Aguiar Vieira (Instituto de Inform\\'atica, Universidade Federal do Rio Grande do Sul), Anderson R. Tavares (Instituto de Inform\\'atica, Universidade Federal do Rio Grande do Sul)""}]","Izaias S. de Lima Neto (Instituto de Inform\'atica, Universidade Federal do Rio Grande do Sul), Marco A. A. de Aguiar Vieira (Instituto de Inform\'atica, Universidade Federal do Rio Grande do Sul), Anderson R. Tavares (Instituto de Inform\'atica, Universidade Federal do Rio Grande do Sul)","{'name': ""Izaias S. de Lima Neto (Instituto de Inform\\'atica, Universidade Federal do Rio Grande do Sul), Marco A. A. de Aguiar Vieira (Instituto de Inform\\'atica, Universidade Federal do Rio Grande do Sul), Anderson R. Tavares (Instituto de Inform\\'atica, Universidade Federal do Rio Grande do Sul)""}",,
86,ECG-Byte: A Tokenizer for End-to-End Generative Electrocardiogram Language Modeling,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'ECG-Byte: A Tokenizer for End-to-End Generative Electrocardiogram Language Modeling'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14373'}]",https://arxiv.org/abs/2412.14373,"arXiv:2412.14373v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have shown remarkable adaptability across domains beyond text, specifically electrocardiograms (ECGs). More specifically, there is a growing body of work exploring the task of generating text from a multi-channeled ECG and corresponding textual prompt. Current approaches typically involve pretraining an ECG-specific encoder with a self-supervised learning (SSL) objective and using the features output by the pretrained encoder to finetune a LLM for natural language generation (NLG). However, these methods are limited by 1) inefficiency from two-stage training and 2) interpretability challenges with encoder-generated features. To address these limitations, we introduce ECG-Byte, an adapted byte pair encoding (BPE) tokenizer pipeline for autoregressive language modeling of ECGs. This approach compresses and encodes ECG signals into tokens, enabling end-to-end LLM training by combining ECG and text tokens directly, while being much more interpretable since the ECG tokens can be directly mapped back to the original signal. Using ECG-Byte, we achieve competitive performance in NLG tasks in only half the time and ~48% of the data required by two-stage approaches.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14373v1 Announce Type: new \nAbstract: Large Language Models (LLMs) have shown remarkable adaptability across domains beyond text, specifically electrocardiograms (ECGs). More specifically, there is a growing body of work exploring the task of generating text from a multi-channeled ECG and corresponding textual prompt. Current approaches typically involve pretraining an ECG-specific encoder with a self-supervised learning (SSL) objective and using the features output by the pretrained encoder to finetune a LLM for natural language generation (NLG). However, these methods are limited by 1) inefficiency from two-stage training and 2) interpretability challenges with encoder-generated features. To address these limitations, we introduce ECG-Byte, an adapted byte pair encoding (BPE) tokenizer pipeline for autoregressive language modeling of ECGs. This approach compresses and encodes ECG signals into tokens, enabling end-to-end LLM training by combining ECG and text tokens directly, while being much more interpretable since the ECG tokens can be directly mapped back to the original signal. Using ECG-Byte, we achieve competitive performance in NLG tasks in only half the time and ~48% of the data required by two-stage approaches.'}",oai:arXiv.org:2412.14373v1,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}, {'term': 'eess.SP', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'William Han, Chaojing Duan, Michael A. Rosenberg, Emerson Liu, Ding Zhao'}]","William Han, Chaojing Duan, Michael A. Rosenberg, Emerson Liu, Ding Zhao","{'name': 'William Han, Chaojing Duan, Michael A. Rosenberg, Emerson Liu, Ding Zhao'}",,
87,Scaling Deep Learning Training with MPMD Pipeline Parallelism,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Scaling Deep Learning Training with MPMD Pipeline Parallelism'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14374'}]",https://arxiv.org/abs/2412.14374,"arXiv:2412.14374v1 Announce Type: new 
Abstract: We present JaxPP, a system for efficiently scaling the training of large deep learning models with flexible pipeline parallelism. We introduce a seamless programming model that allows implementing user-defined pipeline schedules for gradient accumulation. JaxPP automatically distributes tasks, corresponding to pipeline stages, over a cluster of nodes and automatically infers the communication among them. We implement a MPMD runtime for asynchronous execution of SPMD tasks. The pipeline parallelism implementation of JaxPP improves hardware utilization by up to $1.11\times$ with respect to the best performing SPMD configuration.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14374v1 Announce Type: new \nAbstract: We present JaxPP, a system for efficiently scaling the training of large deep learning models with flexible pipeline parallelism. We introduce a seamless programming model that allows implementing user-defined pipeline schedules for gradient accumulation. JaxPP automatically distributes tasks, corresponding to pipeline stages, over a cluster of nodes and automatically infers the communication among them. We implement a MPMD runtime for asynchronous execution of SPMD tasks. The pipeline parallelism implementation of JaxPP improves hardware utilization by up to $1.11\\times$ with respect to the best performing SPMD configuration.'}",oai:arXiv.org:2412.14374v1,False,"[{'term': 'cs.DC', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'cs.PL', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Anxhelo Xhebraj, Sean Lee, Hanfeng Chen, Vinod Grover'}]","Anxhelo Xhebraj, Sean Lee, Hanfeng Chen, Vinod Grover","{'name': 'Anxhelo Xhebraj, Sean Lee, Hanfeng Chen, Vinod Grover'}",,
88,Network Modelling in Analysing Cyber-related Graphs,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Network Modelling in Analysing Cyber-related Graphs'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14375'}]",https://arxiv.org/abs/2412.14375,"arXiv:2412.14375v1 Announce Type: new 
Abstract: In order to improve the resilience of computer infrastructure against cyber attacks and finding ways to mitigate their impact we need to understand their structure and dynamics. Here we propose a novel network-based influence spreading model to investigate event trajectories or paths in various types of attack and causal graphs, which can be directed, weighted, and / or cyclic. In case of attack graphs with acyclic paths, only self-avoiding attack chains are allowed. In the framework of our model a detailed probabilistic analysis beyond the traditional visualisation of attack graphs, based on vulnerabilities, services, and exploitabilities, can be performed. In order to demonstrate the capabilities of the model, we present three use cases with cyber-related graphs, namely two attack graphs and a causal graph. The model can be of benefit to cyber analysts in generating quantitative metrics for prioritisation, summaries, or analysis of larger graphs.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14375v1 Announce Type: new \nAbstract: In order to improve the resilience of computer infrastructure against cyber attacks and finding ways to mitigate their impact we need to understand their structure and dynamics. Here we propose a novel network-based influence spreading model to investigate event trajectories or paths in various types of attack and causal graphs, which can be directed, weighted, and / or cyclic. In case of attack graphs with acyclic paths, only self-avoiding attack chains are allowed. In the framework of our model a detailed probabilistic analysis beyond the traditional visualisation of attack graphs, based on vulnerabilities, services, and exploitabilities, can be performed. In order to demonstrate the capabilities of the model, we present three use cases with cyber-related graphs, namely two attack graphs and a causal graph. The model can be of benefit to cyber analysts in generating quantitative metrics for prioritisation, summaries, or analysis of larger graphs.'}",oai:arXiv.org:2412.14375v1,False,"[{'term': 'cs.SI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Vesa Kuikka, Lauri Pyk\\""al\\""a, Tuomas Takko, Kimmo Kaski'}]","Vesa Kuikka, Lauri Pyk\""al\""a, Tuomas Takko, Kimmo Kaski","{'name': 'Vesa Kuikka, Lauri Pyk\\""al\\""a, Tuomas Takko, Kimmo Kaski'}",,
89,HA-RDet: Hybrid Anchor Rotation Detector for Oriented Object Detection,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'HA-RDet: Hybrid Anchor Rotation Detector for Oriented Object Detection'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14379'}]",https://arxiv.org/abs/2412.14379,"arXiv:2412.14379v1 Announce Type: new 
Abstract: Oriented object detection in aerial images poses a significant challenge due to their varying sizes and orientations. Current state-of-the-art detectors typically rely on either two-stage or one-stage approaches, often employing Anchor-based strategies, which can result in computationally expensive operations due to the redundant number of generated anchors during training. In contrast, Anchor-free mechanisms offer faster processing but suffer from a reduction in the number of training samples, potentially impacting detection accuracy. To address these limitations, we propose the Hybrid-Anchor Rotation Detector (HA-RDet), which combines the advantages of both anchor-based and anchor-free schemes for oriented object detection. By utilizing only one preset anchor for each location on the feature maps and refining these anchors with our Orientation-Aware Convolution technique, HA-RDet achieves competitive accuracies, including 75.41 mAP on DOTA-v1, 65.3 mAP on DIOR-R, and 90.2 mAP on HRSC2016, against current anchor-based state-of-the-art methods, while significantly reducing computational resources.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14379v1 Announce Type: new \nAbstract: Oriented object detection in aerial images poses a significant challenge due to their varying sizes and orientations. Current state-of-the-art detectors typically rely on either two-stage or one-stage approaches, often employing Anchor-based strategies, which can result in computationally expensive operations due to the redundant number of generated anchors during training. In contrast, Anchor-free mechanisms offer faster processing but suffer from a reduction in the number of training samples, potentially impacting detection accuracy. To address these limitations, we propose the Hybrid-Anchor Rotation Detector (HA-RDet), which combines the advantages of both anchor-based and anchor-free schemes for oriented object detection. By utilizing only one preset anchor for each location on the feature maps and refining these anchors with our Orientation-Aware Convolution technique, HA-RDet achieves competitive accuracies, including 75.41 mAP on DOTA-v1, 65.3 mAP on DIOR-R, and 90.2 mAP on HRSC2016, against current anchor-based state-of-the-art methods, while significantly reducing computational resources.'}",oai:arXiv.org:2412.14379v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}",[{'name': 'Phuc D. A. Nguyen'}],Phuc D. A. Nguyen,{'name': 'Phuc D. A. Nguyen'},,
90,Differentially Private Multi-objective Selection: Pareto and Aggregation Approaches,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Differentially Private Multi-objective Selection: Pareto and Aggregation Approaches'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14380'}]",https://arxiv.org/abs/2412.14380,"arXiv:2412.14380v1 Announce Type: new 
Abstract: Differentially private selection mechanisms are fundamental building blocks for privacy-preserving data analysis. While numerous mechanisms exist for single-objective selection, many real-world applications require optimizing multiple competing objectives simultaneously. We present two novel mechanisms for differentially private multi-objective selection: PrivPareto and PrivAgg. PrivPareto uses a novel Pareto score to identify solutions near the Pareto frontier, while PrivAgg enables privacy-preserving weighted aggregation of multiple objectives. Both mechanisms support global and local sensitivity approaches, with comprehensive theoretical analysis showing how to compose sensitivities of multiple utility functions. We demonstrate the practical applicability through two real-world applications: cost-sensitive decision tree construction and multi-objective influential node selection in social networks. The experimental results showed that our local sensitivity-based approaches achieve significantly better utility compared to global sensitivity approaches across both applications and both Pareto and Aggregation approaches. Moreover, the local sensitivity-based approaches are able to perform well with typical privacy budget values $\epsilon \in [0.01, 1]$ in most experiments.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14380v1 Announce Type: new \nAbstract: Differentially private selection mechanisms are fundamental building blocks for privacy-preserving data analysis. While numerous mechanisms exist for single-objective selection, many real-world applications require optimizing multiple competing objectives simultaneously. We present two novel mechanisms for differentially private multi-objective selection: PrivPareto and PrivAgg. PrivPareto uses a novel Pareto score to identify solutions near the Pareto frontier, while PrivAgg enables privacy-preserving weighted aggregation of multiple objectives. Both mechanisms support global and local sensitivity approaches, with comprehensive theoretical analysis showing how to compose sensitivities of multiple utility functions. We demonstrate the practical applicability through two real-world applications: cost-sensitive decision tree construction and multi-objective influential node selection in social networks. The experimental results showed that our local sensitivity-based approaches achieve significantly better utility compared to global sensitivity approaches across both applications and both Pareto and Aggregation approaches. Moreover, the local sensitivity-based approaches are able to perform well with typical privacy budget values $\\epsilon \\in [0.01, 1]$ in most experiments.'}",oai:arXiv.org:2412.14380v1,False,"[{'term': 'cs.CR', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Victor A. E. Farias, Felipe T. Brito, Cheryl Flynn, Javam C. Machado, Divesh Srivastava'}]","Victor A. E. Farias, Felipe T. Brito, Cheryl Flynn, Javam C. Machado, Divesh Srivastava","{'name': 'Victor A. E. Farias, Felipe T. Brito, Cheryl Flynn, Javam C. Machado, Divesh Srivastava'}",,
91,Balans: Multi-Armed Bandits-based Adaptive Large Neighborhood Search for Mixed-Integer Programming Problem,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Balans: Multi-Armed Bandits-based Adaptive Large Neighborhood Search for Mixed-Integer Programming Problem'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14382'}]",https://arxiv.org/abs/2412.14382,"arXiv:2412.14382v1 Announce Type: new 
Abstract: Mixed-Integer Programming (MIP) is a powerful paradigm for modeling and solving various important combinatorial optimization problems. Recently, learning-based approaches have shown potential to speed up MIP solving via offline training that then guides important design decisions during search. However, a significant drawback of these methods is their heavy reliance on offline training, which requires collecting training datasets and computationally costly training epochs yet offering only limited generalization to unseen (larger) instances. In this paper, we propose Balans, an adaptive meta-solver for MIPs with online learning capability that does not require any supervision or apriori training. At its core, Balans is based on adaptive large-neighborhood search, operating on top of a MIP solver by successive applications of destroy and repair neighborhood operators. During the search, the selection among different neighborhood definitions is guided on the fly for the instance at hand via multi-armed bandit algorithms. Our extensive experiments on hard optimization instances show that Balans offers significant performance gains over the default MIP solver, is better than committing to any single best neighborhood, and improves over the state-of-the-art large-neighborhood search for MIPs. Finally, we release Balans as a highly configurable, MIP solver agnostic, open-source software.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14382v1 Announce Type: new \nAbstract: Mixed-Integer Programming (MIP) is a powerful paradigm for modeling and solving various important combinatorial optimization problems. Recently, learning-based approaches have shown potential to speed up MIP solving via offline training that then guides important design decisions during search. However, a significant drawback of these methods is their heavy reliance on offline training, which requires collecting training datasets and computationally costly training epochs yet offering only limited generalization to unseen (larger) instances. In this paper, we propose Balans, an adaptive meta-solver for MIPs with online learning capability that does not require any supervision or apriori training. At its core, Balans is based on adaptive large-neighborhood search, operating on top of a MIP solver by successive applications of destroy and repair neighborhood operators. During the search, the selection among different neighborhood definitions is guided on the fly for the instance at hand via multi-armed bandit algorithms. Our extensive experiments on hard optimization instances show that Balans offers significant performance gains over the default MIP solver, is better than committing to any single best neighborhood, and improves over the state-of-the-art large-neighborhood search for MIPs. Finally, we release Balans as a highly configurable, MIP solver agnostic, open-source software.'}",oai:arXiv.org:2412.14382v1,False,"[{'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'math.OC', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Junyang Cai, Serdar Kadioglu, Bistra Dilkina'}]","Junyang Cai, Serdar Kadioglu, Bistra Dilkina","{'name': 'Junyang Cai, Serdar Kadioglu, Bistra Dilkina'}",,
92,I0T: Embedding Standardization Method Towards Zero Modality Gap,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'I0T: Embedding Standardization Method Towards Zero Modality Gap'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14384'}]",https://arxiv.org/abs/2412.14384,"arXiv:2412.14384v1 Announce Type: new 
Abstract: Contrastive Language-Image Pretraining (CLIP) enables zero-shot inference in downstream tasks such as image-text retrieval and classification. However, recent works extending CLIP suffer from the issue of modality gap, which arises when the image and text embeddings are projected to disparate manifolds, deviating from the intended objective of image-text contrastive learning. We discover that this phenomenon is linked to the modality-specific characteristic that each image/text encoder independently possesses and propose two methods to address the modality gap: (1) a post-hoc embedding standardization method, $\text{I0T}_{\text{post}}$ that reduces the modality gap approximately to zero and (2) a trainable method, $\text{I0T}_{\text{async}}$, to alleviate the modality gap problem by adding two normalization layers for each encoder. Our I0T framework can significantly reduce the modality gap while preserving the original embedding representations of trained models with their locked parameters. In practice, $\text{I0T}_{\text{post}}$ can serve as an alternative explainable automatic evaluation metric of widely used CLIPScore (CLIP-S).","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14384v1 Announce Type: new \nAbstract: Contrastive Language-Image Pretraining (CLIP) enables zero-shot inference in downstream tasks such as image-text retrieval and classification. However, recent works extending CLIP suffer from the issue of modality gap, which arises when the image and text embeddings are projected to disparate manifolds, deviating from the intended objective of image-text contrastive learning. We discover that this phenomenon is linked to the modality-specific characteristic that each image/text encoder independently possesses and propose two methods to address the modality gap: (1) a post-hoc embedding standardization method, $\\text{I0T}_{\\text{post}}$ that reduces the modality gap approximately to zero and (2) a trainable method, $\\text{I0T}_{\\text{async}}$, to alleviate the modality gap problem by adding two normalization layers for each encoder. Our I0T framework can significantly reduce the modality gap while preserving the original embedding representations of trained models with their locked parameters. In practice, $\\text{I0T}_{\\text{post}}$ can serve as an alternative explainable automatic evaluation metric of widely used CLIPScore (CLIP-S).'}",oai:arXiv.org:2412.14384v1,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by-nc-nd/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-nd/4.0/'}","[{'name': 'Na Min An, Eunki Kim, James Thorne, Hyunjung Shim'}]","Na Min An, Eunki Kim, James Thorne, Hyunjung Shim","{'name': 'Na Min An, Eunki Kim, James Thorne, Hyunjung Shim'}",,
93,Clinical Trials Ontology Engineering with Large Language Models,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Clinical Trials Ontology Engineering with Large Language Models'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14387'}]",https://arxiv.org/abs/2412.14387,"arXiv:2412.14387v1 Announce Type: new 
Abstract: Managing clinical trial information is currently a significant challenge for the medical industry, as traditional methods are both time-consuming and costly. This paper proposes a simple yet effective methodology to extract and integrate clinical trial data in a cost-effective and time-efficient manner. Allowing the medical industry to stay up-to-date with medical developments. Comparing time, cost, and quality of the ontologies created by humans, GPT3.5, GPT4, and Llama3 (8b & 70b). Findings suggest that large language models (LLM) are a viable option to automate this process both from a cost and time perspective. This study underscores significant implications for medical research where real-time data integration from clinical trials could become the norm.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14387v1 Announce Type: new \nAbstract: Managing clinical trial information is currently a significant challenge for the medical industry, as traditional methods are both time-consuming and costly. This paper proposes a simple yet effective methodology to extract and integrate clinical trial data in a cost-effective and time-efficient manner. Allowing the medical industry to stay up-to-date with medical developments. Comparing time, cost, and quality of the ontologies created by humans, GPT3.5, GPT4, and Llama3 (8b & 70b). Findings suggest that large language models (LLM) are a viable option to automate this process both from a cost and time perspective. This study underscores significant implications for medical research where real-time data integration from clinical trials could become the norm.'}",oai:arXiv.org:2412.14387v1,False,"[{'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by-nc-sa/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-sa/4.0/'}",[{'name': 'Berkan \\c{C}ak{\\i}r'}],Berkan \c{C}ak{\i}r,{'name': 'Berkan \\c{C}ak{\\i}r'},,
94,Nemesis: Noise-randomized Encryption with Modular Efficiency and Secure Integration in Machine Learning Systems,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Nemesis: Noise-randomized Encryption with Modular Efficiency and Secure Integration in Machine Learning Systems'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14392'}]",https://arxiv.org/abs/2412.14392,"arXiv:2412.14392v1 Announce Type: new 
Abstract: Machine learning (ML) systems that guarantee security and privacy often rely on Fully Homomorphic Encryption (FHE) as a cornerstone technique, enabling computations on encrypted data without exposing sensitive information. However, a critical limitation of FHE is its computational inefficiency, making it impractical for large-scale applications. In this work, we propose \textit{Nemesis}, a framework that accelerates FHE-based systems without compromising accuracy or security. The design of Nemesis is inspired by Rache (SIGMOD'23), which introduced a caching mechanism for encrypted integers and scalars. Nemesis extends this idea with more advanced caching techniques and mathematical tools, enabling efficient operations over multi-slot FHE schemes and overcoming Rache's limitations to support general plaintext structures. We formally prove the security of Nemesis under standard cryptographic assumptions and evaluate its performance extensively on widely used datasets, including MNIST, FashionMNIST, and CIFAR-10. Experimental results show that Nemesis significantly reduces the computational overhead of FHE-based ML systems, paving the way for broader adoption of privacy-preserving technologies.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14392v1 Announce Type: new \nAbstract: Machine learning (ML) systems that guarantee security and privacy often rely on Fully Homomorphic Encryption (FHE) as a cornerstone technique, enabling computations on encrypted data without exposing sensitive information. However, a critical limitation of FHE is its computational inefficiency, making it impractical for large-scale applications. In this work, we propose \\textit{Nemesis}, a framework that accelerates FHE-based systems without compromising accuracy or security. The design of Nemesis is inspired by Rache (SIGMOD'23), which introduced a caching mechanism for encrypted integers and scalars. Nemesis extends this idea with more advanced caching techniques and mathematical tools, enabling efficient operations over multi-slot FHE schemes and overcoming Rache's limitations to support general plaintext structures. We formally prove the security of Nemesis under standard cryptographic assumptions and evaluate its performance extensively on widely used datasets, including MNIST, FashionMNIST, and CIFAR-10. Experimental results show that Nemesis significantly reduces the computational overhead of FHE-based ML systems, paving the way for broader adoption of privacy-preserving technologies.""}",oai:arXiv.org:2412.14392v1,False,"[{'term': 'cs.CR', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by-nc-nd/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-nd/4.0/'}",[{'name': 'Dongfang Zhao'}],Dongfang Zhao,{'name': 'Dongfang Zhao'},,
95,Fingerprinting Codes Meet Geometry: Improved Lower Bounds for Private Query Release and Adaptive Data Analysis,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Fingerprinting Codes Meet Geometry: Improved Lower Bounds for Private Query Release and Adaptive Data Analysis'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14396'}]",https://arxiv.org/abs/2412.14396,"arXiv:2412.14396v1 Announce Type: new 
Abstract: Fingerprinting codes are a crucial tool for proving lower bounds in differential privacy. They have been used to prove tight lower bounds for several fundamental questions, especially in the ``low accuracy'' regime. Unlike reconstruction/discrepancy approaches however, they are more suited for query sets that arise naturally from the fingerprinting codes construction. In this work, we propose a general framework for proving fingerprinting type lower bounds, that allows us to tailor the technique to the geometry of the query set. Our approach allows us to prove several new results, including the following.
  First, we show that any (sample- and population-)accurate algorithm for answering $Q$ arbitrary adaptive counting queries over a universe $\mathcal{X}$ to accuracy $\alpha$ needs $\Omega(\frac{\sqrt{\log |\mathcal{X}|}\cdot \log Q}{\alpha^3})$ samples, matching known upper bounds. This shows that the approaches based on differential privacy are optimal for this question, and improves significantly on the previously known lower bounds of $\frac{\log Q}{\alpha^2}$ and $\min(\sqrt{Q}, \sqrt{\log |\mathcal{X}|})/\alpha^2$.
  Second, we show that any $(\varepsilon,\delta)$-DP algorithm for answering $Q$ counting queries to accuracy $\alpha$ needs $\Omega(\frac{\sqrt{ \log|\mathcal{X}| \log(1/\delta)} \log Q}{\varepsilon\alpha^2})$ samples, matching known upper bounds up to constants. Our framework allows for proving this bound via a direct correlation analysis and improves the prior bound of [BUV'14] by $\sqrt{\log(1/\delta)}$.
  Third, we characterize the sample complexity of answering a set of random $0$-$1$ queries under approximate differential privacy. We give new upper and lower bounds in different regimes. By combining them with known results, we can complete the whole picture.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14396v1 Announce Type: new \nAbstract: Fingerprinting codes are a crucial tool for proving lower bounds in differential privacy. They have been used to prove tight lower bounds for several fundamental questions, especially in the ``low accuracy'' regime. Unlike reconstruction/discrepancy approaches however, they are more suited for query sets that arise naturally from the fingerprinting codes construction. In this work, we propose a general framework for proving fingerprinting type lower bounds, that allows us to tailor the technique to the geometry of the query set. Our approach allows us to prove several new results, including the following.\n  First, we show that any (sample- and population-)accurate algorithm for answering $Q$ arbitrary adaptive counting queries over a universe $\\mathcal{X}$ to accuracy $\\alpha$ needs $\\Omega(\\frac{\\sqrt{\\log |\\mathcal{X}|}\\cdot \\log Q}{\\alpha^3})$ samples, matching known upper bounds. This shows that the approaches based on differential privacy are optimal for this question, and improves significantly on the previously known lower bounds of $\\frac{\\log Q}{\\alpha^2}$ and $\\min(\\sqrt{Q}, \\sqrt{\\log |\\mathcal{X}|})/\\alpha^2$.\n  Second, we show that any $(\\varepsilon,\\delta)$-DP algorithm for answering $Q$ counting queries to accuracy $\\alpha$ needs $\\Omega(\\frac{\\sqrt{ \\log|\\mathcal{X}| \\log(1/\\delta)} \\log Q}{\\varepsilon\\alpha^2})$ samples, matching known upper bounds up to constants. Our framework allows for proving this bound via a direct correlation analysis and improves the prior bound of [BUV'14] by $\\sqrt{\\log(1/\\delta)}$.\n  Third, we characterize the sample complexity of answering a set of random $0$-$1$ queries under approximate differential privacy. We give new upper and lower bounds in different regimes. By combining them with known results, we can complete the whole picture.""}",oai:arXiv.org:2412.14396v1,False,"[{'term': 'cs.DS', 'scheme': None, 'label': None}, {'term': 'cs.CR', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Xin Lyu, Kunal Talwar'}]","Xin Lyu, Kunal Talwar","{'name': 'Xin Lyu, Kunal Talwar'}",,
96,LLMSA: A Compositional Neuro-Symbolic Approach to Compilation-free and Customizable Static Analysis,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'LLMSA: A Compositional Neuro-Symbolic Approach to Compilation-free and Customizable Static Analysis'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14399'}]",https://arxiv.org/abs/2412.14399,"arXiv:2412.14399v1 Announce Type: new 
Abstract: Static analysis is essential for program optimization, bug detection, and debugging, but its reliance on compilation and limited customization hampers practical use. Advances in LLMs enable a new paradigm of compilation-free, customizable analysis via prompting. LLMs excel in interpreting program semantics on small code snippets and allow users to define analysis tasks in natural language with few-shot examples. However, misalignment with program semantics can cause hallucinations, especially in sophisticated semantic analysis upon lengthy code snippets.
  We propose LLMSA, a compositional neuro-symbolic approach for compilation-free, customizable static analysis with reduced hallucinations. Specifically, we propose an analysis policy language to support users decomposing an analysis problem into several sub-problems that target simple syntactic or semantic properties upon smaller code snippets. The problem decomposition enables the LLMs to target more manageable semantic-related sub-problems, while the syntactic ones are resolved by parsing-based analysis without hallucinations. An analysis policy is evaluated with lazy, incremental, and parallel prompting, which mitigates the hallucinations and improves the performance. It is shown that LLMSA achieves comparable and even superior performance to existing techniques in various clients. For instance, it attains 66.27% precision and 78.57% recall in taint vulnerability detection, surpassing an industrial approach in F1 score by 0.20.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14399v1 Announce Type: new \nAbstract: Static analysis is essential for program optimization, bug detection, and debugging, but its reliance on compilation and limited customization hampers practical use. Advances in LLMs enable a new paradigm of compilation-free, customizable analysis via prompting. LLMs excel in interpreting program semantics on small code snippets and allow users to define analysis tasks in natural language with few-shot examples. However, misalignment with program semantics can cause hallucinations, especially in sophisticated semantic analysis upon lengthy code snippets.\n  We propose LLMSA, a compositional neuro-symbolic approach for compilation-free, customizable static analysis with reduced hallucinations. Specifically, we propose an analysis policy language to support users decomposing an analysis problem into several sub-problems that target simple syntactic or semantic properties upon smaller code snippets. The problem decomposition enables the LLMs to target more manageable semantic-related sub-problems, while the syntactic ones are resolved by parsing-based analysis without hallucinations. An analysis policy is evaluated with lazy, incremental, and parallel prompting, which mitigates the hallucinations and improves the performance. It is shown that LLMSA achieves comparable and even superior performance to existing techniques in various clients. For instance, it attains 66.27% precision and 78.57% recall in taint vulnerability detection, surpassing an industrial approach in F1 score by 0.20.'}",oai:arXiv.org:2412.14399v1,False,"[{'term': 'cs.PL', 'scheme': None, 'label': None}, {'term': 'cs.SE', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Chengpeng Wang, Yifei Gao, Wuqi Zhang, Xuwei Liu, Qingkai Shi, Xiangyu Zhang'}]","Chengpeng Wang, Yifei Gao, Wuqi Zhang, Xuwei Liu, Qingkai Shi, Xiangyu Zhang","{'name': 'Chengpeng Wang, Yifei Gao, Wuqi Zhang, Xuwei Liu, Qingkai Shi, Xiangyu Zhang'}",,
97,The One RING: a Robotic Indoor Navigation Generalist,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'The One RING: a Robotic Indoor Navigation Generalist'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14401'}]",https://arxiv.org/abs/2412.14401,"arXiv:2412.14401v1 Announce Type: new 
Abstract: Modern robots vary significantly in shape, size, and sensor configurations used to perceive and interact with their environments. However, most navigation policies are embodiment-specific; a policy learned using one robot's configuration does not typically gracefully generalize to another. Even small changes in the body size or camera viewpoint may cause failures. With the recent surge in custom hardware developments, it is necessary to learn a single policy that can be transferred to other embodiments, eliminating the need to (re)train for each specific robot. In this paper, we introduce RING (Robotic Indoor Navigation Generalist), an embodiment-agnostic policy, trained solely in simulation with diverse randomly initialized embodiments at scale. Specifically, we augment the AI2-THOR simulator with the ability to instantiate robot embodiments with controllable configurations, varying across body size, rotation pivot point, and camera configurations. In the visual object-goal navigation task, RING achieves robust performance on real unseen robot platforms (Stretch RE-1, LoCoBot, Unitree's Go1), achieving an average of 72.1% and 78.9% success rate across 5 embodiments in simulation and 4 robot platforms in the real world. (project website: https://one-ring-policy.allen.ai/)","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14401v1 Announce Type: new \nAbstract: Modern robots vary significantly in shape, size, and sensor configurations used to perceive and interact with their environments. However, most navigation policies are embodiment-specific; a policy learned using one robot's configuration does not typically gracefully generalize to another. Even small changes in the body size or camera viewpoint may cause failures. With the recent surge in custom hardware developments, it is necessary to learn a single policy that can be transferred to other embodiments, eliminating the need to (re)train for each specific robot. In this paper, we introduce RING (Robotic Indoor Navigation Generalist), an embodiment-agnostic policy, trained solely in simulation with diverse randomly initialized embodiments at scale. Specifically, we augment the AI2-THOR simulator with the ability to instantiate robot embodiments with controllable configurations, varying across body size, rotation pivot point, and camera configurations. In the visual object-goal navigation task, RING achieves robust performance on real unseen robot platforms (Stretch RE-1, LoCoBot, Unitree's Go1), achieving an average of 72.1% and 78.9% success rate across 5 embodiments in simulation and 4 robot platforms in the real world. (project website: https://one-ring-policy.allen.ai/)""}",oai:arXiv.org:2412.14401v1,False,"[{'term': 'cs.RO', 'scheme': None, 'label': None}, {'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Ainaz Eftekhar, Luca Weihs, Rose Hendrix, Ege Caglar, Jordi Salvador, Alvaro Herrasti, Winson Han, Eli VanderBil, Aniruddha Kembhavi, Ali Farhadi, Ranjay Krishna, Kiana Ehsani, Kuo-Hao Zeng'}]","Ainaz Eftekhar, Luca Weihs, Rose Hendrix, Ege Caglar, Jordi Salvador, Alvaro Herrasti, Winson Han, Eli VanderBil, Aniruddha Kembhavi, Ali Farhadi, Ranjay Krishna, Kiana Ehsani, Kuo-Hao Zeng","{'name': 'Ainaz Eftekhar, Luca Weihs, Rose Hendrix, Ege Caglar, Jordi Salvador, Alvaro Herrasti, Winson Han, Eli VanderBil, Aniruddha Kembhavi, Ali Farhadi, Ranjay Krishna, Kiana Ehsani, Kuo-Hao Zeng'}",,
98,Enhancing Fingerprint Recognition Systems: Comparative Analysis of Biometric Authentication Algorithms and Techniques for Improved Accuracy and Reliability,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Enhancing Fingerprint Recognition Systems: Comparative Analysis of Biometric Authentication Algorithms and Techniques for Improved Accuracy and Reliability'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14404'}]",https://arxiv.org/abs/2412.14404,"arXiv:2412.14404v1 Announce Type: new 
Abstract: Fingerprint recognition systems stand as pillars in the realm of biometric authentication, providing indispensable security measures across various domains. This study investigates integrating Convolutional Neural Networks (CNNs) with Gabor filters to improve fingerprint recognition accuracy and robustness. Leveraging a diverse dataset sourced from the Sokoto Coventry Fingerprint Dataset, our experiments meticulously evaluate the efficacy of different classification algorithms. Our findings underscore the supremacy of CNN-based approaches, boasting an impressive overall accuracy of 94\%. Furthermore, the amalgamation of Gabor filters with CNN architectures unveils promising strides in discerning altered fingerprints, illuminating new pathways for enhancing biometric authentication systems. While the CNN-Gabor fusion showcases commendable performance, our exploration of hybrid approaches combining multiple classifiers reveals nuanced outcomes. Despite these mixed results, our study illuminates the transformative potential of deep learning methodologies in reshaping the landscape of fingerprint recognition. Through rigorous experimentation and insightful analysis, this research not only contributes to advancing biometric authentication technologies but also sheds light on the intricate interplay between traditional feature extraction methods and cutting-edge deep learning architectures. These findings offer actionable insights for optimizing fingerprint recognition systems for real-world deployment, paving the way for enhanced security and reliability in diverse applications.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14404v1 Announce Type: new \nAbstract: Fingerprint recognition systems stand as pillars in the realm of biometric authentication, providing indispensable security measures across various domains. This study investigates integrating Convolutional Neural Networks (CNNs) with Gabor filters to improve fingerprint recognition accuracy and robustness. Leveraging a diverse dataset sourced from the Sokoto Coventry Fingerprint Dataset, our experiments meticulously evaluate the efficacy of different classification algorithms. Our findings underscore the supremacy of CNN-based approaches, boasting an impressive overall accuracy of 94\\%. Furthermore, the amalgamation of Gabor filters with CNN architectures unveils promising strides in discerning altered fingerprints, illuminating new pathways for enhancing biometric authentication systems. While the CNN-Gabor fusion showcases commendable performance, our exploration of hybrid approaches combining multiple classifiers reveals nuanced outcomes. Despite these mixed results, our study illuminates the transformative potential of deep learning methodologies in reshaping the landscape of fingerprint recognition. Through rigorous experimentation and insightful analysis, this research not only contributes to advancing biometric authentication technologies but also sheds light on the intricate interplay between traditional feature extraction methods and cutting-edge deep learning architectures. These findings offer actionable insights for optimizing fingerprint recognition systems for real-world deployment, paving the way for enhanced security and reliability in diverse applications.'}",oai:arXiv.org:2412.14404v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Temirlan Meiramkhanov, Arailym Tleubayeva'}]","Temirlan Meiramkhanov, Arailym Tleubayeva","{'name': 'Temirlan Meiramkhanov, Arailym Tleubayeva'}",,
99,ChainRank-DPO: Chain Rank Direct Preference Optimization for LLM Rankers,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'ChainRank-DPO: Chain Rank Direct Preference Optimization for LLM Rankers'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14405'}]",https://arxiv.org/abs/2412.14405,"arXiv:2412.14405v1 Announce Type: new 
Abstract: Large language models (LLMs) have demonstrated remarkable effectiveness in text reranking through works like RankGPT, leveraging their human-like reasoning about relevance. However, supervised fine-tuning for ranking often diminishes these models' general-purpose capabilities, including the crucial reasoning abilities that make them valuable for ranking. We introduce a novel approach integrating Chain-of-Thought prompting with an SFT-DPO (Supervised Fine-Tuning followed by Direct Preference Optimization) pipeline to preserve these capabilities while improving ranking performance. Our experiments on TREC 2019 and 2020 Deep Learning datasets show that our approach outperforms the state-of-the-art RankZephyr while maintaining strong performance on the Massive Multitask Language Understanding (MMLU) benchmark, demonstrating effective preservation of general-purpose capabilities through thoughtful fine-tuning strategies. Our code and data will be publicly released upon the acceptance of the paper.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14405v1 Announce Type: new \nAbstract: Large language models (LLMs) have demonstrated remarkable effectiveness in text reranking through works like RankGPT, leveraging their human-like reasoning about relevance. However, supervised fine-tuning for ranking often diminishes these models' general-purpose capabilities, including the crucial reasoning abilities that make them valuable for ranking. We introduce a novel approach integrating Chain-of-Thought prompting with an SFT-DPO (Supervised Fine-Tuning followed by Direct Preference Optimization) pipeline to preserve these capabilities while improving ranking performance. Our experiments on TREC 2019 and 2020 Deep Learning datasets show that our approach outperforms the state-of-the-art RankZephyr while maintaining strong performance on the Massive Multitask Language Understanding (MMLU) benchmark, demonstrating effective preservation of general-purpose capabilities through thoughtful fine-tuning strategies. Our code and data will be publicly released upon the acceptance of the paper.""}",oai:arXiv.org:2412.14405v1,False,"[{'term': 'cs.IR', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Haowei Liu, Xuyang Wu, Guohao Sun, Zhiqiang Tao, Yi Fang'}]","Haowei Liu, Xuyang Wu, Guohao Sun, Zhiqiang Tao, Yi Fang","{'name': 'Haowei Liu, Xuyang Wu, Guohao Sun, Zhiqiang Tao, Yi Fang'}",,
100,Multi-task Representation Learning for Mixed Integer Linear Programming,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Multi-task Representation Learning for Mixed Integer Linear Programming'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14409'}]",https://arxiv.org/abs/2412.14409,"arXiv:2412.14409v1 Announce Type: new 
Abstract: Mixed Integer Linear Programs (MILPs) are highly flexible and powerful tools for modeling and solving complex real-world combinatorial optimization problems. Recently, machine learning (ML)-guided approaches have demonstrated significant potential in improving MILP-solving efficiency. However, these methods typically rely on separate offline data collection and training processes, which limits their scalability and adaptability. This paper introduces the first multi-task learning framework for ML-guided MILP solving. The proposed framework provides MILP embeddings helpful in guiding MILP solving across solvers (e.g., Gurobi and SCIP) and across tasks (e.g., Branching and Solver configuration). Through extensive experiments on three widely used MILP benchmarks, we demonstrate that our multi-task learning model performs similarly to specialized models within the same distribution. Moreover, it significantly outperforms them in generalization across problem sizes and tasks.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14409v1 Announce Type: new \nAbstract: Mixed Integer Linear Programs (MILPs) are highly flexible and powerful tools for modeling and solving complex real-world combinatorial optimization problems. Recently, machine learning (ML)-guided approaches have demonstrated significant potential in improving MILP-solving efficiency. However, these methods typically rely on separate offline data collection and training processes, which limits their scalability and adaptability. This paper introduces the first multi-task learning framework for ML-guided MILP solving. The proposed framework provides MILP embeddings helpful in guiding MILP solving across solvers (e.g., Gurobi and SCIP) and across tasks (e.g., Branching and Solver configuration). Through extensive experiments on three widely used MILP benchmarks, we demonstrate that our multi-task learning model performs similarly to specialized models within the same distribution. Moreover, it significantly outperforms them in generalization across problem sizes and tasks.'}",oai:arXiv.org:2412.14409v1,False,"[{'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'math.OC', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Junyang Cai, Taoan Huang, Bistra Dilkina'}]","Junyang Cai, Taoan Huang, Bistra Dilkina","{'name': 'Junyang Cai, Taoan Huang, Bistra Dilkina'}",,
101,"In-Group Love, Out-Group Hate: A Framework to Measure Affective Polarization via Contentious Online Discussions","{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'In-Group Love, Out-Group Hate: A Framework to Measure Affective Polarization via Contentious Online Discussions'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14414'}]",https://arxiv.org/abs/2412.14414,"arXiv:2412.14414v1 Announce Type: new 
Abstract: Affective polarization, the emotional divide between ideological groups marked by in-group love and out-group hate, has intensified in the United States, driving contentious issues like masking and lockdowns during the COVID-19 pandemic. Despite its societal impact, existing models of opinion change fail to account for emotional dynamics nor offer methods to quantify affective polarization robustly and in real-time. In this paper, we introduce a discrete choice model that captures decision-making within affectively polarized social networks and propose a statistical inference method estimate key parameters -- in-group love and out-group hate -- from social media data. Through empirical validation from online discussions about the COVID-19 pandemic, we demonstrate that our approach accurately captures real-world polarization dynamics and explains the rapid emergence of a partisan gap in attitudes towards masking and lockdowns. This framework allows for tracking affective polarization across contentious issues has broad implications for fostering constructive online dialogues in digital spaces.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14414v1 Announce Type: new \nAbstract: Affective polarization, the emotional divide between ideological groups marked by in-group love and out-group hate, has intensified in the United States, driving contentious issues like masking and lockdowns during the COVID-19 pandemic. Despite its societal impact, existing models of opinion change fail to account for emotional dynamics nor offer methods to quantify affective polarization robustly and in real-time. In this paper, we introduce a discrete choice model that captures decision-making within affectively polarized social networks and propose a statistical inference method estimate key parameters -- in-group love and out-group hate -- from social media data. Through empirical validation from online discussions about the COVID-19 pandemic, we demonstrate that our approach accurately captures real-world polarization dynamics and explains the rapid emergence of a partisan gap in attitudes towards masking and lockdowns. This framework allows for tracking affective polarization across contentious issues has broad implications for fostering constructive online dialogues in digital spaces.'}",oai:arXiv.org:2412.14414v1,False,"[{'term': 'cs.SI', 'scheme': None, 'label': None}, {'term': 'cs.CL', 'scheme': None, 'label': None}, {'term': 'cs.CY', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Buddhika Nettasinghe, Ashwin Rao, Bohan Jiang, Allon Percus, Kristina Lerman'}]","Buddhika Nettasinghe, Ashwin Rao, Bohan Jiang, Allon Percus, Kristina Lerman","{'name': 'Buddhika Nettasinghe, Ashwin Rao, Bohan Jiang, Allon Percus, Kristina Lerman'}",,
102,DriveGPT: Scaling Autoregressive Behavior Models for Driving,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'DriveGPT: Scaling Autoregressive Behavior Models for Driving'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14415'}]",https://arxiv.org/abs/2412.14415,"arXiv:2412.14415v1 Announce Type: new 
Abstract: We present DriveGPT, a scalable behavior model for autonomous driving. We model driving as a sequential decision making task, and learn a transformer model to predict future agent states as tokens in an autoregressive fashion. We scale up our model parameters and training data by multiple orders of magnitude, enabling us to explore the scaling properties in terms of dataset size, model parameters, and compute. We evaluate DriveGPT across different scales in a planning task, through both quantitative metrics and qualitative examples including closed-loop driving in complex real-world scenarios. In a separate prediction task, DriveGPT outperforms a state-of-the-art baseline and exhibits improved performance by pretraining on a large-scale dataset, further validating the benefits of data scaling.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14415v1 Announce Type: new \nAbstract: We present DriveGPT, a scalable behavior model for autonomous driving. We model driving as a sequential decision making task, and learn a transformer model to predict future agent states as tokens in an autoregressive fashion. We scale up our model parameters and training data by multiple orders of magnitude, enabling us to explore the scaling properties in terms of dataset size, model parameters, and compute. We evaluate DriveGPT across different scales in a planning task, through both quantitative metrics and qualitative examples including closed-loop driving in complex real-world scenarios. In a separate prediction task, DriveGPT outperforms a state-of-the-art baseline and exhibits improved performance by pretraining on a large-scale dataset, further validating the benefits of data scaling.'}",oai:arXiv.org:2412.14415v1,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.RO', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by-nc-nd/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-nd/4.0/'}","[{'name': 'Xin Huang, Eric M. Wolff, Paul Vernaza, Tung Phan-Minh, Hongge Chen, David S. Hayden, Mark Edmonds, Brian Pierce, Xinxin Chen, Pratik Elias Jacob, Xiaobai Chen, Chingiz Tairbekov, Pratik Agarwal, Tianshi Gao, Yuning Chai, Siddhartha Srinivasa'}]","Xin Huang, Eric M. Wolff, Paul Vernaza, Tung Phan-Minh, Hongge Chen, David S. Hayden, Mark Edmonds, Brian Pierce, Xinxin Chen, Pratik Elias Jacob, Xiaobai Chen, Chingiz Tairbekov, Pratik Agarwal, Tianshi Gao, Yuning Chai, Siddhartha Srinivasa","{'name': 'Xin Huang, Eric M. Wolff, Paul Vernaza, Tung Phan-Minh, Hongge Chen, David S. Hayden, Mark Edmonds, Brian Pierce, Xinxin Chen, Pratik Elias Jacob, Xiaobai Chen, Chingiz Tairbekov, Pratik Agarwal, Tianshi Gao, Yuning Chai, Siddhartha Srinivasa'}",,
103,Cutting Sequence Diffuser: Sim-to-Real Transferable Planning for Object Shaping by Grinding,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Cutting Sequence Diffuser: Sim-to-Real Transferable Planning for Object Shaping by Grinding'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14417'}]",https://arxiv.org/abs/2412.14417,"arXiv:2412.14417v1 Announce Type: new 
Abstract: Automating object shaping by grinding with a robot is a crucial industrial process that involves removing material with a rotating grinding belt. This process generates removal resistance depending on such process conditions as material type, removal volume, and robot grinding posture, all of which complicate the analytical modeling of shape transitions. Additionally, a data-driven approach based on real-world data is challenging due to high data collection costs and the irreversible nature of the process. This paper proposes a Cutting Sequence Diffuser (CSD) for object shaping by grinding. The CSD, which only requires simple simulation data for model learning, offers an efficient way to plan long-horizon action sequences transferable to the real world. Our method designs a smooth action space with constrained small removal volumes to suppress the complexity of the shape transitions caused by removal resistance, thus reducing the reality gap in simulations. Moreover, by using a diffusion model to generate long-horizon action sequences, our approach reduces the planning time and allows for grinding the target shape while adhering to the constraints of a small removal volume per step. Through evaluations in both simulation and real robot experiments, we confirmed that our CSD was effective for grinding to different materials and various target shapes in a short time.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14417v1 Announce Type: new \nAbstract: Automating object shaping by grinding with a robot is a crucial industrial process that involves removing material with a rotating grinding belt. This process generates removal resistance depending on such process conditions as material type, removal volume, and robot grinding posture, all of which complicate the analytical modeling of shape transitions. Additionally, a data-driven approach based on real-world data is challenging due to high data collection costs and the irreversible nature of the process. This paper proposes a Cutting Sequence Diffuser (CSD) for object shaping by grinding. The CSD, which only requires simple simulation data for model learning, offers an efficient way to plan long-horizon action sequences transferable to the real world. Our method designs a smooth action space with constrained small removal volumes to suppress the complexity of the shape transitions caused by removal resistance, thus reducing the reality gap in simulations. Moreover, by using a diffusion model to generate long-horizon action sequences, our approach reduces the planning time and allows for grinding the target shape while adhering to the constraints of a small removal volume per step. Through evaluations in both simulation and real robot experiments, we confirmed that our CSD was effective for grinding to different materials and various target shapes in a short time.'}",oai:arXiv.org:2412.14417v1,False,"[{'term': 'cs.RO', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Takumi Hachimine, Jun Morimoto, Takamitsu Matsubara'}]","Takumi Hachimine, Jun Morimoto, Takamitsu Matsubara","{'name': 'Takumi Hachimine, Jun Morimoto, Takamitsu Matsubara'}",,
104,An Immersive Multi-Elevation Multi-Seasonal Dataset for 3D Reconstruction and Visualization,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'An Immersive Multi-Elevation Multi-Seasonal Dataset for 3D Reconstruction and Visualization'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14418'}]",https://arxiv.org/abs/2412.14418,"arXiv:2412.14418v1 Announce Type: new 
Abstract: Significant progress has been made in photo-realistic scene reconstruction over recent years. Various disparate efforts have enabled capabilities such as multi-appearance or large-scale modeling; however, there lacks a welldesigned dataset that can evaluate the holistic progress of scene reconstruction. We introduce a collection of imagery of the Johns Hopkins Homewood Campus, acquired at different seasons, times of day, in multiple elevations, and across a large scale. We perform a multi-stage calibration process, which efficiently recover camera parameters from phone and drone cameras. This dataset can enable researchers to rigorously explore challenges in unconstrained settings, including effects of inconsistent illumination, reconstruction from large scale and from significantly different perspectives, etc.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14418v1 Announce Type: new \nAbstract: Significant progress has been made in photo-realistic scene reconstruction over recent years. Various disparate efforts have enabled capabilities such as multi-appearance or large-scale modeling; however, there lacks a welldesigned dataset that can evaluate the holistic progress of scene reconstruction. We introduce a collection of imagery of the Johns Hopkins Homewood Campus, acquired at different seasons, times of day, in multiple elevations, and across a large scale. We perform a multi-stage calibration process, which efficiently recover camera parameters from phone and drone cameras. This dataset can enable researchers to rigorously explore challenges in unconstrained settings, including effects of inconsistent illumination, reconstruction from large scale and from significantly different perspectives, etc.'}",oai:arXiv.org:2412.14418v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Xijun Liu, Yifan Zhou, Yuxiang Guo, Rama Chellappa, Cheng Peng'}]","Xijun Liu, Yifan Zhou, Yuxiang Guo, Rama Chellappa, Cheng Peng","{'name': 'Xijun Liu, Yifan Zhou, Yuxiang Guo, Rama Chellappa, Cheng Peng'}",,
105,Enhancing Diffusion Models for High-Quality Image Generation,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Enhancing Diffusion Models for High-Quality Image Generation'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14422'}]",https://arxiv.org/abs/2412.14422,"arXiv:2412.14422v1 Announce Type: new 
Abstract: This report presents the comprehensive implementation, evaluation, and optimization of Denoising Diffusion Probabilistic Models (DDPMs) and Denoising Diffusion Implicit Models (DDIMs), which are state-of-the-art generative models. During inference, these models take random noise as input and iteratively generate high-quality images as output. The study focuses on enhancing their generative capabilities by incorporating advanced techniques such as Classifier-Free Guidance (CFG), Latent Diffusion Models with Variational Autoencoders (VAE), and alternative noise scheduling strategies. The motivation behind this work is the growing demand for efficient and scalable generative AI models that can produce realistic images across diverse datasets, addressing challenges in applications such as art creation, image synthesis, and data augmentation. Evaluations were conducted on datasets including CIFAR-10 and ImageNet-100, with a focus on improving inference speed, computational efficiency, and image quality metrics like Frechet Inception Distance (FID). Results demonstrate that DDIM + CFG achieves faster inference and superior image quality. Challenges with VAE and noise scheduling are also highlighted, suggesting opportunities for future optimization. This work lays the groundwork for developing scalable, efficient, and high-quality generative AI systems to benefit industries ranging from entertainment to robotics.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14422v1 Announce Type: new \nAbstract: This report presents the comprehensive implementation, evaluation, and optimization of Denoising Diffusion Probabilistic Models (DDPMs) and Denoising Diffusion Implicit Models (DDIMs), which are state-of-the-art generative models. During inference, these models take random noise as input and iteratively generate high-quality images as output. The study focuses on enhancing their generative capabilities by incorporating advanced techniques such as Classifier-Free Guidance (CFG), Latent Diffusion Models with Variational Autoencoders (VAE), and alternative noise scheduling strategies. The motivation behind this work is the growing demand for efficient and scalable generative AI models that can produce realistic images across diverse datasets, addressing challenges in applications such as art creation, image synthesis, and data augmentation. Evaluations were conducted on datasets including CIFAR-10 and ImageNet-100, with a focus on improving inference speed, computational efficiency, and image quality metrics like Frechet Inception Distance (FID). Results demonstrate that DDIM + CFG achieves faster inference and superior image quality. Challenges with VAE and noise scheduling are also highlighted, suggesting opportunities for future optimization. This work lays the groundwork for developing scalable, efficient, and high-quality generative AI systems to benefit industries ranging from entertainment to robotics.'}",oai:arXiv.org:2412.14422v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Jaineet Shah, Michael Gromis, Rickston Pinto'}]","Jaineet Shah, Michael Gromis, Rickston Pinto","{'name': 'Jaineet Shah, Michael Gromis, Rickston Pinto'}",,
106,FedPIA -- Permuting and Integrating Adapters leveraging Wasserstein Barycenters for Finetuning Foundation Models in Multi-Modal Federated Learning,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'FedPIA -- Permuting and Integrating Adapters leveraging Wasserstein Barycenters for Finetuning Foundation Models in Multi-Modal Federated Learning'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14424'}]",https://arxiv.org/abs/2412.14424,"arXiv:2412.14424v1 Announce Type: new 
Abstract: Large Vision-Language Models typically require large text and image datasets for effective fine-tuning. However, collecting data from various sites, especially in healthcare, is challenging due to strict privacy regulations. An alternative is to fine-tune these models on end-user devices, such as in medical clinics, without sending data to a server. These local clients typically have limited computing power and small datasets, which are not enough for fully fine-tuning large VLMs on their own. A naive solution to these scenarios is to leverage parameter-efficient fine-tuning (PEFT) strategies and apply federated learning (FL) algorithms to combine the learned adapter weights, thereby respecting the resource limitations and data privacy. However, this approach does not fully leverage the knowledge from multiple adapters trained on diverse data distributions and for diverse tasks. The adapters are adversely impacted by data heterogeneity and task heterogeneity across clients resulting in suboptimal convergence. To this end, we propose a novel framework called FedPIA that improves upon the naive combinations of FL and PEFT by introducing Permutation and Integration of the local Adapters in the server and global Adapters in the clients exploiting Wasserstein barycenters for improved blending of client-specific and client-agnostic knowledge. This layerwise permutation helps to bridge the gap in the parameter space of local and global adapters before integration. We conduct over 2000 client-level experiments utilizing 48 medical image datasets across five different medical vision-language FL task settings encompassing visual question answering as well as image and report-based multi-label disease detection. Our experiments involving diverse client settings, ten different modalities, and two VLM backbones demonstrate that FedPIA consistently outperforms the state-of-the-art PEFT-FL baselines.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14424v1 Announce Type: new \nAbstract: Large Vision-Language Models typically require large text and image datasets for effective fine-tuning. However, collecting data from various sites, especially in healthcare, is challenging due to strict privacy regulations. An alternative is to fine-tune these models on end-user devices, such as in medical clinics, without sending data to a server. These local clients typically have limited computing power and small datasets, which are not enough for fully fine-tuning large VLMs on their own. A naive solution to these scenarios is to leverage parameter-efficient fine-tuning (PEFT) strategies and apply federated learning (FL) algorithms to combine the learned adapter weights, thereby respecting the resource limitations and data privacy. However, this approach does not fully leverage the knowledge from multiple adapters trained on diverse data distributions and for diverse tasks. The adapters are adversely impacted by data heterogeneity and task heterogeneity across clients resulting in suboptimal convergence. To this end, we propose a novel framework called FedPIA that improves upon the naive combinations of FL and PEFT by introducing Permutation and Integration of the local Adapters in the server and global Adapters in the clients exploiting Wasserstein barycenters for improved blending of client-specific and client-agnostic knowledge. This layerwise permutation helps to bridge the gap in the parameter space of local and global adapters before integration. We conduct over 2000 client-level experiments utilizing 48 medical image datasets across five different medical vision-language FL task settings encompassing visual question answering as well as image and report-based multi-label disease detection. Our experiments involving diverse client settings, ten different modalities, and two VLM backbones demonstrate that FedPIA consistently outperforms the state-of-the-art PEFT-FL baselines.'}",oai:arXiv.org:2412.14424v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Pramit Saha, Divyanshu Mishra, Felix Wagner, Konstantinos Kamnitsas, J. Alison Noble'}]","Pramit Saha, Divyanshu Mishra, Felix Wagner, Konstantinos Kamnitsas, J. Alison Noble","{'name': 'Pramit Saha, Divyanshu Mishra, Felix Wagner, Konstantinos Kamnitsas, J. Alison Noble'}",,
107,All-in-One Tuning and Structural Pruning for Domain-Specific LLMs,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'All-in-One Tuning and Structural Pruning for Domain-Specific LLMs'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14426'}]",https://arxiv.org/abs/2412.14426,"arXiv:2412.14426v1 Announce Type: new 
Abstract: Existing pruning techniques for large language models (LLMs) targeting domain-specific applications typically follow a two-stage process: pruning the pretrained general-purpose LLMs and then fine-tuning the pruned LLMs on specific domains. However, the pruning decisions, derived from the pretrained weights, remain unchanged during fine-tuning, even if the weights have been updated. Therefore, such a combination of the pruning decisions and the finetuned weights may be suboptimal, leading to non-negligible performance degradation. To address these limitations, we propose ATP: All-in-One Tuning and Structural Pruning, a unified one-stage structural pruning and fine-tuning approach that dynamically identifies the current optimal substructure throughout the fine-tuning phase via a trainable pruning decision generator. Moreover, given the limited available data for domain-specific applications, Low-Rank Adaptation (LoRA) becomes a common technique to fine-tune the LLMs. In ATP, we introduce LoRA-aware forward and sparsity regularization to ensure that the substructures corresponding to the learned pruning decisions can be directly removed after the ATP process. ATP outperforms the state-of-the-art two-stage pruning methods on tasks in the legal and healthcare domains. More specifically, ATP recovers up to 88% and 91% performance of the dense model when pruning 40% parameters of LLaMA2-7B and LLaMA3-8B models, respectively.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14426v1 Announce Type: new \nAbstract: Existing pruning techniques for large language models (LLMs) targeting domain-specific applications typically follow a two-stage process: pruning the pretrained general-purpose LLMs and then fine-tuning the pruned LLMs on specific domains. However, the pruning decisions, derived from the pretrained weights, remain unchanged during fine-tuning, even if the weights have been updated. Therefore, such a combination of the pruning decisions and the finetuned weights may be suboptimal, leading to non-negligible performance degradation. To address these limitations, we propose ATP: All-in-One Tuning and Structural Pruning, a unified one-stage structural pruning and fine-tuning approach that dynamically identifies the current optimal substructure throughout the fine-tuning phase via a trainable pruning decision generator. Moreover, given the limited available data for domain-specific applications, Low-Rank Adaptation (LoRA) becomes a common technique to fine-tune the LLMs. In ATP, we introduce LoRA-aware forward and sparsity regularization to ensure that the substructures corresponding to the learned pruning decisions can be directly removed after the ATP process. ATP outperforms the state-of-the-art two-stage pruning methods on tasks in the legal and healthcare domains. More specifically, ATP recovers up to 88% and 91% performance of the dense model when pruning 40% parameters of LLaMA2-7B and LLaMA3-8B models, respectively.'}",oai:arXiv.org:2412.14426v1,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Lei Lu, Zhepeng Wang, Ruexue Bao, Mengbing Wang, Fangyi Li, Yawen Wu, Weiwen Jiang, Jie Xu, Yanzhi Wang, Shangqian Gao'}]","Lei Lu, Zhepeng Wang, Ruexue Bao, Mengbing Wang, Fangyi Li, Yawen Wu, Weiwen Jiang, Jie Xu, Yanzhi Wang, Shangqian Gao","{'name': 'Lei Lu, Zhepeng Wang, Ruexue Bao, Mengbing Wang, Fangyi Li, Yawen Wu, Weiwen Jiang, Jie Xu, Yanzhi Wang, Shangqian Gao'}",,
108,WildSAT: Learning Satellite Image Representations from Wildlife Observations,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'WildSAT: Learning Satellite Image Representations from Wildlife Observations'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14428'}]",https://arxiv.org/abs/2412.14428,"arXiv:2412.14428v1 Announce Type: new 
Abstract: What does the presence of a species reveal about a geographic location? We posit that habitat, climate, and environmental preferences reflected in species distributions provide a rich source of supervision for learning satellite image representations. We introduce WildSAT, which pairs satellite images with millions of geo-tagged wildlife observations readily-available on citizen science platforms. WildSAT uses a contrastive learning framework to combine information from species distribution maps with text descriptions that capture habitat and range details, alongside satellite images, to train or fine-tune models. On a range of downstream satellite image recognition tasks, this significantly improves the performance of both randomly initialized models and pre-trained models from sources like ImageNet or specialized satellite image datasets. Additionally, the alignment with text enables zero-shot retrieval, allowing for search based on general descriptions of locations. We demonstrate that WildSAT achieves better representations than recent methods that utilize other forms of cross-modal supervision, such as aligning satellite images with ground images or wildlife photos. Finally, we analyze the impact of various design choices on downstream performance, highlighting the general applicability of our approach.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14428v1 Announce Type: new \nAbstract: What does the presence of a species reveal about a geographic location? We posit that habitat, climate, and environmental preferences reflected in species distributions provide a rich source of supervision for learning satellite image representations. We introduce WildSAT, which pairs satellite images with millions of geo-tagged wildlife observations readily-available on citizen science platforms. WildSAT uses a contrastive learning framework to combine information from species distribution maps with text descriptions that capture habitat and range details, alongside satellite images, to train or fine-tune models. On a range of downstream satellite image recognition tasks, this significantly improves the performance of both randomly initialized models and pre-trained models from sources like ImageNet or specialized satellite image datasets. Additionally, the alignment with text enables zero-shot retrieval, allowing for search based on general descriptions of locations. We demonstrate that WildSAT achieves better representations than recent methods that utilize other forms of cross-modal supervision, such as aligning satellite images with ground images or wildlife photos. Finally, we analyze the impact of various design choices on downstream performance, highlighting the general applicability of our approach.'}",oai:arXiv.org:2412.14428v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'q-bio.QM', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Rangel Daroya, Elijah Cole, Oisin Mac Aodha, Grant Van Horn, Subhransu Maji'}]","Rangel Daroya, Elijah Cole, Oisin Mac Aodha, Grant Van Horn, Subhransu Maji","{'name': 'Rangel Daroya, Elijah Cole, Oisin Mac Aodha, Grant Van Horn, Subhransu Maji'}",,
109,Balanced Gradient Sample Retrieval for Enhanced Knowledge Retention in Proxy-based Continual Learning,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Balanced Gradient Sample Retrieval for Enhanced Knowledge Retention in Proxy-based Continual Learning'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14430'}]",https://arxiv.org/abs/2412.14430,"arXiv:2412.14430v1 Announce Type: new 
Abstract: Continual learning in deep neural networks often suffers from catastrophic forgetting, where representations for previous tasks are overwritten during subsequent training. We propose a novel sample retrieval strategy from the memory buffer that leverages both gradient-conflicting and gradient-aligned samples to effectively retain knowledge about past tasks within a supervised contrastive learning framework. Gradient-conflicting samples are selected for their potential to reduce interference by re-aligning gradients, thereby preserving past task knowledge. Meanwhile, gradient-aligned samples are incorporated to reinforce stable, shared representations across tasks. By balancing gradient correction from conflicting samples with alignment reinforcement from aligned ones, our approach increases the diversity among retrieved instances and achieves superior alignment in parameter space, significantly enhancing knowledge retention and mitigating proxy drift. Empirical results demonstrate that using both sample types outperforms methods relying solely on one sample type or random retrieval. Experiments on popular continual learning benchmarks in computer vision validate our method's state-of-the-art performance in mitigating forgetting while maintaining competitive accuracy on new tasks.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14430v1 Announce Type: new \nAbstract: Continual learning in deep neural networks often suffers from catastrophic forgetting, where representations for previous tasks are overwritten during subsequent training. We propose a novel sample retrieval strategy from the memory buffer that leverages both gradient-conflicting and gradient-aligned samples to effectively retain knowledge about past tasks within a supervised contrastive learning framework. Gradient-conflicting samples are selected for their potential to reduce interference by re-aligning gradients, thereby preserving past task knowledge. Meanwhile, gradient-aligned samples are incorporated to reinforce stable, shared representations across tasks. By balancing gradient correction from conflicting samples with alignment reinforcement from aligned ones, our approach increases the diversity among retrieved instances and achieves superior alignment in parameter space, significantly enhancing knowledge retention and mitigating proxy drift. Empirical results demonstrate that using both sample types outperforms methods relying solely on one sample type or random retrieval. Experiments on popular continual learning benchmarks in computer vision validate our method's state-of-the-art performance in mitigating forgetting while maintaining competitive accuracy on new tasks.""}",oai:arXiv.org:2412.14430v1,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Hongye Xu, Jan Wasilewski, Bartosz Krawczyk'}]","Hongye Xu, Jan Wasilewski, Bartosz Krawczyk","{'name': 'Hongye Xu, Jan Wasilewski, Bartosz Krawczyk'}",,
110,IntroStyle: Training-Free Introspective Style Attribution using Diffusion Features,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'IntroStyle: Training-Free Introspective Style Attribution using Diffusion Features'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14432'}]",https://arxiv.org/abs/2412.14432,"arXiv:2412.14432v1 Announce Type: new 
Abstract: Text-to-image (T2I) models have gained widespread adoption among content creators and the general public. However, this has sparked significant concerns regarding data privacy and copyright infringement among artists. Consequently, there is an increasing demand for T2I models to incorporate mechanisms that prevent the generation of specific artistic styles, thereby safeguarding intellectual property rights. Existing methods for style extraction typically necessitate the collection of custom datasets and the training of specialized models. This, however, is resource-intensive, time-consuming, and often impractical for real-time applications. Moreover, it may not adequately address the dynamic nature of artistic styles and the rapidly evolving landscape of digital art. We present a novel, training-free framework to solve the style attribution problem, using the features produced by a diffusion model alone, without any external modules or retraining. This is denoted as introspective style attribution (IntroStyle) and demonstrates superior performance to state-of-the-art models for style retrieval. We also introduce a synthetic dataset of Style Hacks (SHacks) to isolate artistic style and evaluate fine-grained style attribution performance.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14432v1 Announce Type: new \nAbstract: Text-to-image (T2I) models have gained widespread adoption among content creators and the general public. However, this has sparked significant concerns regarding data privacy and copyright infringement among artists. Consequently, there is an increasing demand for T2I models to incorporate mechanisms that prevent the generation of specific artistic styles, thereby safeguarding intellectual property rights. Existing methods for style extraction typically necessitate the collection of custom datasets and the training of specialized models. This, however, is resource-intensive, time-consuming, and often impractical for real-time applications. Moreover, it may not adequately address the dynamic nature of artistic styles and the rapidly evolving landscape of digital art. We present a novel, training-free framework to solve the style attribution problem, using the features produced by a diffusion model alone, without any external modules or retraining. This is denoted as introspective style attribution (IntroStyle) and demonstrates superior performance to state-of-the-art models for style retrieval. We also introduce a synthetic dataset of Style Hacks (SHacks) to isolate artistic style and evaluate fine-grained style attribution performance.'}",oai:arXiv.org:2412.14432v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'eess.IV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by-nc-sa/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-sa/4.0/'}","[{'name': 'Anand Kumar, Jiteng Mu, Nuno Vasconcelos'}]","Anand Kumar, Jiteng Mu, Nuno Vasconcelos","{'name': 'Anand Kumar, Jiteng Mu, Nuno Vasconcelos'}",,
111,Cherry-Picking in Time Series Forecasting: How to Select Datasets to Make Your Model Shine,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Cherry-Picking in Time Series Forecasting: How to Select Datasets to Make Your Model Shine'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14435'}]",https://arxiv.org/abs/2412.14435,"arXiv:2412.14435v1 Announce Type: new 
Abstract: The importance of time series forecasting drives continuous research and the development of new approaches to tackle this problem. Typically, these methods are introduced through empirical studies that frequently claim superior accuracy for the proposed approaches. Nevertheless, concerns are rising about the reliability and generalizability of these results due to limitations in experimental setups. This paper addresses a critical limitation: the number and representativeness of the datasets used. We investigate the impact of dataset selection bias, particularly the practice of cherry-picking datasets, on the performance evaluation of forecasting methods. Through empirical analysis with a diverse set of benchmark datasets, our findings reveal that cherry-picking datasets can significantly distort the perceived performance of methods, often exaggerating their effectiveness. Furthermore, our results demonstrate that by selectively choosing just four datasets - what most studies report - 46% of methods could be deemed best in class, and 77% could rank within the top three. Additionally, recent deep learning-based approaches show high sensitivity to dataset selection, whereas classical methods exhibit greater robustness. Finally, our results indicate that, when empirically validating forecasting algorithms on a subset of the benchmarks, increasing the number of datasets tested from 3 to 6 reduces the risk of incorrectly identifying an algorithm as the best one by approximately 40%. Our study highlights the critical need for comprehensive evaluation frameworks that more accurately reflect real-world scenarios. Adopting such frameworks will ensure the development of robust and reliable forecasting methods.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14435v1 Announce Type: new \nAbstract: The importance of time series forecasting drives continuous research and the development of new approaches to tackle this problem. Typically, these methods are introduced through empirical studies that frequently claim superior accuracy for the proposed approaches. Nevertheless, concerns are rising about the reliability and generalizability of these results due to limitations in experimental setups. This paper addresses a critical limitation: the number and representativeness of the datasets used. We investigate the impact of dataset selection bias, particularly the practice of cherry-picking datasets, on the performance evaluation of forecasting methods. Through empirical analysis with a diverse set of benchmark datasets, our findings reveal that cherry-picking datasets can significantly distort the perceived performance of methods, often exaggerating their effectiveness. Furthermore, our results demonstrate that by selectively choosing just four datasets - what most studies report - 46% of methods could be deemed best in class, and 77% could rank within the top three. Additionally, recent deep learning-based approaches show high sensitivity to dataset selection, whereas classical methods exhibit greater robustness. Finally, our results indicate that, when empirically validating forecasting algorithms on a subset of the benchmarks, increasing the number of datasets tested from 3 to 6 reduces the risk of incorrectly identifying an algorithm as the best one by approximately 40%. Our study highlights the critical need for comprehensive evaluation frameworks that more accurately reflect real-world scenarios. Adopting such frameworks will ensure the development of robust and reliable forecasting methods.'}",oai:arXiv.org:2412.14435v1,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Luis Roque, Carlos Soares, Vitor Cerqueira, Luis Torgo'}]","Luis Roque, Carlos Soares, Vitor Cerqueira, Luis Torgo","{'name': 'Luis Roque, Carlos Soares, Vitor Cerqueira, Luis Torgo'}",,
112,ORBIT: Cost-Effective Dataset Curation for Large Language Model Domain Adaptation with an Astronomy Case Study,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'ORBIT: Cost-Effective Dataset Curation for Large Language Model Domain Adaptation with an Astronomy Case Study'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14436'}]",https://arxiv.org/abs/2412.14436,"arXiv:2412.14436v1 Announce Type: new 
Abstract: Recent advances in language modeling demonstrate the need for high-quality domain-specific training data, especially for tasks that require specialized knowledge. General-purpose models, while versatile, often lack the depth needed for expert-level tasks because of limited domain-specific information. Domain adaptation training can enhance these models, but it demands substantial, high-quality data. To address this, we propose ORBIT, a cost-efficient methodology for curating massive, high-quality domain-specific datasets from noisy web sources, tailored for training specialist large language models. Using astronomy as a primary case study, we refined the 1.3T-token FineWeb-Edu dataset into a high-quality, 10B-token subset focused on astronomy. Fine-tuning \textsc{LLaMA-3-8B} on a 1B-token astronomy subset improved performance on the MMLU astronomy benchmark from 69\% to 76\% and achieved top results on AstroBench, an astronomy-specific benchmark. Moreover, our model (Orbit-LLaMA) outperformed \textsc{LLaMA-3-8B-base}, with GPT-4o evaluations preferring it in 73\% of cases across 1000 astronomy-specific questions. Additionally, we validated ORBIT's generalizability by applying it to law and medicine, achieving a significant improvement of data quality compared to an unfiltered baseline. We open-source the ORBIT methodology, including the curated datasets, the codebase, and the resulting model at \href{https://github.com/ModeEric/ORBIT-Llama}{https://github.com/ModeEric/ORBIT-Llama}.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14436v1 Announce Type: new \nAbstract: Recent advances in language modeling demonstrate the need for high-quality domain-specific training data, especially for tasks that require specialized knowledge. General-purpose models, while versatile, often lack the depth needed for expert-level tasks because of limited domain-specific information. Domain adaptation training can enhance these models, but it demands substantial, high-quality data. To address this, we propose ORBIT, a cost-efficient methodology for curating massive, high-quality domain-specific datasets from noisy web sources, tailored for training specialist large language models. Using astronomy as a primary case study, we refined the 1.3T-token FineWeb-Edu dataset into a high-quality, 10B-token subset focused on astronomy. Fine-tuning \\textsc{LLaMA-3-8B} on a 1B-token astronomy subset improved performance on the MMLU astronomy benchmark from 69\\% to 76\\% and achieved top results on AstroBench, an astronomy-specific benchmark. Moreover, our model (Orbit-LLaMA) outperformed \\textsc{LLaMA-3-8B-base}, with GPT-4o evaluations preferring it in 73\\% of cases across 1000 astronomy-specific questions. Additionally, we validated ORBIT's generalizability by applying it to law and medicine, achieving a significant improvement of data quality compared to an unfiltered baseline. We open-source the ORBIT methodology, including the curated datasets, the codebase, and the resulting model at \\href{https://github.com/ModeEric/ORBIT-Llama}{https://github.com/ModeEric/ORBIT-Llama}.""}",oai:arXiv.org:2412.14436v1,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Eric Modesitt, Ke Yang, Spencer Hulsey, Chengxiang Zhai, Volodymyr Kindratenko'}]","Eric Modesitt, Ke Yang, Spencer Hulsey, Chengxiang Zhai, Volodymyr Kindratenko","{'name': 'Eric Modesitt, Ke Yang, Spencer Hulsey, Chengxiang Zhai, Volodymyr Kindratenko'}",,
113,Fine-Grained Computation in 3-Space: Matrix Multiplication and Graph Problems,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Fine-Grained Computation in 3-Space: Matrix Multiplication and Graph Problems'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14441'}]",https://arxiv.org/abs/2412.14441,"arXiv:2412.14441v1 Announce Type: new 
Abstract: Obeying constraints imposed by classical physics, we give optimal fine-grained algorithms for matrix multiplication and problems involving graphs and mazes, where all calculations are done in 3-dimensional space. We assume that whatever the technology is, a bit requires a minimum volume and communication travels at a bounded speed. These imply that multiplying $n \times n$ matrices takes $\Omega(n^{2/3})$ time, and we show that this can be achieved by a fine-grained 3-d mesh of $n^2$ processors. While the constants are impractically large, this is asymptotically faster than parallel implementations of Strassen's algorithm, while the lower bound shows that some claims about parallelizing faster serial algorithms are impossible in 3-space. If the matrices are not over a ring then multiplication can be done in $\Theta(n^{3/4})$ time by expanding to a mesh larger than the input. In 2-d (such as the surface of a chip) this approach is useless and $\Theta(n)$ systolic algorithms are optimal even when the matrices are over a ring. Similarly, for path and maze problems there are approaches useful in 3-d but not 2-d.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14441v1 Announce Type: new \nAbstract: Obeying constraints imposed by classical physics, we give optimal fine-grained algorithms for matrix multiplication and problems involving graphs and mazes, where all calculations are done in 3-dimensional space. We assume that whatever the technology is, a bit requires a minimum volume and communication travels at a bounded speed. These imply that multiplying $n \\times n$ matrices takes $\\Omega(n^{2/3})$ time, and we show that this can be achieved by a fine-grained 3-d mesh of $n^2$ processors. While the constants are impractically large, this is asymptotically faster than parallel implementations of Strassen's algorithm, while the lower bound shows that some claims about parallelizing faster serial algorithms are impossible in 3-space. If the matrices are not over a ring then multiplication can be done in $\\Theta(n^{3/4})$ time by expanding to a mesh larger than the input. In 2-d (such as the surface of a chip) this approach is useless and $\\Theta(n)$ systolic algorithms are optimal even when the matrices are over a ring. Similarly, for path and maze problems there are approaches useful in 3-d but not 2-d.""}",oai:arXiv.org:2412.14441v1,False,"[{'term': 'cs.DS', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}",[{'name': 'Quentin F. Stout'}],Quentin F. Stout,{'name': 'Quentin F. Stout'},,
114,EPN: An Ego Vehicle Planning-Informed Network for Target Trajectory Prediction,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'EPN: An Ego Vehicle Planning-Informed Network for Target Trajectory Prediction'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14442'}]",https://arxiv.org/abs/2412.14442,"arXiv:2412.14442v1 Announce Type: new 
Abstract: Trajectory prediction plays a crucial role in improving the safety and reliability of autonomous vehicles, serving as an intermediate link between perception and planning. However, due to the highly dynamic and multimodal nature of the task, accurately predicting the future trajectory of a target vehicle remains a significant challenge. To address these challenges, we propose an Ego vehicle Planning-informed Network (EPN) for multimodal trajectory prediction. Current trajectory prediction methods typically use the historical trajectory and vehicle attributes as inputs, focusing primarily on how historical information influences the future trajectory of the target vehicle. In real-world driving scenarios, however, the future trajectory of a vehicle is influenced not only by its own historical data but also by the behavior of other vehicles on the road. To address this, we incorporate the future planned trajectory of the ego vehicle as an additional input to simulate the mutual influence between the ego vehicle's planned trajectory and the predicted trajectory of the target vehicle. Furthermore, to tackle the challenges of intention ambiguity and large prediction errors often encountered in methods based on driving intentions, we propose a target's endpoint prediction module. This module first predicts the possible endpoints of the target vehicle, then refines these predictions through a correction mechanism, and finally generates a complete multimodal predicted trajectory based on the corrected endpoints. Experimental results demonstrate that, compared to other trajectory prediction methods, EPN achieves an average reduction of 34.9%, 30.7%, and 30.4% in RMSE, ADE, and FDE evaluation metrics on the NGSIM dataset, and an average reduction of 64.6%, 64.5%, and 64.3% in RMSE, ADE, and FDE on the HighD dataset. These results highlight the strong performance of EPN in trajectory prediction.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14442v1 Announce Type: new \nAbstract: Trajectory prediction plays a crucial role in improving the safety and reliability of autonomous vehicles, serving as an intermediate link between perception and planning. However, due to the highly dynamic and multimodal nature of the task, accurately predicting the future trajectory of a target vehicle remains a significant challenge. To address these challenges, we propose an Ego vehicle Planning-informed Network (EPN) for multimodal trajectory prediction. Current trajectory prediction methods typically use the historical trajectory and vehicle attributes as inputs, focusing primarily on how historical information influences the future trajectory of the target vehicle. In real-world driving scenarios, however, the future trajectory of a vehicle is influenced not only by its own historical data but also by the behavior of other vehicles on the road. To address this, we incorporate the future planned trajectory of the ego vehicle as an additional input to simulate the mutual influence between the ego vehicle's planned trajectory and the predicted trajectory of the target vehicle. Furthermore, to tackle the challenges of intention ambiguity and large prediction errors often encountered in methods based on driving intentions, we propose a target's endpoint prediction module. This module first predicts the possible endpoints of the target vehicle, then refines these predictions through a correction mechanism, and finally generates a complete multimodal predicted trajectory based on the corrected endpoints. Experimental results demonstrate that, compared to other trajectory prediction methods, EPN achieves an average reduction of 34.9%, 30.7%, and 30.4% in RMSE, ADE, and FDE evaluation metrics on the NGSIM dataset, and an average reduction of 64.6%, 64.5%, and 64.3% in RMSE, ADE, and FDE on the HighD dataset. These results highlight the strong performance of EPN in trajectory prediction.""}",oai:arXiv.org:2412.14442v1,False,"[{'term': 'cs.RO', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Saiqian Peng, Duanfeng Chu, Guanjie Li, Liping Lu, Jinxiang Wang'}]","Saiqian Peng, Duanfeng Chu, Guanjie Li, Liping Lu, Jinxiang Wang","{'name': 'Saiqian Peng, Duanfeng Chu, Guanjie Li, Liping Lu, Jinxiang Wang'}",,
115,GenHMR: Generative Human Mesh Recovery,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'GenHMR: Generative Human Mesh Recovery'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14444'}]",https://arxiv.org/abs/2412.14444,"arXiv:2412.14444v1 Announce Type: new 
Abstract: Human mesh recovery (HMR) is crucial in many computer vision applications; from health to arts and entertainment. HMR from monocular images has predominantly been addressed by deterministic methods that output a single prediction for a given 2D image. However, HMR from a single image is an ill-posed problem due to depth ambiguity and occlusions. Probabilistic methods have attempted to address this by generating and fusing multiple plausible 3D reconstructions, but their performance has often lagged behind deterministic approaches. In this paper, we introduce GenHMR, a novel generative framework that reformulates monocular HMR as an image-conditioned generative task, explicitly modeling and mitigating uncertainties in the 2D-to-3D mapping process. GenHMR comprises two key components: (1) a pose tokenizer to convert 3D human poses into a sequence of discrete tokens in a latent space, and (2) an image-conditional masked transformer to learn the probabilistic distributions of the pose tokens, conditioned on the input image prompt along with randomly masked token sequence. During inference, the model samples from the learned conditional distribution to iteratively decode high-confidence pose tokens, thereby reducing 3D reconstruction uncertainties. To further refine the reconstruction, a 2D pose-guided refinement technique is proposed to directly fine-tune the decoded pose tokens in the latent space, which forces the projected 3D body mesh to align with the 2D pose clues. Experiments on benchmark datasets demonstrate that GenHMR significantly outperforms state-of-the-art methods. Project website can be found at https://m-usamasaleem.github.io/publication/GenHMR/GenHMR.html","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14444v1 Announce Type: new \nAbstract: Human mesh recovery (HMR) is crucial in many computer vision applications; from health to arts and entertainment. HMR from monocular images has predominantly been addressed by deterministic methods that output a single prediction for a given 2D image. However, HMR from a single image is an ill-posed problem due to depth ambiguity and occlusions. Probabilistic methods have attempted to address this by generating and fusing multiple plausible 3D reconstructions, but their performance has often lagged behind deterministic approaches. In this paper, we introduce GenHMR, a novel generative framework that reformulates monocular HMR as an image-conditioned generative task, explicitly modeling and mitigating uncertainties in the 2D-to-3D mapping process. GenHMR comprises two key components: (1) a pose tokenizer to convert 3D human poses into a sequence of discrete tokens in a latent space, and (2) an image-conditional masked transformer to learn the probabilistic distributions of the pose tokens, conditioned on the input image prompt along with randomly masked token sequence. During inference, the model samples from the learned conditional distribution to iteratively decode high-confidence pose tokens, thereby reducing 3D reconstruction uncertainties. To further refine the reconstruction, a 2D pose-guided refinement technique is proposed to directly fine-tune the decoded pose tokens in the latent space, which forces the projected 3D body mesh to align with the 2D pose clues. Experiments on benchmark datasets demonstrate that GenHMR significantly outperforms state-of-the-art methods. Project website can be found at https://m-usamasaleem.github.io/publication/GenHMR/GenHMR.html'}",oai:arXiv.org:2412.14444v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.GR', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Muhammad Usama Saleem, Ekkasit Pinyoanuntapong, Pu Wang, Hongfei Xue, Srijan Das, Chen Chen'}]","Muhammad Usama Saleem, Ekkasit Pinyoanuntapong, Pu Wang, Hongfei Xue, Srijan Das, Chen Chen","{'name': 'Muhammad Usama Saleem, Ekkasit Pinyoanuntapong, Pu Wang, Hongfei Xue, Srijan Das, Chen Chen'}",,
116,VLM-AD: End-to-End Autonomous Driving through Vision-Language Model Supervision,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'VLM-AD: End-to-End Autonomous Driving through Vision-Language Model Supervision'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14446'}]",https://arxiv.org/abs/2412.14446,"arXiv:2412.14446v1 Announce Type: new 
Abstract: Human drivers rely on commonsense reasoning to navigate diverse and dynamic real-world scenarios. Existing end-to-end (E2E) autonomous driving (AD) models are typically optimized to mimic driving patterns observed in data, without capturing the underlying reasoning processes. This limitation constrains their ability to handle challenging driving scenarios. To close this gap, we propose VLM-AD, a method that leverages vision-language models (VLMs) as teachers to enhance training by providing additional supervision that incorporates unstructured reasoning information and structured action labels. Such supervision enhances the model's ability to learn richer feature representations that capture the rationale behind driving patterns. Importantly, our method does not require a VLM during inference, making it practical for real-time deployment. When integrated with state-of-the-art methods, VLM-AD achieves significant improvements in planning accuracy and reduced collision rates on the nuScenes dataset.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14446v1 Announce Type: new \nAbstract: Human drivers rely on commonsense reasoning to navigate diverse and dynamic real-world scenarios. Existing end-to-end (E2E) autonomous driving (AD) models are typically optimized to mimic driving patterns observed in data, without capturing the underlying reasoning processes. This limitation constrains their ability to handle challenging driving scenarios. To close this gap, we propose VLM-AD, a method that leverages vision-language models (VLMs) as teachers to enhance training by providing additional supervision that incorporates unstructured reasoning information and structured action labels. Such supervision enhances the model's ability to learn richer feature representations that capture the rationale behind driving patterns. Importantly, our method does not require a VLM during inference, making it practical for real-time deployment. When integrated with state-of-the-art methods, VLM-AD achieves significant improvements in planning accuracy and reduced collision rates on the nuScenes dataset.""}",oai:arXiv.org:2412.14446v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by-nc-sa/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-sa/4.0/'}","[{'name': 'Yi Xu, Yuxin Hu, Zaiwei Zhang, Gregory P. Meyer, Siva Karthik Mustikovela, Siddhartha Srinivasa, Eric M. Wolff, Xin Huang'}]","Yi Xu, Yuxin Hu, Zaiwei Zhang, Gregory P. Meyer, Siva Karthik Mustikovela, Siddhartha Srinivasa, Eric M. Wolff, Xin Huang","{'name': 'Yi Xu, Yuxin Hu, Zaiwei Zhang, Gregory P. Meyer, Siva Karthik Mustikovela, Siddhartha Srinivasa, Eric M. Wolff, Xin Huang'}",,
117,Color Enhancement for V-PCC Compressed Point Cloud via 2D Attribute Map Optimization,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Color Enhancement for V-PCC Compressed Point Cloud via 2D Attribute Map Optimization'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14449'}]",https://arxiv.org/abs/2412.14449,"arXiv:2412.14449v1 Announce Type: new 
Abstract: Video-based point cloud compression (V-PCC) converts the dynamic point cloud data into video sequences using traditional video codecs for efficient encoding. However, this lossy compression scheme introduces artifacts that degrade the color attributes of the data. This paper introduces a framework designed to enhance the color quality in the V-PCC compressed point clouds. We propose the lightweight de-compression Unet (LDC-Unet), a 2D neural network, to optimize the projection maps generated during V-PCC encoding. The optimized 2D maps will then be back-projected to the 3D space to enhance the corresponding point cloud attributes. Additionally, we introduce a transfer learning strategy and develop a customized natural image dataset for the initial training. The model was then fine-tuned using the projection maps of the compressed point clouds. The whole strategy effectively addresses the scarcity of point cloud training data. Our experiments, conducted on the public 8i voxelized full bodies long sequences (8iVSLF) dataset, demonstrate the effectiveness of our proposed method in improving the color quality.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14449v1 Announce Type: new \nAbstract: Video-based point cloud compression (V-PCC) converts the dynamic point cloud data into video sequences using traditional video codecs for efficient encoding. However, this lossy compression scheme introduces artifacts that degrade the color attributes of the data. This paper introduces a framework designed to enhance the color quality in the V-PCC compressed point clouds. We propose the lightweight de-compression Unet (LDC-Unet), a 2D neural network, to optimize the projection maps generated during V-PCC encoding. The optimized 2D maps will then be back-projected to the 3D space to enhance the corresponding point cloud attributes. Additionally, we introduce a transfer learning strategy and develop a customized natural image dataset for the initial training. The model was then fine-tuned using the projection maps of the compressed point clouds. The whole strategy effectively addresses the scarcity of point cloud training data. Our experiments, conducted on the public 8i voxelized full bodies long sequences (8iVSLF) dataset, demonstrate the effectiveness of our proposed method in improving the color quality.'}",oai:arXiv.org:2412.14449v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'eess.IV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Jingwei Bao, Yu Liu, Zeliang Li, Shuyuan Zhu, Siu-Kei Au Yeung'}]","Jingwei Bao, Yu Liu, Zeliang Li, Shuyuan Zhu, Siu-Kei Au Yeung","{'name': 'Jingwei Bao, Yu Liu, Zeliang Li, Shuyuan Zhu, Siu-Kei Au Yeung'}",,
118,CLDG: Contrastive Learning on Dynamic Graphs,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'CLDG: Contrastive Learning on Dynamic Graphs'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14451'}]",https://arxiv.org/abs/2412.14451,"arXiv:2412.14451v1 Announce Type: new 
Abstract: The graph with complex annotations is the most potent data type, whose constantly evolving motivates further exploration of the unsupervised dynamic graph representation. One of the representative paradigms is graph contrastive learning. It constructs self-supervised signals by maximizing the mutual information between the statistic graph's augmentation views. However, the semantics and labels may change within the augmentation process, causing a significant performance drop in downstream tasks. This drawback becomes greatly magnified on dynamic graphs. To address this problem, we designed a simple yet effective framework named CLDG. Firstly, we elaborate that dynamic graphs have temporal translation invariance at different levels. Then, we proposed a sampling layer to extract the temporally-persistent signals. It will encourage the node to maintain consistent local and global representations, i.e., temporal translation invariance under the timespan views. The extensive experiments demonstrate the effectiveness and efficiency of the method on seven datasets by outperforming eight unsupervised state-of-the-art baselines and showing competitiveness against four semi-supervised methods. Compared with the existing dynamic graph method, the number of model parameters and training time is reduced by an average of 2,001.86 times and 130.31 times on seven datasets, respectively.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14451v1 Announce Type: new \nAbstract: The graph with complex annotations is the most potent data type, whose constantly evolving motivates further exploration of the unsupervised dynamic graph representation. One of the representative paradigms is graph contrastive learning. It constructs self-supervised signals by maximizing the mutual information between the statistic graph's augmentation views. However, the semantics and labels may change within the augmentation process, causing a significant performance drop in downstream tasks. This drawback becomes greatly magnified on dynamic graphs. To address this problem, we designed a simple yet effective framework named CLDG. Firstly, we elaborate that dynamic graphs have temporal translation invariance at different levels. Then, we proposed a sampling layer to extract the temporally-persistent signals. It will encourage the node to maintain consistent local and global representations, i.e., temporal translation invariance under the timespan views. The extensive experiments demonstrate the effectiveness and efficiency of the method on seven datasets by outperforming eight unsupervised state-of-the-art baselines and showing competitiveness against four semi-supervised methods. Compared with the existing dynamic graph method, the number of model parameters and training time is reduced by an average of 2,001.86 times and 130.31 times on seven datasets, respectively.""}",oai:arXiv.org:2412.14451v1,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Yiming Xu, Bin Shi, Teng Ma, Bo Dong, Haoyi Zhou, Qinghua Zheng'}]","Yiming Xu, Bin Shi, Teng Ma, Bo Dong, Haoyi Zhou, Qinghua Zheng","{'name': 'Yiming Xu, Bin Shi, Teng Ma, Bo Dong, Haoyi Zhou, Qinghua Zheng'}",10.1109/ICDE55515.2023.00059,
119,Multimodal Latent Diffusion Model for Complex Sewing Pattern Generation,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Multimodal Latent Diffusion Model for Complex Sewing Pattern Generation'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14453'}]",https://arxiv.org/abs/2412.14453,"arXiv:2412.14453v1 Announce Type: new 
Abstract: Generating sewing patterns in garment design is receiving increasing attention due to its CG-friendly and flexible-editing nature. Previous sewing pattern generation methods have been able to produce exquisite clothing, but struggle to design complex garments with detailed control. To address these issues, we propose SewingLDM, a multi-modal generative model that generates sewing patterns controlled by text prompts, body shapes, and garment sketches. Initially, we extend the original vector of sewing patterns into a more comprehensive representation to cover more intricate details and then compress them into a compact latent space. To learn the sewing pattern distribution in the latent space, we design a two-step training strategy to inject the multi-modal conditions, \ie, body shapes, text prompts, and garment sketches, into a diffusion model, ensuring the generated garments are body-suited and detail-controlled. Comprehensive qualitative and quantitative experiments show the effectiveness of our proposed method, significantly surpassing previous approaches in terms of complex garment design and various body adaptability. Our project page: https://shengqiliu1.github.io/SewingLDM.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14453v1 Announce Type: new \nAbstract: Generating sewing patterns in garment design is receiving increasing attention due to its CG-friendly and flexible-editing nature. Previous sewing pattern generation methods have been able to produce exquisite clothing, but struggle to design complex garments with detailed control. To address these issues, we propose SewingLDM, a multi-modal generative model that generates sewing patterns controlled by text prompts, body shapes, and garment sketches. Initially, we extend the original vector of sewing patterns into a more comprehensive representation to cover more intricate details and then compress them into a compact latent space. To learn the sewing pattern distribution in the latent space, we design a two-step training strategy to inject the multi-modal conditions, \\ie, body shapes, text prompts, and garment sketches, into a diffusion model, ensuring the generated garments are body-suited and detail-controlled. Comprehensive qualitative and quantitative experiments show the effectiveness of our proposed method, significantly surpassing previous approaches in terms of complex garment design and various body adaptability. Our project page: https://shengqiliu1.github.io/SewingLDM.'}",oai:arXiv.org:2412.14453v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.GR', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Shengqi Liu, Yuhao Cheng, Zhuo Chen, Xingyu Ren, Wenhan Zhu, Lincheng Li, Mengxiao Bi, Xiaokang Yang, Yichao Yan'}]","Shengqi Liu, Yuhao Cheng, Zhuo Chen, Xingyu Ren, Wenhan Zhu, Lincheng Li, Mengxiao Bi, Xiaokang Yang, Yichao Yan","{'name': 'Shengqi Liu, Yuhao Cheng, Zhuo Chen, Xingyu Ren, Wenhan Zhu, Lincheng Li, Mengxiao Bi, Xiaokang Yang, Yichao Yan'}",,
120,Are Longer Prompts Always Better? Prompt Selection in Large Language Models for Recommendation Systems,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Are Longer Prompts Always Better? Prompt Selection in Large Language Models for Recommendation Systems'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14454'}]",https://arxiv.org/abs/2412.14454,"arXiv:2412.14454v1 Announce Type: new 
Abstract: In large language models (LLM)-based recommendation systems (LLM-RSs), accurately predicting user preferences by leveraging the general knowledge of LLMs is possible without requiring extensive training data. By converting recommendation tasks into natural language inputs called prompts, LLM-RSs can efficiently solve issues that have been difficult to address due to data scarcity but are crucial in applications such as cold-start and cross-domain problems. However, when applying this in practice, selecting the prompt that matches tasks and data is essential. Although numerous prompts have been proposed in LLM-RSs and representing the target user in prompts significantly impacts recommendation accuracy, there are still no clear guidelines for selecting specific prompts.
  In this paper, we categorize and analyze prompts from previous research to establish practical prompt selection guidelines. Through 450 experiments with 90 prompts and five real-world datasets, we examined the relationship between prompts and dataset characteristics in recommendation accuracy. We found that no single prompt consistently outperforms others; thus, selecting prompts on the basis of dataset characteristics is crucial. Here, we propose a prompt selection method that achieves higher accuracy with minimal validation data. Because increasing the number of prompts to explore raises costs, we also introduce a cost-efficient strategy using high-performance and cost-efficient LLMs, significantly reducing exploration costs while maintaining high prediction accuracy. Our work offers valuable insights into the prompt selection, advancing accurate and efficient LLM-RSs.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14454v1 Announce Type: new \nAbstract: In large language models (LLM)-based recommendation systems (LLM-RSs), accurately predicting user preferences by leveraging the general knowledge of LLMs is possible without requiring extensive training data. By converting recommendation tasks into natural language inputs called prompts, LLM-RSs can efficiently solve issues that have been difficult to address due to data scarcity but are crucial in applications such as cold-start and cross-domain problems. However, when applying this in practice, selecting the prompt that matches tasks and data is essential. Although numerous prompts have been proposed in LLM-RSs and representing the target user in prompts significantly impacts recommendation accuracy, there are still no clear guidelines for selecting specific prompts.\n  In this paper, we categorize and analyze prompts from previous research to establish practical prompt selection guidelines. Through 450 experiments with 90 prompts and five real-world datasets, we examined the relationship between prompts and dataset characteristics in recommendation accuracy. We found that no single prompt consistently outperforms others; thus, selecting prompts on the basis of dataset characteristics is crucial. Here, we propose a prompt selection method that achieves higher accuracy with minimal validation data. Because increasing the number of prompts to explore raises costs, we also introduce a cost-efficient strategy using high-performance and cost-efficient LLMs, significantly reducing exploration costs while maintaining high prediction accuracy. Our work offers valuable insights into the prompt selection, advancing accurate and efficient LLM-RSs.'}",oai:arXiv.org:2412.14454v1,False,"[{'term': 'cs.IR', 'scheme': None, 'label': None}, {'term': 'cs.CL', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Genki Kusano, Kosuke Akimoto, Kunihiro Takeoka'}]","Genki Kusano, Kosuke Akimoto, Kunihiro Takeoka","{'name': 'Genki Kusano, Kosuke Akimoto, Kunihiro Takeoka'}",,
121,LEDiff: Latent Exposure Diffusion for HDR Generation,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'LEDiff: Latent Exposure Diffusion for HDR Generation'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14456'}]",https://arxiv.org/abs/2412.14456,"arXiv:2412.14456v1 Announce Type: new 
Abstract: While consumer displays increasingly support more than 10 stops of dynamic range, most image assets such as internet photographs and generative AI content remain limited to 8-bit low dynamic range (LDR), constraining their utility across high dynamic range (HDR) applications. Currently, no generative model can produce high-bit, high-dynamic range content in a generalizable way. Existing LDR-to-HDR conversion methods often struggle to produce photorealistic details and physically-plausible dynamic range in the clipped areas. We introduce LEDiff, a method that enables a generative model with HDR content generation through latent space fusion inspired by image-space exposure fusion techniques. It also functions as an LDR-to-HDR converter, expanding the dynamic range of existing low-dynamic range images. Our approach uses a small HDR dataset to enable a pretrained diffusion model to recover detail and dynamic range in clipped highlights and shadows. LEDiff brings HDR capabilities to existing generative models and converts any LDR image to HDR, creating photorealistic HDR outputs for image generation, image-based lighting (HDR environment map generation), and photographic effects such as depth of field simulation, where linear HDR data is essential for realistic quality.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14456v1 Announce Type: new \nAbstract: While consumer displays increasingly support more than 10 stops of dynamic range, most image assets such as internet photographs and generative AI content remain limited to 8-bit low dynamic range (LDR), constraining their utility across high dynamic range (HDR) applications. Currently, no generative model can produce high-bit, high-dynamic range content in a generalizable way. Existing LDR-to-HDR conversion methods often struggle to produce photorealistic details and physically-plausible dynamic range in the clipped areas. We introduce LEDiff, a method that enables a generative model with HDR content generation through latent space fusion inspired by image-space exposure fusion techniques. It also functions as an LDR-to-HDR converter, expanding the dynamic range of existing low-dynamic range images. Our approach uses a small HDR dataset to enable a pretrained diffusion model to recover detail and dynamic range in clipped highlights and shadows. LEDiff brings HDR capabilities to existing generative models and converts any LDR image to HDR, creating photorealistic HDR outputs for image generation, image-based lighting (HDR environment map generation), and photographic effects such as depth of field simulation, where linear HDR data is essential for realistic quality.'}",oai:arXiv.org:2412.14456v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'eess.IV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Chao Wang, Zhihao Xia, Thomas Leimkuehler, Karol Myszkowski, Xuaner Zhang'}]","Chao Wang, Zhihao Xia, Thomas Leimkuehler, Karol Myszkowski, Xuaner Zhang","{'name': 'Chao Wang, Zhihao Xia, Thomas Leimkuehler, Karol Myszkowski, Xuaner Zhang'}",,
122,VISA: Retrieval Augmented Generation with Visual Source Attribution,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'VISA: Retrieval Augmented Generation with Visual Source Attribution'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14457'}]",https://arxiv.org/abs/2412.14457,"arXiv:2412.14457v1 Announce Type: new 
Abstract: Generation with source attribution is important for enhancing the verifiability of retrieval-augmented generation (RAG) systems. However, existing approaches in RAG primarily link generated content to document-level references, making it challenging for users to locate evidence among multiple content-rich retrieved documents. To address this challenge, we propose Retrieval-Augmented Generation with Visual Source Attribution (VISA), a novel approach that combines answer generation with visual source attribution. Leveraging large vision-language models (VLMs), VISA identifies the evidence and highlights the exact regions that support the generated answers with bounding boxes in the retrieved document screenshots. To evaluate its effectiveness, we curated two datasets: Wiki-VISA, based on crawled Wikipedia webpage screenshots, and Paper-VISA, derived from PubLayNet and tailored to the medical domain. Experimental results demonstrate the effectiveness of VISA for visual source attribution on documents' original look, as well as highlighting the challenges for improvement. Code, data, and model checkpoints will be released.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14457v1 Announce Type: new \nAbstract: Generation with source attribution is important for enhancing the verifiability of retrieval-augmented generation (RAG) systems. However, existing approaches in RAG primarily link generated content to document-level references, making it challenging for users to locate evidence among multiple content-rich retrieved documents. To address this challenge, we propose Retrieval-Augmented Generation with Visual Source Attribution (VISA), a novel approach that combines answer generation with visual source attribution. Leveraging large vision-language models (VLMs), VISA identifies the evidence and highlights the exact regions that support the generated answers with bounding boxes in the retrieved document screenshots. To evaluate its effectiveness, we curated two datasets: Wiki-VISA, based on crawled Wikipedia webpage screenshots, and Paper-VISA, derived from PubLayNet and tailored to the medical domain. Experimental results demonstrate the effectiveness of VISA for visual source attribution on documents' original look, as well as highlighting the challenges for improvement. Code, data, and model checkpoints will be released.""}",oai:arXiv.org:2412.14457v1,False,"[{'term': 'cs.IR', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Xueguang Ma, Shengyao Zhuang, Bevan Koopman, Guido Zuccon, Wenhu Chen, Jimmy Lin'}]","Xueguang Ma, Shengyao Zhuang, Bevan Koopman, Guido Zuccon, Wenhu Chen, Jimmy Lin","{'name': 'Xueguang Ma, Shengyao Zhuang, Bevan Koopman, Guido Zuccon, Wenhu Chen, Jimmy Lin'}",,
123,A tensor-train reduced basis solver for parameterized partial differential equations,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'A tensor-train reduced basis solver for parameterized partial differential equations'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14460'}]",https://arxiv.org/abs/2412.14460,"arXiv:2412.14460v1 Announce Type: new 
Abstract: In this manuscript we present the tensor-train reduced basis method, a novel projection-based reduced-order model for the efficient solution of parameterized partial differential equations. Despite their popularity and considerable computational advantages with respect to their full order counterparts, reduced-order models are typically characterized by a considerable offline computational cost. The proposed approach addresses this issue by efficiently representing high dimensional finite element quantities with the tensor train format. This method entails numerous benefits, namely, the smaller number of operations required to compute the reduced subspaces, the cheaper hyper-reduction strategy employed to reduce the complexity of the PDE residual and Jacobian, and the decreased dimensionality of the projection subspaces for a fixed accuracy. We provide a posteriori estimates that demonstrate the accuracy of the proposed method, we test its computational performance for the heat equation and transient linear elasticity on three-dimensional Cartesian geometries.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14460v1 Announce Type: new \nAbstract: In this manuscript we present the tensor-train reduced basis method, a novel projection-based reduced-order model for the efficient solution of parameterized partial differential equations. Despite their popularity and considerable computational advantages with respect to their full order counterparts, reduced-order models are typically characterized by a considerable offline computational cost. The proposed approach addresses this issue by efficiently representing high dimensional finite element quantities with the tensor train format. This method entails numerous benefits, namely, the smaller number of operations required to compute the reduced subspaces, the cheaper hyper-reduction strategy employed to reduce the complexity of the PDE residual and Jacobian, and the decreased dimensionality of the projection subspaces for a fixed accuracy. We provide a posteriori estimates that demonstrate the accuracy of the proposed method, we test its computational performance for the heat equation and transient linear elasticity on three-dimensional Cartesian geometries.'}",oai:arXiv.org:2412.14460v1,False,"[{'term': 'math.NA', 'scheme': None, 'label': None}, {'term': 'cs.NA', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Nicholas Mueller, Yiran Zhao, Santiago Badia, Tiangang Cui'}]","Nicholas Mueller, Yiran Zhao, Santiago Badia, Tiangang Cui","{'name': 'Nicholas Mueller, Yiran Zhao, Santiago Badia, Tiangang Cui'}",,
124,From Human Annotation to LLMs: SILICON Annotation Workflow for Management Research,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'From Human Annotation to LLMs: SILICON Annotation Workflow for Management Research'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14461'}]",https://arxiv.org/abs/2412.14461,"arXiv:2412.14461v1 Announce Type: new 
Abstract: Unstructured text data annotation and analysis are fundamental to management research, often relying on human annotators through crowdsourcing platforms. While Large Language Models (LLMs) promise to provide a cost-effective and efficient alternative to human annotation, there lacks a systematic workflow that evaluate when LLMs are suitable or how to proceed with LLM-based text annotation in a reproducible manner. This paper addresses this methodological gap by introducing the ``SILICON"" (\textbf{S}ystematic \textbf{I}nference with \textbf{L}LMs for \textbf{I}nformation \textbf{C}lassificati\textbf{o}n and \textbf{N}otation) workflow. The workflow integrates established principles of human annotation with systematic prompt optimization and model selection, addressing challenges such as developing robust annotation guidelines, establishing high-quality human baselines, optimizing prompts, and ensuring reproducibility across LLMs. We validate the SILICON workflow through seven case studies covering common management research tasks, including business proposal evaluation, dialog intent and breakdown analysis, review attribute detection. Our findings highlight the importance of validating annotation guideline agreement, the superiority of expert-developed human baselines over crowdsourced ones, the iterative nature of prompt optimization, and the necessity of testing multiple LLMs. Notably, we propose a regression-based methodology to empirically compare LLM outputs across prompts and models. Our workflow advances management research by establishing reproducible processes for LLM-based annotation that maintain scientific rigor. We provide practical guidance for researchers to effectively navigate the evolving landscape of generative AI tools effectively while maintaining transparency and reproducibility.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14461v1 Announce Type: new \nAbstract: Unstructured text data annotation and analysis are fundamental to management research, often relying on human annotators through crowdsourcing platforms. While Large Language Models (LLMs) promise to provide a cost-effective and efficient alternative to human annotation, there lacks a systematic workflow that evaluate when LLMs are suitable or how to proceed with LLM-based text annotation in a reproducible manner. This paper addresses this methodological gap by introducing the ``SILICON"" (\\textbf{S}ystematic \\textbf{I}nference with \\textbf{L}LMs for \\textbf{I}nformation \\textbf{C}lassificati\\textbf{o}n and \\textbf{N}otation) workflow. The workflow integrates established principles of human annotation with systematic prompt optimization and model selection, addressing challenges such as developing robust annotation guidelines, establishing high-quality human baselines, optimizing prompts, and ensuring reproducibility across LLMs. We validate the SILICON workflow through seven case studies covering common management research tasks, including business proposal evaluation, dialog intent and breakdown analysis, review attribute detection. Our findings highlight the importance of validating annotation guideline agreement, the superiority of expert-developed human baselines over crowdsourced ones, the iterative nature of prompt optimization, and the necessity of testing multiple LLMs. Notably, we propose a regression-based methodology to empirically compare LLM outputs across prompts and models. Our workflow advances management research by establishing reproducible processes for LLM-based annotation that maintain scientific rigor. We provide practical guidance for researchers to effectively navigate the evolving landscape of generative AI tools effectively while maintaining transparency and reproducibility.'}",oai:arXiv.org:2412.14461v1,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by-sa/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-sa/4.0/'}","[{'name': 'Xiang Cheng, Raveesh Mayya, Jo\\~ao Sedoc'}]","Xiang Cheng, Raveesh Mayya, Jo\~ao Sedoc","{'name': 'Xiang Cheng, Raveesh Mayya, Jo\\~ao Sedoc'}",,
125,Affordance-Aware Object Insertion via Mask-Aware Dual Diffusion,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Affordance-Aware Object Insertion via Mask-Aware Dual Diffusion'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14462'}]",https://arxiv.org/abs/2412.14462,"arXiv:2412.14462v1 Announce Type: new 
Abstract: As a common image editing operation, image composition involves integrating foreground objects into background scenes. In this paper, we expand the application of the concept of Affordance from human-centered image composition tasks to a more general object-scene composition framework, addressing the complex interplay between foreground objects and background scenes. Following the principle of Affordance, we define the affordance-aware object insertion task, which aims to seamlessly insert any object into any scene with various position prompts. To address the limited data issue and incorporate this task, we constructed the SAM-FB dataset, which contains over 3 million examples across more than 3,000 object categories. Furthermore, we propose the Mask-Aware Dual Diffusion (MADD) model, which utilizes a dual-stream architecture to simultaneously denoise the RGB image and the insertion mask. By explicitly modeling the insertion mask in the diffusion process, MADD effectively facilitates the notion of affordance. Extensive experimental results show that our method outperforms the state-of-the-art methods and exhibits strong generalization performance on in-the-wild images. Please refer to our code on https://github.com/KaKituken/affordance-aware-any.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14462v1 Announce Type: new \nAbstract: As a common image editing operation, image composition involves integrating foreground objects into background scenes. In this paper, we expand the application of the concept of Affordance from human-centered image composition tasks to a more general object-scene composition framework, addressing the complex interplay between foreground objects and background scenes. Following the principle of Affordance, we define the affordance-aware object insertion task, which aims to seamlessly insert any object into any scene with various position prompts. To address the limited data issue and incorporate this task, we constructed the SAM-FB dataset, which contains over 3 million examples across more than 3,000 object categories. Furthermore, we propose the Mask-Aware Dual Diffusion (MADD) model, which utilizes a dual-stream architecture to simultaneously denoise the RGB image and the insertion mask. By explicitly modeling the insertion mask in the diffusion process, MADD effectively facilitates the notion of affordance. Extensive experimental results show that our method outperforms the state-of-the-art methods and exhibits strong generalization performance on in-the-wild images. Please refer to our code on https://github.com/KaKituken/affordance-aware-any.'}",oai:arXiv.org:2412.14462v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Jixuan He, Wanhua Li, Ye Liu, Junsik Kim, Donglai Wei, Hanspeter Pfister'}]","Jixuan He, Wanhua Li, Ye Liu, Junsik Kim, Donglai Wei, Hanspeter Pfister","{'name': 'Jixuan He, Wanhua Li, Ye Liu, Junsik Kim, Donglai Wei, Hanspeter Pfister'}",,
126,LiftRefine: Progressively Refined View Synthesis from 3D Lifting with Volume-Triplane Representations,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'LiftRefine: Progressively Refined View Synthesis from 3D Lifting with Volume-Triplane Representations'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14464'}]",https://arxiv.org/abs/2412.14464,"arXiv:2412.14464v1 Announce Type: new 
Abstract: We propose a new view synthesis method via synthesizing a 3D neural field from both single or few-view input images. To address the ill-posed nature of the image-to-3D generation problem, we devise a two-stage method that involves a reconstruction model and a diffusion model for view synthesis. Our reconstruction model first lifts one or more input images to the 3D space from a volume as the coarse-scale 3D representation followed by a tri-plane as the fine-scale 3D representation. To mitigate the ambiguity in occluded regions, our diffusion model then hallucinates missing details in the rendered images from tri-planes. We then introduce a new progressive refinement technique that iteratively applies the reconstruction and diffusion model to gradually synthesize novel views, boosting the overall quality of the 3D representations and their rendering. Empirical evaluation demonstrates the superiority of our method over state-of-the-art methods on the synthetic SRN-Car dataset, the in-the-wild CO3D dataset, and large-scale Objaverse dataset while achieving both sampling efficacy and multi-view consistency.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14464v1 Announce Type: new \nAbstract: We propose a new view synthesis method via synthesizing a 3D neural field from both single or few-view input images. To address the ill-posed nature of the image-to-3D generation problem, we devise a two-stage method that involves a reconstruction model and a diffusion model for view synthesis. Our reconstruction model first lifts one or more input images to the 3D space from a volume as the coarse-scale 3D representation followed by a tri-plane as the fine-scale 3D representation. To mitigate the ambiguity in occluded regions, our diffusion model then hallucinates missing details in the rendered images from tri-planes. We then introduce a new progressive refinement technique that iteratively applies the reconstruction and diffusion model to gradually synthesize novel views, boosting the overall quality of the 3D representations and their rendering. Empirical evaluation demonstrates the superiority of our method over state-of-the-art methods on the synthetic SRN-Car dataset, the in-the-wild CO3D dataset, and large-scale Objaverse dataset while achieving both sampling efficacy and multi-view consistency.'}",oai:arXiv.org:2412.14464v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.GR', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Tung Do, Thuan Hoang Nguyen, Anh Tuan Tran, Rang Nguyen, Binh-Son Hua'}]","Tung Do, Thuan Hoang Nguyen, Anh Tuan Tran, Rang Nguyen, Binh-Son Hua","{'name': 'Tung Do, Thuan Hoang Nguyen, Anh Tuan Tran, Rang Nguyen, Binh-Son Hua'}",,
127,DiffusionTrend: A Minimalist Approach to Virtual Fashion Try-On,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'DiffusionTrend: A Minimalist Approach to Virtual Fashion Try-On'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14465'}]",https://arxiv.org/abs/2412.14465,"arXiv:2412.14465v1 Announce Type: new 
Abstract: We introduce DiffusionTrend for virtual fashion try-on, which forgoes the need for retraining diffusion models. Using advanced diffusion models, DiffusionTrend harnesses latent information rich in prior information to capture the nuances of garment details. Throughout the diffusion denoising process, these details are seamlessly integrated into the model image generation, expertly directed by a precise garment mask crafted by a lightweight and compact CNN. Although our DiffusionTrend model initially demonstrates suboptimal metric performance, our exploratory approach offers some important advantages: (1) It circumvents resource-intensive retraining of diffusion models on large datasets. (2) It eliminates the necessity for various complex and user-unfriendly model inputs. (3) It delivers a visually compelling try-on experience, underscoring the potential of training-free diffusion model. This initial foray into the application of untrained diffusion models in virtual try-on technology potentially paves the way for further exploration and refinement in this industrially and academically valuable field.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14465v1 Announce Type: new \nAbstract: We introduce DiffusionTrend for virtual fashion try-on, which forgoes the need for retraining diffusion models. Using advanced diffusion models, DiffusionTrend harnesses latent information rich in prior information to capture the nuances of garment details. Throughout the diffusion denoising process, these details are seamlessly integrated into the model image generation, expertly directed by a precise garment mask crafted by a lightweight and compact CNN. Although our DiffusionTrend model initially demonstrates suboptimal metric performance, our exploratory approach offers some important advantages: (1) It circumvents resource-intensive retraining of diffusion models on large datasets. (2) It eliminates the necessity for various complex and user-unfriendly model inputs. (3) It delivers a visually compelling try-on experience, underscoring the potential of training-free diffusion model. This initial foray into the application of untrained diffusion models in virtual try-on technology potentially paves the way for further exploration and refinement in this industrially and academically valuable field.'}",oai:arXiv.org:2412.14465v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Wengyi Zhan, Mingbao Lin, Shuicheng Yan, Rongrong Ji'}]","Wengyi Zhan, Mingbao Lin, Shuicheng Yan, Rongrong Ji","{'name': 'Wengyi Zhan, Mingbao Lin, Shuicheng Yan, Rongrong Ji'}",,
128,Towards Provable Security in Industrial Control Systems Via Dynamic Protocol Attestation,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Towards Provable Security in Industrial Control Systems Via Dynamic Protocol Attestation'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14467'}]",https://arxiv.org/abs/2412.14467,"arXiv:2412.14467v1 Announce Type: new 
Abstract: Industrial control systems (ICSs) increasingly rely on digital technologies vulnerable to cyber attacks. Cyber attackers can infiltrate ICSs and execute malicious actions. Individually, each action seems innocuous. But taken together, they cause the system to enter an unsafe state. These attacks have resulted in dramatic consequences such as physical damage, economic loss, and environmental catastrophes. This paper introduces a methodology that restricts actions using protocols. These protocols only allow safe actions to execute. Protocols are written in a domain specific language we have embedded in an interactive theorem prover (ITP). The ITP enables formal, machine-checked proofs to ensure protocols maintain safety properties. We use dynamic attestation to ensure ICSs conform to their protocol even if an adversary compromises a component. Since protocol conformance prevents unsafe actions, the previously mentioned cyber attacks become impossible. We demonstrate the effectiveness of our methodology using an example from the Fischertechnik Industry 4.0 platform. We measure dynamic attestation's impact on latency and throughput. Our approach is a starting point for studying how to combine formal methods and protocol design to thwart attacks intended to cripple ICSs.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14467v1 Announce Type: new \nAbstract: Industrial control systems (ICSs) increasingly rely on digital technologies vulnerable to cyber attacks. Cyber attackers can infiltrate ICSs and execute malicious actions. Individually, each action seems innocuous. But taken together, they cause the system to enter an unsafe state. These attacks have resulted in dramatic consequences such as physical damage, economic loss, and environmental catastrophes. This paper introduces a methodology that restricts actions using protocols. These protocols only allow safe actions to execute. Protocols are written in a domain specific language we have embedded in an interactive theorem prover (ITP). The ITP enables formal, machine-checked proofs to ensure protocols maintain safety properties. We use dynamic attestation to ensure ICSs conform to their protocol even if an adversary compromises a component. Since protocol conformance prevents unsafe actions, the previously mentioned cyber attacks become impossible. We demonstrate the effectiveness of our methodology using an example from the Fischertechnik Industry 4.0 platform. We measure dynamic attestation's impact on latency and throughput. Our approach is a starting point for studying how to combine formal methods and protocol design to thwart attacks intended to cripple ICSs.""}",oai:arXiv.org:2412.14467v1,False,"[{'term': 'cs.CR', 'scheme': None, 'label': None}, {'term': 'cs.FL', 'scheme': None, 'label': None}, {'term': 'cs.LO', 'scheme': None, 'label': None}, {'term': 'cs.PL', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Arthur Amorim, Trevor Kann, Max Taylor, Lance Joneckis'}]","Arthur Amorim, Trevor Kann, Max Taylor, Lance Joneckis","{'name': 'Arthur Amorim, Trevor Kann, Max Taylor, Lance Joneckis'}",,
129,HashAttention: Semantic Sparsity for Faster Inference,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'HashAttention: Semantic Sparsity for Faster Inference'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14468'}]",https://arxiv.org/abs/2412.14468,"arXiv:2412.14468v1 Announce Type: new 
Abstract: Utilizing longer contexts is increasingly essential to power better AI systems. However, the cost of attending to long contexts is high due to the involved softmax computation. While the scaled dot-product attention (SDPA) exhibits token sparsity, with only a few pivotal tokens significantly contributing to attention, leveraging this sparsity effectively remains an open challenge. Previous methods either suffer from model degradation or require considerable additional resources. We propose HashAttention --a principled approach casting pivotal token identification as a recommendation problem. Given a query, HashAttention encodes keys and queries in Hamming space capturing the required semantic similarity using learned mapping functions. HashAttention efficiently identifies pivotal tokens for a given query in this Hamming space using bitwise operations, and only these pivotal tokens are used for attention computation, significantly improving overall attention efficiency. HashAttention can reduce the number of tokens used by a factor of $1/32\times$ for the Llama-3.1-8B model with LongBench, keeping average quality loss within 0.6 points, while using only 32 bits per token auxiliary memory. At $32\times$ sparsity, HashAttention is $3{-}6\times$ faster than LightLLM and $2.5{-}4.5\times$ faster than gpt-fast on Nvidia-L4 GPU.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14468v1 Announce Type: new \nAbstract: Utilizing longer contexts is increasingly essential to power better AI systems. However, the cost of attending to long contexts is high due to the involved softmax computation. While the scaled dot-product attention (SDPA) exhibits token sparsity, with only a few pivotal tokens significantly contributing to attention, leveraging this sparsity effectively remains an open challenge. Previous methods either suffer from model degradation or require considerable additional resources. We propose HashAttention --a principled approach casting pivotal token identification as a recommendation problem. Given a query, HashAttention encodes keys and queries in Hamming space capturing the required semantic similarity using learned mapping functions. HashAttention efficiently identifies pivotal tokens for a given query in this Hamming space using bitwise operations, and only these pivotal tokens are used for attention computation, significantly improving overall attention efficiency. HashAttention can reduce the number of tokens used by a factor of $1/32\\times$ for the Llama-3.1-8B model with LongBench, keeping average quality loss within 0.6 points, while using only 32 bits per token auxiliary memory. At $32\\times$ sparsity, HashAttention is $3{-}6\\times$ faster than LightLLM and $2.5{-}4.5\\times$ faster than gpt-fast on Nvidia-L4 GPU.'}",oai:arXiv.org:2412.14468v1,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Aditya Desai, Shuo Yang, Alejandro Cuadron, Ana Klimovic, Matei Zaharia, Joseph E. Gonzalez, Ion Stoica'}]","Aditya Desai, Shuo Yang, Alejandro Cuadron, Ana Klimovic, Matei Zaharia, Joseph E. Gonzalez, Ion Stoica","{'name': 'Aditya Desai, Shuo Yang, Alejandro Cuadron, Ana Klimovic, Matei Zaharia, Joseph E. Gonzalez, Ion Stoica'}",,
130,Who is Helping Whom? Student Concerns about AI- Teacher Collaboration in Higher Education Classrooms,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Who is Helping Whom? Student Concerns about AI- Teacher Collaboration in Higher Education Classrooms'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14469'}]",https://arxiv.org/abs/2412.14469,"arXiv:2412.14469v1 Announce Type: new 
Abstract: AI's integration into education promises to equip teachers with data-driven insights and intervene in student learning. Despite the intended advancements, there is a lack of understanding of interactions and emerging dynamics in classrooms where various stakeholders including teachers, students, and AI, collaborate. This paper aims to understand how students perceive the implications of AI in Education in terms of classroom collaborative dynamics, especially AI used to observe students and notify teachers to provide targeted help. Using the story completion method, we analyzed narratives from 65 participants, highlighting three challenges: AI decontextualizing of the educational context; AI-teacher cooperation with bias concerns and power disparities; and AI's impact on student behavior that further challenges AI's effectiveness. We argue that for effective and ethical AI-facilitated cooperative education, future AIEd design must factor in the situated nature of implementation. Designers must consider the broader nuances of the education context, impacts on multiple stakeholders, dynamics involving these stakeholders, and the interplay among potential consequences for AI systems and stakeholders. It is crucial to understand the values in the situated context, the capacity and limitations of both AI and humans for effective cooperation, and any implications to the relevant ecosystem.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14469v1 Announce Type: new \nAbstract: AI's integration into education promises to equip teachers with data-driven insights and intervene in student learning. Despite the intended advancements, there is a lack of understanding of interactions and emerging dynamics in classrooms where various stakeholders including teachers, students, and AI, collaborate. This paper aims to understand how students perceive the implications of AI in Education in terms of classroom collaborative dynamics, especially AI used to observe students and notify teachers to provide targeted help. Using the story completion method, we analyzed narratives from 65 participants, highlighting three challenges: AI decontextualizing of the educational context; AI-teacher cooperation with bias concerns and power disparities; and AI's impact on student behavior that further challenges AI's effectiveness. We argue that for effective and ethical AI-facilitated cooperative education, future AIEd design must factor in the situated nature of implementation. Designers must consider the broader nuances of the education context, impacts on multiple stakeholders, dynamics involving these stakeholders, and the interplay among potential consequences for AI systems and stakeholders. It is crucial to understand the values in the situated context, the capacity and limitations of both AI and humans for effective cooperation, and any implications to the relevant ecosystem.""}",oai:arXiv.org:2412.14469v1,False,"[{'term': 'cs.CY', 'scheme': None, 'label': None}, {'term': 'cs.HC', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by-nc-nd/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-nd/4.0/'}","[{'name': 'Bingyi Han, Simon Coghlan, George Buchanan, Dana McKay'}]","Bingyi Han, Simon Coghlan, George Buchanan, Dana McKay","{'name': 'Bingyi Han, Simon Coghlan, George Buchanan, Dana McKay'}",,
131,Agent-SafetyBench: Evaluating the Safety of LLM Agents,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Agent-SafetyBench: Evaluating the Safety of LLM Agents'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14470'}]",https://arxiv.org/abs/2412.14470,"arXiv:2412.14470v1 Announce Type: new 
Abstract: As large language models (LLMs) are increasingly deployed as agents, their integration into interactive environments and tool use introduce new safety challenges beyond those associated with the models themselves. However, the absence of comprehensive benchmarks for evaluating agent safety presents a significant barrier to effective assessment and further improvement. In this paper, we introduce Agent-SafetyBench, a comprehensive benchmark designed to evaluate the safety of LLM agents. Agent-SafetyBench encompasses 349 interaction environments and 2,000 test cases, evaluating 8 categories of safety risks and covering 10 common failure modes frequently encountered in unsafe interactions. Our evaluation of 16 popular LLM agents reveals a concerning result: none of the agents achieves a safety score above 60%. This highlights significant safety challenges in LLM agents and underscores the considerable need for improvement. Through quantitative analysis, we identify critical failure modes and summarize two fundamental safety detects in current LLM agents: lack of robustness and lack of risk awareness. Furthermore, our findings suggest that reliance on defense prompts alone is insufficient to address these safety issues, emphasizing the need for more advanced and robust strategies. We release Agent-SafetyBench at \url{https://github.com/thu-coai/Agent-SafetyBench} to facilitate further research and innovation in agent safety evaluation and improvement.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14470v1 Announce Type: new \nAbstract: As large language models (LLMs) are increasingly deployed as agents, their integration into interactive environments and tool use introduce new safety challenges beyond those associated with the models themselves. However, the absence of comprehensive benchmarks for evaluating agent safety presents a significant barrier to effective assessment and further improvement. In this paper, we introduce Agent-SafetyBench, a comprehensive benchmark designed to evaluate the safety of LLM agents. Agent-SafetyBench encompasses 349 interaction environments and 2,000 test cases, evaluating 8 categories of safety risks and covering 10 common failure modes frequently encountered in unsafe interactions. Our evaluation of 16 popular LLM agents reveals a concerning result: none of the agents achieves a safety score above 60%. This highlights significant safety challenges in LLM agents and underscores the considerable need for improvement. Through quantitative analysis, we identify critical failure modes and summarize two fundamental safety detects in current LLM agents: lack of robustness and lack of risk awareness. Furthermore, our findings suggest that reliance on defense prompts alone is insufficient to address these safety issues, emphasizing the need for more advanced and robust strategies. We release Agent-SafetyBench at \\url{https://github.com/thu-coai/Agent-SafetyBench} to facilitate further research and innovation in agent safety evaluation and improvement.'}",oai:arXiv.org:2412.14470v1,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Zhexin Zhang, Shiyao Cui, Yida Lu, Jingzhuo Zhou, Junxiao Yang, Hongning Wang, Minlie Huang'}]","Zhexin Zhang, Shiyao Cui, Yida Lu, Jingzhuo Zhou, Junxiao Yang, Hongning Wang, Minlie Huang","{'name': 'Zhexin Zhang, Shiyao Cui, Yida Lu, Jingzhuo Zhou, Junxiao Yang, Hongning Wang, Minlie Huang'}",,
132,Why We Build Local Large Language Models: An Observational Analysis from 35 Japanese and Multilingual LLMs,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Why We Build Local Large Language Models: An Observational Analysis from 35 Japanese and Multilingual LLMs'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14471'}]",https://arxiv.org/abs/2412.14471,"arXiv:2412.14471v1 Announce Type: new 
Abstract: Why do we build local large language models (LLMs)? What should a local LLM learn from the target language? Which abilities can be transferred from other languages? Do language-specific scaling laws exist? To explore these research questions, we evaluated 35 Japanese, English, and multilingual LLMs on 19 evaluation benchmarks for Japanese and English, taking Japanese as a local language. Adopting an observational approach, we analyzed correlations of benchmark scores, and conducted principal component analysis (PCA) on the scores to derive \textit{ability factors} of local LLMs. We found that training on English text can improve the scores of academic subjects in Japanese (JMMLU). In addition, it is unnecessary to specifically train on Japanese text to enhance abilities for solving Japanese code generation, arithmetic reasoning, commonsense, and reading comprehension tasks. In contrast, training on Japanese text could improve question-answering tasks about Japanese knowledge and English-Japanese translation, which indicates that abilities for solving these two tasks can be regarded as \textit{Japanese abilities} for LLMs. Furthermore, we confirmed that the Japanese abilities scale with the computational budget for Japanese text.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14471v1 Announce Type: new \nAbstract: Why do we build local large language models (LLMs)? What should a local LLM learn from the target language? Which abilities can be transferred from other languages? Do language-specific scaling laws exist? To explore these research questions, we evaluated 35 Japanese, English, and multilingual LLMs on 19 evaluation benchmarks for Japanese and English, taking Japanese as a local language. Adopting an observational approach, we analyzed correlations of benchmark scores, and conducted principal component analysis (PCA) on the scores to derive \\textit{ability factors} of local LLMs. We found that training on English text can improve the scores of academic subjects in Japanese (JMMLU). In addition, it is unnecessary to specifically train on Japanese text to enhance abilities for solving Japanese code generation, arithmetic reasoning, commonsense, and reading comprehension tasks. In contrast, training on Japanese text could improve question-answering tasks about Japanese knowledge and English-Japanese translation, which indicates that abilities for solving these two tasks can be regarded as \\textit{Japanese abilities} for LLMs. Furthermore, we confirmed that the Japanese abilities scale with the computational budget for Japanese text.'}",oai:arXiv.org:2412.14471v1,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Koshiro Saito, Sakae Mizuki, Masanari Ohi, Taishi Nakamura, Taihei Shiotani, Koki Maeda, Youmi Ma, Kakeru Hattori, Kazuki Fujii, Takumi Okamoto, Shigeki Ishida, Hiroya Takamura, Rio Yokota, Naoaki Okazaki'}]","Koshiro Saito, Sakae Mizuki, Masanari Ohi, Taishi Nakamura, Taihei Shiotani, Koki Maeda, Youmi Ma, Kakeru Hattori, Kazuki Fujii, Takumi Okamoto, Shigeki Ishida, Hiroya Takamura, Rio Yokota, Naoaki Okazaki","{'name': 'Koshiro Saito, Sakae Mizuki, Masanari Ohi, Taishi Nakamura, Taihei Shiotani, Koki Maeda, Youmi Ma, Kakeru Hattori, Kazuki Fujii, Takumi Okamoto, Shigeki Ishida, Hiroya Takamura, Rio Yokota, Naoaki Okazaki'}",,
133,Promptable Representation Distribution Learning and Data Augmentation for Gigapixel Histopathology WSI Analysis,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Promptable Representation Distribution Learning and Data Augmentation for Gigapixel Histopathology WSI Analysis'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14473'}]",https://arxiv.org/abs/2412.14473,"arXiv:2412.14473v1 Announce Type: new 
Abstract: Gigapixel image analysis, particularly for whole slide images (WSIs), often relies on multiple instance learning (MIL). Under the paradigm of MIL, patch image representations are extracted and then fixed during the training of the MIL classifiers for efficiency consideration. However, the invariance of representations makes it difficult to perform data augmentation for WSI-level model training, which significantly limits the performance of the downstream WSI analysis. The current data augmentation methods for gigapixel images either introduce additional computational costs or result in a loss of semantic information, which is hard to meet the requirements for efficiency and stability needed for WSI model training. In this paper, we propose a Promptable Representation Distribution Learning framework (PRDL) for both patch-level representation learning and WSI-level data augmentation. Meanwhile, we explore the use of prompts to guide data augmentation in feature space, which achieves promptable data augmentation for training robust WSI-level models. The experimental results have demonstrated that the proposed method stably outperforms state-of-the-art methods.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14473v1 Announce Type: new \nAbstract: Gigapixel image analysis, particularly for whole slide images (WSIs), often relies on multiple instance learning (MIL). Under the paradigm of MIL, patch image representations are extracted and then fixed during the training of the MIL classifiers for efficiency consideration. However, the invariance of representations makes it difficult to perform data augmentation for WSI-level model training, which significantly limits the performance of the downstream WSI analysis. The current data augmentation methods for gigapixel images either introduce additional computational costs or result in a loss of semantic information, which is hard to meet the requirements for efficiency and stability needed for WSI model training. In this paper, we propose a Promptable Representation Distribution Learning framework (PRDL) for both patch-level representation learning and WSI-level data augmentation. Meanwhile, we explore the use of prompts to guide data augmentation in feature space, which achieves promptable data augmentation for training robust WSI-level models. The experimental results have demonstrated that the proposed method stably outperforms state-of-the-art methods.'}",oai:arXiv.org:2412.14473v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Kunming Tang, Zhiguo Jiang, Jun Shi, Wei Wang, Haibo Wu, Yushan Zheng'}]","Kunming Tang, Zhiguo Jiang, Jun Shi, Wei Wang, Haibo Wu, Yushan Zheng","{'name': 'Kunming Tang, Zhiguo Jiang, Jun Shi, Wei Wang, Haibo Wu, Yushan Zheng'}",,
134,Benign Overfitting in Out-of-Distribution Generalization of Linear Models,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Benign Overfitting in Out-of-Distribution Generalization of Linear Models'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14474'}]",https://arxiv.org/abs/2412.14474,"arXiv:2412.14474v1 Announce Type: new 
Abstract: Benign overfitting refers to the phenomenon where an over-parameterized model fits the training data perfectly, including noise in the data, but still generalizes well to the unseen test data. While prior work provides some theoretical understanding of this phenomenon under the in-distribution setup, modern machine learning often operates in a more challenging Out-of-Distribution (OOD) regime, where the target (test) distribution can be rather different from the source (training) distribution. In this work, we take an initial step towards understanding benign overfitting in the OOD regime by focusing on the basic setup of over-parameterized linear models under covariate shift. We provide non-asymptotic guarantees proving that benign overfitting occurs in standard ridge regression, even under the OOD regime when the target covariance satisfies certain structural conditions. We identify several vital quantities relating to source and target covariance, which govern the performance of OOD generalization. Our result is sharp, which provably recovers prior in-distribution benign overfitting guarantee [Tsigler and Bartlett, 2023], as well as under-parameterized OOD guarantee [Ge et al., 2024] when specializing to each setup. Moreover, we also present theoretical results for a more general family of target covariance matrix, where standard ridge regression only achieves a slow statistical rate of $O(1/\sqrt{n})$ for the excess risk, while Principal Component Regression (PCR) is guaranteed to achieve the fast rate $O(1/n)$, where $n$ is the number of samples.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14474v1 Announce Type: new \nAbstract: Benign overfitting refers to the phenomenon where an over-parameterized model fits the training data perfectly, including noise in the data, but still generalizes well to the unseen test data. While prior work provides some theoretical understanding of this phenomenon under the in-distribution setup, modern machine learning often operates in a more challenging Out-of-Distribution (OOD) regime, where the target (test) distribution can be rather different from the source (training) distribution. In this work, we take an initial step towards understanding benign overfitting in the OOD regime by focusing on the basic setup of over-parameterized linear models under covariate shift. We provide non-asymptotic guarantees proving that benign overfitting occurs in standard ridge regression, even under the OOD regime when the target covariance satisfies certain structural conditions. We identify several vital quantities relating to source and target covariance, which govern the performance of OOD generalization. Our result is sharp, which provably recovers prior in-distribution benign overfitting guarantee [Tsigler and Bartlett, 2023], as well as under-parameterized OOD guarantee [Ge et al., 2024] when specializing to each setup. Moreover, we also present theoretical results for a more general family of target covariance matrix, where standard ridge regression only achieves a slow statistical rate of $O(1/\\sqrt{n})$ for the excess risk, while Principal Component Regression (PCR) is guaranteed to achieve the fast rate $O(1/n)$, where $n$ is the number of samples.'}",oai:arXiv.org:2412.14474v1,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'stat.ML', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Shange Tang, Jiayun Wu, Jianqing Fan, Chi Jin'}]","Shange Tang, Jiayun Wu, Jianqing Fan, Chi Jin","{'name': 'Shange Tang, Jiayun Wu, Jianqing Fan, Chi Jin'}",,
135,MegaPairs: Massive Data Synthesis For Universal Multimodal Retrieval,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'MegaPairs: Massive Data Synthesis For Universal Multimodal Retrieval'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14475'}]",https://arxiv.org/abs/2412.14475,"arXiv:2412.14475v1 Announce Type: new 
Abstract: Despite the rapidly growing demand for multimodal retrieval, progress in this field remains severely constrained by a lack of training data. In this paper, we introduce MegaPairs, a novel data synthesis method that leverages vision language models (VLMs) and open-domain images, together with a massive synthetic dataset generated from this method. Our empirical analysis shows that MegaPairs generates high-quality data, enabling the multimodal retriever to significantly outperform the baseline model trained on 70$\times$ more data from existing datasets. Moreover, since MegaPairs solely relies on general image corpora and open-source VLMs, it can be easily scaled up, enabling continuous improvements in retrieval performance. In this stage, we produced more than 26 million training instances and trained several models of varying sizes using this data. These new models achieve state-of-the-art zero-shot performance across 4 popular composed image retrieval (CIR) benchmarks and the highest overall performance on the 36 datasets provided by MMEB. They also demonstrate notable performance improvements with additional downstream fine-tuning. Our produced dataset, well-trained models, and data synthesis pipeline will be made publicly available to facilitate the future development of this field.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14475v1 Announce Type: new \nAbstract: Despite the rapidly growing demand for multimodal retrieval, progress in this field remains severely constrained by a lack of training data. In this paper, we introduce MegaPairs, a novel data synthesis method that leverages vision language models (VLMs) and open-domain images, together with a massive synthetic dataset generated from this method. Our empirical analysis shows that MegaPairs generates high-quality data, enabling the multimodal retriever to significantly outperform the baseline model trained on 70$\\times$ more data from existing datasets. Moreover, since MegaPairs solely relies on general image corpora and open-source VLMs, it can be easily scaled up, enabling continuous improvements in retrieval performance. In this stage, we produced more than 26 million training instances and trained several models of varying sizes using this data. These new models achieve state-of-the-art zero-shot performance across 4 popular composed image retrieval (CIR) benchmarks and the highest overall performance on the 36 datasets provided by MMEB. They also demonstrate notable performance improvements with additional downstream fine-tuning. Our produced dataset, well-trained models, and data synthesis pipeline will be made publicly available to facilitate the future development of this field.'}",oai:arXiv.org:2412.14475v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.CL', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Junjie Zhou, Zheng Liu, Ze Liu, Shitao Xiao, Yueze Wang, Bo Zhao, Chen Jason Zhang, Defu Lian, Yongping Xiong'}]","Junjie Zhou, Zheng Liu, Ze Liu, Shitao Xiao, Yueze Wang, Bo Zhao, Chen Jason Zhang, Defu Lian, Yongping Xiong","{'name': 'Junjie Zhou, Zheng Liu, Ze Liu, Shitao Xiao, Yueze Wang, Bo Zhao, Chen Jason Zhang, Defu Lian, Yongping Xiong'}",,
136,HEC-GCN: Hypergraph Enhanced Cascading Graph Convolution Network for Multi-Behavior Recommendation,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'HEC-GCN: Hypergraph Enhanced Cascading Graph Convolution Network for Multi-Behavior Recommendation'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14476'}]",https://arxiv.org/abs/2412.14476,"arXiv:2412.14476v1 Announce Type: new 
Abstract: Multi-behavior recommendation (MBR) has garnered growing attention recently due to its ability to mitigate the sparsity issue by inferring user preferences from various auxiliary behaviors to improve predictions for the target behavior. Although existing research on MBR has yielded impressive results, they still face two major limitations. First, previous methods mainly focus on modeling fine-grained interaction information between users and items under each behavior, which may suffer from sparsity issue. Second, existing models usually concentrate on exploiting dependencies between two consecutive behaviors, leaving intra- and inter-behavior consistency largely unexplored. To the end, we propose a novel approach named Hypergraph Enhanced Cascading Graph Convolution Network for multi-behavior recommendation (HEC-GCN). To be specific, we first explore both fine- and coarse-grained correlations among users or items of each behavior by simultaneously modeling the behavior-specific interaction graph and its corresponding hypergraph in a cascaded manner. Then, we propose a behavior consistency-guided alignment strategy that ensures consistent representations between the interaction graph and its associated hypergraph for each behavior, while also maintaining representation consistency across different behaviors. Extensive experiments and analyses on three public benchmark datasets demonstrate that our proposed approach is consistently superior to previous state-of-the-art methods due to its capability to effectively attenuate the sparsity issue as well as preserve both intra- and inter-behavior consistencies. The code is available at https://github.com/marqu22/HEC-GCN.git.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14476v1 Announce Type: new \nAbstract: Multi-behavior recommendation (MBR) has garnered growing attention recently due to its ability to mitigate the sparsity issue by inferring user preferences from various auxiliary behaviors to improve predictions for the target behavior. Although existing research on MBR has yielded impressive results, they still face two major limitations. First, previous methods mainly focus on modeling fine-grained interaction information between users and items under each behavior, which may suffer from sparsity issue. Second, existing models usually concentrate on exploiting dependencies between two consecutive behaviors, leaving intra- and inter-behavior consistency largely unexplored. To the end, we propose a novel approach named Hypergraph Enhanced Cascading Graph Convolution Network for multi-behavior recommendation (HEC-GCN). To be specific, we first explore both fine- and coarse-grained correlations among users or items of each behavior by simultaneously modeling the behavior-specific interaction graph and its corresponding hypergraph in a cascaded manner. Then, we propose a behavior consistency-guided alignment strategy that ensures consistent representations between the interaction graph and its associated hypergraph for each behavior, while also maintaining representation consistency across different behaviors. Extensive experiments and analyses on three public benchmark datasets demonstrate that our proposed approach is consistently superior to previous state-of-the-art methods due to its capability to effectively attenuate the sparsity issue as well as preserve both intra- and inter-behavior consistencies. The code is available at https://github.com/marqu22/HEC-GCN.git.'}",oai:arXiv.org:2412.14476v1,False,"[{'term': 'cs.IR', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Yabo Yin, Xiaofei Zhu, Wenshan Wang, Yihao Zhang, Pengfei Wang, Yixing Fan, Jiafeng Guo'}]","Yabo Yin, Xiaofei Zhu, Wenshan Wang, Yihao Zhang, Pengfei Wang, Yixing Fan, Jiafeng Guo","{'name': 'Yabo Yin, Xiaofei Zhu, Wenshan Wang, Yihao Zhang, Pengfei Wang, Yixing Fan, Jiafeng Guo'}",,
137,Graph-Structured Topic Modeling for Documents with Spatial or Covariate Dependencies,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Graph-Structured Topic Modeling for Documents with Spatial or Covariate Dependencies'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14477'}]",https://arxiv.org/abs/2412.14477,"arXiv:2412.14477v1 Announce Type: new 
Abstract: We address the challenge of incorporating document-level metadata into topic modeling to improve topic mixture estimation. To overcome the computational complexity and lack of theoretical guarantees in existing Bayesian methods, we extend probabilistic latent semantic indexing (pLSI), a frequentist framework for topic modeling, by incorporating document-level covariates or known similarities between documents through a graph formalism. Modeling documents as nodes and edges denoting similarities, we propose a new estimator based on a fast graph-regularized iterative singular value decomposition (SVD) that encourages similar documents to share similar topic mixture proportions. We characterize the estimation error of our proposed method by deriving high-probability bounds and develop a specialized cross-validation method to optimize our regularization parameters. We validate our model through comprehensive experiments on synthetic datasets and three real-world corpora, demonstrating improved performance and faster inference compared to existing Bayesian methods.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14477v1 Announce Type: new \nAbstract: We address the challenge of incorporating document-level metadata into topic modeling to improve topic mixture estimation. To overcome the computational complexity and lack of theoretical guarantees in existing Bayesian methods, we extend probabilistic latent semantic indexing (pLSI), a frequentist framework for topic modeling, by incorporating document-level covariates or known similarities between documents through a graph formalism. Modeling documents as nodes and edges denoting similarities, we propose a new estimator based on a fast graph-regularized iterative singular value decomposition (SVD) that encourages similar documents to share similar topic mixture proportions. We characterize the estimation error of our proposed method by deriving high-probability bounds and develop a specialized cross-validation method to optimize our regularization parameters. We validate our model through comprehensive experiments on synthetic datasets and three real-world corpora, demonstrating improved performance and faster inference compared to existing Bayesian methods.'}",oai:arXiv.org:2412.14477v1,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'stat.ME', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Yeo Jin Jung, Claire Donnat'}]","Yeo Jin Jung, Claire Donnat","{'name': 'Yeo Jin Jung, Claire Donnat'}",,
138,Frenzy: A Memory-Aware Serverless LLM Training System for Heterogeneous GPU Clusters,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Frenzy: A Memory-Aware Serverless LLM Training System for Heterogeneous GPU Clusters'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14479'}]",https://arxiv.org/abs/2412.14479,"arXiv:2412.14479v1 Announce Type: new 
Abstract: Existing work only effective on a given number of GPUs, often neglecting the complexities involved in manually determining the specific types and quantities of GPUs needed, which can be a significant burden for developers. To address this issue, we propose Frenzy, a memory-aware serverless computing method for heterogeneous GPU clusters. Frenzy allows users to submit models without worrying about underlying hardware resources. First, Frenzy predicts the required number and type of GPUs by estimating the GPU memory usage of the LLM. Then, it employs a low-overhead heterogeneity-aware scheduling method to optimize training efficiency. We validated Frenzy's performance by conducting multi-task LLM training tests on a heterogeneous GPU cluster with three different GPU types. The results show that Frenzy's memory usage prediction accuracy exceeds 92\%, the scheduling overhead is reduced by 10 times, and it reduces the average job completion time by 12\% to 18\% compared to state-of-the-art methods.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14479v1 Announce Type: new \nAbstract: Existing work only effective on a given number of GPUs, often neglecting the complexities involved in manually determining the specific types and quantities of GPUs needed, which can be a significant burden for developers. To address this issue, we propose Frenzy, a memory-aware serverless computing method for heterogeneous GPU clusters. Frenzy allows users to submit models without worrying about underlying hardware resources. First, Frenzy predicts the required number and type of GPUs by estimating the GPU memory usage of the LLM. Then, it employs a low-overhead heterogeneity-aware scheduling method to optimize training efficiency. We validated Frenzy's performance by conducting multi-task LLM training tests on a heterogeneous GPU cluster with three different GPU types. The results show that Frenzy's memory usage prediction accuracy exceeds 92\\%, the scheduling overhead is reduced by 10 times, and it reduces the average job completion time by 12\\% to 18\\% compared to state-of-the-art methods.""}",oai:arXiv.org:2412.14479v1,False,"[{'term': 'cs.DC', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Zihan Chang, Sheng Xiao, Shuibing He, Siling Yang, Zhe Pan, Dong Li'}]","Zihan Chang, Sheng Xiao, Shuibing He, Siling Yang, Zhe Pan, Dong Li","{'name': 'Zihan Chang, Sheng Xiao, Shuibing He, Siling Yang, Zhe Pan, Dong Li'}",,
139,GraphEQA: Using 3D Semantic Scene Graphs for Real-time Embodied Question Answering,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'GraphEQA: Using 3D Semantic Scene Graphs for Real-time Embodied Question Answering'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14480'}]",https://arxiv.org/abs/2412.14480,"arXiv:2412.14480v1 Announce Type: new 
Abstract: In Embodied Question Answering (EQA), agents must explore and develop a semantic understanding of an unseen environment in order to answer a situated question with confidence. This remains a challenging problem in robotics, due to the difficulties in obtaining useful semantic representations, updating these representations online, and leveraging prior world knowledge for efficient exploration and planning. Aiming to address these limitations, we propose GraphEQA, a novel approach that utilizes real-time 3D metric-semantic scene graphs (3DSGs) and task relevant images as multi-modal memory for grounding Vision-Language Models (VLMs) to perform EQA tasks in unseen environments. We employ a hierarchical planning approach that exploits the hierarchical nature of 3DSGs for structured planning and semantic-guided exploration. Through experiments in simulation on the HM-EQA dataset and in the real world in home and office environments, we demonstrate that our method outperforms key baselines by completing EQA tasks with higher success rates and fewer planning steps.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14480v1 Announce Type: new \nAbstract: In Embodied Question Answering (EQA), agents must explore and develop a semantic understanding of an unseen environment in order to answer a situated question with confidence. This remains a challenging problem in robotics, due to the difficulties in obtaining useful semantic representations, updating these representations online, and leveraging prior world knowledge for efficient exploration and planning. Aiming to address these limitations, we propose GraphEQA, a novel approach that utilizes real-time 3D metric-semantic scene graphs (3DSGs) and task relevant images as multi-modal memory for grounding Vision-Language Models (VLMs) to perform EQA tasks in unseen environments. We employ a hierarchical planning approach that exploits the hierarchical nature of 3DSGs for structured planning and semantic-guided exploration. Through experiments in simulation on the HM-EQA dataset and in the real world in home and office environments, we demonstrate that our method outperforms key baselines by completing EQA tasks with higher success rates and fewer planning steps.'}",oai:arXiv.org:2412.14480v1,False,"[{'term': 'cs.RO', 'scheme': None, 'label': None}, {'term': 'cs.CL', 'scheme': None, 'label': None}, {'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/publicdomain/zero/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/publicdomain/zero/1.0/'}","[{'name': 'Saumya Saxena, Blake Buchanan, Chris Paxton, Bingqing Chen, Narunas Vaskevicius, Luigi Palmieri, Jonathan Francis, Oliver Kroemer'}]","Saumya Saxena, Blake Buchanan, Chris Paxton, Bingqing Chen, Narunas Vaskevicius, Luigi Palmieri, Jonathan Francis, Oliver Kroemer","{'name': 'Saumya Saxena, Blake Buchanan, Chris Paxton, Bingqing Chen, Narunas Vaskevicius, Luigi Palmieri, Jonathan Francis, Oliver Kroemer'}",,
140,The Shape of Agency: Designing for Personal Agency in Qualitative Data Analysis,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'The Shape of Agency: Designing for Personal Agency in Qualitative Data Analysis'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14481'}]",https://arxiv.org/abs/2412.14481,"arXiv:2412.14481v1 Announce Type: new 
Abstract: Computational thematic analysis is rapidly emerging as a method of using large text corpora to understand the lived experience of people across the continuum of health care: patients, practitioners, and everyone in between. However, many qualitative researchers do not have the necessary programming skills to write machine learning code on their own, but also seek to maintain ownership, intimacy, and control over their analysis. In this work we explore the use of data visualizations to foster researcher agency and make computational thematic analysis more accessible to domain experts. We used a design science research approach to develop a datavis prototype over four phases: (1) problem comprehension, (2) specifying needs and requirements, (3) prototype development, and (4) feedback on the prototype. We show that qualitative researchers have a wide range of cognitive needs when conducting data analysis and place high importance upon choices and freedom, wanting to feel autonomy over their own research and not be replaced or hindered by AI.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14481v1 Announce Type: new \nAbstract: Computational thematic analysis is rapidly emerging as a method of using large text corpora to understand the lived experience of people across the continuum of health care: patients, practitioners, and everyone in between. However, many qualitative researchers do not have the necessary programming skills to write machine learning code on their own, but also seek to maintain ownership, intimacy, and control over their analysis. In this work we explore the use of data visualizations to foster researcher agency and make computational thematic analysis more accessible to domain experts. We used a design science research approach to develop a datavis prototype over four phases: (1) problem comprehension, (2) specifying needs and requirements, (3) prototype development, and (4) feedback on the prototype. We show that qualitative researchers have a wide range of cognitive needs when conducting data analysis and place high importance upon choices and freedom, wanting to feel autonomy over their own research and not be replaced or hindered by AI.'}",oai:arXiv.org:2412.14481v1,False,"[{'term': 'cs.HC', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Luka Ugaya Mazza, Plinio Morita, James R. Wallace'}]","Luka Ugaya Mazza, Plinio Morita, James R. Wallace","{'name': 'Luka Ugaya Mazza, Plinio Morita, James R. Wallace'}",,
141,Embedding high-resolution touch across robotic hands enables adaptive human-like grasping,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Embedding high-resolution touch across robotic hands enables adaptive human-like grasping'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14482'}]",https://arxiv.org/abs/2412.14482,"arXiv:2412.14482v1 Announce Type: new 
Abstract: Developing robotic hands that adapt to real-world dynamics remains a fundamental challenge in robotics and machine intelligence. Despite significant advances in replicating human hand kinematics and control algorithms, robotic systems still struggle to match human capabilities in dynamic environments, primarily due to inadequate tactile feedback. To bridge this gap, we present F-TAC Hand, a biomimetic hand featuring high-resolution tactile sensing (0.1mm spatial resolution) across 70% of its surface area. Through optimized hand design, we overcome traditional challenges in integrating high-resolution tactile sensors while preserving the full range of motion. The hand, powered by our generative algorithm that synthesizes human-like hand configurations, demonstrates robust grasping capabilities in dynamic real-world conditions. Extensive evaluation across 600 real-world trials demonstrates that this tactile-embodied system significantly outperforms non-tactile alternatives in complex manipulation tasks (p<0.0001). These results provide empirical evidence for the critical role of rich tactile embodiment in developing advanced robotic intelligence, offering new perspectives on the relationship between physical sensing capabilities and intelligent behavior.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14482v1 Announce Type: new \nAbstract: Developing robotic hands that adapt to real-world dynamics remains a fundamental challenge in robotics and machine intelligence. Despite significant advances in replicating human hand kinematics and control algorithms, robotic systems still struggle to match human capabilities in dynamic environments, primarily due to inadequate tactile feedback. To bridge this gap, we present F-TAC Hand, a biomimetic hand featuring high-resolution tactile sensing (0.1mm spatial resolution) across 70% of its surface area. Through optimized hand design, we overcome traditional challenges in integrating high-resolution tactile sensors while preserving the full range of motion. The hand, powered by our generative algorithm that synthesizes human-like hand configurations, demonstrates robust grasping capabilities in dynamic real-world conditions. Extensive evaluation across 600 real-world trials demonstrates that this tactile-embodied system significantly outperforms non-tactile alternatives in complex manipulation tasks (p<0.0001). These results provide empirical evidence for the critical role of rich tactile embodiment in developing advanced robotic intelligence, offering new perspectives on the relationship between physical sensing capabilities and intelligent behavior.'}",oai:arXiv.org:2412.14482v1,False,"[{'term': 'cs.RO', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Zihang Zhao, Wanlin Li, Yuyang Li, Tengyu Liu, Boren Li, Meng Wang, Kai Du, Hangxin Liu, Yixin Zhu, Qining Wang, Kaspar Althoefer, Song-Chun Zhu'}]","Zihang Zhao, Wanlin Li, Yuyang Li, Tengyu Liu, Boren Li, Meng Wang, Kai Du, Hangxin Liu, Yixin Zhu, Qining Wang, Kaspar Althoefer, Song-Chun Zhu","{'name': 'Zihang Zhao, Wanlin Li, Yuyang Li, Tengyu Liu, Boren Li, Meng Wang, Kai Du, Hangxin Liu, Yixin Zhu, Qining Wang, Kaspar Althoefer, Song-Chun Zhu'}",,
142,DirectorLLM for Human-Centric Video Generation,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'DirectorLLM for Human-Centric Video Generation'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14484'}]",https://arxiv.org/abs/2412.14484,"arXiv:2412.14484v1 Announce Type: new 
Abstract: In this paper, we introduce DirectorLLM, a novel video generation model that employs a large language model (LLM) to orchestrate human poses within videos. As foundational text-to-video models rapidly evolve, the demand for high-quality human motion and interaction grows. To address this need and enhance the authenticity of human motions, we extend the LLM from a text generator to a video director and human motion simulator. Utilizing open-source resources from Llama 3, we train the DirectorLLM to generate detailed instructional signals, such as human poses, to guide video generation. This approach offloads the simulation of human motion from the video generator to the LLM, effectively creating informative outlines for human-centric scenes. These signals are used as conditions by the video renderer, facilitating more realistic and prompt-following video generation. As an independent LLM module, it can be applied to different video renderers, including UNet and DiT, with minimal effort. Experiments on automatic evaluation benchmarks and human evaluations show that our model outperforms existing ones in generating videos with higher human motion fidelity, improved prompt faithfulness, and enhanced rendered subject naturalness.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14484v1 Announce Type: new \nAbstract: In this paper, we introduce DirectorLLM, a novel video generation model that employs a large language model (LLM) to orchestrate human poses within videos. As foundational text-to-video models rapidly evolve, the demand for high-quality human motion and interaction grows. To address this need and enhance the authenticity of human motions, we extend the LLM from a text generator to a video director and human motion simulator. Utilizing open-source resources from Llama 3, we train the DirectorLLM to generate detailed instructional signals, such as human poses, to guide video generation. This approach offloads the simulation of human motion from the video generator to the LLM, effectively creating informative outlines for human-centric scenes. These signals are used as conditions by the video renderer, facilitating more realistic and prompt-following video generation. As an independent LLM module, it can be applied to different video renderers, including UNet and DiT, with minimal effort. Experiments on automatic evaluation benchmarks and human evaluations show that our model outperforms existing ones in generating videos with higher human motion fidelity, improved prompt faithfulness, and enhanced rendered subject naturalness.'}",oai:arXiv.org:2412.14484v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Kunpeng Song, Tingbo Hou, Zecheng He, Haoyu Ma, Jialiang Wang, Animesh Sinha, Sam Tsai, Yaqiao Luo, Xiaoliang Dai, Li Chen, Xide Xia, Peizhao Zhang, Peter Vajda, Ahmed Elgammal, Felix Juefei-Xu'}]","Kunpeng Song, Tingbo Hou, Zecheng He, Haoyu Ma, Jialiang Wang, Animesh Sinha, Sam Tsai, Yaqiao Luo, Xiaoliang Dai, Li Chen, Xide Xia, Peizhao Zhang, Peter Vajda, Ahmed Elgammal, Felix Juefei-Xu","{'name': 'Kunpeng Song, Tingbo Hou, Zecheng He, Haoyu Ma, Jialiang Wang, Animesh Sinha, Sam Tsai, Yaqiao Luo, Xiaoliang Dai, Li Chen, Xide Xia, Peizhao Zhang, Peter Vajda, Ahmed Elgammal, Felix Juefei-Xu'}",,
143,Towards Projected and Incremental Pseudo-Boolean Model Counting,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Towards Projected and Incremental Pseudo-Boolean Model Counting'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14485'}]",https://arxiv.org/abs/2412.14485,"arXiv:2412.14485v1 Announce Type: new 
Abstract: Model counting is a fundamental task that involves determining the number of satisfying assignments to a logical formula, typically in conjunctive normal form (CNF). While CNF model counting has received extensive attention over recent decades, interest in Pseudo-Boolean (PB) model counting is just emerging partly due to the greater flexibility of PB formulas. As such, we observed feature gaps in existing PB counters such as a lack of support for projected and incremental settings, which could hinder adoption.
  In this work, our main contribution is the introduction of the PB model counter PBCount2, the first exact PB model counter with support for projected and incremental model counting. Our counter, PBCount2, uses our Least Occurrence Weighted Min Degree (LOW-MD) computation ordering heuristic to support projected model counting and a cache mechanism to enable incremental model counting. In our evaluations, PBCount2 completed at least 1.40x the number of benchmarks of competing methods for projected model counting and at least 1.18x of competing methods in incremental model counting.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14485v1 Announce Type: new \nAbstract: Model counting is a fundamental task that involves determining the number of satisfying assignments to a logical formula, typically in conjunctive normal form (CNF). While CNF model counting has received extensive attention over recent decades, interest in Pseudo-Boolean (PB) model counting is just emerging partly due to the greater flexibility of PB formulas. As such, we observed feature gaps in existing PB counters such as a lack of support for projected and incremental settings, which could hinder adoption.\n  In this work, our main contribution is the introduction of the PB model counter PBCount2, the first exact PB model counter with support for projected and incremental model counting. Our counter, PBCount2, uses our Least Occurrence Weighted Min Degree (LOW-MD) computation ordering heuristic to support projected model counting and a cache mechanism to enable incremental model counting. In our evaluations, PBCount2 completed at least 1.40x the number of benchmarks of competing methods for projected model counting and at least 1.18x of competing methods in incremental model counting.'}",oai:arXiv.org:2412.14485v1,False,"[{'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.LO', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Suwei Yang, Kuldeep S. Meel'}]","Suwei Yang, Kuldeep S. Meel","{'name': 'Suwei Yang, Kuldeep S. Meel'}",,
144,Moving Beyond LDA: A Comparison of Unsupervised Topic Modelling Techniques for Qualitative Data Analysis of Online Communities,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Moving Beyond LDA: A Comparison of Unsupervised Topic Modelling Techniques for Qualitative Data Analysis of Online Communities'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14486'}]",https://arxiv.org/abs/2412.14486,"arXiv:2412.14486v1 Announce Type: new 
Abstract: Social media constitutes a rich and influential source of information for qualitative researchers. Although computational techniques like topic modelling assist with managing the volume and diversity of social media content, qualitative researcher's lack of programming expertise creates a significant barrier to their adoption. In this paper we explore how BERTopic, an advanced Large Language Model (LLM)-based topic modelling technique, can support qualitative data analysis of social media. We conducted interviews and hands-on evaluations in which qualitative researchers compared topics from three modelling techniques: LDA, NMF, and BERTopic. BERTopic was favoured by 8 of 12 participants for its ability to provide detailed, coherent clusters for deeper understanding and actionable insights. Participants also prioritised topic relevance, logical organisation, and the capacity to reveal unexpected relationships within the data. Our findings underscore the potential of LLM-based techniques for supporting qualitative analysis.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14486v1 Announce Type: new \nAbstract: Social media constitutes a rich and influential source of information for qualitative researchers. Although computational techniques like topic modelling assist with managing the volume and diversity of social media content, qualitative researcher's lack of programming expertise creates a significant barrier to their adoption. In this paper we explore how BERTopic, an advanced Large Language Model (LLM)-based topic modelling technique, can support qualitative data analysis of social media. We conducted interviews and hands-on evaluations in which qualitative researchers compared topics from three modelling techniques: LDA, NMF, and BERTopic. BERTopic was favoured by 8 of 12 participants for its ability to provide detailed, coherent clusters for deeper understanding and actionable insights. Participants also prioritised topic relevance, logical organisation, and the capacity to reveal unexpected relationships within the data. Our findings underscore the potential of LLM-based techniques for supporting qualitative analysis.""}",oai:arXiv.org:2412.14486v1,False,"[{'term': 'cs.HC', 'scheme': None, 'label': None}, {'term': 'cs.IR', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Amandeep Kaur, James R. Wallace'}]","Amandeep Kaur, James R. Wallace","{'name': 'Amandeep Kaur, James R. Wallace'}",,
145,Token Preference Optimization with Self-Calibrated Visual-Anchored Rewards for Hallucination Mitigation,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Token Preference Optimization with Self-Calibrated Visual-Anchored Rewards for Hallucination Mitigation'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14487'}]",https://arxiv.org/abs/2412.14487,"arXiv:2412.14487v1 Announce Type: new 
Abstract: Direct Preference Optimization (DPO) has been demonstrated to be highly effective in mitigating hallucinations in Large Vision Language Models (LVLMs) by aligning their outputs more closely with human preferences. Despite the recent progress, existing methods suffer from two drawbacks: 1) Lack of scalable token-level rewards; and 2) Neglect of visual-anchored tokens. To this end, we propose a novel Token Preference Optimization model with self-calibrated rewards (dubbed as TPO), which adaptively attends to visual-correlated tokens without fine-grained annotations. Specifically, we introduce a token-level \emph{visual-anchored} \emph{reward} as the difference of the logistic distributions of generated tokens conditioned on the raw image and the corrupted one. In addition, to highlight the informative visual-anchored tokens, a visual-aware training objective is proposed to enhance more accurate token-level optimization. Extensive experimental results have manifested the state-of-the-art performance of the proposed TPO. For example, by building on top of LLAVA-1.5-7B, our TPO boosts the performance absolute improvement for hallucination benchmarks.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14487v1 Announce Type: new \nAbstract: Direct Preference Optimization (DPO) has been demonstrated to be highly effective in mitigating hallucinations in Large Vision Language Models (LVLMs) by aligning their outputs more closely with human preferences. Despite the recent progress, existing methods suffer from two drawbacks: 1) Lack of scalable token-level rewards; and 2) Neglect of visual-anchored tokens. To this end, we propose a novel Token Preference Optimization model with self-calibrated rewards (dubbed as TPO), which adaptively attends to visual-correlated tokens without fine-grained annotations. Specifically, we introduce a token-level \\emph{visual-anchored} \\emph{reward} as the difference of the logistic distributions of generated tokens conditioned on the raw image and the corrupted one. In addition, to highlight the informative visual-anchored tokens, a visual-aware training objective is proposed to enhance more accurate token-level optimization. Extensive experimental results have manifested the state-of-the-art performance of the proposed TPO. For example, by building on top of LLAVA-1.5-7B, our TPO boosts the performance absolute improvement for hallucination benchmarks.'}",oai:arXiv.org:2412.14487v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Jihao Gu, Yingyao Wang, Meng Cao, Pi Bu, Jun Song, Yancheng He, Shilong Li, Bo Zheng'}]","Jihao Gu, Yingyao Wang, Meng Cao, Pi Bu, Jun Song, Yancheng He, Shilong Li, Bo Zheng","{'name': 'Jihao Gu, Yingyao Wang, Meng Cao, Pi Bu, Jun Song, Yancheng He, Shilong Li, Bo Zheng'}",,
146,QADM-Net: Quality-adaptive Dynamic Network for Reliable Multimodal Classification,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'QADM-Net: Quality-adaptive Dynamic Network for Reliable Multimodal Classification'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14489'}]",https://arxiv.org/abs/2412.14489,"arXiv:2412.14489v1 Announce Type: new 
Abstract: Integrating complementary information from different data modalities can yield representation with stronger expressive ability. However, data quality varies across multimodal samples, highlighting the need for learning reliable multimodal representations, especially in safety-critical applications. This paper focuses on an aspect that existing methods in this domain commonly overlook: the importance of network dynamics and adaptability in providing reliable results from diverse samples. Specifically, it highlights the model's ability to dynamically adjust its capacity and behaviour according to different samples, using the adjusted network for predicting each sample. To this end, we propose a novel framework for multimodal reliable classification termed Quality-adaptive Dynamic Multimodal Network (QADM-Net). QADM-Net first introduces a confidence-guided dynamic depths mechanism to achieve the appropriate network capacity. This mechanism adjusts the network depth according to the difficulty of each sample, which is determined by the quality of its modalities. Subsequently, we develop an informativeness-based dynamic parameters mechanism that enables QADM-Net to perform unique inference behaviour on each of the diverse samples with feature-level quality variation presented in their feature vectors. In this way, QADM-Net adequately adapts its capacity and behaviour on each sample by investigating the quality variation of samples at both modality and feature levels, thus enhancing the reliability of classification results. Experiments conducted on four datasets demonstrate that QADM-Net significantly outperforms state-of-the-art methods in classification performance and exhibits strong adaptability to data with diverse quality.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14489v1 Announce Type: new \nAbstract: Integrating complementary information from different data modalities can yield representation with stronger expressive ability. However, data quality varies across multimodal samples, highlighting the need for learning reliable multimodal representations, especially in safety-critical applications. This paper focuses on an aspect that existing methods in this domain commonly overlook: the importance of network dynamics and adaptability in providing reliable results from diverse samples. Specifically, it highlights the model's ability to dynamically adjust its capacity and behaviour according to different samples, using the adjusted network for predicting each sample. To this end, we propose a novel framework for multimodal reliable classification termed Quality-adaptive Dynamic Multimodal Network (QADM-Net). QADM-Net first introduces a confidence-guided dynamic depths mechanism to achieve the appropriate network capacity. This mechanism adjusts the network depth according to the difficulty of each sample, which is determined by the quality of its modalities. Subsequently, we develop an informativeness-based dynamic parameters mechanism that enables QADM-Net to perform unique inference behaviour on each of the diverse samples with feature-level quality variation presented in their feature vectors. In this way, QADM-Net adequately adapts its capacity and behaviour on each sample by investigating the quality variation of samples at both modality and feature levels, thus enhancing the reliability of classification results. Experiments conducted on four datasets demonstrate that QADM-Net significantly outperforms state-of-the-art methods in classification performance and exhibits strong adaptability to data with diverse quality.""}",oai:arXiv.org:2412.14489v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Shu Shen, Tong Zhang, C. L. Philip Chen'}]","Shu Shen, Tong Zhang, C. L. Philip Chen","{'name': 'Shu Shen, Tong Zhang, C. L. Philip Chen'}",,
147,MAIDS: Malicious Agent Identification-based Data Security Model for Cloud Environments,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'MAIDS: Malicious Agent Identification-based Data Security Model for Cloud Environments'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14490'}]",https://arxiv.org/abs/2412.14490,"arXiv:2412.14490v1 Announce Type: new 
Abstract: With the vigorous development of cloud computing, most organizations have shifted their data and applications to the cloud environment for storage, computation, and sharing purposes. During storage and data sharing across the participating entities, a malicious agent may gain access to outsourced data from the cloud environment. A malicious agent is an entity that deliberately breaches the data. This information accessed might be misused or revealed to unauthorized parties. Therefore, data protection and prediction of malicious agents have become a demanding task that needs to be addressed appropriately. To deal with this crucial and challenging issue, this paper presents a Malicious Agent Identification-based Data Security (MAIDS) Model which utilizes XGBoost machine learning classification algorithm for securing data allocation and communication among different participating entities in the cloud system. The proposed model explores and computes intended multiple security parameters associated with online data communication or transactions. Correspondingly, a security-focused knowledge database is produced for developing the XGBoost Classifier-based Malicious Agent Prediction (XC-MAP) unit. Unlike the existing approaches, which only identify malicious agents after data leaks, MAIDS proactively identifies malicious agents by examining their eligibility for respective data access. In this way, the model provides a comprehensive solution to safeguard crucial data from both intentional and non-intentional breaches, by granting data to authorized agents only by evaluating the agents behavior and predicting the malicious agent before granting data.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14490v1 Announce Type: new \nAbstract: With the vigorous development of cloud computing, most organizations have shifted their data and applications to the cloud environment for storage, computation, and sharing purposes. During storage and data sharing across the participating entities, a malicious agent may gain access to outsourced data from the cloud environment. A malicious agent is an entity that deliberately breaches the data. This information accessed might be misused or revealed to unauthorized parties. Therefore, data protection and prediction of malicious agents have become a demanding task that needs to be addressed appropriately. To deal with this crucial and challenging issue, this paper presents a Malicious Agent Identification-based Data Security (MAIDS) Model which utilizes XGBoost machine learning classification algorithm for securing data allocation and communication among different participating entities in the cloud system. The proposed model explores and computes intended multiple security parameters associated with online data communication or transactions. Correspondingly, a security-focused knowledge database is produced for developing the XGBoost Classifier-based Malicious Agent Prediction (XC-MAP) unit. Unlike the existing approaches, which only identify malicious agents after data leaks, MAIDS proactively identifies malicious agents by examining their eligibility for respective data access. In this way, the model provides a comprehensive solution to safeguard crucial data from both intentional and non-intentional breaches, by granting data to authorized agents only by evaluating the agents behavior and predicting the malicious agent before granting data.'}",oai:arXiv.org:2412.14490v1,False,"[{'term': 'cs.CR', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by-sa/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-sa/4.0/'}","[{'name': 'Kishu Gupta, Deepika Saxena, Rishabh Gupta, Ashutosh Kumar Singh'}]","Kishu Gupta, Deepika Saxena, Rishabh Gupta, Ashutosh Kumar Singh","{'name': 'Kishu Gupta, Deepika Saxena, Rishabh Gupta, Ashutosh Kumar Singh'}",10.1007/s10586-023-04263-9,"Cluster Comput 27, 6167 to 6184, (2024)"
148,Mediation Analysis for Probabilities of Causation,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Mediation Analysis for Probabilities of Causation'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14491'}]",https://arxiv.org/abs/2412.14491,"arXiv:2412.14491v1 Announce Type: new 
Abstract: Probabilities of causation (PoC) offer valuable insights for informed decision-making. This paper introduces novel variants of PoC-controlled direct, natural direct, and natural indirect probability of necessity and sufficiency (PNS). These metrics quantify the necessity and sufficiency of a treatment for producing an outcome, accounting for different causal pathways. We develop identification theorems for these new PoC measures, allowing for their estimation from observational data. We demonstrate the practical application of our results through an analysis of a real-world psychology dataset.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14491v1 Announce Type: new \nAbstract: Probabilities of causation (PoC) offer valuable insights for informed decision-making. This paper introduces novel variants of PoC-controlled direct, natural direct, and natural indirect probability of necessity and sufficiency (PNS). These metrics quantify the necessity and sufficiency of a treatment for producing an outcome, accounting for different causal pathways. We develop identification theorems for these new PoC measures, allowing for their estimation from observational data. We demonstrate the practical application of our results through an analysis of a real-world psychology dataset.'}",oai:arXiv.org:2412.14491v1,False,"[{'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Yuta Kawakami, Jin Tian'}]","Yuta Kawakami, Jin Tian","{'name': 'Yuta Kawakami, Jin Tian'}",,
149,FaultExplainer: Leveraging Large Language Models for Interpretable Fault Detection and Diagnosis,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'FaultExplainer: Leveraging Large Language Models for Interpretable Fault Detection and Diagnosis'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14492'}]",https://arxiv.org/abs/2412.14492,"arXiv:2412.14492v1 Announce Type: new 
Abstract: Machine learning algorithms are increasingly being applied to fault detection and diagnosis (FDD) in chemical processes. However, existing data-driven FDD platforms often lack interpretability for process operators and struggle to identify root causes of previously unseen faults. This paper presents FaultExplainer, an interactive tool designed to improve fault detection, diagnosis, and explanation in the Tennessee Eastman Process (TEP). FaultExplainer integrates real-time sensor data visualization, Principal Component Analysis (PCA)-based fault detection, and identification of top contributing variables within an interactive user interface powered by large language models (LLMs). We evaluate the LLMs' reasoning capabilities in two scenarios: one where historical root causes are provided, and one where they are not to mimic the challenge of previously unseen faults. Experimental results using GPT-4o and o1-preview models demonstrate the system's strengths in generating plausible and actionable explanations, while also highlighting its limitations, including reliance on PCA-selected features and occasional hallucinations.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14492v1 Announce Type: new \nAbstract: Machine learning algorithms are increasingly being applied to fault detection and diagnosis (FDD) in chemical processes. However, existing data-driven FDD platforms often lack interpretability for process operators and struggle to identify root causes of previously unseen faults. This paper presents FaultExplainer, an interactive tool designed to improve fault detection, diagnosis, and explanation in the Tennessee Eastman Process (TEP). FaultExplainer integrates real-time sensor data visualization, Principal Component Analysis (PCA)-based fault detection, and identification of top contributing variables within an interactive user interface powered by large language models (LLMs). We evaluate the LLMs' reasoning capabilities in two scenarios: one where historical root causes are provided, and one where they are not to mimic the challenge of previously unseen faults. Experimental results using GPT-4o and o1-preview models demonstrate the system's strengths in generating plausible and actionable explanations, while also highlighting its limitations, including reliance on PCA-selected features and occasional hallucinations.""}",oai:arXiv.org:2412.14492v1,False,"[{'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'cs.SY', 'scheme': None, 'label': None}, {'term': 'eess.SY', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by-nc-nd/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-nd/4.0/'}","[{'name': 'Abdullah Khan, Rahul Nahar, Hao Chen, Gonzalo E. Constante Flores, Can Li'}]","Abdullah Khan, Rahul Nahar, Hao Chen, Gonzalo E. Constante Flores, Can Li","{'name': 'Abdullah Khan, Rahul Nahar, Hao Chen, Gonzalo E. Constante Flores, Can Li'}",,
150,Drive-1-to-3: Enriching Diffusion Priors for Novel View Synthesis of Real Vehicles,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Drive-1-to-3: Enriching Diffusion Priors for Novel View Synthesis of Real Vehicles'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14494'}]",https://arxiv.org/abs/2412.14494,"arXiv:2412.14494v1 Announce Type: new 
Abstract: The recent advent of large-scale 3D data, e.g. Objaverse, has led to impressive progress in training pose-conditioned diffusion models for novel view synthesis. However, due to the synthetic nature of such 3D data, their performance drops significantly when applied to real-world images. This paper consolidates a set of good practices to finetune large pretrained models for a real-world task -- harvesting vehicle assets for autonomous driving applications. To this end, we delve into the discrepancies between the synthetic data and real driving data, then develop several strategies to account for them properly. Specifically, we start with a virtual camera rotation of real images to ensure geometric alignment with synthetic data and consistency with the pose manifold defined by pretrained models. We also identify important design choices in object-centric data curation to account for varying object distances in real driving scenes -- learn across varying object scales with fixed camera focal length. Further, we perform occlusion-aware training in latent spaces to account for ubiquitous occlusions in real data, and handle large viewpoint changes by leveraging a symmetric prior. Our insights lead to effective finetuning that results in a $68.8\%$ reduction in FID for novel view synthesis over prior arts.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14494v1 Announce Type: new \nAbstract: The recent advent of large-scale 3D data, e.g. Objaverse, has led to impressive progress in training pose-conditioned diffusion models for novel view synthesis. However, due to the synthetic nature of such 3D data, their performance drops significantly when applied to real-world images. This paper consolidates a set of good practices to finetune large pretrained models for a real-world task -- harvesting vehicle assets for autonomous driving applications. To this end, we delve into the discrepancies between the synthetic data and real driving data, then develop several strategies to account for them properly. Specifically, we start with a virtual camera rotation of real images to ensure geometric alignment with synthetic data and consistency with the pose manifold defined by pretrained models. We also identify important design choices in object-centric data curation to account for varying object distances in real driving scenes -- learn across varying object scales with fixed camera focal length. Further, we perform occlusion-aware training in latent spaces to account for ubiquitous occlusions in real data, and handle large viewpoint changes by leveraging a symmetric prior. Our insights lead to effective finetuning that results in a $68.8\\%$ reduction in FID for novel view synthesis over prior arts.'}",oai:arXiv.org:2412.14494v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Chuang Lin, Bingbing Zhuang, Shanlin Sun, Ziyu Jiang, Jianfei Cai, Manmohan Chandraker'}]","Chuang Lin, Bingbing Zhuang, Shanlin Sun, Ziyu Jiang, Jianfei Cai, Manmohan Chandraker","{'name': 'Chuang Lin, Bingbing Zhuang, Shanlin Sun, Ziyu Jiang, Jianfei Cai, Manmohan Chandraker'}",,
151,FedMUP: Federated Learning driven Malicious User Prediction Model for Secure Data Distribution in Cloud Environments,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'FedMUP: Federated Learning driven Malicious User Prediction Model for Secure Data Distribution in Cloud Environments'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14495'}]",https://arxiv.org/abs/2412.14495,"arXiv:2412.14495v1 Announce Type: new 
Abstract: Cloud computing is flourishing at a rapid pace. Significant consequences related to data security appear as a malicious user may get unauthorized access to sensitive data which may be misused, further. This raises an alarm-ringing situation to tackle the crucial issue related to data security and proactive malicious user prediction. This article proposes a Federated learning driven Malicious User Prediction Model for Secure Data Distribution in Cloud Environments (FedMUP). This approach firstly analyses user behavior to acquire multiple security risk parameters. Afterward, it employs the federated learning-driven malicious user prediction approach to reveal doubtful users, proactively. FedMUP trains the local model on their local dataset and transfers computed values rather than actual raw data to obtain an updated global model based on averaging various local versions. This updated model is shared repeatedly at regular intervals with the user for retraining to acquire a better, and more efficient model capable of predicting malicious users more precisely. Extensive experimental work and comparison of the proposed model with state-of-the-art approaches demonstrate the efficiency of the proposed work. Significant improvement is observed in the key performance indicators such as malicious user prediction accuracy, precision, recall, and f1-score up to 14.32%, 17.88%, 14.32%, and 18.35%, respectively.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14495v1 Announce Type: new \nAbstract: Cloud computing is flourishing at a rapid pace. Significant consequences related to data security appear as a malicious user may get unauthorized access to sensitive data which may be misused, further. This raises an alarm-ringing situation to tackle the crucial issue related to data security and proactive malicious user prediction. This article proposes a Federated learning driven Malicious User Prediction Model for Secure Data Distribution in Cloud Environments (FedMUP). This approach firstly analyses user behavior to acquire multiple security risk parameters. Afterward, it employs the federated learning-driven malicious user prediction approach to reveal doubtful users, proactively. FedMUP trains the local model on their local dataset and transfers computed values rather than actual raw data to obtain an updated global model based on averaging various local versions. This updated model is shared repeatedly at regular intervals with the user for retraining to acquire a better, and more efficient model capable of predicting malicious users more precisely. Extensive experimental work and comparison of the proposed model with state-of-the-art approaches demonstrate the efficiency of the proposed work. Significant improvement is observed in the key performance indicators such as malicious user prediction accuracy, precision, recall, and f1-score up to 14.32%, 17.88%, 14.32%, and 18.35%, respectively.'}",oai:arXiv.org:2412.14495v1,False,"[{'term': 'cs.CR', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by-sa/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-sa/4.0/'}","[{'name': 'Kishu Gupta, Deepika Saxena, Rishabh Gupta, Jatinder Kumar, Ashutosh Kumar Singh'}]","Kishu Gupta, Deepika Saxena, Rishabh Gupta, Jatinder Kumar, Ashutosh Kumar Singh","{'name': 'Kishu Gupta, Deepika Saxena, Rishabh Gupta, Jatinder Kumar, Ashutosh Kumar Singh'}",10.1016/j.asoc.2024.111519,"Fedmup: Federated learning driven malicious user prediction model for secure data distribution in cloud environments, Applied Soft Computing, vol. 157, p. 111519, 2024"
152,Content-style disentangled representation for controllable artistic image stylization and generation,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Content-style disentangled representation for controllable artistic image stylization and generation'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14496'}]",https://arxiv.org/abs/2412.14496,"arXiv:2412.14496v1 Announce Type: new 
Abstract: Controllable artistic image stylization and generation aims to render the content provided by text or image with the learned artistic style, where content and style decoupling is the key to achieve satisfactory results. However, current methods for content and style disentanglement primarily rely on image information for supervision, which leads to two problems: 1) models can only support one modality for style or content input;2) incomplete disentanglement resulting in semantic interference from the reference image. To address the above issues, this paper proposes a content-style representation disentangling method for controllable artistic image stylization and generation. We construct a WikiStyle+ dataset consists of artworks with corresponding textual descriptions for style and content. Based on the multimodal dataset, we propose a disentangled content and style representations guided diffusion model. The disentangled representations are first learned by Q-Formers and then injected into a pre-trained diffusion model using learnable multi-step cross-attention layers for better controllable stylization. This approach allows model to accommodate inputs from different modalities. Experimental results show that our method achieves a thorough disentanglement of content and style in reference images under multimodal supervision, thereby enabling a harmonious integration of content and style in the generated outputs, successfully producing style-consistent and expressive stylized images.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14496v1 Announce Type: new \nAbstract: Controllable artistic image stylization and generation aims to render the content provided by text or image with the learned artistic style, where content and style decoupling is the key to achieve satisfactory results. However, current methods for content and style disentanglement primarily rely on image information for supervision, which leads to two problems: 1) models can only support one modality for style or content input;2) incomplete disentanglement resulting in semantic interference from the reference image. To address the above issues, this paper proposes a content-style representation disentangling method for controllable artistic image stylization and generation. We construct a WikiStyle+ dataset consists of artworks with corresponding textual descriptions for style and content. Based on the multimodal dataset, we propose a disentangled content and style representations guided diffusion model. The disentangled representations are first learned by Q-Formers and then injected into a pre-trained diffusion model using learnable multi-step cross-attention layers for better controllable stylization. This approach allows model to accommodate inputs from different modalities. Experimental results show that our method achieves a thorough disentanglement of content and style in reference images under multimodal supervision, thereby enabling a harmonious integration of content and style in the generated outputs, successfully producing style-consistent and expressive stylized images.'}",oai:arXiv.org:2412.14496v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Ma Zhuoqi, Zhang Yixuan, You Zejun, Tian Long, Liu Xiyang'}]","Ma Zhuoqi, Zhang Yixuan, You Zejun, Tian Long, Liu Xiyang","{'name': 'Ma Zhuoqi, Zhang Yixuan, You Zejun, Tian Long, Liu Xiyang'}",,
153,Treatment Effects Estimation on Networked Observational Data using Disentangled Variational Graph Autoencoder,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Treatment Effects Estimation on Networked Observational Data using Disentangled Variational Graph Autoencoder'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14497'}]",https://arxiv.org/abs/2412.14497,"arXiv:2412.14497v1 Announce Type: new 
Abstract: Estimating individual treatment effect (ITE) from observational data has gained increasing attention across various domains, with a key challenge being the identification of latent confounders affecting both treatment and outcome. Networked observational data offer new opportunities to address this issue by utilizing network information to infer latent confounders. However, most existing approaches assume observed variables and network information serve only as proxy variables for latent confounders, which often fails in practice, as some variables influence treatment but not outcomes, and vice versa. Recent advances in disentangled representation learning, which disentangle latent factors into instrumental, confounding, and adjustment factors, have shown promise for ITE estimation. Building on this, we propose a novel disentangled variational graph autoencoder that learns disentangled factors for treatment effect estimation on networked observational data. Our graph encoder further ensures factor independence using the Hilbert-Schmidt Independence Criterion. Extensive experiments on two semi-synthetic datasets derived from real-world social networks and one synthetic dataset demonstrate that our method achieves state-of-the-art performance.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14497v1 Announce Type: new \nAbstract: Estimating individual treatment effect (ITE) from observational data has gained increasing attention across various domains, with a key challenge being the identification of latent confounders affecting both treatment and outcome. Networked observational data offer new opportunities to address this issue by utilizing network information to infer latent confounders. However, most existing approaches assume observed variables and network information serve only as proxy variables for latent confounders, which often fails in practice, as some variables influence treatment but not outcomes, and vice versa. Recent advances in disentangled representation learning, which disentangle latent factors into instrumental, confounding, and adjustment factors, have shown promise for ITE estimation. Building on this, we propose a novel disentangled variational graph autoencoder that learns disentangled factors for treatment effect estimation on networked observational data. Our graph encoder further ensures factor independence using the Hilbert-Schmidt Independence Criterion. Extensive experiments on two semi-synthetic datasets derived from real-world social networks and one synthetic dataset demonstrate that our method achieves state-of-the-art performance.'}",oai:arXiv.org:2412.14497v1,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'stat.ML', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Di Fan, Renlei Jiang, Yunhao Wen, Chuanhou Gao'}]","Di Fan, Renlei Jiang, Yunhao Wen, Chuanhou Gao","{'name': 'Di Fan, Renlei Jiang, Yunhao Wen, Chuanhou Gao'}",,
154,Guided Diffusion Model for Sensor Data Obfuscation,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Guided Diffusion Model for Sensor Data Obfuscation'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14499'}]",https://arxiv.org/abs/2412.14499,"arXiv:2412.14499v1 Announce Type: new 
Abstract: Sensor data collected by Internet of Things (IoT) devices carries detailed information about individuals in their vicinity. Sharing this data with a semi-trusted service provider may compromise the individuals' privacy, as sensitive information can be extracted by powerful machine learning models. Data obfuscation empowered by generative models is a promising approach to generate synthetic sensor data such that the useful information contained in the original data is preserved and the sensitive information is obscured. This newly generated data will then be shared with the service provider instead of the original sensor data. In this work, we propose PrivDiffuser, a novel data obfuscation technique based on a denoising diffusion model that attains a superior trade-off between data utility and privacy through effective guidance techniques. Specifically, we extract latent representations that contain information about public and private attributes from sensor data to guide the diffusion model, and impose mutual information-based regularization when learning the latent representations to alleviate the entanglement of public and private attributes, thereby increasing the effectiveness of guidance. Evaluation on three real-world datasets containing different sensing modalities reveals that PrivDiffuser yields a better privacy-utility trade-off than the state-of-the-art obfuscation model, decreasing the utility loss by up to $1.81\%$ and the privacy loss by up to $3.42\%$. Moreover, we showed that users with diverse privacy needs can use PrivDiffuser to protect their privacy without having to retrain the model.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14499v1 Announce Type: new \nAbstract: Sensor data collected by Internet of Things (IoT) devices carries detailed information about individuals in their vicinity. Sharing this data with a semi-trusted service provider may compromise the individuals' privacy, as sensitive information can be extracted by powerful machine learning models. Data obfuscation empowered by generative models is a promising approach to generate synthetic sensor data such that the useful information contained in the original data is preserved and the sensitive information is obscured. This newly generated data will then be shared with the service provider instead of the original sensor data. In this work, we propose PrivDiffuser, a novel data obfuscation technique based on a denoising diffusion model that attains a superior trade-off between data utility and privacy through effective guidance techniques. Specifically, we extract latent representations that contain information about public and private attributes from sensor data to guide the diffusion model, and impose mutual information-based regularization when learning the latent representations to alleviate the entanglement of public and private attributes, thereby increasing the effectiveness of guidance. Evaluation on three real-world datasets containing different sensing modalities reveals that PrivDiffuser yields a better privacy-utility trade-off than the state-of-the-art obfuscation model, decreasing the utility loss by up to $1.81\\%$ and the privacy loss by up to $3.42\\%$. Moreover, we showed that users with diverse privacy needs can use PrivDiffuser to protect their privacy without having to retrain the model.""}",oai:arXiv.org:2412.14499v1,False,"[{'term': 'cs.CR', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Xin Yang, Omid Ardakanian'}]","Xin Yang, Omid Ardakanian","{'name': 'Xin Yang, Omid Ardakanian'}",,
155,The Digital Ecosystem of Beliefs: does evolution favour AI over humans?,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'The Digital Ecosystem of Beliefs: does evolution favour AI over humans?'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14500'}]",https://arxiv.org/abs/2412.14500,"arXiv:2412.14500v1 Announce Type: new 
Abstract: As AI systems are integrated into social networks, there are AI safety concerns that AI-generated content may dominate the web, e.g. in popularity or impact on beliefs.To understand such questions, this paper proposes the Digital Ecosystem of Beliefs (Digico), the first evolutionary framework for controlled experimentation with multi-population interactions in simulated social networks. The framework models a population of agents which change their messaging strategies due to evolutionary updates following a Universal Darwinism approach, interact via messages, influence each other's beliefs through dynamics based on a contagion model, and maintain their beliefs through cognitive Lamarckian inheritance. Initial experiments with an abstract implementation of Digico show that: a) when AIs have faster messaging, evolution, and more influence in the recommendation algorithm, they get 80% to 95% of the views, depending on the size of the influence benefit; b) AIs designed for propaganda can typically convince 50% of humans to adopt extreme beliefs, and up to 85% when agents believe only a limited number of channels; c) a penalty for content that violates agents' beliefs reduces propaganda effectiveness by up to 8%. We further discuss implications for control (e.g. legislation) and Digico as a means of studying evolutionary principles.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14500v1 Announce Type: new \nAbstract: As AI systems are integrated into social networks, there are AI safety concerns that AI-generated content may dominate the web, e.g. in popularity or impact on beliefs.To understand such questions, this paper proposes the Digital Ecosystem of Beliefs (Digico), the first evolutionary framework for controlled experimentation with multi-population interactions in simulated social networks. The framework models a population of agents which change their messaging strategies due to evolutionary updates following a Universal Darwinism approach, interact via messages, influence each other's beliefs through dynamics based on a contagion model, and maintain their beliefs through cognitive Lamarckian inheritance. Initial experiments with an abstract implementation of Digico show that: a) when AIs have faster messaging, evolution, and more influence in the recommendation algorithm, they get 80% to 95% of the views, depending on the size of the influence benefit; b) AIs designed for propaganda can typically convince 50% of humans to adopt extreme beliefs, and up to 85% when agents believe only a limited number of channels; c) a penalty for content that violates agents' beliefs reduces propaganda effectiveness by up to 8%. We further discuss implications for control (e.g. legislation) and Digico as a means of studying evolutionary principles.""}",oai:arXiv.org:2412.14500v1,False,"[{'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.MA', 'scheme': None, 'label': None}, {'term': 'cs.NE', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'David M. Bossens, Shanshan Feng, Yew-Soon Ong'}]","David M. Bossens, Shanshan Feng, Yew-Soon Ong","{'name': 'David M. Bossens, Shanshan Feng, Yew-Soon Ong'}",,
156,Do Large Language Models Defend Inferentialist Semantics?: On the Logical Expressivism and Anti-Representationalism of LLMs,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Do Large Language Models Defend Inferentialist Semantics?: On the Logical Expressivism and Anti-Representationalism of LLMs'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14501'}]",https://arxiv.org/abs/2412.14501,"arXiv:2412.14501v1 Announce Type: new 
Abstract: The philosophy of language, which has historically been developed through an anthropocentric lens, is now being forced to move towards post-anthropocentrism due to the advent of large language models (LLMs) like ChatGPT (OpenAI), Claude (Anthropic), which are considered to possess linguistic abilities comparable to those of humans. Traditionally, LLMs have been explained through distributional semantics as their foundational semantics. However, recent research is exploring alternative foundational semantics beyond distributional semantics. This paper proposes Robert Brandom's inferentialist semantics as an suitable foundational semantics for LLMs, specifically focusing on the issue of linguistic representationalism within this post-anthropocentric trend. Here, we show that the anti-representationalism and logical expressivism of inferential semantics, as well as quasi-compositionality, are useful in interpreting the characteristics and behaviors of LLMs. Further, we propose a \emph{consensus theory of truths} for LLMs. This paper argues that the characteristics of LLMs challenge mainstream assumptions in philosophy of language, such as semantic externalism and compositionality. We believe the argument in this paper leads to a re-evaluation of anti\hyphen{}representationalist views of language, potentially leading to new developments in the philosophy of language.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14501v1 Announce Type: new \nAbstract: The philosophy of language, which has historically been developed through an anthropocentric lens, is now being forced to move towards post-anthropocentrism due to the advent of large language models (LLMs) like ChatGPT (OpenAI), Claude (Anthropic), which are considered to possess linguistic abilities comparable to those of humans. Traditionally, LLMs have been explained through distributional semantics as their foundational semantics. However, recent research is exploring alternative foundational semantics beyond distributional semantics. This paper proposes Robert Brandom's inferentialist semantics as an suitable foundational semantics for LLMs, specifically focusing on the issue of linguistic representationalism within this post-anthropocentric trend. Here, we show that the anti-representationalism and logical expressivism of inferential semantics, as well as quasi-compositionality, are useful in interpreting the characteristics and behaviors of LLMs. Further, we propose a \\emph{consensus theory of truths} for LLMs. This paper argues that the characteristics of LLMs challenge mainstream assumptions in philosophy of language, such as semantic externalism and compositionality. We believe the argument in this paper leads to a re-evaluation of anti\\hyphen{}representationalist views of language, potentially leading to new developments in the philosophy of language.""}",oai:arXiv.org:2412.14501v1,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Yuzuki Arai, Sho Tsugawa'}]","Yuzuki Arai, Sho Tsugawa","{'name': 'Yuzuki Arai, Sho Tsugawa'}",,
157,A hybrid framework for effective and efficient machine unlearning,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'A hybrid framework for effective and efficient machine unlearning'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14505'}]",https://arxiv.org/abs/2412.14505,"arXiv:2412.14505v1 Announce Type: new 
Abstract: Recently machine unlearning (MU) is proposed to remove the imprints of revoked samples from the already trained model parameters, to solve users' privacy concern. Different from the runtime expensive retraining from scratch, there exist two research lines, exact MU and approximate MU with different favorites in terms of accuracy and efficiency. In this paper, we present a novel hybrid strategy on top of them to achieve an overall success. It implements the unlearning operation with an acceptable computation cost, while simultaneously improving the accuracy as much as possible. Specifically, it runs reasonable unlearning techniques by estimating the retraining workloads caused by revocations. If the workload is lightweight, it performs retraining to derive the model parameters consistent with the accurate ones retrained from scratch. Otherwise, it outputs the unlearned model by directly modifying the current parameters, for better efficiency. In particular, to improve the accuracy in the latter case, we propose an optimized version to amend the output model with lightweight runtime penalty. We particularly study the boundary of two approaches in our frameworks to adaptively make the smart selection. Extensive experiments on real datasets validate that our proposals can improve the unlearning efficiency by 1.5$\times$ to 8$\times$ while achieving comparable accuracy.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14505v1 Announce Type: new \nAbstract: Recently machine unlearning (MU) is proposed to remove the imprints of revoked samples from the already trained model parameters, to solve users' privacy concern. Different from the runtime expensive retraining from scratch, there exist two research lines, exact MU and approximate MU with different favorites in terms of accuracy and efficiency. In this paper, we present a novel hybrid strategy on top of them to achieve an overall success. It implements the unlearning operation with an acceptable computation cost, while simultaneously improving the accuracy as much as possible. Specifically, it runs reasonable unlearning techniques by estimating the retraining workloads caused by revocations. If the workload is lightweight, it performs retraining to derive the model parameters consistent with the accurate ones retrained from scratch. Otherwise, it outputs the unlearned model by directly modifying the current parameters, for better efficiency. In particular, to improve the accuracy in the latter case, we propose an optimized version to amend the output model with lightweight runtime penalty. We particularly study the boundary of two approaches in our frameworks to adaptively make the smart selection. Extensive experiments on real datasets validate that our proposals can improve the unlearning efficiency by 1.5$\\times$ to 8$\\times$ while achieving comparable accuracy.""}",oai:arXiv.org:2412.14505v1,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Mingxin Li, Yizhen Yu, Ning Wang, Zhigang Wang, Xiaodong Wang, Haipeng Qu, Jia Xu, Shen Su, Zhichao Yin'}]","Mingxin Li, Yizhen Yu, Ning Wang, Zhigang Wang, Xiaodong Wang, Haipeng Qu, Jia Xu, Shen Su, Zhichao Yin","{'name': 'Mingxin Li, Yizhen Yu, Ning Wang, Zhigang Wang, Xiaodong Wang, Haipeng Qu, Jia Xu, Shen Su, Zhichao Yin'}",,
158,A Super-pixel-based Approach to the Stable Interpretation of Neural Networks,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'A Super-pixel-based Approach to the Stable Interpretation of Neural Networks'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14509'}]",https://arxiv.org/abs/2412.14509,"arXiv:2412.14509v1 Announce Type: new 
Abstract: Saliency maps are widely used in the computer vision community for interpreting neural network classifiers. However, due to the randomness of training samples and optimization algorithms, the resulting saliency maps suffer from a significant level of stochasticity, making it difficult for domain experts to capture the intrinsic factors that influence the neural network's decision. In this work, we propose a novel pixel partitioning strategy to boost the stability and generalizability of gradient-based saliency maps. Through both theoretical analysis and numerical experiments, we demonstrate that the grouping of pixels reduces the variance of the saliency map and improves the generalization behavior of the interpretation method. Furthermore, we propose a sensible grouping strategy based on super-pixels which cluster pixels into groups that align well with the semantic meaning of the images. We perform several numerical experiments on CIFAR-10 and ImageNet. Our empirical results suggest that the super-pixel-based interpretation maps consistently improve the stability and quality over the pixel-based saliency maps.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14509v1 Announce Type: new \nAbstract: Saliency maps are widely used in the computer vision community for interpreting neural network classifiers. However, due to the randomness of training samples and optimization algorithms, the resulting saliency maps suffer from a significant level of stochasticity, making it difficult for domain experts to capture the intrinsic factors that influence the neural network's decision. In this work, we propose a novel pixel partitioning strategy to boost the stability and generalizability of gradient-based saliency maps. Through both theoretical analysis and numerical experiments, we demonstrate that the grouping of pixels reduces the variance of the saliency map and improves the generalization behavior of the interpretation method. Furthermore, we propose a sensible grouping strategy based on super-pixels which cluster pixels into groups that align well with the semantic meaning of the images. We perform several numerical experiments on CIFAR-10 and ImageNet. Our empirical results suggest that the super-pixel-based interpretation maps consistently improve the stability and quality over the pixel-based saliency maps.""}",oai:arXiv.org:2412.14509v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Shizhan Gong, Jingwei Zhang, Qi Dou, Farzan Farnia'}]","Shizhan Gong, Jingwei Zhang, Qi Dou, Farzan Farnia","{'name': 'Shizhan Gong, Jingwei Zhang, Qi Dou, Farzan Farnia'}",,
159,PA-RAG: RAG Alignment via Multi-Perspective Preference Optimization,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'PA-RAG: RAG Alignment via Multi-Perspective Preference Optimization'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14510'}]",https://arxiv.org/abs/2412.14510,"arXiv:2412.14510v1 Announce Type: new 
Abstract: The emergence of Retrieval-augmented generation (RAG) has alleviated the issues of outdated and hallucinatory content in the generation of large language models (LLMs), yet it still reveals numerous limitations. When a general-purpose LLM serves as the RAG generator, it often suffers from inadequate response informativeness, response robustness, and citation quality. Past approaches to tackle these limitations, either by incorporating additional steps beyond generating responses or optimizing the generator through supervised fine-tuning (SFT), still failed to align with the RAG requirement thoroughly. Consequently, optimizing the RAG generator from multiple preference perspectives while maintaining its end-to-end LLM form remains a challenge. To bridge this gap, we propose Multiple Perspective Preference Alignment for Retrieval-Augmented Generation (PA-RAG), a method for optimizing the generator of RAG systems to align with RAG requirements comprehensively. Specifically, we construct high-quality instruction fine-tuning data and multi-perspective preference data by sampling varied quality responses from the generator across different prompt documents quality scenarios. Subsequently, we optimize the generator using SFT and Direct Preference Optimization (DPO). Extensive experiments conducted on four question-answer datasets across three LLMs demonstrate that PA-RAG can significantly enhance the performance of RAG generators. Our code and datasets are available at https://github.com/wujwyi/PA-RAG.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14510v1 Announce Type: new \nAbstract: The emergence of Retrieval-augmented generation (RAG) has alleviated the issues of outdated and hallucinatory content in the generation of large language models (LLMs), yet it still reveals numerous limitations. When a general-purpose LLM serves as the RAG generator, it often suffers from inadequate response informativeness, response robustness, and citation quality. Past approaches to tackle these limitations, either by incorporating additional steps beyond generating responses or optimizing the generator through supervised fine-tuning (SFT), still failed to align with the RAG requirement thoroughly. Consequently, optimizing the RAG generator from multiple preference perspectives while maintaining its end-to-end LLM form remains a challenge. To bridge this gap, we propose Multiple Perspective Preference Alignment for Retrieval-Augmented Generation (PA-RAG), a method for optimizing the generator of RAG systems to align with RAG requirements comprehensively. Specifically, we construct high-quality instruction fine-tuning data and multi-perspective preference data by sampling varied quality responses from the generator across different prompt documents quality scenarios. Subsequently, we optimize the generator using SFT and Direct Preference Optimization (DPO). Extensive experiments conducted on four question-answer datasets across three LLMs demonstrate that PA-RAG can significantly enhance the performance of RAG generators. Our code and datasets are available at https://github.com/wujwyi/PA-RAG.'}",oai:arXiv.org:2412.14510v1,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Jiayi Wu, Hengyi Cai, Lingyong Yan, Hao Sun, Xiang Li, Shuaiqiang Wang, Dawei Yin, Ming Gao'}]","Jiayi Wu, Hengyi Cai, Lingyong Yan, Hao Sun, Xiang Li, Shuaiqiang Wang, Dawei Yin, Ming Gao","{'name': 'Jiayi Wu, Hengyi Cai, Lingyong Yan, Hao Sun, Xiang Li, Shuaiqiang Wang, Dawei Yin, Ming Gao'}",,
160,High-Accuracy Model Predictive Control with Inverse Hysteresis for High-Speed Trajectory Tracking of Piezoelectric Fast Steering Mirror,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'High-Accuracy Model Predictive Control with Inverse Hysteresis for High-Speed Trajectory Tracking of Piezoelectric Fast Steering Mirror'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14511'}]",https://arxiv.org/abs/2412.14511,"arXiv:2412.14511v1 Announce Type: new 
Abstract: Piezoelectric fast steering mirrors (PFSM) are widely utilized in beam precision-pointing systems but encounter considerable challenges in achieving high-precision tracking of fast trajectories due to nonlinear hysteresis and mechanical dual-axis cross-coupling. This paper proposes a model predictive control (MPC) approach integrated with a hysteresis inverse based on the Hammerstein modeling structure of the PFSM. The MPC is designed to decouple the rate-dependent dual-axis linear components, with an augmented error integral variable introduced in the state space to eliminate steady-state errors. Moreover, proofs of zero steady-state error and disturbance rejection are provided. The hysteresis inverse model is then cascaded to compensate for the rate-independent nonlinear components. Finally, PFSM tracking experiments are conducted on step, sinusoidal, triangular, and composite trajectories. Compared to traditional model-free and existing model-based controllers, the proposed method significantly enhances tracking accuracy, demonstrating superior tracking performance and robustness to frequency variations. These results offer valuable insights for engineering applications.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14511v1 Announce Type: new \nAbstract: Piezoelectric fast steering mirrors (PFSM) are widely utilized in beam precision-pointing systems but encounter considerable challenges in achieving high-precision tracking of fast trajectories due to nonlinear hysteresis and mechanical dual-axis cross-coupling. This paper proposes a model predictive control (MPC) approach integrated with a hysteresis inverse based on the Hammerstein modeling structure of the PFSM. The MPC is designed to decouple the rate-dependent dual-axis linear components, with an augmented error integral variable introduced in the state space to eliminate steady-state errors. Moreover, proofs of zero steady-state error and disturbance rejection are provided. The hysteresis inverse model is then cascaded to compensate for the rate-independent nonlinear components. Finally, PFSM tracking experiments are conducted on step, sinusoidal, triangular, and composite trajectories. Compared to traditional model-free and existing model-based controllers, the proposed method significantly enhances tracking accuracy, demonstrating superior tracking performance and robustness to frequency variations. These results offer valuable insights for engineering applications.'}",oai:arXiv.org:2412.14511v1,False,"[{'term': 'eess.SY', 'scheme': None, 'label': None}, {'term': 'cs.SY', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Sen Yang, Xiaofeng Li'}]","Sen Yang, Xiaofeng Li","{'name': 'Sen Yang, Xiaofeng Li'}",,
161,Vulnerable Connectivity Caused by Local Communities in Spatial Networks,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Vulnerable Connectivity Caused by Local Communities in Spatial Networks'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14513'}]",https://arxiv.org/abs/2412.14513,"arXiv:2412.14513v1 Announce Type: new 
Abstract: It has been shown that randomly formed communities in topological networks reduce the robustness of connectivity. However, in spatial networks, where community structures are not random but constrained by physical and geographical factors, the effect of these structures on the robustness is unclear. This paper investigates the emergence of local communities in road and communication networks, whose nodes are located by population data of major urban areas in Japan, and connected shortly with low cost. We show that, as the strength of community increases, the spatial networks become more vulnerable to both intentional attacks and random failures. As an application point of view, this result suggests that the densely setting nodes of equipments should be avoided under short links in constructing a spatial network. These findings contribute to understanding the important relation between local communities and the robustness of connectivity against attacks and disasters especially in spatial networks.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14513v1 Announce Type: new \nAbstract: It has been shown that randomly formed communities in topological networks reduce the robustness of connectivity. However, in spatial networks, where community structures are not random but constrained by physical and geographical factors, the effect of these structures on the robustness is unclear. This paper investigates the emergence of local communities in road and communication networks, whose nodes are located by population data of major urban areas in Japan, and connected shortly with low cost. We show that, as the strength of community increases, the spatial networks become more vulnerable to both intentional attacks and random failures. As an application point of view, this result suggests that the densely setting nodes of equipments should be avoided under short links in constructing a spatial network. These findings contribute to understanding the important relation between local communities and the robustness of connectivity against attacks and disasters especially in spatial networks.'}",oai:arXiv.org:2412.14513v1,False,"[{'term': 'cs.SI', 'scheme': None, 'label': None}, {'term': 'physics.soc-ph', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Yingzhou Mou, Yukio Hayashi'}]","Yingzhou Mou, Yukio Hayashi","{'name': 'Yingzhou Mou, Yukio Hayashi'}",,
162,Relational Programming with Foundation Models,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Relational Programming with Foundation Models'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14515'}]",https://arxiv.org/abs/2412.14515,"arXiv:2412.14515v1 Announce Type: new 
Abstract: Foundation models have vast potential to enable diverse AI applications. The powerful yet incomplete nature of these models has spurred a wide range of mechanisms to augment them with capabilities such as in-context learning, information retrieval, and code interpreting. We propose Vieira, a declarative framework that unifies these mechanisms in a general solution for programming with foundation models. Vieira follows a probabilistic relational paradigm and treats foundation models as stateless functions with relational inputs and outputs. It supports neuro-symbolic applications by enabling the seamless combination of such models with logic programs, as well as complex, multi-modal applications by streamlining the composition of diverse sub-models. We implement Vieira by extending the Scallop compiler with a foreign interface that supports foundation models as plugins. We implement plugins for 12 foundation models including GPT, CLIP, and SAM. We evaluate Vieira on 9 challenging tasks that span language, vision, and structured and vector databases. Our evaluation shows that programs in Vieira are concise, can incorporate modern foundation models, and have comparable or better accuracy than competitive baselines.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14515v1 Announce Type: new \nAbstract: Foundation models have vast potential to enable diverse AI applications. The powerful yet incomplete nature of these models has spurred a wide range of mechanisms to augment them with capabilities such as in-context learning, information retrieval, and code interpreting. We propose Vieira, a declarative framework that unifies these mechanisms in a general solution for programming with foundation models. Vieira follows a probabilistic relational paradigm and treats foundation models as stateless functions with relational inputs and outputs. It supports neuro-symbolic applications by enabling the seamless combination of such models with logic programs, as well as complex, multi-modal applications by streamlining the composition of diverse sub-models. We implement Vieira by extending the Scallop compiler with a foreign interface that supports foundation models as plugins. We implement plugins for 12 foundation models including GPT, CLIP, and SAM. We evaluate Vieira on 9 challenging tasks that span language, vision, and structured and vector databases. Our evaluation shows that programs in Vieira are concise, can incorporate modern foundation models, and have comparable or better accuracy than competitive baselines.'}",oai:arXiv.org:2412.14515v1,False,"[{'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.PL', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by-nc-sa/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-sa/4.0/'}","[{'name': 'Ziyang Li, Jiani Huang, Jason Liu, Felix Zhu, Eric Zhao, William Dodds, Neelay Velingker, Rajeev Alur, Mayur Naik'}]","Ziyang Li, Jiani Huang, Jason Liu, Felix Zhu, Eric Zhao, William Dodds, Neelay Velingker, Rajeev Alur, Mayur Naik","{'name': 'Ziyang Li, Jiani Huang, Jason Liu, Felix Zhu, Eric Zhao, William Dodds, Neelay Velingker, Rajeev Alur, Mayur Naik'}",10.1609/aaai.v38i9.28934,
163,Cal-DPO: Calibrated Direct Preference Optimization for Language Model Alignment,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Cal-DPO: Calibrated Direct Preference Optimization for Language Model Alignment'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14516'}]",https://arxiv.org/abs/2412.14516,"arXiv:2412.14516v1 Announce Type: new 
Abstract: We study the problem of aligning large language models (LLMs) with human preference data. Contrastive preference optimization has shown promising results in aligning LLMs with available preference data by optimizing the implicit reward associated with the policy. However, the contrastive objective focuses mainly on the relative values of implicit rewards associated with two responses while ignoring their actual values, resulting in suboptimal alignment with human preferences. To address this limitation, we propose calibrated direct preference optimization (Cal-DPO), a simple yet effective algorithm. We show that substantial improvement in alignment with the given preferences can be achieved simply by calibrating the implicit reward to ensure that the learned implicit rewards are comparable in scale to the ground-truth rewards. We demonstrate the theoretical advantages of Cal-DPO over existing approaches. The results of our experiments on a variety of standard benchmarks show that Cal-DPO remarkably improves off-the-shelf methods.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14516v1 Announce Type: new \nAbstract: We study the problem of aligning large language models (LLMs) with human preference data. Contrastive preference optimization has shown promising results in aligning LLMs with available preference data by optimizing the implicit reward associated with the policy. However, the contrastive objective focuses mainly on the relative values of implicit rewards associated with two responses while ignoring their actual values, resulting in suboptimal alignment with human preferences. To address this limitation, we propose calibrated direct preference optimization (Cal-DPO), a simple yet effective algorithm. We show that substantial improvement in alignment with the given preferences can be achieved simply by calibrating the implicit reward to ensure that the learned implicit rewards are comparable in scale to the ground-truth rewards. We demonstrate the theoretical advantages of Cal-DPO over existing approaches. The results of our experiments on a variety of standard benchmarks show that Cal-DPO remarkably improves off-the-shelf methods.'}",oai:arXiv.org:2412.14516v1,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'cs.CL', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Teng Xiao, Yige Yuan, Huaisheng Zhu, Mingxiao Li, Vasant G Honavar'}]","Teng Xiao, Yige Yuan, Huaisheng Zhu, Mingxiao Li, Vasant G Honavar","{'name': 'Teng Xiao, Yige Yuan, Huaisheng Zhu, Mingxiao Li, Vasant G Honavar'}",,
164,Efficient Self-Supervised Video Hashing with Selective State Spaces,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Efficient Self-Supervised Video Hashing with Selective State Spaces'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14518'}]",https://arxiv.org/abs/2412.14518,"arXiv:2412.14518v1 Announce Type: new 
Abstract: Self-supervised video hashing (SSVH) is a practical task in video indexing and retrieval. Although Transformers are predominant in SSVH for their impressive temporal modeling capabilities, they often suffer from computational and memory inefficiencies. Drawing inspiration from Mamba, an advanced state-space model, we explore its potential in SSVH to achieve a better balance between efficacy and efficiency. We introduce S5VH, a Mamba-based video hashing model with an improved self-supervised learning paradigm. Specifically, we design bidirectional Mamba layers for both the encoder and decoder, which are effective and efficient in capturing temporal relationships thanks to the data-dependent selective scanning mechanism with linear complexity. In our learning strategy, we transform global semantics in the feature space into semantically consistent and discriminative hash centers, followed by a center alignment loss as a global learning signal. Our self-local-global (SLG) paradigm significantly improves learning efficiency, leading to faster and better convergence. Extensive experiments demonstrate S5VH's improvements over state-of-the-art methods, superior transferability, and scalable advantages in inference efficiency. Code is available at https://github.com/gimpong/AAAI25-S5VH.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14518v1 Announce Type: new \nAbstract: Self-supervised video hashing (SSVH) is a practical task in video indexing and retrieval. Although Transformers are predominant in SSVH for their impressive temporal modeling capabilities, they often suffer from computational and memory inefficiencies. Drawing inspiration from Mamba, an advanced state-space model, we explore its potential in SSVH to achieve a better balance between efficacy and efficiency. We introduce S5VH, a Mamba-based video hashing model with an improved self-supervised learning paradigm. Specifically, we design bidirectional Mamba layers for both the encoder and decoder, which are effective and efficient in capturing temporal relationships thanks to the data-dependent selective scanning mechanism with linear complexity. In our learning strategy, we transform global semantics in the feature space into semantically consistent and discriminative hash centers, followed by a center alignment loss as a global learning signal. Our self-local-global (SLG) paradigm significantly improves learning efficiency, leading to faster and better convergence. Extensive experiments demonstrate S5VH's improvements over state-of-the-art methods, superior transferability, and scalable advantages in inference efficiency. Code is available at https://github.com/gimpong/AAAI25-S5VH.""}",oai:arXiv.org:2412.14518v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.IR', 'scheme': None, 'label': None}, {'term': 'cs.MM', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Jinpeng Wang, Niu Lian, Jun Li, Yuting Wang, Yan Feng, Bin Chen, Yongbing Zhang, Shu-Tao Xia'}]","Jinpeng Wang, Niu Lian, Jun Li, Yuting Wang, Yan Feng, Bin Chen, Yongbing Zhang, Shu-Tao Xia","{'name': 'Jinpeng Wang, Niu Lian, Jun Li, Yuting Wang, Yan Feng, Bin Chen, Yongbing Zhang, Shu-Tao Xia'}",,
165,Optimizing Big Active Data Management Systems,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Optimizing Big Active Data Management Systems'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14519'}]",https://arxiv.org/abs/2412.14519,"arXiv:2412.14519v1 Announce Type: new 
Abstract: Within the dynamic world of Big Data, traditional systems typically operate in a passive mode, processing and responding to user queries by returning the requested data. However, this methodology falls short of meeting the evolving demands of users who not only wish to analyze data but also to receive proactive updates on topics of interest. To bridge this gap, Big Active Data (BAD) frameworks have been proposed to support extensive data subscriptions and analytics for millions of subscribers. As data volumes and the number of interested users continue to increase, the imperative to optimize BAD systems for enhanced scalability, performance, and efficiency becomes paramount. To this end, this paper introduces three main optimizations, namely: strategic aggregation, intelligent modifications to the query plan, and early result filtering, all aimed at reinforcing a BAD platform's capability to actively manage and efficiently process soaring rates of incoming data and distribute notifications to larger numbers of subscribers.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14519v1 Announce Type: new \nAbstract: Within the dynamic world of Big Data, traditional systems typically operate in a passive mode, processing and responding to user queries by returning the requested data. However, this methodology falls short of meeting the evolving demands of users who not only wish to analyze data but also to receive proactive updates on topics of interest. To bridge this gap, Big Active Data (BAD) frameworks have been proposed to support extensive data subscriptions and analytics for millions of subscribers. As data volumes and the number of interested users continue to increase, the imperative to optimize BAD systems for enhanced scalability, performance, and efficiency becomes paramount. To this end, this paper introduces three main optimizations, namely: strategic aggregation, intelligent modifications to the query plan, and early result filtering, all aimed at reinforcing a BAD platform's capability to actively manage and efficiently process soaring rates of incoming data and distribute notifications to larger numbers of subscribers.""}",oai:arXiv.org:2412.14519v1,False,"[{'term': 'cs.DB', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Shahrzad Haji Amin Shirazi, Xikui Wang, Michael J. Carey, Vassilis J. Tsotras'}]","Shahrzad Haji Amin Shirazi, Xikui Wang, Michael J. Carey, Vassilis J. Tsotras","{'name': 'Shahrzad Haji Amin Shirazi, Xikui Wang, Michael J. Carey, Vassilis J. Tsotras'}",,
166,Dynamic User Interface Generation for Enhanced Human-Computer Interaction Using Variational Autoencoders,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Dynamic User Interface Generation for Enhanced Human-Computer Interaction Using Variational Autoencoders'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14521'}]",https://arxiv.org/abs/2412.14521,"arXiv:2412.14521v1 Announce Type: new 
Abstract: This study presents a novel approach for intelligent user interaction interface generation and optimization, grounded in the variational autoencoder (VAE) model. With the rapid advancement of intelligent technologies, traditional interface design methods struggle to meet the evolving demands for diversity and personalization, often lacking flexibility in real-time adjustments to enhance the user experience. Human-Computer Interaction (HCI) plays a critical role in addressing these challenges by focusing on creating interfaces that are functional, intuitive, and responsive to user needs. This research leverages the RICO dataset to train the VAE model, enabling the simulation and creation of user interfaces that align with user aesthetics and interaction habits. By integrating real-time user behavior data, the system dynamically refines and optimizes the interface, improving usability and underscoring the importance of HCI in achieving a seamless user experience. Experimental findings indicate that the VAE-based approach significantly enhances the quality and precision of interface generation compared to other methods, including autoencoders (AE), generative adversarial networks (GAN), conditional GANs (cGAN), deep belief networks (DBN), and VAE-GAN. This work contributes valuable insights into HCI, providing robust technical solutions for automated interface generation and enhanced user experience optimization.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14521v1 Announce Type: new \nAbstract: This study presents a novel approach for intelligent user interaction interface generation and optimization, grounded in the variational autoencoder (VAE) model. With the rapid advancement of intelligent technologies, traditional interface design methods struggle to meet the evolving demands for diversity and personalization, often lacking flexibility in real-time adjustments to enhance the user experience. Human-Computer Interaction (HCI) plays a critical role in addressing these challenges by focusing on creating interfaces that are functional, intuitive, and responsive to user needs. This research leverages the RICO dataset to train the VAE model, enabling the simulation and creation of user interfaces that align with user aesthetics and interaction habits. By integrating real-time user behavior data, the system dynamically refines and optimizes the interface, improving usability and underscoring the importance of HCI in achieving a seamless user experience. Experimental findings indicate that the VAE-based approach significantly enhances the quality and precision of interface generation compared to other methods, including autoencoders (AE), generative adversarial networks (GAN), conditional GANs (cGAN), deep belief networks (DBN), and VAE-GAN. This work contributes valuable insights into HCI, providing robust technical solutions for automated interface generation and enhanced user experience optimization.'}",oai:arXiv.org:2412.14521v1,False,"[{'term': 'cs.HC', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Runsheng Zhang (University of Southern California), Shixiao Wang (School of Visual Arts), Tianfang Xie (Georgia Institute of Technology), Shiyu Duan (Carnegie Mellon University), Mengmeng Chen (New York University)'}]","Runsheng Zhang (University of Southern California), Shixiao Wang (School of Visual Arts), Tianfang Xie (Georgia Institute of Technology), Shiyu Duan (Carnegie Mellon University), Mengmeng Chen (New York University)","{'name': 'Runsheng Zhang (University of Southern California), Shixiao Wang (School of Visual Arts), Tianfang Xie (Georgia Institute of Technology), Shiyu Duan (Carnegie Mellon University), Mengmeng Chen (New York University)'}",,
167,CAE-T: A Channelwise AutoEncoder with Transformer for EEG Abnormality Detection,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'CAE-T: A Channelwise AutoEncoder with Transformer for EEG Abnormality Detection'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14522'}]",https://arxiv.org/abs/2412.14522,"arXiv:2412.14522v1 Announce Type: new 
Abstract: Electroencephalogram (EEG) signals are critical for detecting abnormal brain activity, but their high dimensionality and complexity pose significant challenges for effective analysis. In this paper, we propose CAE-T, a novel framework that combines a channelwise CNN-based autoencoder with a single-head transformer classifier for efficient EEG abnormality detection. The channelwise autoencoder compresses raw EEG signals while preserving channel independence, reducing computational costs and retaining biologically meaningful features. The compressed representations are then fed into the transformer-based classifier, which efficiently models long-term dependencies to distinguish between normal and abnormal signals. Evaluated on the TUH Abnormal EEG Corpus, the proposed model achieves 85.0% accuracy, 76.2% sensitivity, and 91.2% specificity at the per-case level, outperforming baseline models such as EEGNet, Deep4Conv, and FusionCNN. Furthermore, CAE-T requires only 202M FLOPs and 2.9M parameters, making it significantly more efficient than transformer-based alternatives. The framework retains interpretability through its channelwise design, demonstrating great potential for future applications in neuroscience research and clinical practice. The source code is available at https://github.com/YossiZhao/CAE-T.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14522v1 Announce Type: new \nAbstract: Electroencephalogram (EEG) signals are critical for detecting abnormal brain activity, but their high dimensionality and complexity pose significant challenges for effective analysis. In this paper, we propose CAE-T, a novel framework that combines a channelwise CNN-based autoencoder with a single-head transformer classifier for efficient EEG abnormality detection. The channelwise autoencoder compresses raw EEG signals while preserving channel independence, reducing computational costs and retaining biologically meaningful features. The compressed representations are then fed into the transformer-based classifier, which efficiently models long-term dependencies to distinguish between normal and abnormal signals. Evaluated on the TUH Abnormal EEG Corpus, the proposed model achieves 85.0% accuracy, 76.2% sensitivity, and 91.2% specificity at the per-case level, outperforming baseline models such as EEGNet, Deep4Conv, and FusionCNN. Furthermore, CAE-T requires only 202M FLOPs and 2.9M parameters, making it significantly more efficient than transformer-based alternatives. The framework retains interpretability through its channelwise design, demonstrating great potential for future applications in neuroscience research and clinical practice. The source code is available at https://github.com/YossiZhao/CAE-T.'}",oai:arXiv.org:2412.14522v1,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.NE', 'scheme': None, 'label': None}, {'term': 'eess.SP', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/publicdomain/zero/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/publicdomain/zero/1.0/'}","[{'name': 'Youshen Zhao, Keiji Iramina'}]","Youshen Zhao, Keiji Iramina","{'name': 'Youshen Zhao, Keiji Iramina'}",,
168,Knowledge Distillation in RNN-Attention Models for Early Prediction of Student Performance,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Knowledge Distillation in RNN-Attention Models for Early Prediction of Student Performance'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14526'}]",https://arxiv.org/abs/2412.14526,"arXiv:2412.14526v1 Announce Type: new 
Abstract: Educational data mining (EDM) is a part of applied computing that focuses on automatically analyzing data from learning contexts. Early prediction for identifying at-risk students is a crucial and widely researched topic in EDM research. It enables instructors to support at-risk students to stay on track, preventing student dropout or failure. Previous studies have predicted students' learning performance to identify at-risk students by using machine learning on data collected from e-learning platforms. However, most studies aimed to identify at-risk students utilizing the entire course data after the course finished. This does not correspond to the real-world scenario that at-risk students may drop out before the course ends. To address this problem, we introduce an RNN-Attention-KD (knowledge distillation) framework to predict at-risk students early throughout a course. It leverages the strengths of Recurrent Neural Networks (RNNs) in handling time-sequence data to predict students' performance at each time step and employs an attention mechanism to focus on relevant time steps for improved predictive accuracy. At the same time, KD is applied to compress the time steps to facilitate early prediction. In an empirical evaluation, RNN-Attention-KD outperforms traditional neural network models in terms of recall and F1-measure. For example, it obtained recall and F1-measure of 0.49 and 0.51 for Weeks 1--3 and 0.51 and 0.61 for Weeks 1--6 across all datasets from four years of a university course. Then, an ablation study investigated the contributions of different knowledge transfer methods (distillation objectives). We found that hint loss from the hidden layer of RNN and context vector loss from the attention module on RNN could enhance the model's prediction performance for identifying at-risk students. These results are relevant for EDM researchers employing deep learning models.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14526v1 Announce Type: new \nAbstract: Educational data mining (EDM) is a part of applied computing that focuses on automatically analyzing data from learning contexts. Early prediction for identifying at-risk students is a crucial and widely researched topic in EDM research. It enables instructors to support at-risk students to stay on track, preventing student dropout or failure. Previous studies have predicted students' learning performance to identify at-risk students by using machine learning on data collected from e-learning platforms. However, most studies aimed to identify at-risk students utilizing the entire course data after the course finished. This does not correspond to the real-world scenario that at-risk students may drop out before the course ends. To address this problem, we introduce an RNN-Attention-KD (knowledge distillation) framework to predict at-risk students early throughout a course. It leverages the strengths of Recurrent Neural Networks (RNNs) in handling time-sequence data to predict students' performance at each time step and employs an attention mechanism to focus on relevant time steps for improved predictive accuracy. At the same time, KD is applied to compress the time steps to facilitate early prediction. In an empirical evaluation, RNN-Attention-KD outperforms traditional neural network models in terms of recall and F1-measure. For example, it obtained recall and F1-measure of 0.49 and 0.51 for Weeks 1--3 and 0.51 and 0.61 for Weeks 1--6 across all datasets from four years of a university course. Then, an ablation study investigated the contributions of different knowledge transfer methods (distillation objectives). We found that hint loss from the hidden layer of RNN and context vector loss from the attention module on RNN could enhance the model's prediction performance for identifying at-risk students. These results are relevant for EDM researchers employing deep learning models.""}",oai:arXiv.org:2412.14526v1,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'cs.CY', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': ""Sukrit Leelaluk, Cheng Tang, Valdemar \\v{S}v\\'abensk\\'y, Atsushi Shimada""}]","Sukrit Leelaluk, Cheng Tang, Valdemar \v{S}v\'abensk\'y, Atsushi Shimada","{'name': ""Sukrit Leelaluk, Cheng Tang, Valdemar \\v{S}v\\'abensk\\'y, Atsushi Shimada""}",10.1145/3672608.3707805,
169,Multi-Level Optimal Transport for Universal Cross-Tokenizer Knowledge Distillation on Language Models,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Multi-Level Optimal Transport for Universal Cross-Tokenizer Knowledge Distillation on Language Models'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14528'}]",https://arxiv.org/abs/2412.14528,"arXiv:2412.14528v1 Announce Type: new 
Abstract: Knowledge distillation (KD) has become a prevalent technique for compressing large language models (LLMs). Existing KD methods are constrained by the need for identical tokenizers (i.e., vocabularies) between teacher and student models, limiting their versatility in handling LLMs of different architecture families. In this paper, we introduce the Multi-Level Optimal Transport (MultiLevelOT), a novel approach that advances the optimal transport for universal cross-tokenizer knowledge distillation. Our method aligns the logit distributions of the teacher and the student at both token and sequence levels using diverse cost matrices, eliminating the need for dimensional or token-by-token correspondence. At the token level, MultiLevelOT integrates both global and local information by jointly optimizing all tokens within a sequence to enhance robustness. At the sequence level, we efficiently capture complex distribution structures of logits via the Sinkhorn distance, which approximates the Wasserstein distance for divergence measures. Extensive experiments on tasks such as extractive QA, generative QA, and summarization demonstrate that the MultiLevelOT outperforms state-of-the-art cross-tokenizer KD methods under various settings. Our approach is robust to different student and teacher models across model families, architectures, and parameter sizes.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14528v1 Announce Type: new \nAbstract: Knowledge distillation (KD) has become a prevalent technique for compressing large language models (LLMs). Existing KD methods are constrained by the need for identical tokenizers (i.e., vocabularies) between teacher and student models, limiting their versatility in handling LLMs of different architecture families. In this paper, we introduce the Multi-Level Optimal Transport (MultiLevelOT), a novel approach that advances the optimal transport for universal cross-tokenizer knowledge distillation. Our method aligns the logit distributions of the teacher and the student at both token and sequence levels using diverse cost matrices, eliminating the need for dimensional or token-by-token correspondence. At the token level, MultiLevelOT integrates both global and local information by jointly optimizing all tokens within a sequence to enhance robustness. At the sequence level, we efficiently capture complex distribution structures of logits via the Sinkhorn distance, which approximates the Wasserstein distance for divergence measures. Extensive experiments on tasks such as extractive QA, generative QA, and summarization demonstrate that the MultiLevelOT outperforms state-of-the-art cross-tokenizer KD methods under various settings. Our approach is robust to different student and teacher models across model families, architectures, and parameter sizes.'}",oai:arXiv.org:2412.14528v1,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Xiao Cui, Mo Zhu, Yulei Qin, Liang Xie, Wengang Zhou, Houqiang Li'}]","Xiao Cui, Mo Zhu, Yulei Qin, Liang Xie, Wengang Zhou, Houqiang Li","{'name': 'Xiao Cui, Mo Zhu, Yulei Qin, Liang Xie, Wengang Zhou, Houqiang Li'}",,
170,Leveraging Time Series Categorization and Temporal Fusion Transformers to Improve Cryptocurrency Price Forecasting,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Leveraging Time Series Categorization and Temporal Fusion Transformers to Improve Cryptocurrency Price Forecasting'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14529'}]",https://arxiv.org/abs/2412.14529,"arXiv:2412.14529v1 Announce Type: new 
Abstract: Organizing and managing cryptocurrency portfolios and decision-making on transactions is crucial in this market. Optimal selection of assets is one of the main challenges that requires accurate prediction of the price of cryptocurrencies. In this work, we categorize the financial time series into several similar subseries to increase prediction accuracy by learning each subseries category with similar behavior. For each category of the subseries, we create a deep learning model based on the attention mechanism to predict the next step of each subseries. Due to the limited amount of cryptocurrency data for training models, if the number of categories increases, the amount of training data for each model will decrease, and some complex models will not be trained well due to the large number of parameters. To overcome this challenge, we propose to combine the time series data of other cryptocurrencies to increase the amount of data for each category, hence increasing the accuracy of the models corresponding to each category.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14529v1 Announce Type: new \nAbstract: Organizing and managing cryptocurrency portfolios and decision-making on transactions is crucial in this market. Optimal selection of assets is one of the main challenges that requires accurate prediction of the price of cryptocurrencies. In this work, we categorize the financial time series into several similar subseries to increase prediction accuracy by learning each subseries category with similar behavior. For each category of the subseries, we create a deep learning model based on the attention mechanism to predict the next step of each subseries. Due to the limited amount of cryptocurrency data for training models, if the number of categories increases, the amount of training data for each model will decrease, and some complex models will not be trained well due to the large number of parameters. To overcome this challenge, we propose to combine the time series data of other cryptocurrencies to increase the amount of data for each category, hence increasing the accuracy of the models corresponding to each category.'}",oai:arXiv.org:2412.14529v1,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'cs.CE', 'scheme': None, 'label': None}, {'term': 'q-fin.ST', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by-nc-sa/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-sa/4.0/'}","[{'name': 'Arash Peik, Mohammad Ali Zare Chahooki, Amin Milani Fard, Mehdi Agha Sarram'}]","Arash Peik, Mohammad Ali Zare Chahooki, Amin Milani Fard, Mehdi Agha Sarram","{'name': 'Arash Peik, Mohammad Ali Zare Chahooki, Amin Milani Fard, Mehdi Agha Sarram'}",,
171,Consistent Human Image and Video Generation with Spatially Conditioned Diffusion,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Consistent Human Image and Video Generation with Spatially Conditioned Diffusion'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14531'}]",https://arxiv.org/abs/2412.14531,"arXiv:2412.14531v1 Announce Type: new 
Abstract: Consistent human-centric image and video synthesis aims to generate images or videos with new poses while preserving appearance consistency with a given reference image, which is crucial for low-cost visual content creation. Recent advances based on diffusion models typically rely on separate networks for reference appearance feature extraction and target visual generation, leading to inconsistent domain gaps between references and targets. In this paper, we frame the task as a spatially-conditioned inpainting problem, where the target image is inpainted to maintain appearance consistency with the reference. This approach enables the reference features to guide the generation of pose-compliant targets within a unified denoising network, thereby mitigating domain gaps. Additionally, to better maintain the reference appearance information, we impose a causal feature interaction framework, in which reference features can only query from themselves, while target features can query appearance information from both the reference and the target. To further enhance computational efficiency and flexibility, in practical implementation, we decompose the spatially-conditioned generation process into two stages: reference appearance extraction and conditioned target generation. Both stages share a single denoising network, with interactions restricted to self-attention layers. This proposed method ensures flexible control over the appearance of generated human images and videos. By fine-tuning existing base diffusion models on human video data, our method demonstrates strong generalization to unseen human identities and poses without requiring additional per-instance fine-tuning. Experimental results validate the effectiveness of our approach, showing competitive performance compared to existing methods for consistent human image and video synthesis.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14531v1 Announce Type: new \nAbstract: Consistent human-centric image and video synthesis aims to generate images or videos with new poses while preserving appearance consistency with a given reference image, which is crucial for low-cost visual content creation. Recent advances based on diffusion models typically rely on separate networks for reference appearance feature extraction and target visual generation, leading to inconsistent domain gaps between references and targets. In this paper, we frame the task as a spatially-conditioned inpainting problem, where the target image is inpainted to maintain appearance consistency with the reference. This approach enables the reference features to guide the generation of pose-compliant targets within a unified denoising network, thereby mitigating domain gaps. Additionally, to better maintain the reference appearance information, we impose a causal feature interaction framework, in which reference features can only query from themselves, while target features can query appearance information from both the reference and the target. To further enhance computational efficiency and flexibility, in practical implementation, we decompose the spatially-conditioned generation process into two stages: reference appearance extraction and conditioned target generation. Both stages share a single denoising network, with interactions restricted to self-attention layers. This proposed method ensures flexible control over the appearance of generated human images and videos. By fine-tuning existing base diffusion models on human video data, our method demonstrates strong generalization to unseen human identities and poses without requiring additional per-instance fine-tuning. Experimental results validate the effectiveness of our approach, showing competitive performance compared to existing methods for consistent human image and video synthesis.'}",oai:arXiv.org:2412.14531v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Mingdeng Cao, Chong Mou, Ziyang Yuan, Xintao Wang, Zhaoyang Zhang, Ying Shan, Yinqiang Zheng'}]","Mingdeng Cao, Chong Mou, Ziyang Yuan, Xintao Wang, Zhaoyang Zhang, Ying Shan, Yinqiang Zheng","{'name': 'Mingdeng Cao, Chong Mou, Ziyang Yuan, Xintao Wang, Zhaoyang Zhang, Ying Shan, Yinqiang Zheng'}",,
172,ClusterTalk: Corpus Exploration Framework using Multi-Dimensional Exploratory Search,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'ClusterTalk: Corpus Exploration Framework using Multi-Dimensional Exploratory Search'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14533'}]",https://arxiv.org/abs/2412.14533,"arXiv:2412.14533v1 Announce Type: new 
Abstract: Exploratory search of large text corpora is essential in domains like biomedical research, where large amounts of research literature are continuously generated. This paper presents ClusterTalk (The demo video and source code are available at: https://github.com/achouhan93/ClusterTalk), a framework for corpus exploration using multi-dimensional exploratory search. Our system integrates document clustering with faceted search, allowing users to interactively refine their exploration and ask corpus and document-level queries. Compared to traditional one-dimensional search approaches like keyword search or clustering, this system improves the discoverability of information by encouraging a deeper interaction with the corpus. We demonstrate the functionality of the ClusterTalk framework based on four million PubMed abstracts for the four-year time frame.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14533v1 Announce Type: new \nAbstract: Exploratory search of large text corpora is essential in domains like biomedical research, where large amounts of research literature are continuously generated. This paper presents ClusterTalk (The demo video and source code are available at: https://github.com/achouhan93/ClusterTalk), a framework for corpus exploration using multi-dimensional exploratory search. Our system integrates document clustering with faceted search, allowing users to interactively refine their exploration and ask corpus and document-level queries. Compared to traditional one-dimensional search approaches like keyword search or clustering, this system improves the discoverability of information by encouraging a deeper interaction with the corpus. We demonstrate the functionality of the ClusterTalk framework based on four million PubMed abstracts for the four-year time frame.'}",oai:arXiv.org:2412.14533v1,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by-sa/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-sa/4.0/'}","[{'name': 'Ashish Chouhan, Saifeldin Mandour, Michael Gertz'}]","Ashish Chouhan, Saifeldin Mandour, Michael Gertz","{'name': 'Ashish Chouhan, Saifeldin Mandour, Michael Gertz'}",,
173,DAMPER: A Dual-Stage Medical Report Generation Framework with Coarse-Grained MeSH Alignment and Fine-Grained Hypergraph Matching,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'DAMPER: A Dual-Stage Medical Report Generation Framework with Coarse-Grained MeSH Alignment and Fine-Grained Hypergraph Matching'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14535'}]",https://arxiv.org/abs/2412.14535,"arXiv:2412.14535v1 Announce Type: new 
Abstract: Medical report generation is crucial for clinical diagnosis and patient management, summarizing diagnoses and recommendations based on medical imaging. However, existing work often overlook the clinical pipeline involved in report writing, where physicians typically conduct an initial quick review followed by a detailed examination. Moreover, current alignment methods may lead to misaligned relationships. To address these issues, we propose DAMPER, a dual-stage framework for medical report generation that mimics the clinical pipeline of report writing in two stages. In the first stage, a MeSH-Guided Coarse-Grained Alignment (MCG) stage that aligns chest X-ray (CXR) image features with medical subject headings (MeSH) features to generate a rough keyphrase representation of the overall impression. In the second stage, a Hypergraph-Enhanced Fine-Grained Alignment (HFG) stage that constructs hypergraphs for image patches and report annotations, modeling high-order relationships within each modality and performing hypergraph matching to capture semantic correlations between image regions and textual phrases. Finally,the coarse-grained visual features, generated MeSH representations, and visual hypergraph features are fed into a report decoder to produce the final medical report. Extensive experiments on public datasets demonstrate the effectiveness of DAMPER in generating comprehensive and accurate medical reports, outperforming state-of-the-art methods across various evaluation metrics.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14535v1 Announce Type: new \nAbstract: Medical report generation is crucial for clinical diagnosis and patient management, summarizing diagnoses and recommendations based on medical imaging. However, existing work often overlook the clinical pipeline involved in report writing, where physicians typically conduct an initial quick review followed by a detailed examination. Moreover, current alignment methods may lead to misaligned relationships. To address these issues, we propose DAMPER, a dual-stage framework for medical report generation that mimics the clinical pipeline of report writing in two stages. In the first stage, a MeSH-Guided Coarse-Grained Alignment (MCG) stage that aligns chest X-ray (CXR) image features with medical subject headings (MeSH) features to generate a rough keyphrase representation of the overall impression. In the second stage, a Hypergraph-Enhanced Fine-Grained Alignment (HFG) stage that constructs hypergraphs for image patches and report annotations, modeling high-order relationships within each modality and performing hypergraph matching to capture semantic correlations between image regions and textual phrases. Finally,the coarse-grained visual features, generated MeSH representations, and visual hypergraph features are fed into a report decoder to produce the final medical report. Extensive experiments on public datasets demonstrate the effectiveness of DAMPER in generating comprehensive and accurate medical reports, outperforming state-of-the-art methods across various evaluation metrics.'}",oai:arXiv.org:2412.14535v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Xiaofei Huang, Wenting Chen, Jie Liu, Qisheng Lu, Xiaoling Luo, Linlin Shen'}]","Xiaofei Huang, Wenting Chen, Jie Liu, Qisheng Lu, Xiaoling Luo, Linlin Shen","{'name': 'Xiaofei Huang, Wenting Chen, Jie Liu, Qisheng Lu, Xiaoling Luo, Linlin Shen'}",,AAAI 2025
174,ST-ReP: Learning Predictive Representations Efficiently for Spatial-Temporal Forecasting,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'ST-ReP: Learning Predictive Representations Efficiently for Spatial-Temporal Forecasting'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14537'}]",https://arxiv.org/abs/2412.14537,"arXiv:2412.14537v1 Announce Type: new 
Abstract: Spatial-temporal forecasting is crucial and widely applicable in various domains such as traffic, energy, and climate. Benefiting from the abundance of unlabeled spatial-temporal data, self-supervised methods are increasingly adapted to learn spatial-temporal representations. However, it encounters three key challenges: 1) the difficulty in selecting reliable negative pairs due to the homogeneity of variables, hindering contrastive learning methods; 2) overlooking spatial correlations across variables over time; 3) limitations of efficiency and scalability in existing self-supervised learning methods. To tackle these, we propose a lightweight representation-learning model ST-ReP, integrating current value reconstruction and future value prediction into the pre-training framework for spatial-temporal forecasting. And we design a new spatial-temporal encoder to model fine-grained relationships. Moreover, multi-time scale analysis is incorporated into the self-supervised loss to enhance predictive capability. Experimental results across diverse domains demonstrate that the proposed model surpasses pre-training-based baselines, showcasing its ability to learn compact and semantically enriched representations while exhibiting superior scalability.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14537v1 Announce Type: new \nAbstract: Spatial-temporal forecasting is crucial and widely applicable in various domains such as traffic, energy, and climate. Benefiting from the abundance of unlabeled spatial-temporal data, self-supervised methods are increasingly adapted to learn spatial-temporal representations. However, it encounters three key challenges: 1) the difficulty in selecting reliable negative pairs due to the homogeneity of variables, hindering contrastive learning methods; 2) overlooking spatial correlations across variables over time; 3) limitations of efficiency and scalability in existing self-supervised learning methods. To tackle these, we propose a lightweight representation-learning model ST-ReP, integrating current value reconstruction and future value prediction into the pre-training framework for spatial-temporal forecasting. And we design a new spatial-temporal encoder to model fine-grained relationships. Moreover, multi-time scale analysis is incorporated into the self-supervised loss to enhance predictive capability. Experimental results across diverse domains demonstrate that the proposed model surpasses pre-training-based baselines, showcasing its ability to learn compact and semantically enriched representations while exhibiting superior scalability.'}",oai:arXiv.org:2412.14537v1,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Qi Zheng, Zihao Yao, Yaying Zhang'}]","Qi Zheng, Zihao Yao, Yaying Zhang","{'name': 'Qi Zheng, Zihao Yao, Yaying Zhang'}",,
175,"Overview of AI and Communication for 6G Network: Fundamentals, Challenges, and Future Research Opportunities","{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Overview of AI and Communication for 6G Network: Fundamentals, Challenges, and Future Research Opportunities'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14538'}]",https://arxiv.org/abs/2412.14538,"arXiv:2412.14538v1 Announce Type: new 
Abstract: With the increasing demand for seamless connectivity and intelligent communication, the integration of artificial intelligence (AI) and communication for sixth-generation (6G) network is emerging as a revolutionary architecture. This paper presents a comprehensive overview of AI and communication for 6G networks, emphasizing their foundational principles, inherent challenges, and future research opportunities. We commence with a retrospective analysis of AI and the evolution of large-scale AI models, underscoring their pivotal roles in shaping contemporary communication technologies. The discourse then transitions to a detailed exposition of the envisioned integration of AI within 6G networks, delineated across three progressive developmental stages. The initial stage, AI for Network, focuses on employing AI to augment network performance, optimize efficiency, and enhance user service experiences. The subsequent stage, Network for AI, highlights the role of the network in facilitating and buttressing AI operations and presents key enabling technologies, including digital twins for AI and semantic communication. In the final stage, AI as a Service, it is anticipated that future 6G networks will innately provide AI functions as services and support application scenarios like immersive communication and intelligent industrial robots. Specifically, we have defined the quality of AI service, which refers to the measurement framework system of AI services within the network. In addition to these developmental stages, we thoroughly examine the standardization processes pertinent to AI in network contexts, highlighting key milestones and ongoing efforts. Finally, we outline promising future research opportunities that could drive the evolution and refinement of AI and communication for 6G, positioning them as a cornerstone of next-generation communication infrastructure.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14538v1 Announce Type: new \nAbstract: With the increasing demand for seamless connectivity and intelligent communication, the integration of artificial intelligence (AI) and communication for sixth-generation (6G) network is emerging as a revolutionary architecture. This paper presents a comprehensive overview of AI and communication for 6G networks, emphasizing their foundational principles, inherent challenges, and future research opportunities. We commence with a retrospective analysis of AI and the evolution of large-scale AI models, underscoring their pivotal roles in shaping contemporary communication technologies. The discourse then transitions to a detailed exposition of the envisioned integration of AI within 6G networks, delineated across three progressive developmental stages. The initial stage, AI for Network, focuses on employing AI to augment network performance, optimize efficiency, and enhance user service experiences. The subsequent stage, Network for AI, highlights the role of the network in facilitating and buttressing AI operations and presents key enabling technologies, including digital twins for AI and semantic communication. In the final stage, AI as a Service, it is anticipated that future 6G networks will innately provide AI functions as services and support application scenarios like immersive communication and intelligent industrial robots. Specifically, we have defined the quality of AI service, which refers to the measurement framework system of AI services within the network. In addition to these developmental stages, we thoroughly examine the standardization processes pertinent to AI in network contexts, highlighting key milestones and ongoing efforts. Finally, we outline promising future research opportunities that could drive the evolution and refinement of AI and communication for 6G, positioning them as a cornerstone of next-generation communication infrastructure.'}",oai:arXiv.org:2412.14538v1,False,"[{'term': 'cs.NI', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'eess.SP', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Qimei Cui, Xiaohu You, Ni Wei, Guoshun Nan, Xuefei Zhang, Jianhua Zhang, Xinchen Lyu, Ming Ai, Xiaofeng Tao, Zhiyong Feng, Ping Zhang, Qingqing Wu, Meixia Tao, Yongming Huang, Chongwen Huang, Guangyi Liu, Chenghui Peng, Zhiwen Pan, Tao Sun, Dusit Niyato, Tao Chen, Muhammad Khurram Khan, Abbas Jamalipour, Mohsen Guizani, Chau Yuen'}]","Qimei Cui, Xiaohu You, Ni Wei, Guoshun Nan, Xuefei Zhang, Jianhua Zhang, Xinchen Lyu, Ming Ai, Xiaofeng Tao, Zhiyong Feng, Ping Zhang, Qingqing Wu, Meixia Tao, Yongming Huang, Chongwen Huang, Guangyi Liu, Chenghui Peng, Zhiwen Pan, Tao Sun, Dusit Niyato, Tao Chen, Muhammad Khurram Khan, Abbas Jamalipour, Mohsen Guizani, Chau Yuen","{'name': 'Qimei Cui, Xiaohu You, Ni Wei, Guoshun Nan, Xuefei Zhang, Jianhua Zhang, Xinchen Lyu, Ming Ai, Xiaofeng Tao, Zhiyong Feng, Ping Zhang, Qingqing Wu, Meixia Tao, Yongming Huang, Chongwen Huang, Guangyi Liu, Chenghui Peng, Zhiwen Pan, Tao Sun, Dusit Niyato, Tao Chen, Muhammad Khurram Khan, Abbas Jamalipour, Mohsen Guizani, Chau Yuen'}",,
176,Downscaling Precipitation with Bias-informed Conditional Diffusion Model,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Downscaling Precipitation with Bias-informed Conditional Diffusion Model'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14539'}]",https://arxiv.org/abs/2412.14539,"arXiv:2412.14539v1 Announce Type: new 
Abstract: Climate change is intensifying rainfall extremes, making high-resolution precipitation projections crucial for society to better prepare for impacts such as flooding. However, current Global Climate Models (GCMs) operate at spatial resolutions too coarse for localized analyses. To address this limitation, deep learning-based statistical downscaling methods offer promising solutions, providing high-resolution precipitation projections with a moderate computational cost. In this work, we introduce a bias-informed conditional diffusion model for statistical downscaling of precipitation. Specifically, our model leverages a conditional diffusion approach to learn distribution priors from large-scale, high-resolution precipitation datasets. The long-tail distribution of precipitation poses a unique challenge for training diffusion models; to address this, we apply gamma correction during preprocessing. Additionally, to correct biases in the downscaled results, we employ a guided-sampling strategy to enhance bias correction. Our experiments demonstrate that the proposed model achieves highly accurate results in an 8 times downscaling setting, outperforming previous deterministic methods. The code and dataset are available at https://github.com/RoseLV/research_super-resolution","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14539v1 Announce Type: new \nAbstract: Climate change is intensifying rainfall extremes, making high-resolution precipitation projections crucial for society to better prepare for impacts such as flooding. However, current Global Climate Models (GCMs) operate at spatial resolutions too coarse for localized analyses. To address this limitation, deep learning-based statistical downscaling methods offer promising solutions, providing high-resolution precipitation projections with a moderate computational cost. In this work, we introduce a bias-informed conditional diffusion model for statistical downscaling of precipitation. Specifically, our model leverages a conditional diffusion approach to learn distribution priors from large-scale, high-resolution precipitation datasets. The long-tail distribution of precipitation poses a unique challenge for training diffusion models; to address this, we apply gamma correction during preprocessing. Additionally, to correct biases in the downscaled results, we employ a guided-sampling strategy to enhance bias correction. Our experiments demonstrate that the proposed model achieves highly accurate results in an 8 times downscaling setting, outperforming previous deterministic methods. The code and dataset are available at https://github.com/RoseLV/research_super-resolution'}",oai:arXiv.org:2412.14539v1,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'physics.ao-ph', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Ran Lyu (Virginia Tech), Linhan Wang (Virginia Tech), Yanshen Sun (Virginia Tech), Hedanqiu Bai (Texas A&M University), Chang-Tien Lu (Virginia Tech)'}]","Ran Lyu (Virginia Tech), Linhan Wang (Virginia Tech), Yanshen Sun (Virginia Tech), Hedanqiu Bai (Texas A&M University), Chang-Tien Lu (Virginia Tech)","{'name': 'Ran Lyu (Virginia Tech), Linhan Wang (Virginia Tech), Yanshen Sun (Virginia Tech), Hedanqiu Bai (Texas A&M University), Chang-Tien Lu (Virginia Tech)'}",,
177,Transformer models are gauge invariant: A mathematical connection between AI and particle physics,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Transformer models are gauge invariant: A mathematical connection between AI and particle physics'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14543'}]",https://arxiv.org/abs/2412.14543,"arXiv:2412.14543v1 Announce Type: new 
Abstract: In particle physics, the fundamental forces are subject to symmetries called gauge invariance. It is a redundancy in the mathematical description of any physical system. In this article I will demonstrate that the transformer architecture exhibits the same properties, and show that the default representation of transformers has partially, but not fully removed the gauge invariance.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14543v1 Announce Type: new \nAbstract: In particle physics, the fundamental forces are subject to symmetries called gauge invariance. It is a redundancy in the mathematical description of any physical system. In this article I will demonstrate that the transformer architecture exhibits the same properties, and show that the default representation of transformers has partially, but not fully removed the gauge invariance.'}",oai:arXiv.org:2412.14543v1,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'hep-th', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}",[{'name': 'Leo van Nierop'}],Leo van Nierop,{'name': 'Leo van Nierop'},,
178,Summary of Point Transformer with Federated Learning for Predicting Breast Cancer HER2 Status from Hematoxylin and Eosin-Stained Whole Slide Images,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Summary of Point Transformer with Federated Learning for Predicting Breast Cancer HER2 Status from Hematoxylin and Eosin-Stained Whole Slide Images'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14545'}]",https://arxiv.org/abs/2412.14545,"arXiv:2412.14545v1 Announce Type: new 
Abstract: This study introduces a federated learning-based approach to predict HER2 status from hematoxylin and eosin (HE)-stained whole slide images (WSIs), reducing costs and speeding up treatment decisions. To address label imbalance and feature representation challenges in multisite datasets, a point transformer is proposed, incorporating dynamic label distribution, an auxiliary classifier, and farthest cosine sampling. Extensive experiments demonstrate state-of-the-art performance across four sites (2687 WSIs) and strong generalization to two unseen sites (229 WSIs).","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14545v1 Announce Type: new \nAbstract: This study introduces a federated learning-based approach to predict HER2 status from hematoxylin and eosin (HE)-stained whole slide images (WSIs), reducing costs and speeding up treatment decisions. To address label imbalance and feature representation challenges in multisite datasets, a point transformer is proposed, incorporating dynamic label distribution, an auxiliary classifier, and farthest cosine sampling. Extensive experiments demonstrate state-of-the-art performance across four sites (2687 WSIs) and strong generalization to two unseen sites (229 WSIs).'}",oai:arXiv.org:2412.14545v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/publicdomain/zero/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/publicdomain/zero/1.0/'}","[{'name': 'Kamorudeen A. Amuda, Almustapha A. Wakili'}]","Kamorudeen A. Amuda, Almustapha A. Wakili","{'name': 'Kamorudeen A. Amuda, Almustapha A. Wakili'}",,
179,{S$^3$-Mamba}: Small-Size-Sensitive Mamba for Lesion Segmentation,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': '{S$^3$-Mamba}: Small-Size-Sensitive Mamba for Lesion Segmentation'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14546'}]",https://arxiv.org/abs/2412.14546,"arXiv:2412.14546v1 Announce Type: new 
Abstract: Small lesions play a critical role in early disease diagnosis and intervention of severe infections. Popular models often face challenges in segmenting small lesions, as it occupies only a minor portion of an image, while down\_sampling operations may inevitably lose focus on local features of small lesions. To tackle the challenges, we propose a {\bf S}mall-{\bf S}ize-{\bf S}ensitive {\bf Mamba} ({\bf S$^3$-Mamba}), which promotes the sensitivity to small lesions across three dimensions: channel, spatial, and training strategy. Specifically, an Enhanced Visual State Space block is designed to focus on small lesions through multiple residual connections to preserve local features, and selectively amplify important details while suppressing irrelevant ones through channel-wise attention. A Tensor-based Cross-feature Multi-scale Attention is designed to integrate input image features and intermediate-layer features with edge features and exploit the attentive support of features across multiple scales, thereby retaining spatial details of small lesions at various granularities. Finally, we introduce a novel regularized curriculum learning to automatically assess lesion size and sample difficulty, and gradually focus from easy samples to hard ones like small lesions. Extensive experiments on three medical image segmentation datasets show the superiority of our S$^3$-Mamba, especially in segmenting small lesions. Our code is available at https://github.com/ErinWang2023/S3-Mamba.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14546v1 Announce Type: new \nAbstract: Small lesions play a critical role in early disease diagnosis and intervention of severe infections. Popular models often face challenges in segmenting small lesions, as it occupies only a minor portion of an image, while down\\_sampling operations may inevitably lose focus on local features of small lesions. To tackle the challenges, we propose a {\\bf S}mall-{\\bf S}ize-{\\bf S}ensitive {\\bf Mamba} ({\\bf S$^3$-Mamba}), which promotes the sensitivity to small lesions across three dimensions: channel, spatial, and training strategy. Specifically, an Enhanced Visual State Space block is designed to focus on small lesions through multiple residual connections to preserve local features, and selectively amplify important details while suppressing irrelevant ones through channel-wise attention. A Tensor-based Cross-feature Multi-scale Attention is designed to integrate input image features and intermediate-layer features with edge features and exploit the attentive support of features across multiple scales, thereby retaining spatial details of small lesions at various granularities. Finally, we introduce a novel regularized curriculum learning to automatically assess lesion size and sample difficulty, and gradually focus from easy samples to hard ones like small lesions. Extensive experiments on three medical image segmentation datasets show the superiority of our S$^3$-Mamba, especially in segmenting small lesions. Our code is available at https://github.com/ErinWang2023/S3-Mamba.'}",oai:arXiv.org:2412.14546v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Gui Wang, Yuexiang Li, Wenting Chen, Meidan Ding, Wooi Ping Cheah, Rong Qu, Jianfeng Ren, Linlin Shen'}]","Gui Wang, Yuexiang Li, Wenting Chen, Meidan Ding, Wooi Ping Cheah, Rong Qu, Jianfeng Ren, Linlin Shen","{'name': 'Gui Wang, Yuexiang Li, Wenting Chen, Meidan Ding, Wooi Ping Cheah, Rong Qu, Jianfeng Ren, Linlin Shen'}",,
180,Bright-NeRF:Brightening Neural Radiance Field with Color Restoration from Low-light Raw Images,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Bright-NeRF:Brightening Neural Radiance Field with Color Restoration from Low-light Raw Images'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14547'}]",https://arxiv.org/abs/2412.14547,"arXiv:2412.14547v1 Announce Type: new 
Abstract: Neural Radiance Fields (NeRFs) have demonstrated prominent performance in novel view synthesis. However, their input heavily relies on image acquisition under normal light conditions, making it challenging to learn accurate scene representation in low-light environments where images typically exhibit significant noise and severe color distortion. To address these challenges, we propose a novel approach, Bright-NeRF, which learns enhanced and high-quality radiance fields from multi-view low-light raw images in an unsupervised manner. Our method simultaneously achieves color restoration, denoising, and enhanced novel view synthesis. Specifically, we leverage a physically-inspired model of the sensor's response to illumination and introduce a chromatic adaptation loss to constrain the learning of response, enabling consistent color perception of objects regardless of lighting conditions. We further utilize the raw data's properties to expose the scene's intensity automatically. Additionally, we have collected a multi-view low-light raw image dataset to advance research in this field. Experimental results demonstrate that our proposed method significantly outperforms existing 2D and 3D approaches. Our code and dataset will be made publicly available.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14547v1 Announce Type: new \nAbstract: Neural Radiance Fields (NeRFs) have demonstrated prominent performance in novel view synthesis. However, their input heavily relies on image acquisition under normal light conditions, making it challenging to learn accurate scene representation in low-light environments where images typically exhibit significant noise and severe color distortion. To address these challenges, we propose a novel approach, Bright-NeRF, which learns enhanced and high-quality radiance fields from multi-view low-light raw images in an unsupervised manner. Our method simultaneously achieves color restoration, denoising, and enhanced novel view synthesis. Specifically, we leverage a physically-inspired model of the sensor's response to illumination and introduce a chromatic adaptation loss to constrain the learning of response, enabling consistent color perception of objects regardless of lighting conditions. We further utilize the raw data's properties to expose the scene's intensity automatically. Additionally, we have collected a multi-view low-light raw image dataset to advance research in this field. Experimental results demonstrate that our proposed method significantly outperforms existing 2D and 3D approaches. Our code and dataset will be made publicly available.""}",oai:arXiv.org:2412.14547v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'eess.IV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Min Wang, Xin Huang, Guoqing Zhou, Qifeng Guo, Qing Wang'}]","Min Wang, Xin Huang, Guoqing Zhou, Qifeng Guo, Qing Wang","{'name': 'Min Wang, Xin Huang, Guoqing Zhou, Qifeng Guo, Qing Wang'}",,
181,The Current Challenges of Software Engineering in the Era of Large Language Models,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'The Current Challenges of Software Engineering in the Era of Large Language Models'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14554'}]",https://arxiv.org/abs/2412.14554,"arXiv:2412.14554v1 Announce Type: new 
Abstract: With the advent of large language models (LLMs) in the artificial intelligence (AI) area, the field of software engineering (SE) has also witnessed a paradigm shift. These models, by leveraging the power of deep learning and massive amounts of data, have demonstrated an unprecedented capacity to understand, generate, and operate programming languages. They can assist developers in completing a broad spectrum of software development activities, encompassing software design, automated programming, and maintenance, which potentially reduces huge human efforts. Integrating LLMs within the SE landscape (LLM4SE) has become a burgeoning trend, necessitating exploring this emergent landscape's challenges and opportunities.
  The paper aims at revisiting the software development life cycle (SDLC) under LLMs, and highlighting challenges and opportunities of the new paradigm. The paper first summarizes the overall process of LLM4SE, and then elaborates on the current challenges based on a through discussion. The discussion was held among more than 20 participants from academia and industry, specializing in fields such as software engineering and artificial intelligence. Specifically, we achieve 26 key challenges from seven aspects, including software requirement & design, coding assistance, testing code generation, code review, code maintenance, software vulnerability management, and data, training, and evaluation. We hope the achieved challenges would benefit future research in the LLM4SE field.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14554v1 Announce Type: new \nAbstract: With the advent of large language models (LLMs) in the artificial intelligence (AI) area, the field of software engineering (SE) has also witnessed a paradigm shift. These models, by leveraging the power of deep learning and massive amounts of data, have demonstrated an unprecedented capacity to understand, generate, and operate programming languages. They can assist developers in completing a broad spectrum of software development activities, encompassing software design, automated programming, and maintenance, which potentially reduces huge human efforts. Integrating LLMs within the SE landscape (LLM4SE) has become a burgeoning trend, necessitating exploring this emergent landscape's challenges and opportunities.\n  The paper aims at revisiting the software development life cycle (SDLC) under LLMs, and highlighting challenges and opportunities of the new paradigm. The paper first summarizes the overall process of LLM4SE, and then elaborates on the current challenges based on a through discussion. The discussion was held among more than 20 participants from academia and industry, specializing in fields such as software engineering and artificial intelligence. Specifically, we achieve 26 key challenges from seven aspects, including software requirement & design, coding assistance, testing code generation, code review, code maintenance, software vulnerability management, and data, training, and evaluation. We hope the achieved challenges would benefit future research in the LLM4SE field.""}",oai:arXiv.org:2412.14554v1,False,"[{'term': 'cs.SE', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Cuiyun Gao, Xing Hu, Shan Gao, Xin Xia, Zhi Jin'}]","Cuiyun Gao, Xing Hu, Shan Gao, Xin Xia, Zhi Jin","{'name': 'Cuiyun Gao, Xing Hu, Shan Gao, Xin Xia, Zhi Jin'}",,
182,Single-Loop Federated Actor-Critic across Heterogeneous Environments,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Single-Loop Federated Actor-Critic across Heterogeneous Environments'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14555'}]",https://arxiv.org/abs/2412.14555,"arXiv:2412.14555v1 Announce Type: new 
Abstract: Federated reinforcement learning (FRL) has emerged as a promising paradigm, enabling multiple agents to collaborate and learn a shared policy adaptable across heterogeneous environments. Among the various reinforcement learning (RL) algorithms, the actor-critic (AC) algorithm stands out for its low variance and high sample efficiency. However, little to nothing is known theoretically about AC in a federated manner, especially each agent interacts with a potentially different environment. The lack of such results is attributed to various technical challenges: a two-level structure illustrating the coupling effect between the actor and the critic, heterogeneous environments, Markovian sampling and multiple local updates. In response, we study \textit{Single-loop Federated Actor Critic} (SFAC) where agents perform actor-critic learning in a two-level federated manner while interacting with heterogeneous environments. We then provide bounds on the convergence error of SFAC. The results show that the convergence error asymptotically converges to a near-stationary point, with the extent proportional to environment heterogeneity. Moreover, the sample complexity exhibits a linear speed-up through the federation of agents. We evaluate the performance of SFAC through numerical experiments using common RL benchmarks, which demonstrate its effectiveness.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14555v1 Announce Type: new \nAbstract: Federated reinforcement learning (FRL) has emerged as a promising paradigm, enabling multiple agents to collaborate and learn a shared policy adaptable across heterogeneous environments. Among the various reinforcement learning (RL) algorithms, the actor-critic (AC) algorithm stands out for its low variance and high sample efficiency. However, little to nothing is known theoretically about AC in a federated manner, especially each agent interacts with a potentially different environment. The lack of such results is attributed to various technical challenges: a two-level structure illustrating the coupling effect between the actor and the critic, heterogeneous environments, Markovian sampling and multiple local updates. In response, we study \\textit{Single-loop Federated Actor Critic} (SFAC) where agents perform actor-critic learning in a two-level federated manner while interacting with heterogeneous environments. We then provide bounds on the convergence error of SFAC. The results show that the convergence error asymptotically converges to a near-stationary point, with the extent proportional to environment heterogeneity. Moreover, the sample complexity exhibits a linear speed-up through the federation of agents. We evaluate the performance of SFAC through numerical experiments using common RL benchmarks, which demonstrate its effectiveness.'}",oai:arXiv.org:2412.14555v1,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'cs.DC', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Ye Zhu, Xiaowen Gong'}]","Ye Zhu, Xiaowen Gong","{'name': 'Ye Zhu, Xiaowen Gong'}",,
183,CitaLaw: Enhancing LLM with Citations in Legal Domain,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'CitaLaw: Enhancing LLM with Citations in Legal Domain'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14556'}]",https://arxiv.org/abs/2412.14556,"arXiv:2412.14556v1 Announce Type: new 
Abstract: In this paper, we propose CitaLaw, the first benchmark designed to evaluate LLMs' ability to produce legally sound responses with appropriate citations. CitaLaw features a diverse set of legal questions for both laypersons and practitioners, paired with a comprehensive corpus of law articles and precedent cases as a reference pool. This framework enables LLM-based systems to retrieve supporting citations from the reference corpus and align these citations with the corresponding sentences in their responses. Moreover, we introduce syllogism-inspired evaluation methods to assess the legal alignment between retrieved references and LLM-generated responses, as well as their consistency with user questions. Extensive experiments on 2 open-domain and 7 legal-specific LLMs demonstrate that integrating legal references substantially enhances response quality. Furthermore, our proposed syllogism-based evaluation method exhibits strong agreement with human judgments.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14556v1 Announce Type: new \nAbstract: In this paper, we propose CitaLaw, the first benchmark designed to evaluate LLMs' ability to produce legally sound responses with appropriate citations. CitaLaw features a diverse set of legal questions for both laypersons and practitioners, paired with a comprehensive corpus of law articles and precedent cases as a reference pool. This framework enables LLM-based systems to retrieve supporting citations from the reference corpus and align these citations with the corresponding sentences in their responses. Moreover, we introduce syllogism-inspired evaluation methods to assess the legal alignment between retrieved references and LLM-generated responses, as well as their consistency with user questions. Extensive experiments on 2 open-domain and 7 legal-specific LLMs demonstrate that integrating legal references substantially enhances response quality. Furthermore, our proposed syllogism-based evaluation method exhibits strong agreement with human judgments.""}",oai:arXiv.org:2412.14556v1,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Kepu Zhang, Weijie Yu, Sunhao Dai, Jun Xu'}]","Kepu Zhang, Weijie Yu, Sunhao Dai, Jun Xu","{'name': 'Kepu Zhang, Weijie Yu, Sunhao Dai, Jun Xu'}",,
184,ScaMo: Exploring the Scaling Law in Autoregressive Motion Generation Model,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'ScaMo: Exploring the Scaling Law in Autoregressive Motion Generation Model'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14559'}]",https://arxiv.org/abs/2412.14559,"arXiv:2412.14559v1 Announce Type: new 
Abstract: The scaling law has been validated in various domains, such as natural language processing (NLP) and massive computer vision tasks; however, its application to motion generation remains largely unexplored. In this paper, we introduce a scalable motion generation framework that includes the motion tokenizer Motion FSQ-VAE and a text-prefix autoregressive transformer. Through comprehensive experiments, we observe the scaling behavior of this system. For the first time, we confirm the existence of scaling laws within the context of motion generation. Specifically, our results demonstrate that the normalized test loss of our prefix autoregressive models adheres to a logarithmic law in relation to compute budgets. Furthermore, we also confirm the power law between Non-Vocabulary Parameters, Vocabulary Parameters, and Data Tokens with respect to compute budgets respectively. Leveraging the scaling law, we predict the optimal transformer size, vocabulary size, and data requirements for a compute budget of $1e18$. The test loss of the system, when trained with the optimal model size, vocabulary size, and required data, aligns precisely with the predicted test loss, thereby validating the scaling law.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14559v1 Announce Type: new \nAbstract: The scaling law has been validated in various domains, such as natural language processing (NLP) and massive computer vision tasks; however, its application to motion generation remains largely unexplored. In this paper, we introduce a scalable motion generation framework that includes the motion tokenizer Motion FSQ-VAE and a text-prefix autoregressive transformer. Through comprehensive experiments, we observe the scaling behavior of this system. For the first time, we confirm the existence of scaling laws within the context of motion generation. Specifically, our results demonstrate that the normalized test loss of our prefix autoregressive models adheres to a logarithmic law in relation to compute budgets. Furthermore, we also confirm the power law between Non-Vocabulary Parameters, Vocabulary Parameters, and Data Tokens with respect to compute budgets respectively. Leveraging the scaling law, we predict the optimal transformer size, vocabulary size, and data requirements for a compute budget of $1e18$. The test loss of the system, when trained with the optimal model size, vocabulary size, and required data, aligns precisely with the predicted test loss, thereby validating the scaling law.'}",oai:arXiv.org:2412.14559v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Shunlin Lu, Jingbo Wang, Zeyu Lu, Ling-Hao Chen, Wenxun Dai, Junting Dong, Zhiyang Dou, Bo Dai, Ruimao Zhang'}]","Shunlin Lu, Jingbo Wang, Zeyu Lu, Ling-Hao Chen, Wenxun Dai, Junting Dong, Zhiyang Dou, Bo Dai, Ruimao Zhang","{'name': 'Shunlin Lu, Jingbo Wang, Zeyu Lu, Ling-Hao Chen, Wenxun Dai, Junting Dong, Zhiyang Dou, Bo Dai, Ruimao Zhang'}",,
185,GBRIP: Granular Ball Representation for Imbalanced Partial Label Learning,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'GBRIP: Granular Ball Representation for Imbalanced Partial Label Learning'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14561'}]",https://arxiv.org/abs/2412.14561,"arXiv:2412.14561v1 Announce Type: new 
Abstract: Partial label learning (PLL) is a complicated weakly supervised multi-classification task compounded by class imbalance. Currently, existing methods only rely on inter-class pseudo-labeling from inter-class features, often overlooking the significant impact of the intra-class imbalanced features combined with the inter-class. To address these limitations, we introduce Granular Ball Representation for Imbalanced PLL (GBRIP), a novel framework for imbalanced PLL. GBRIP utilizes coarse-grained granular ball representation and multi-center loss to construct a granular ball-based nfeature space through unsupervised learning, effectively capturing the feature distribution within each class. GBRIP mitigates the impact of confusing features by systematically refining label disambiguation and estimating imbalance distributions. The novel multi-center loss function enhances learning by emphasizing the relationships between samples and their respective centers within the granular balls. Extensive experiments on standard benchmarks demonstrate that GBRIP outperforms existing state-of-the-art methods, offering a robust solution to the challenges of imbalanced PLL.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14561v1 Announce Type: new \nAbstract: Partial label learning (PLL) is a complicated weakly supervised multi-classification task compounded by class imbalance. Currently, existing methods only rely on inter-class pseudo-labeling from inter-class features, often overlooking the significant impact of the intra-class imbalanced features combined with the inter-class. To address these limitations, we introduce Granular Ball Representation for Imbalanced PLL (GBRIP), a novel framework for imbalanced PLL. GBRIP utilizes coarse-grained granular ball representation and multi-center loss to construct a granular ball-based nfeature space through unsupervised learning, effectively capturing the feature distribution within each class. GBRIP mitigates the impact of confusing features by systematically refining label disambiguation and estimating imbalance distributions. The novel multi-center loss function enhances learning by emphasizing the relationships between samples and their respective centers within the granular balls. Extensive experiments on standard benchmarks demonstrate that GBRIP outperforms existing state-of-the-art methods, offering a robust solution to the challenges of imbalanced PLL.'}",oai:arXiv.org:2412.14561v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Jintao Huang, Yiu-ming Cheung, Chi-man Vong, Wenbin Qian'}]","Jintao Huang, Yiu-ming Cheung, Chi-man Vong, Wenbin Qian","{'name': 'Jintao Huang, Yiu-ming Cheung, Chi-man Vong, Wenbin Qian'}",,
186,AIArena: A Blockchain-Based Decentralized AI Training Platform,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'AIArena: A Blockchain-Based Decentralized AI Training Platform'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14566'}]",https://arxiv.org/abs/2412.14566,"arXiv:2412.14566v1 Announce Type: new 
Abstract: The rapid advancement of AI has underscored critical challenges in its development and implementation, largely due to centralized control by a few major corporations. This concentration of power intensifies biases within AI models, resulting from inadequate governance and oversight mechanisms. Additionally, it limits public involvement and heightens concerns about the integrity of model generation. Such monopolistic control over data and AI outputs threatens both innovation and fair data usage, as users inadvertently contribute data that primarily benefits these corporations. In this work, we propose AIArena, a blockchain-based decentralized AI training platform designed to democratize AI development and alignment through on-chain incentive mechanisms. AIArena fosters an open and collaborative environment where participants can contribute models and computing resources. Its on-chain consensus mechanism ensures fair rewards for participants based on their contributions. We instantiate and implement AIArena on the public Base blockchain Sepolia testnet, and the evaluation results demonstrate the feasibility of AIArena in real-world applications.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14566v1 Announce Type: new \nAbstract: The rapid advancement of AI has underscored critical challenges in its development and implementation, largely due to centralized control by a few major corporations. This concentration of power intensifies biases within AI models, resulting from inadequate governance and oversight mechanisms. Additionally, it limits public involvement and heightens concerns about the integrity of model generation. Such monopolistic control over data and AI outputs threatens both innovation and fair data usage, as users inadvertently contribute data that primarily benefits these corporations. In this work, we propose AIArena, a blockchain-based decentralized AI training platform designed to democratize AI development and alignment through on-chain incentive mechanisms. AIArena fosters an open and collaborative environment where participants can contribute models and computing resources. Its on-chain consensus mechanism ensures fair rewards for participants based on their contributions. We instantiate and implement AIArena on the public Base blockchain Sepolia testnet, and the evaluation results demonstrate the feasibility of AIArena in real-world applications.'}",oai:arXiv.org:2412.14566v1,False,"[{'term': 'cs.CR', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.DC', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Zhipeng Wang, Rui Sun, Elizabeth Lui, Tuo Zhou, Yizhe Wen, Jiahao Sun'}]","Zhipeng Wang, Rui Sun, Elizabeth Lui, Tuo Zhou, Yizhe Wen, Jiahao Sun","{'name': 'Zhipeng Wang, Rui Sun, Elizabeth Lui, Tuo Zhou, Yizhe Wen, Jiahao Sun'}",,
187,Improving Geometry in Sparse-View 3DGS via Reprojection-based DoF Separation,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Improving Geometry in Sparse-View 3DGS via Reprojection-based DoF Separation'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14568'}]",https://arxiv.org/abs/2412.14568,"arXiv:2412.14568v1 Announce Type: new 
Abstract: Recent learning-based Multi-View Stereo models have demonstrated state-of-the-art performance in sparse-view 3D reconstruction. However, directly applying 3D Gaussian Splatting (3DGS) as a refinement step following these models presents challenges. We hypothesize that the excessive positional degrees of freedom (DoFs) in Gaussians induce geometry distortion, fitting color patterns at the cost of structural fidelity. To address this, we propose reprojection-based DoF separation, a method distinguishing positional DoFs in terms of uncertainty: image-plane-parallel DoFs and ray-aligned DoF. To independently manage each DoF, we introduce a reprojection process along with tailored constraints for each DoF. Through experiments across various datasets, we confirm that separating the positional DoFs of Gaussians and applying targeted constraints effectively suppresses geometric artifacts, producing reconstruction results that are both visually and geometrically plausible.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14568v1 Announce Type: new \nAbstract: Recent learning-based Multi-View Stereo models have demonstrated state-of-the-art performance in sparse-view 3D reconstruction. However, directly applying 3D Gaussian Splatting (3DGS) as a refinement step following these models presents challenges. We hypothesize that the excessive positional degrees of freedom (DoFs) in Gaussians induce geometry distortion, fitting color patterns at the cost of structural fidelity. To address this, we propose reprojection-based DoF separation, a method distinguishing positional DoFs in terms of uncertainty: image-plane-parallel DoFs and ray-aligned DoF. To independently manage each DoF, we introduce a reprojection process along with tailored constraints for each DoF. Through experiments across various datasets, we confirm that separating the positional DoFs of Gaussians and applying targeted constraints effectively suppresses geometric artifacts, producing reconstruction results that are both visually and geometrically plausible.'}",oai:arXiv.org:2412.14568v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by-nc-sa/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-sa/4.0/'}","[{'name': 'Yongsung Kim, Minjun Park, Jooyoung Choi, Sungroh Yoon'}]","Yongsung Kim, Minjun Park, Jooyoung Choi, Sungroh Yoon","{'name': 'Yongsung Kim, Minjun Park, Jooyoung Choi, Sungroh Yoon'}",,
188,Global Spatio-Temporal Fusion-based Traffic Prediction Algorithm with Anomaly Aware,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Global Spatio-Temporal Fusion-based Traffic Prediction Algorithm with Anomaly Aware'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14569'}]",https://arxiv.org/abs/2412.14569,"arXiv:2412.14569v1 Announce Type: new 
Abstract: Traffic prediction is an indispensable component of urban planning and traffic management. Achieving accurate traffic prediction hinges on the ability to capture the potential spatio-temporal relationships among road sensors. However, the majority of existing works focus on local short-term spatio-temporal correlations, failing to fully consider the interactions of different sensors in the long-term state. In addition, these works do not analyze the influences of anomalous factors, or have insufficient ability to extract personalized features of anomalous factors, which make them ineffectively capture their spatio-temporal influences on traffic prediction. To address the aforementioned issues, We propose a global spatio-temporal fusion-based traffic prediction algorithm that incorporates anomaly awareness. Initially, based on the designed anomaly detection network, we construct an efficient anomalous factors impacting module (AFIM), to evaluate the spatio-temporal impact of unexpected external events on traffic prediction. Furthermore, we propose a multi-scale spatio-temporal feature fusion module (MTSFFL) based on the transformer architecture, to obtain all possible both long and short term correlations among different sensors in a wide-area traffic environment for accurate prediction of traffic flow. Finally, experiments are implemented based on real-scenario public transportation datasets (PEMS04 and PEMS08) to demonstrate that our approach can achieve state-of-the-art performance.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14569v1 Announce Type: new \nAbstract: Traffic prediction is an indispensable component of urban planning and traffic management. Achieving accurate traffic prediction hinges on the ability to capture the potential spatio-temporal relationships among road sensors. However, the majority of existing works focus on local short-term spatio-temporal correlations, failing to fully consider the interactions of different sensors in the long-term state. In addition, these works do not analyze the influences of anomalous factors, or have insufficient ability to extract personalized features of anomalous factors, which make them ineffectively capture their spatio-temporal influences on traffic prediction. To address the aforementioned issues, We propose a global spatio-temporal fusion-based traffic prediction algorithm that incorporates anomaly awareness. Initially, based on the designed anomaly detection network, we construct an efficient anomalous factors impacting module (AFIM), to evaluate the spatio-temporal impact of unexpected external events on traffic prediction. Furthermore, we propose a multi-scale spatio-temporal feature fusion module (MTSFFL) based on the transformer architecture, to obtain all possible both long and short term correlations among different sensors in a wide-area traffic environment for accurate prediction of traffic flow. Finally, experiments are implemented based on real-scenario public transportation datasets (PEMS04 and PEMS08) to demonstrate that our approach can achieve state-of-the-art performance.'}",oai:arXiv.org:2412.14569v1,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Chaoqun Liu, Xuanpeng Li, Chen Gong, Guangyu Li'}]","Chaoqun Liu, Xuanpeng Li, Chen Gong, Guangyu Li","{'name': 'Chaoqun Liu, Xuanpeng Li, Chen Gong, Guangyu Li'}",,
189,Characterising Simulation-Based Program Equilibria,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Characterising Simulation-Based Program Equilibria'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14570'}]",https://arxiv.org/abs/2412.14570,"arXiv:2412.14570v1 Announce Type: new 
Abstract: In Tennenholtz's program equilibrium, players of a game submit programs to play on their behalf. Each program receives the other programs' source code and outputs an action. This can model interactions involving AI agents, mutually transparent institutions, or commitments. Tennenholtz (2004) proves a folk theorem for program games, but the equilibria constructed are very brittle. We therefore consider simulation-based programs -- i.e., programs that work by running opponents' programs. These are relatively robust (in particular, two programs that act the same are treated the same) and are more practical than proof-based approaches. Oesterheld's (2019) $\epsilon$Grounded$\pi$Bot is such an approach. Unfortunately, it is not generally applicable to games of three or more players, and only allows for a limited range of equilibria in two player games. In this paper, we propose a generalisation to Oesterheld's (2019) $\epsilon$Grounded$\pi$Bot. We prove a folk theorem for our programs in a setting with access to a shared source of randomness. We then characterise their equilibria in a setting without shared randomness. Both with and without shared randomness, we achieve a much wider range of equilibria than Oesterheld's (2019) $\epsilon$Grounded$\pi$Bot. Finally, we explore the limits of simulation-based program equilibrium, showing that the Tennenholtz folk theorem cannot be attained by simulation-based programs without access to shared randomness.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14570v1 Announce Type: new \nAbstract: In Tennenholtz's program equilibrium, players of a game submit programs to play on their behalf. Each program receives the other programs' source code and outputs an action. This can model interactions involving AI agents, mutually transparent institutions, or commitments. Tennenholtz (2004) proves a folk theorem for program games, but the equilibria constructed are very brittle. We therefore consider simulation-based programs -- i.e., programs that work by running opponents' programs. These are relatively robust (in particular, two programs that act the same are treated the same) and are more practical than proof-based approaches. Oesterheld's (2019) $\\epsilon$Grounded$\\pi$Bot is such an approach. Unfortunately, it is not generally applicable to games of three or more players, and only allows for a limited range of equilibria in two player games. In this paper, we propose a generalisation to Oesterheld's (2019) $\\epsilon$Grounded$\\pi$Bot. We prove a folk theorem for our programs in a setting with access to a shared source of randomness. We then characterise their equilibria in a setting without shared randomness. Both with and without shared randomness, we achieve a much wider range of equilibria than Oesterheld's (2019) $\\epsilon$Grounded$\\pi$Bot. Finally, we explore the limits of simulation-based program equilibrium, showing that the Tennenholtz folk theorem cannot be attained by simulation-based programs without access to shared randomness.""}",oai:arXiv.org:2412.14570v1,False,"[{'term': 'cs.GT', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.MA', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Emery Cooper, Caspar Oesterheld, Vincent Conitzer'}]","Emery Cooper, Caspar Oesterheld, Vincent Conitzer","{'name': 'Emery Cooper, Caspar Oesterheld, Vincent Conitzer'}",,
190,SCKD: Semi-Supervised Cross-Modality Knowledge Distillation for 4D Radar Object Detection,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'SCKD: Semi-Supervised Cross-Modality Knowledge Distillation for 4D Radar Object Detection'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14571'}]",https://arxiv.org/abs/2412.14571,"arXiv:2412.14571v1 Announce Type: new 
Abstract: 3D object detection is one of the fundamental perception tasks for autonomous vehicles. Fulfilling such a task with a 4D millimeter-wave radar is very attractive since the sensor is able to acquire 3D point clouds similar to Lidar while maintaining robust measurements under adverse weather. However, due to the high sparsity and noise associated with the radar point clouds, the performance of the existing methods is still much lower than expected. In this paper, we propose a novel Semi-supervised Cross-modality Knowledge Distillation (SCKD) method for 4D radar-based 3D object detection. It characterizes the capability of learning the feature from a Lidar-radar-fused teacher network with semi-supervised distillation. We first propose an adaptive fusion module in the teacher network to boost its performance. Then, two feature distillation modules are designed to facilitate the cross-modality knowledge transfer. Finally, a semi-supervised output distillation is proposed to increase the effectiveness and flexibility of the distillation framework. With the same network structure, our radar-only student trained by SCKD boosts the mAP by 10.38% over the baseline and outperforms the state-of-the-art works on the VoD dataset. The experiment on ZJUODset also shows 5.12% mAP improvements on the moderate difficulty level over the baseline when extra unlabeled data are available. Code is available at https://github.com/Ruoyu-Xu/SCKD.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14571v1 Announce Type: new \nAbstract: 3D object detection is one of the fundamental perception tasks for autonomous vehicles. Fulfilling such a task with a 4D millimeter-wave radar is very attractive since the sensor is able to acquire 3D point clouds similar to Lidar while maintaining robust measurements under adverse weather. However, due to the high sparsity and noise associated with the radar point clouds, the performance of the existing methods is still much lower than expected. In this paper, we propose a novel Semi-supervised Cross-modality Knowledge Distillation (SCKD) method for 4D radar-based 3D object detection. It characterizes the capability of learning the feature from a Lidar-radar-fused teacher network with semi-supervised distillation. We first propose an adaptive fusion module in the teacher network to boost its performance. Then, two feature distillation modules are designed to facilitate the cross-modality knowledge transfer. Finally, a semi-supervised output distillation is proposed to increase the effectiveness and flexibility of the distillation framework. With the same network structure, our radar-only student trained by SCKD boosts the mAP by 10.38% over the baseline and outperforms the state-of-the-art works on the VoD dataset. The experiment on ZJUODset also shows 5.12% mAP improvements on the moderate difficulty level over the baseline when extra unlabeled data are available. Code is available at https://github.com/Ruoyu-Xu/SCKD.'}",oai:arXiv.org:2412.14571v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'eess.IV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Ruoyu Xu, Zhiyu Xiang, Chenwei Zhang, Hanzhi Zhong, Xijun Zhao, Ruina Dang, Peng Xu, Tianyu Pu, Eryun Liu'}]","Ruoyu Xu, Zhiyu Xiang, Chenwei Zhang, Hanzhi Zhong, Xijun Zhao, Ruina Dang, Peng Xu, Tianyu Pu, Eryun Liu","{'name': 'Ruoyu Xu, Zhiyu Xiang, Chenwei Zhang, Hanzhi Zhong, Xijun Zhao, Ruina Dang, Peng Xu, Tianyu Pu, Eryun Liu'}",,
191,Sliding Windows Are Not the End: Exploring Full Ranking with Long-Context Large Language Models,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Sliding Windows Are Not the End: Exploring Full Ranking with Long-Context Large Language Models'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14574'}]",https://arxiv.org/abs/2412.14574,"arXiv:2412.14574v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have shown exciting performance in listwise passage ranking. Due to the limited input length, existing methods often adopt the sliding window strategy. Such a strategy, though effective, is inefficient as it involves repetitive and serialized processing, which usually re-evaluates relevant passages multiple times. As a result, it incurs redundant API costs, which are proportional to the number of inference tokens. The development of long-context LLMs enables the full ranking of all passages within a single inference, avoiding redundant API costs. In this paper, we conduct a comprehensive study of long-context LLMs for ranking tasks in terms of efficiency and effectiveness. Surprisingly, our experiments reveal that full ranking with long-context LLMs can deliver superior performance in the supervised fine-tuning setting with a huge efficiency improvement. Furthermore, we identify two limitations of fine-tuning the full ranking model based on existing methods: (1) sliding window strategy fails to produce a full ranking list as a training label, and (2) the language modeling loss cannot emphasize top-ranked passage IDs in the label. To alleviate these issues, we propose a new complete listwise label construction approach and a novel importance-aware learning objective for full ranking. Experiments show the superior performance of our method over baselines. Our codes are available at \url{https://github.com/8421BCD/fullrank}.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14574v1 Announce Type: new \nAbstract: Large Language Models (LLMs) have shown exciting performance in listwise passage ranking. Due to the limited input length, existing methods often adopt the sliding window strategy. Such a strategy, though effective, is inefficient as it involves repetitive and serialized processing, which usually re-evaluates relevant passages multiple times. As a result, it incurs redundant API costs, which are proportional to the number of inference tokens. The development of long-context LLMs enables the full ranking of all passages within a single inference, avoiding redundant API costs. In this paper, we conduct a comprehensive study of long-context LLMs for ranking tasks in terms of efficiency and effectiveness. Surprisingly, our experiments reveal that full ranking with long-context LLMs can deliver superior performance in the supervised fine-tuning setting with a huge efficiency improvement. Furthermore, we identify two limitations of fine-tuning the full ranking model based on existing methods: (1) sliding window strategy fails to produce a full ranking list as a training label, and (2) the language modeling loss cannot emphasize top-ranked passage IDs in the label. To alleviate these issues, we propose a new complete listwise label construction approach and a novel importance-aware learning objective for full ranking. Experiments show the superior performance of our method over baselines. Our codes are available at \\url{https://github.com/8421BCD/fullrank}.'}",oai:arXiv.org:2412.14574v1,False,"[{'term': 'cs.IR', 'scheme': None, 'label': None}, {'term': 'cs.CL', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Wenhan Liu, Xinyu Ma, Yutao Zhu, Ziliang Zhao, Shuaiqiang Wang, Dawei Yin, Zhicheng Dou'}]","Wenhan Liu, Xinyu Ma, Yutao Zhu, Ziliang Zhao, Shuaiqiang Wang, Dawei Yin, Zhicheng Dou","{'name': 'Wenhan Liu, Xinyu Ma, Yutao Zhu, Ziliang Zhao, Shuaiqiang Wang, Dawei Yin, Zhicheng Dou'}",,
192,Alignment-Free RGB-T Salient Object Detection: A Large-scale Dataset and Progressive Correlation Network,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Alignment-Free RGB-T Salient Object Detection: A Large-scale Dataset and Progressive Correlation Network'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14576'}]",https://arxiv.org/abs/2412.14576,"arXiv:2412.14576v1 Announce Type: new 
Abstract: Alignment-free RGB-Thermal (RGB-T) salient object detection (SOD) aims to achieve robust performance in complex scenes by directly leveraging the complementary information from unaligned visible-thermal image pairs, without requiring manual alignment. However, the labor-intensive process of collecting and annotating image pairs limits the scale of existing benchmarks, hindering the advancement of alignment-free RGB-T SOD. In this paper, we construct a large-scale and high-diversity unaligned RGB-T SOD dataset named UVT20K, comprising 20,000 image pairs, 407 scenes, and 1256 object categories. All samples are collected from real-world scenarios with various challenges, such as low illumination, image clutter, complex salient objects, and so on. To support the exploration for further research, each sample in UVT20K is annotated with a comprehensive set of ground truths, including saliency masks, scribbles, boundaries, and challenge attributes. In addition, we propose a Progressive Correlation Network (PCNet), which models inter- and intra-modal correlations on the basis of explicit alignment to achieve accurate predictions in unaligned image pairs. Extensive experiments conducted on unaligned and aligned datasets demonstrate the effectiveness of our method.Code and dataset are available at https://github.com/Angknpng/PCNet.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14576v1 Announce Type: new \nAbstract: Alignment-free RGB-Thermal (RGB-T) salient object detection (SOD) aims to achieve robust performance in complex scenes by directly leveraging the complementary information from unaligned visible-thermal image pairs, without requiring manual alignment. However, the labor-intensive process of collecting and annotating image pairs limits the scale of existing benchmarks, hindering the advancement of alignment-free RGB-T SOD. In this paper, we construct a large-scale and high-diversity unaligned RGB-T SOD dataset named UVT20K, comprising 20,000 image pairs, 407 scenes, and 1256 object categories. All samples are collected from real-world scenarios with various challenges, such as low illumination, image clutter, complex salient objects, and so on. To support the exploration for further research, each sample in UVT20K is annotated with a comprehensive set of ground truths, including saliency masks, scribbles, boundaries, and challenge attributes. In addition, we propose a Progressive Correlation Network (PCNet), which models inter- and intra-modal correlations on the basis of explicit alignment to achieve accurate predictions in unaligned image pairs. Extensive experiments conducted on unaligned and aligned datasets demonstrate the effectiveness of our method.Code and dataset are available at https://github.com/Angknpng/PCNet.'}",oai:arXiv.org:2412.14576v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Kunpeng Wang, Keke Chen, Chenglong Li, Zhengzheng Tu, Bin Luo'}]","Kunpeng Wang, Keke Chen, Chenglong Li, Zhengzheng Tu, Bin Luo","{'name': 'Kunpeng Wang, Keke Chen, Chenglong Li, Zhengzheng Tu, Bin Luo'}",,
193,GSRender: Deduplicated Occupancy Prediction via Weakly Supervised 3D Gaussian Splatting,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'GSRender: Deduplicated Occupancy Prediction via Weakly Supervised 3D Gaussian Splatting'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14579'}]",https://arxiv.org/abs/2412.14579,"arXiv:2412.14579v1 Announce Type: new 
Abstract: 3D occupancy perception is gaining increasing attention due to its capability to offer detailed and precise environment representations. Previous weakly-supervised NeRF methods balance efficiency and accuracy, with mIoU varying by 5-10 points due to sampling count along camera rays. Recently, real-time Gaussian splatting has gained widespread popularity in 3D reconstruction, and the occupancy prediction task can also be viewed as a reconstruction task. Consequently, we propose GSRender, which naturally employs 3D Gaussian Splatting for occupancy prediction, simplifying the sampling process. In addition, the limitations of 2D supervision result in duplicate predictions along the same camera ray. We implemented the Ray Compensation (RC) module, which mitigates this issue by compensating for features from adjacent frames. Finally, we redesigned the loss to eliminate the impact of dynamic objects from adjacent frames. Extensive experiments demonstrate that our approach achieves SOTA (state-of-the-art) results in RayIoU (+6.0), while narrowing the gap with 3D supervision methods. Our code will be released soon.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14579v1 Announce Type: new \nAbstract: 3D occupancy perception is gaining increasing attention due to its capability to offer detailed and precise environment representations. Previous weakly-supervised NeRF methods balance efficiency and accuracy, with mIoU varying by 5-10 points due to sampling count along camera rays. Recently, real-time Gaussian splatting has gained widespread popularity in 3D reconstruction, and the occupancy prediction task can also be viewed as a reconstruction task. Consequently, we propose GSRender, which naturally employs 3D Gaussian Splatting for occupancy prediction, simplifying the sampling process. In addition, the limitations of 2D supervision result in duplicate predictions along the same camera ray. We implemented the Ray Compensation (RC) module, which mitigates this issue by compensating for features from adjacent frames. Finally, we redesigned the loss to eliminate the impact of dynamic objects from adjacent frames. Extensive experiments demonstrate that our approach achieves SOTA (state-of-the-art) results in RayIoU (+6.0), while narrowing the gap with 3D supervision methods. Our code will be released soon.'}",oai:arXiv.org:2412.14579v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Qianpu Sun, Changyong Shu, Sifan Zhou, Zichen Yu, Yan Chen, Dawei Yang, Yuan Chun'}]","Qianpu Sun, Changyong Shu, Sifan Zhou, Zichen Yu, Yan Chen, Dawei Yang, Yuan Chun","{'name': 'Qianpu Sun, Changyong Shu, Sifan Zhou, Zichen Yu, Yan Chen, Dawei Yang, Yuan Chun'}",,
194,DiffSim: Taming Diffusion Models for Evaluating Visual Similarity,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'DiffSim: Taming Diffusion Models for Evaluating Visual Similarity'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14580'}]",https://arxiv.org/abs/2412.14580,"arXiv:2412.14580v1 Announce Type: new 
Abstract: Diffusion models have fundamentally transformed the field of generative models, making the assessment of similarity between customized model outputs and reference inputs critically important. However, traditional perceptual similarity metrics operate primarily at the pixel and patch levels, comparing low-level colors and textures but failing to capture mid-level similarities and differences in image layout, object pose, and semantic content. Contrastive learning-based CLIP and self-supervised learning-based DINO are often used to measure semantic similarity, but they highly compress image features, inadequately assessing appearance details. This paper is the first to discover that pretrained diffusion models can be utilized for measuring visual similarity and introduces the DiffSim method, addressing the limitations of traditional metrics in capturing perceptual consistency in custom generation tasks. By aligning features in the attention layers of the denoising U-Net, DiffSim evaluates both appearance and style similarity, showing superior alignment with human visual preferences. Additionally, we introduce the Sref and IP benchmarks to evaluate visual similarity at the level of style and instance, respectively. Comprehensive evaluations across multiple benchmarks demonstrate that DiffSim achieves state-of-the-art performance, providing a robust tool for measuring visual coherence in generative models.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14580v1 Announce Type: new \nAbstract: Diffusion models have fundamentally transformed the field of generative models, making the assessment of similarity between customized model outputs and reference inputs critically important. However, traditional perceptual similarity metrics operate primarily at the pixel and patch levels, comparing low-level colors and textures but failing to capture mid-level similarities and differences in image layout, object pose, and semantic content. Contrastive learning-based CLIP and self-supervised learning-based DINO are often used to measure semantic similarity, but they highly compress image features, inadequately assessing appearance details. This paper is the first to discover that pretrained diffusion models can be utilized for measuring visual similarity and introduces the DiffSim method, addressing the limitations of traditional metrics in capturing perceptual consistency in custom generation tasks. By aligning features in the attention layers of the denoising U-Net, DiffSim evaluates both appearance and style similarity, showing superior alignment with human visual preferences. Additionally, we introduce the Sref and IP benchmarks to evaluate visual similarity at the level of style and instance, respectively. Comprehensive evaluations across multiple benchmarks demonstrate that DiffSim achieves state-of-the-art performance, providing a robust tool for measuring visual coherence in generative models.'}",oai:arXiv.org:2412.14580v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by-sa/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-sa/4.0/'}","[{'name': 'Yiren Song, Xiaokang Liu, Mike Zheng Shou'}]","Yiren Song, Xiaokang Liu, Mike Zheng Shou","{'name': 'Yiren Song, Xiaokang Liu, Mike Zheng Shou'}",,
195,CORD: Balancing COnsistency and Rank Distillation for Robust Retrieval-Augmented Generation,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'CORD: Balancing COnsistency and Rank Distillation for Robust Retrieval-Augmented Generation'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14581'}]",https://arxiv.org/abs/2412.14581,"arXiv:2412.14581v1 Announce Type: new 
Abstract: With the adoption of retrieval-augmented generation (RAG), large language models (LLMs) are expected to ground their generation to the retrieved contexts. Yet, this is hindered by position bias of LLMs, failing to evenly attend to all contexts. Previous work has addressed this by synthesizing contexts with perturbed positions of gold segment, creating a position-diversified train set. We extend this intuition to propose consistency regularization with augmentation and distillation. First, we augment each training instance with its position perturbation to encourage consistent predictions, regardless of ordering. We also distill behaviors of this pair, although it can be counterproductive in certain RAG scenarios where the given order from the retriever is crucial for generation quality. We thus propose CORD, balancing COnsistency and Rank Distillation. CORD adaptively samples noise-controlled perturbations from an interpolation space, ensuring both consistency and respect for the rank prior. Empirical results show this balance enables CORD to outperform consistently in diverse RAG benchmarks.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14581v1 Announce Type: new \nAbstract: With the adoption of retrieval-augmented generation (RAG), large language models (LLMs) are expected to ground their generation to the retrieved contexts. Yet, this is hindered by position bias of LLMs, failing to evenly attend to all contexts. Previous work has addressed this by synthesizing contexts with perturbed positions of gold segment, creating a position-diversified train set. We extend this intuition to propose consistency regularization with augmentation and distillation. First, we augment each training instance with its position perturbation to encourage consistent predictions, regardless of ordering. We also distill behaviors of this pair, although it can be counterproductive in certain RAG scenarios where the given order from the retriever is crucial for generation quality. We thus propose CORD, balancing COnsistency and Rank Distillation. CORD adaptively samples noise-controlled perturbations from an interpolation space, ensuring both consistency and respect for the rank prior. Empirical results show this balance enables CORD to outperform consistently in diverse RAG benchmarks.'}",oai:arXiv.org:2412.14581v1,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': ""Youngwon Lee, Seung-won Hwang, Daniel Campos, Filip Grali\\'nski, Zhewei Yao, Yuxiong He""}]","Youngwon Lee, Seung-won Hwang, Daniel Campos, Filip Grali\'nski, Zhewei Yao, Yuxiong He","{'name': ""Youngwon Lee, Seung-won Hwang, Daniel Campos, Filip Grali\\'nski, Zhewei Yao, Yuxiong He""}",,
196,Loss Minimization for Electrical Flows over Spanning Trees on Grids,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Loss Minimization for Electrical Flows over Spanning Trees on Grids'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14583'}]",https://arxiv.org/abs/2412.14583,"arXiv:2412.14583v1 Announce Type: new 
Abstract: We study the electrical distribution network reconfiguration problem, defined as follows. We are given an undirected graph with a root vertex, demand at each non-root vertex, and resistance on each edge. Then, we want to find a spanning tree of the graph that specifies the routing of power from the root to each vertex so that all the demands are satisfied and the energy loss is minimized. This problem is known to be NP-hard in general. When restricted to grids with uniform resistance and the root located at a corner, Gupta, Khodabaksh, Mortagy and Nikolova [Mathematical Programming 2022] invented the so-called Min-Min algorithm whose approximation factor is theoretically guaranteed. Our contributions are twofold. First, we prove that the problem is NP-hard even for grids; this resolves the open problem posed by Gupta et al. Second, we give a refined analysis of the Min-Min algorithm and improve its approximation factor under the same setup. In the analysis, we formulate the problem of giving an upper bound for the approximation factor as a non-linear optimization problem that maximizes a convex function over a polytope, which is less commonly employed in the analysis of approximation algorithms than linear optimization problems.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14583v1 Announce Type: new \nAbstract: We study the electrical distribution network reconfiguration problem, defined as follows. We are given an undirected graph with a root vertex, demand at each non-root vertex, and resistance on each edge. Then, we want to find a spanning tree of the graph that specifies the routing of power from the root to each vertex so that all the demands are satisfied and the energy loss is minimized. This problem is known to be NP-hard in general. When restricted to grids with uniform resistance and the root located at a corner, Gupta, Khodabaksh, Mortagy and Nikolova [Mathematical Programming 2022] invented the so-called Min-Min algorithm whose approximation factor is theoretically guaranteed. Our contributions are twofold. First, we prove that the problem is NP-hard even for grids; this resolves the open problem posed by Gupta et al. Second, we give a refined analysis of the Min-Min algorithm and improve its approximation factor under the same setup. In the analysis, we formulate the problem of giving an upper bound for the approximation factor as a non-linear optimization problem that maximizes a convex function over a polytope, which is less commonly employed in the analysis of approximation algorithms than linear optimization problems.'}",oai:arXiv.org:2412.14583v1,False,"[{'term': 'cs.DS', 'scheme': None, 'label': None}, {'term': 'math.OC', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Takehiro Ito, Naonori Kakimura, Naoyuki Kamiyama, Yusuke Kobayashi, Yoshio Okamoto'}]","Takehiro Ito, Naonori Kakimura, Naoyuki Kamiyama, Yusuke Kobayashi, Yoshio Okamoto","{'name': 'Takehiro Ito, Naonori Kakimura, Naoyuki Kamiyama, Yusuke Kobayashi, Yoshio Okamoto'}",,
197,Simulation-Free Hierarchical Latent Policy Planning for Proactive Dialogues,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Simulation-Free Hierarchical Latent Policy Planning for Proactive Dialogues'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14584'}]",https://arxiv.org/abs/2412.14584,"arXiv:2412.14584v1 Announce Type: new 
Abstract: Recent advancements in proactive dialogues have garnered significant attention, particularly for more complex objectives (e.g. emotion support and persuasion). Unlike traditional task-oriented dialogues, proactive dialogues demand advanced policy planning and adaptability, requiring rich scenarios and comprehensive policy repositories to develop such systems. However, existing approaches tend to rely on Large Language Models (LLMs) for user simulation and online learning, leading to biases that diverge from realistic scenarios and result in suboptimal efficiency. Moreover, these methods depend on manually defined, context-independent, coarse-grained policies, which not only incur high expert costs but also raise concerns regarding their completeness. In our work, we highlight the potential for automatically discovering policies directly from raw, real-world dialogue records. To this end, we introduce a novel dialogue policy planning framework, LDPP. It fully automates the process from mining policies in dialogue records to learning policy planning. Specifically, we employ a variant of the Variational Autoencoder to discover fine-grained policies represented as latent vectors. After automatically annotating the data with these latent policy labels, we propose an Offline Hierarchical Reinforcement Learning (RL) algorithm in the latent space to develop effective policy planning capabilities. Our experiments demonstrate that LDPP outperforms existing methods on two proactive scenarios, even surpassing ChatGPT with only a 1.8-billion-parameter LLM.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14584v1 Announce Type: new \nAbstract: Recent advancements in proactive dialogues have garnered significant attention, particularly for more complex objectives (e.g. emotion support and persuasion). Unlike traditional task-oriented dialogues, proactive dialogues demand advanced policy planning and adaptability, requiring rich scenarios and comprehensive policy repositories to develop such systems. However, existing approaches tend to rely on Large Language Models (LLMs) for user simulation and online learning, leading to biases that diverge from realistic scenarios and result in suboptimal efficiency. Moreover, these methods depend on manually defined, context-independent, coarse-grained policies, which not only incur high expert costs but also raise concerns regarding their completeness. In our work, we highlight the potential for automatically discovering policies directly from raw, real-world dialogue records. To this end, we introduce a novel dialogue policy planning framework, LDPP. It fully automates the process from mining policies in dialogue records to learning policy planning. Specifically, we employ a variant of the Variational Autoencoder to discover fine-grained policies represented as latent vectors. After automatically annotating the data with these latent policy labels, we propose an Offline Hierarchical Reinforcement Learning (RL) algorithm in the latent space to develop effective policy planning capabilities. Our experiments demonstrate that LDPP outperforms existing methods on two proactive scenarios, even surpassing ChatGPT with only a 1.8-billion-parameter LLM.'}",oai:arXiv.org:2412.14584v1,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Tao He, Lizi Liao, Yixin Cao, Yuanxing Liu, Yiheng Sun, Zerui Chen, Ming Liu, Bing Qin'}]","Tao He, Lizi Liao, Yixin Cao, Yuanxing Liu, Yiheng Sun, Zerui Chen, Ming Liu, Bing Qin","{'name': 'Tao He, Lizi Liao, Yixin Cao, Yuanxing Liu, Yiheng Sun, Zerui Chen, Ming Liu, Bing Qin'}",,
198,HiCM$^2$: Hierarchical Compact Memory Modeling for Dense Video Captioning,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'HiCM$^2$: Hierarchical Compact Memory Modeling for Dense Video Captioning'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14585'}]",https://arxiv.org/abs/2412.14585,"arXiv:2412.14585v1 Announce Type: new 
Abstract: With the growing demand for solutions to real-world video challenges, interest in dense video captioning (DVC) has been on the rise. DVC involves the automatic captioning and localization of untrimmed videos. Several studies highlight the challenges of DVC and introduce improved methods utilizing prior knowledge, such as pre-training and external memory. In this research, we propose a model that leverages the prior knowledge of human-oriented hierarchical compact memory inspired by human memory hierarchy and cognition. To mimic human-like memory recall, we construct a hierarchical memory and a hierarchical memory reading module. We build an efficient hierarchical compact memory by employing clustering of memory events and summarization using large language models. Comparative experiments demonstrate that this hierarchical memory recall process improves the performance of DVC by achieving state-of-the-art performance on YouCook2 and ViTT datasets.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14585v1 Announce Type: new \nAbstract: With the growing demand for solutions to real-world video challenges, interest in dense video captioning (DVC) has been on the rise. DVC involves the automatic captioning and localization of untrimmed videos. Several studies highlight the challenges of DVC and introduce improved methods utilizing prior knowledge, such as pre-training and external memory. In this research, we propose a model that leverages the prior knowledge of human-oriented hierarchical compact memory inspired by human memory hierarchy and cognition. To mimic human-like memory recall, we construct a hierarchical memory and a hierarchical memory reading module. We build an efficient hierarchical compact memory by employing clustering of memory events and summarization using large language models. Comparative experiments demonstrate that this hierarchical memory recall process improves the performance of DVC by achieving state-of-the-art performance on YouCook2 and ViTT datasets.'}",oai:arXiv.org:2412.14585v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by-nc-nd/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-nd/4.0/'}","[{'name': 'Minkuk Kim, Hyeon Bae Kim, Jinyoung Moon, Jinwoo Choi, Seong Tae Kim'}]","Minkuk Kim, Hyeon Bae Kim, Jinyoung Moon, Jinwoo Choi, Seong Tae Kim","{'name': 'Minkuk Kim, Hyeon Bae Kim, Jinyoung Moon, Jinwoo Choi, Seong Tae Kim'}",,
199,Spike2Former: Efficient Spiking Transformer for High-performance Image Segmentation,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Spike2Former: Efficient Spiking Transformer for High-performance Image Segmentation'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14587'}]",https://arxiv.org/abs/2412.14587,"arXiv:2412.14587v1 Announce Type: new 
Abstract: Spiking Neural Networks (SNNs) have a low-power advantage but perform poorly in image segmentation tasks. The reason is that directly converting neural networks with complex architectural designs for segmentation tasks into spiking versions leads to performance degradation and non-convergence. To address this challenge, we first identify the modules in the architecture design that lead to the severe reduction in spike firing, make targeted improvements, and propose Spike2Former architecture. Second, we propose normalized integer spiking neurons to solve the training stability problem of SNNs with complex architectures. We set a new state-of-the-art for SNNs in various semantic segmentation datasets, with a significant improvement of +12.7% mIoU and 5.0 efficiency on ADE20K, +14.3% mIoU and 5.2 efficiency on VOC2012, and +9.1% mIoU and 6.6 efficiency on CityScapes.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14587v1 Announce Type: new \nAbstract: Spiking Neural Networks (SNNs) have a low-power advantage but perform poorly in image segmentation tasks. The reason is that directly converting neural networks with complex architectural designs for segmentation tasks into spiking versions leads to performance degradation and non-convergence. To address this challenge, we first identify the modules in the architecture design that lead to the severe reduction in spike firing, make targeted improvements, and propose Spike2Former architecture. Second, we propose normalized integer spiking neurons to solve the training stability problem of SNNs with complex architectures. We set a new state-of-the-art for SNNs in various semantic segmentation datasets, with a significant improvement of +12.7% mIoU and 5.0 efficiency on ADE20K, +14.3% mIoU and 5.2 efficiency on VOC2012, and +9.1% mIoU and 6.6 efficiency on CityScapes.'}",oai:arXiv.org:2412.14587v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.NE', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Zhenxin Lei, Man Yao, Jiakui Hu, Xinhao Luo, Yanye Lu, Bo Xu, Guoqi Li'}]","Zhenxin Lei, Man Yao, Jiakui Hu, Xinhao Luo, Yanye Lu, Bo Xu, Guoqi Li","{'name': 'Zhenxin Lei, Man Yao, Jiakui Hu, Xinhao Luo, Yanye Lu, Bo Xu, Guoqi Li'}",,
200,Beyond Guilt: Legal Judgment Prediction with Trichotomous Reasoning,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Beyond Guilt: Legal Judgment Prediction with Trichotomous Reasoning'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14588'}]",https://arxiv.org/abs/2412.14588,"arXiv:2412.14588v1 Announce Type: new 
Abstract: In legal practice, judges apply the trichotomous dogmatics of criminal law, sequentially assessing the elements of the offense, unlawfulness, and culpability to determine whether an individual's conduct constitutes a crime. Although current legal large language models (LLMs) show promising accuracy in judgment prediction, they lack trichotomous reasoning capabilities due to the absence of an appropriate benchmark dataset, preventing them from predicting innocent outcomes. As a result, every input is automatically assigned a charge, limiting their practical utility in legal contexts. To bridge this gap, we introduce LJPIV, the first benchmark dataset for Legal Judgment Prediction with Innocent Verdicts. Adhering to the trichotomous dogmatics, we extend three widely-used legal datasets through LLM-based augmentation and manual verification. Our experiments with state-of-the-art legal LLMs and novel strategies that integrate trichotomous reasoning into zero-shot prompting and fine-tuning reveal: (1) current legal LLMs have significant room for improvement, with even the best models achieving an F1 score of less than 0.3 on LJPIV; and (2) our strategies notably enhance both in-domain and cross-domain judgment prediction accuracy, especially for cases resulting in an innocent verdict.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14588v1 Announce Type: new \nAbstract: In legal practice, judges apply the trichotomous dogmatics of criminal law, sequentially assessing the elements of the offense, unlawfulness, and culpability to determine whether an individual's conduct constitutes a crime. Although current legal large language models (LLMs) show promising accuracy in judgment prediction, they lack trichotomous reasoning capabilities due to the absence of an appropriate benchmark dataset, preventing them from predicting innocent outcomes. As a result, every input is automatically assigned a charge, limiting their practical utility in legal contexts. To bridge this gap, we introduce LJPIV, the first benchmark dataset for Legal Judgment Prediction with Innocent Verdicts. Adhering to the trichotomous dogmatics, we extend three widely-used legal datasets through LLM-based augmentation and manual verification. Our experiments with state-of-the-art legal LLMs and novel strategies that integrate trichotomous reasoning into zero-shot prompting and fine-tuning reveal: (1) current legal LLMs have significant room for improvement, with even the best models achieving an F1 score of less than 0.3 on LJPIV; and (2) our strategies notably enhance both in-domain and cross-domain judgment prediction accuracy, especially for cases resulting in an innocent verdict.""}",oai:arXiv.org:2412.14588v1,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Kepu Zhang, Haoyue Yang, Xu Tang, Weijie Yu, Jun Xu'}]","Kepu Zhang, Haoyue Yang, Xu Tang, Weijie Yu, Jun Xu","{'name': 'Kepu Zhang, Haoyue Yang, Xu Tang, Weijie Yu, Jun Xu'}",,
201,MixLLM: LLM Quantization with Global Mixed-precision between Output-features and Highly-efficient System Design,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'MixLLM: LLM Quantization with Global Mixed-precision between Output-features and Highly-efficient System Design'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14590'}]",https://arxiv.org/abs/2412.14590,"arXiv:2412.14590v1 Announce Type: new 
Abstract: Quantization has become one of the most effective methodologies to compress LLMs into smaller size. However, the existing quantization solutions still show limitations of either non-negligible accuracy drop or system inefficiency. In this paper, we make a comprehensive analysis of the general quantization principles on their effect to the triangle of accuracy, memory consumption and system efficiency. We propose MixLLM that explores the new optimization space of mixed-precision quantization between output features based on the insight that different output features matter differently in the model. MixLLM identifies the output features with high salience in the global view rather than within each single layer, effectively assigning the larger bit-width to output features that need it most to achieve good accuracy with low memory consumption. We present the sweet spot of quantization configuration of algorithm-system co-design that leads to high accuracy and system efficiency. To address the system challenge, we design the two-step dequantization to make use of the int8 Tensor Core easily and fast data type conversion to reduce dequantization overhead significantly, and present the software pipeline to overlap the memory access, dequantization and the MatMul to the best. Extensive experiments show that with only 10% more bits, the PPL increasement can be reduced from about 0.5 in SOTA to within 0.2 for Llama 3.1 70B, while on average MMLU-Pro improves by 0.93 over the SOTA of three popular models. In addition to its superior accuracy, MixLLM also achieves state-of-the-art system efficiency.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14590v1 Announce Type: new \nAbstract: Quantization has become one of the most effective methodologies to compress LLMs into smaller size. However, the existing quantization solutions still show limitations of either non-negligible accuracy drop or system inefficiency. In this paper, we make a comprehensive analysis of the general quantization principles on their effect to the triangle of accuracy, memory consumption and system efficiency. We propose MixLLM that explores the new optimization space of mixed-precision quantization between output features based on the insight that different output features matter differently in the model. MixLLM identifies the output features with high salience in the global view rather than within each single layer, effectively assigning the larger bit-width to output features that need it most to achieve good accuracy with low memory consumption. We present the sweet spot of quantization configuration of algorithm-system co-design that leads to high accuracy and system efficiency. To address the system challenge, we design the two-step dequantization to make use of the int8 Tensor Core easily and fast data type conversion to reduce dequantization overhead significantly, and present the software pipeline to overlap the memory access, dequantization and the MatMul to the best. Extensive experiments show that with only 10% more bits, the PPL increasement can be reduced from about 0.5 in SOTA to within 0.2 for Llama 3.1 70B, while on average MMLU-Pro improves by 0.93 over the SOTA of three popular models. In addition to its superior accuracy, MixLLM also achieves state-of-the-art system efficiency.'}",oai:arXiv.org:2412.14590v1,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Zhen Zheng, Xiaonan Song, Chuanjie Liu'}]","Zhen Zheng, Xiaonan Song, Chuanjie Liu","{'name': 'Zhen Zheng, Xiaonan Song, Chuanjie Liu'}",,
202,"Multi-Sensor Object Anomaly Detection: Unifying Appearance, Geometry, and Internal Properties","{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Multi-Sensor Object Anomaly Detection: Unifying Appearance, Geometry, and Internal Properties'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14592'}]",https://arxiv.org/abs/2412.14592,"arXiv:2412.14592v1 Announce Type: new 
Abstract: Object anomaly detection is essential for industrial quality inspection, yet traditional single-sensor methods face critical limitations. They fail to capture the wide range of anomaly types, as single sensors are often constrained to either external appearance, geometric structure, or internal properties. To overcome these challenges, we introduce MulSen-AD, the first high-resolution, multi-sensor anomaly detection dataset tailored for industrial applications. MulSen-AD unifies data from RGB cameras, laser scanners, and lock-in infrared thermography, effectively capturing external appearance, geometric deformations, and internal defects. The dataset spans 15 industrial products with diverse, real-world anomalies. We also present MulSen-AD Bench, a benchmark designed to evaluate multi-sensor methods, and propose MulSen-TripleAD, a decision-level fusion algorithm that integrates these three modalities for robust, unsupervised object anomaly detection. Our experiments demonstrate that multi-sensor fusion substantially outperforms single-sensor approaches, achieving 96.1% AUROC in object-level detection accuracy. These results highlight the importance of integrating multi-sensor data for comprehensive industrial anomaly detection.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14592v1 Announce Type: new \nAbstract: Object anomaly detection is essential for industrial quality inspection, yet traditional single-sensor methods face critical limitations. They fail to capture the wide range of anomaly types, as single sensors are often constrained to either external appearance, geometric structure, or internal properties. To overcome these challenges, we introduce MulSen-AD, the first high-resolution, multi-sensor anomaly detection dataset tailored for industrial applications. MulSen-AD unifies data from RGB cameras, laser scanners, and lock-in infrared thermography, effectively capturing external appearance, geometric deformations, and internal defects. The dataset spans 15 industrial products with diverse, real-world anomalies. We also present MulSen-AD Bench, a benchmark designed to evaluate multi-sensor methods, and propose MulSen-TripleAD, a decision-level fusion algorithm that integrates these three modalities for robust, unsupervised object anomaly detection. Our experiments demonstrate that multi-sensor fusion substantially outperforms single-sensor approaches, achieving 96.1% AUROC in object-level detection accuracy. These results highlight the importance of integrating multi-sensor data for comprehensive industrial anomaly detection.'}",oai:arXiv.org:2412.14592v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Wenqiao Li, Bozhong Zheng, Xiaohao Xu, Jinye Gan, Fading Lu, Xiang Li, Na Ni, Zheng Tian, Xiaonan Huang, Shenghua Gao, Yingna Wu'}]","Wenqiao Li, Bozhong Zheng, Xiaohao Xu, Jinye Gan, Fading Lu, Xiang Li, Na Ni, Zheng Tian, Xiaonan Huang, Shenghua Gao, Yingna Wu","{'name': 'Wenqiao Li, Bozhong Zheng, Xiaohao Xu, Jinye Gan, Fading Lu, Xiang Li, Na Ni, Zheng Tian, Xiaonan Huang, Shenghua Gao, Yingna Wu'}",,
203,On the Lebesgue constant of the Morrow-Patterson points,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'On the Lebesgue constant of the Morrow-Patterson points'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14595'}]",https://arxiv.org/abs/2412.14595,"arXiv:2412.14595v1 Announce Type: new 
Abstract: The study of interpolation nodes and their associated Lebesgue constants are central to numerical analysis, impacting the stability and accuracy of polynomial approximations. In this paper, we will explore the Morrow-Patterson points, a set of interpolation nodes introduced to construct cubature formulas of a minimum number of points in the square for a fixed degree $n$. We prove that their Lebesgue constant growth is ${\cal O}(n^2)$ as was conjectured based on numerical evidence about twenty years ago in the paper by Caliari, M., De Marchi, S., Vianello, M., {\it Bivariate polynomial interpolation on the square at new nodal sets}, Appl. Math. Comput. 165(2) (2005), 261--274.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14595v1 Announce Type: new \nAbstract: The study of interpolation nodes and their associated Lebesgue constants are central to numerical analysis, impacting the stability and accuracy of polynomial approximations. In this paper, we will explore the Morrow-Patterson points, a set of interpolation nodes introduced to construct cubature formulas of a minimum number of points in the square for a fixed degree $n$. We prove that their Lebesgue constant growth is ${\\cal O}(n^2)$ as was conjectured based on numerical evidence about twenty years ago in the paper by Caliari, M., De Marchi, S., Vianello, M., {\\it Bivariate polynomial interpolation on the square at new nodal sets}, Appl. Math. Comput. 165(2) (2005), 261--274.'}",oai:arXiv.org:2412.14595v1,False,"[{'term': 'math.NA', 'scheme': None, 'label': None}, {'term': 'cs.NA', 'scheme': None, 'label': None}, {'term': 'math.CA', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Tomasz Beberok, Leokadia Bia{\\l}as-Cie\\.z, Stefano De Marchi'}]","Tomasz Beberok, Leokadia Bia{\l}as-Cie\.z, Stefano De Marchi","{'name': 'Tomasz Beberok, Leokadia Bia{\\l}as-Cie\\.z, Stefano De Marchi'}",,
204,LDP: Generalizing to Multilingual Visual Information Extraction by Language Decoupled Pretraining,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'LDP: Generalizing to Multilingual Visual Information Extraction by Language Decoupled Pretraining'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14596'}]",https://arxiv.org/abs/2412.14596,"arXiv:2412.14596v1 Announce Type: new 
Abstract: Visual Information Extraction (VIE) plays a crucial role in the comprehension of semi-structured documents, and several pre-trained models have been developed to enhance performance. However, most of these works are monolingual (usually English). Due to the extremely unbalanced quantity and quality of pre-training corpora between English and other languages, few works can extend to non-English scenarios. In this paper, we conduct systematic experiments to show that vision and layout modality hold invariance among images with different languages. If decoupling language bias from document images, a vision-layout-based model can achieve impressive cross-lingual generalization. Accordingly, we present a simple but effective multilingual training paradigm LDP (Language Decoupled Pre-training) for better utilization of monolingual pre-training data. Our proposed model LDM (Language Decoupled Model) is first pre-trained on the language-independent data, where the language knowledge is decoupled by a diffusion model, and then the LDM is fine-tuned on the downstream languages. Extensive experiments show that the LDM outperformed all SOTA multilingual pre-trained models, and also maintains competitiveness on downstream monolingual/English benchmarks.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14596v1 Announce Type: new \nAbstract: Visual Information Extraction (VIE) plays a crucial role in the comprehension of semi-structured documents, and several pre-trained models have been developed to enhance performance. However, most of these works are monolingual (usually English). Due to the extremely unbalanced quantity and quality of pre-training corpora between English and other languages, few works can extend to non-English scenarios. In this paper, we conduct systematic experiments to show that vision and layout modality hold invariance among images with different languages. If decoupling language bias from document images, a vision-layout-based model can achieve impressive cross-lingual generalization. Accordingly, we present a simple but effective multilingual training paradigm LDP (Language Decoupled Pre-training) for better utilization of monolingual pre-training data. Our proposed model LDM (Language Decoupled Model) is first pre-trained on the language-independent data, where the language knowledge is decoupled by a diffusion model, and then the LDM is fine-tuned on the downstream languages. Extensive experiments show that the LDM outperformed all SOTA multilingual pre-trained models, and also maintains competitiveness on downstream monolingual/English benchmarks.'}",oai:arXiv.org:2412.14596v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.CL', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Huawen Shen, Gengluo Li, Jinwen Zhong, Yu Zhou'}]","Huawen Shen, Gengluo Li, Jinwen Zhong, Yu Zhou","{'name': 'Huawen Shen, Gengluo Li, Jinwen Zhong, Yu Zhou'}",,
205,"Can We Get Rid of Handcrafted Feature Extractors? SparseViT: Nonsemantics-Centered, Parameter-Efficient Image Manipulation Localization Through Spare-Coding Transformer","{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Can We Get Rid of Handcrafted Feature Extractors? SparseViT: Nonsemantics-Centered, Parameter-Efficient Image Manipulation Localization Through Spare-Coding Transformer'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14598'}]",https://arxiv.org/abs/2412.14598,"arXiv:2412.14598v1 Announce Type: new 
Abstract: Non-semantic features or semantic-agnostic features, which are irrelevant to image context but sensitive to image manipulations, are recognized as evidential to Image Manipulation Localization (IML). Since manual labels are impossible, existing works rely on handcrafted methods to extract non-semantic features. Handcrafted non-semantic features jeopardize IML model's generalization ability in unseen or complex scenarios. Therefore, for IML, the elephant in the room is: How to adaptively extract non-semantic features? Non-semantic features are context-irrelevant and manipulation-sensitive. That is, within an image, they are consistent across patches unless manipulation occurs. Then, spare and discrete interactions among image patches are sufficient for extracting non-semantic features. However, image semantics vary drastically on different patches, requiring dense and continuous interactions among image patches for learning semantic representations. Hence, in this paper, we propose a Sparse Vision Transformer (SparseViT), which reformulates the dense, global self-attention in ViT into a sparse, discrete manner. Such sparse self-attention breaks image semantics and forces SparseViT to adaptively extract non-semantic features for images. Besides, compared with existing IML models, the sparse self-attention mechanism largely reduced the model size (max 80% in FLOPs), achieving stunning parameter efficiency and computation reduction. Extensive experiments demonstrate that, without any handcrafted feature extractors, SparseViT is superior in both generalization and efficiency across benchmark datasets.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14598v1 Announce Type: new \nAbstract: Non-semantic features or semantic-agnostic features, which are irrelevant to image context but sensitive to image manipulations, are recognized as evidential to Image Manipulation Localization (IML). Since manual labels are impossible, existing works rely on handcrafted methods to extract non-semantic features. Handcrafted non-semantic features jeopardize IML model's generalization ability in unseen or complex scenarios. Therefore, for IML, the elephant in the room is: How to adaptively extract non-semantic features? Non-semantic features are context-irrelevant and manipulation-sensitive. That is, within an image, they are consistent across patches unless manipulation occurs. Then, spare and discrete interactions among image patches are sufficient for extracting non-semantic features. However, image semantics vary drastically on different patches, requiring dense and continuous interactions among image patches for learning semantic representations. Hence, in this paper, we propose a Sparse Vision Transformer (SparseViT), which reformulates the dense, global self-attention in ViT into a sparse, discrete manner. Such sparse self-attention breaks image semantics and forces SparseViT to adaptively extract non-semantic features for images. Besides, compared with existing IML models, the sparse self-attention mechanism largely reduced the model size (max 80% in FLOPs), achieving stunning parameter efficiency and computation reduction. Extensive experiments demonstrate that, without any handcrafted feature extractors, SparseViT is superior in both generalization and efficiency across benchmark datasets.""}",oai:arXiv.org:2412.14598v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Lei Su, Xiaochen Ma, Xuekang Zhu, Chaoqun Niu, Zeyu Lei, Ji-Zhe Zhou'}]","Lei Su, Xiaochen Ma, Xuekang Zhu, Chaoqun Niu, Zeyu Lei, Ji-Zhe Zhou","{'name': 'Lei Su, Xiaochen Ma, Xuekang Zhu, Chaoqun Niu, Zeyu Lei, Ji-Zhe Zhou'}",,
206,Towards Scalable and Deep Graph Neural Networks via Noise Masking,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Towards Scalable and Deep Graph Neural Networks via Noise Masking'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14602'}]",https://arxiv.org/abs/2412.14602,"arXiv:2412.14602v1 Announce Type: new 
Abstract: In recent years, Graph Neural Networks (GNNs) have achieved remarkable success in many graph mining tasks. However, scaling them to large graphs is challenging due to the high computational and storage costs of repeated feature propagation and non-linear transformation during training. One commonly employed approach to address this challenge is model-simplification, which only executes the Propagation (P) once in the pre-processing, and Combine (C) these receptive fields in different ways and then feed them into a simple model for better performance. Despite their high predictive performance and scalability, these methods still face two limitations. First, existing approaches mainly focus on exploring different C methods from the model perspective, neglecting the crucial problem of performance degradation with increasing P depth from the data-centric perspective, known as the over-smoothing problem. Second, pre-processing overhead takes up most of the end-to-end processing time, especially for large-scale graphs. To address these limitations, we present random walk with noise masking (RMask), a plug-and-play module compatible with the existing model-simplification works. This module enables the exploration of deeper GNNs while preserving their scalability. Unlike the previous model-simplification works, we focus on continuous P and found that the noise existing inside each P is the cause of the over-smoothing issue, and use the efficient masking mechanism to eliminate them. Experimental results on six real-world datasets demonstrate that model-simplification works equipped with RMask yield superior performance compared to their original version and can make a good trade-off between accuracy and efficiency.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14602v1 Announce Type: new \nAbstract: In recent years, Graph Neural Networks (GNNs) have achieved remarkable success in many graph mining tasks. However, scaling them to large graphs is challenging due to the high computational and storage costs of repeated feature propagation and non-linear transformation during training. One commonly employed approach to address this challenge is model-simplification, which only executes the Propagation (P) once in the pre-processing, and Combine (C) these receptive fields in different ways and then feed them into a simple model for better performance. Despite their high predictive performance and scalability, these methods still face two limitations. First, existing approaches mainly focus on exploring different C methods from the model perspective, neglecting the crucial problem of performance degradation with increasing P depth from the data-centric perspective, known as the over-smoothing problem. Second, pre-processing overhead takes up most of the end-to-end processing time, especially for large-scale graphs. To address these limitations, we present random walk with noise masking (RMask), a plug-and-play module compatible with the existing model-simplification works. This module enables the exploration of deeper GNNs while preserving their scalability. Unlike the previous model-simplification works, we focus on continuous P and found that the noise existing inside each P is the cause of the over-smoothing issue, and use the efficient masking mechanism to eliminate them. Experimental results on six real-world datasets demonstrate that model-simplification works equipped with RMask yield superior performance compared to their original version and can make a good trade-off between accuracy and efficiency.'}",oai:arXiv.org:2412.14602v1,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Yuxuan Liang, Wentao Zhang, Zeang Sheng, Ling Yang, Quanqing Xu, Jiawei Jiang, Yunhai Tong, Bin Cu'}]","Yuxuan Liang, Wentao Zhang, Zeang Sheng, Ling Yang, Quanqing Xu, Jiawei Jiang, Yunhai Tong, Bin Cu","{'name': 'Yuxuan Liang, Wentao Zhang, Zeang Sheng, Ling Yang, Quanqing Xu, Jiawei Jiang, Yunhai Tong, Bin Cu'}",,
207,Successive optimization of optics and post-processing with differentiable coherent PSF operator and field information,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Successive optimization of optics and post-processing with differentiable coherent PSF operator and field information'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14603'}]",https://arxiv.org/abs/2412.14603,"arXiv:2412.14603v1 Announce Type: new 
Abstract: Recently, the joint design of optical systems and downstream algorithms is showing significant potential. However, existing rays-described methods are limited to optimizing geometric degradation, making it difficult to fully represent the optical characteristics of complex, miniaturized lenses constrained by wavefront aberration or diffraction effects. In this work, we introduce a precise optical simulation model, and every operation in pipeline is differentiable. This model employs a novel initial value strategy to enhance the reliability of intersection calculation on high aspherics. Moreover, it utilizes a differential operator to reduce memory consumption during coherent point spread function calculations. To efficiently address various degradation, we design a joint optimization procedure that leverages field information. Guided by a general restoration network, the proposed method not only enhances the image quality, but also successively improves the optical performance across multiple lenses that are already in professional level. This joint optimization pipeline offers innovative insights into the practical design of sophisticated optical systems and post-processing algorithms. The source code will be made publicly available at https://github.com/Zrr-ZJU/Successive-optimization","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14603v1 Announce Type: new \nAbstract: Recently, the joint design of optical systems and downstream algorithms is showing significant potential. However, existing rays-described methods are limited to optimizing geometric degradation, making it difficult to fully represent the optical characteristics of complex, miniaturized lenses constrained by wavefront aberration or diffraction effects. In this work, we introduce a precise optical simulation model, and every operation in pipeline is differentiable. This model employs a novel initial value strategy to enhance the reliability of intersection calculation on high aspherics. Moreover, it utilizes a differential operator to reduce memory consumption during coherent point spread function calculations. To efficiently address various degradation, we design a joint optimization procedure that leverages field information. Guided by a general restoration network, the proposed method not only enhances the image quality, but also successively improves the optical performance across multiple lenses that are already in professional level. This joint optimization pipeline offers innovative insights into the practical design of sophisticated optical systems and post-processing algorithms. The source code will be made publicly available at https://github.com/Zrr-ZJU/Successive-optimization'}",oai:arXiv.org:2412.14603v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'physics.optics', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Zheng Ren, Jingwen Zhou, Wenguan Zhang, Jiapu Yan, Bingkun Chen, Huajun Feng, Shiqi Chen'}]","Zheng Ren, Jingwen Zhou, Wenguan Zhang, Jiapu Yan, Bingkun Chen, Huajun Feng, Shiqi Chen","{'name': 'Zheng Ren, Jingwen Zhou, Wenguan Zhang, Jiapu Yan, Bingkun Chen, Huajun Feng, Shiqi Chen'}",,
208,Computational Sociology of Humans and Machines; Conflict and Collaboration,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Computational Sociology of Humans and Machines; Conflict and Collaboration'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14606'}]",https://arxiv.org/abs/2412.14606,"arXiv:2412.14606v1 Announce Type: new 
Abstract: This Chapter examines the dynamics of conflict and collaboration in human-machine systems, with a particular focus on large-scale, internet-based collaborative platforms. While these platforms represent successful examples of collective knowledge production, they are also sites of significant conflict, as diverse participants with differing intentions and perspectives interact. The analysis identifies recurring patterns of interaction, including serial attacks, reciprocal revenge, and third-party interventions. These microstructures reveal the role of experience, cultural differences, and topic sensitivity in shaping human-human, human-machine, and machine-machine interactions. The chapter further investigates the role of algorithmic agents and bots, highlighting their dual nature: they enhance collaboration by automating tasks but can also contribute to persistent conflicts with both humans and other machines. We conclude with policy recommendations that emphasize transparency, balance, cultural sensitivity, and governance to maximize the benefits of human-machine synergy while minimizing potential detriments.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14606v1 Announce Type: new \nAbstract: This Chapter examines the dynamics of conflict and collaboration in human-machine systems, with a particular focus on large-scale, internet-based collaborative platforms. While these platforms represent successful examples of collective knowledge production, they are also sites of significant conflict, as diverse participants with differing intentions and perspectives interact. The analysis identifies recurring patterns of interaction, including serial attacks, reciprocal revenge, and third-party interventions. These microstructures reveal the role of experience, cultural differences, and topic sensitivity in shaping human-human, human-machine, and machine-machine interactions. The chapter further investigates the role of algorithmic agents and bots, highlighting their dual nature: they enhance collaboration by automating tasks but can also contribute to persistent conflicts with both humans and other machines. We conclude with policy recommendations that emphasize transparency, balance, cultural sensitivity, and governance to maximize the benefits of human-machine synergy while minimizing potential detriments.'}",oai:arXiv.org:2412.14606v1,False,"[{'term': 'cs.CY', 'scheme': None, 'label': None}, {'term': 'cs.HC', 'scheme': None, 'label': None}, {'term': 'cs.SI', 'scheme': None, 'label': None}, {'term': 'physics.soc-ph', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}",[{'name': 'Taha Yasseri'}],Taha Yasseri,{'name': 'Taha Yasseri'},,
209,Reachability in Vector Addition System with States Parameterized by Geometric Dimension,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Reachability in Vector Addition System with States Parameterized by Geometric Dimension'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14608'}]",https://arxiv.org/abs/2412.14608,"arXiv:2412.14608v1 Announce Type: new 
Abstract: The geometric dimension of a Vector Addition System with States (VASS), emerged in Leroux and Schmitz (2019) and formalized by Fu, Yang, and Zheng (2024), quantifies the dimension of the vector space spanned by cycle effects in the system. This paper explores the VASS reachability problem through the lens of geometric dimension, revealing key differences from the traditional dimensional parameterization. Notably, we establish that the reachability problem for both geometrically 1-dimensional and 2-dimensional VASS is PSPACE-complete, achieved by extending the pumping technique originally proposed by Czerwi\'nski et al. (2019).","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14608v1 Announce Type: new \nAbstract: The geometric dimension of a Vector Addition System with States (VASS), emerged in Leroux and Schmitz (2019) and formalized by Fu, Yang, and Zheng (2024), quantifies the dimension of the vector space spanned by cycle effects in the system. This paper explores the VASS reachability problem through the lens of geometric dimension, revealing key differences from the traditional dimensional parameterization. Notably, we establish that the reachability problem for both geometrically 1-dimensional and 2-dimensional VASS is PSPACE-complete, achieved by extending the pumping technique originally proposed by Czerwi\\'nski et al. (2019).""}",oai:arXiv.org:2412.14608v1,False,"[{'term': 'cs.FL', 'scheme': None, 'label': None}, {'term': 'cs.LO', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}",[{'name': 'Yangluo Zheng'}],Yangluo Zheng,{'name': 'Yangluo Zheng'},,
210,"Is This You, LLM? Recognizing AI-written Programs with Multilingual Code Stylometry","{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Is This You, LLM? Recognizing AI-written Programs with Multilingual Code Stylometry'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14611'}]",https://arxiv.org/abs/2412.14611,"arXiv:2412.14611v1 Announce Type: new 
Abstract: With the increasing popularity of LLM-based code completers, like GitHub Copilot, the interest in automatically detecting AI-generated code is also increasing-in particular in contexts where the use of LLMs to program is forbidden by policy due to security, intellectual property, or ethical concerns.We introduce a novel technique for AI code stylometry, i.e., the ability to distinguish code generated by LLMs from code written by humans, based on a transformer-based encoder classifier. Differently from previous work, our classifier is capable of detecting AI-written code across 10 different programming languages with a single machine learning model, maintaining high average accuracy across all languages (84.1% $\pm$ 3.8%).Together with the classifier we also release H-AIRosettaMP, a novel open dataset for AI code stylometry tasks, consisting of 121 247 code snippets in 10 popular programming languages, labeled as either human-written or AI-generated. The experimental pipeline (dataset, training code, resulting models) is the first fully reproducible one for the AI code stylometry task. Most notably our experiments rely only on open LLMs, rather than on proprietary/closed ones like ChatGPT.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14611v1 Announce Type: new \nAbstract: With the increasing popularity of LLM-based code completers, like GitHub Copilot, the interest in automatically detecting AI-generated code is also increasing-in particular in contexts where the use of LLMs to program is forbidden by policy due to security, intellectual property, or ethical concerns.We introduce a novel technique for AI code stylometry, i.e., the ability to distinguish code generated by LLMs from code written by humans, based on a transformer-based encoder classifier. Differently from previous work, our classifier is capable of detecting AI-written code across 10 different programming languages with a single machine learning model, maintaining high average accuracy across all languages (84.1% $\\pm$ 3.8%).Together with the classifier we also release H-AIRosettaMP, a novel open dataset for AI code stylometry tasks, consisting of 121 247 code snippets in 10 popular programming languages, labeled as either human-written or AI-generated. The experimental pipeline (dataset, training code, resulting models) is the first fully reproducible one for the AI code stylometry task. Most notably our experiments rely only on open LLMs, rather than on proprietary/closed ones like ChatGPT.'}",oai:arXiv.org:2412.14611v1,False,"[{'term': 'cs.SE', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Andrea Gurioli (DISI, UNIBO), Maurizio Gabbrielli (DISI, UNIBO), Stefano Zacchiroli (IP Paris, LTCI, ACES, INFRES)'}]","Andrea Gurioli (DISI, UNIBO), Maurizio Gabbrielli (DISI, UNIBO), Stefano Zacchiroli (IP Paris, LTCI, ACES, INFRES)","{'name': 'Andrea Gurioli (DISI, UNIBO), Maurizio Gabbrielli (DISI, UNIBO), Stefano Zacchiroli (IP Paris, LTCI, ACES, INFRES)'}",,"IEEE International Conference on Software Analysis, Evolution and Reengineering, SANER 2025, Mar 2025, Montr{\'e}al, Canada"
211,KARRIEREWEGE: A Large Scale Career Path Prediction Dataset,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'KARRIEREWEGE: A Large Scale Career Path Prediction Dataset'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14612'}]",https://arxiv.org/abs/2412.14612,"arXiv:2412.14612v1 Announce Type: new 
Abstract: Accurate career path prediction can support many stakeholders, like job seekers, recruiters, HR, and project managers. However, publicly available data and tools for career path prediction are scarce. In this work, we introduce KARRIEREWEGE, a comprehensive, publicly available dataset containing over 500k career paths, significantly surpassing the size of previously available datasets. We link the dataset to the ESCO taxonomy to offer a valuable resource for predicting career trajectories. To tackle the problem of free-text inputs typically found in resumes, we enhance it by synthesizing job titles and descriptions resulting in KARRIEREWEGE+. This allows for accurate predictions from unstructured data, closely aligning with real-world application challenges. We benchmark existing state-of-the-art (SOTA) models on our dataset and a prior benchmark and observe improved performance and robustness, particularly for free-text use cases, due to the synthesized data.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14612v1 Announce Type: new \nAbstract: Accurate career path prediction can support many stakeholders, like job seekers, recruiters, HR, and project managers. However, publicly available data and tools for career path prediction are scarce. In this work, we introduce KARRIEREWEGE, a comprehensive, publicly available dataset containing over 500k career paths, significantly surpassing the size of previously available datasets. We link the dataset to the ESCO taxonomy to offer a valuable resource for predicting career trajectories. To tackle the problem of free-text inputs typically found in resumes, we enhance it by synthesizing job titles and descriptions resulting in KARRIEREWEGE+. This allows for accurate predictions from unstructured data, closely aligning with real-world application challenges. We benchmark existing state-of-the-art (SOTA) models on our dataset and a prior benchmark and observe improved performance and robustness, particularly for free-text use cases, due to the synthesized data.'}",oai:arXiv.org:2412.14612v1,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Elena Senger, Yuri Campbell, Rob van der Goot, Barbara Plank'}]","Elena Senger, Yuri Campbell, Rob van der Goot, Barbara Plank","{'name': 'Elena Senger, Yuri Campbell, Rob van der Goot, Barbara Plank'}",,
212,"HarmonicEval: Multi-modal, Multi-task, Multi-criteria Automatic Evaluation Using a Vision Language Model","{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'HarmonicEval: Multi-modal, Multi-task, Multi-criteria Automatic Evaluation Using a Vision Language Model'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14613'}]",https://arxiv.org/abs/2412.14613,"arXiv:2412.14613v1 Announce Type: new 
Abstract: Vision-language models (VLMs) have shown impressive abilities in text and image understanding. However, existing metrics for evaluating the text generated by VLMs focus exclusively on overall quality, leading to two limitations: 1) it is challenging to identify which aspects of the text need improvement from the overall score; 2) metrics may overlook specific evaluation criteria when predicting an overall score. To address these limitations, we propose HarmonicEval, a reference-free evaluation metric that aggregates criterion-wise scores to produce the overall score in a bottom-up manner. Furthermore, we construct the Multi-task Multi-criteria Human Evaluation (MMHE) dataset, which comprises 18,000 expert human judgments across four vision-language tasks. Our experiments demonstrate that HarmonicEval achieves higher correlations with human judgments than conventional metrics while providing numerical scores for each criterion.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14613v1 Announce Type: new \nAbstract: Vision-language models (VLMs) have shown impressive abilities in text and image understanding. However, existing metrics for evaluating the text generated by VLMs focus exclusively on overall quality, leading to two limitations: 1) it is challenging to identify which aspects of the text need improvement from the overall score; 2) metrics may overlook specific evaluation criteria when predicting an overall score. To address these limitations, we propose HarmonicEval, a reference-free evaluation metric that aggregates criterion-wise scores to produce the overall score in a bottom-up manner. Furthermore, we construct the Multi-task Multi-criteria Human Evaluation (MMHE) dataset, which comprises 18,000 expert human judgments across four vision-language tasks. Our experiments demonstrate that HarmonicEval achieves higher correlations with human judgments than conventional metrics while providing numerical scores for each criterion.'}",oai:arXiv.org:2412.14613v1,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Masanari Ohi, Masahiro Kaneko, Naoaki Okazaki, Nakamasa Inoue'}]","Masanari Ohi, Masahiro Kaneko, Naoaki Okazaki, Nakamasa Inoue","{'name': 'Masanari Ohi, Masahiro Kaneko, Naoaki Okazaki, Nakamasa Inoue'}",,
213,A Model-free Biomimetics Algorithm for Deterministic Partially Observable Markov Decision Process,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'A Model-free Biomimetics Algorithm for Deterministic Partially Observable Markov Decision Process'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14614'}]",https://arxiv.org/abs/2412.14614,"arXiv:2412.14614v1 Announce Type: new 
Abstract: Partially Observable Markov Decision Process (POMDP) is a mathematical framework for modeling decision-making under uncertainty, where the agent's observations are incomplete and the underlying system dynamics are probabilistic. Solving the POMDP problem within the model-free paradigm is challenging for agents due to the inherent difficulty in accurately identifying and distinguishing between states and observations. We define such a difficult problem as a DETerministic Partially Observable Markov Decision Process (DET-POMDP) problem, which is a specific setting of POMDP. In this problem, states and observations are in a many-to-one relationship. The state is obscured, and its relationship is less apparent to the agent. This creates obstacles for the agent to infer the state through observations. To effectively address this problem, we convert DET-POMDP into a fully observable MDP using a model-free biomimetics algorithm called BIOMAP. BIOMAP is based on the MDP Graph Automaton framework to distinguish authentic environmental information from fraudulent data. Thus, it enhances the agent's ability to develop stable policies against DET-POMDP. The experimental results highlight the superior capabilities of BIOMAP in maintaining operational effectiveness and environmental reparability in the presence of environmental deceptions when compared with existing POMDP solvers. This research opens up new avenues for the deployment of reliable POMDP-based systems in fields that are particularly susceptible to DET-POMDP problems.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14614v1 Announce Type: new \nAbstract: Partially Observable Markov Decision Process (POMDP) is a mathematical framework for modeling decision-making under uncertainty, where the agent's observations are incomplete and the underlying system dynamics are probabilistic. Solving the POMDP problem within the model-free paradigm is challenging for agents due to the inherent difficulty in accurately identifying and distinguishing between states and observations. We define such a difficult problem as a DETerministic Partially Observable Markov Decision Process (DET-POMDP) problem, which is a specific setting of POMDP. In this problem, states and observations are in a many-to-one relationship. The state is obscured, and its relationship is less apparent to the agent. This creates obstacles for the agent to infer the state through observations. To effectively address this problem, we convert DET-POMDP into a fully observable MDP using a model-free biomimetics algorithm called BIOMAP. BIOMAP is based on the MDP Graph Automaton framework to distinguish authentic environmental information from fraudulent data. Thus, it enhances the agent's ability to develop stable policies against DET-POMDP. The experimental results highlight the superior capabilities of BIOMAP in maintaining operational effectiveness and environmental reparability in the presence of environmental deceptions when compared with existing POMDP solvers. This research opens up new avenues for the deployment of reliable POMDP-based systems in fields that are particularly susceptible to DET-POMDP problems.""}",oai:arXiv.org:2412.14614v1,False,"[{'term': 'eess.SY', 'scheme': None, 'label': None}, {'term': 'cs.SY', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Yide Yu, Yue Liu, Xiaochen Yuan, Dennis Wong, Huijie Li, Yan Ma'}]","Yide Yu, Yue Liu, Xiaochen Yuan, Dennis Wong, Huijie Li, Yan Ma","{'name': 'Yide Yu, Yue Liu, Xiaochen Yuan, Dennis Wong, Huijie Li, Yan Ma'}",,
214,Additive codes attaining the Griesmer bound,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Additive codes attaining the Griesmer bound'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14615'}]",https://arxiv.org/abs/2412.14615,"arXiv:2412.14615v1 Announce Type: new 
Abstract: Additive codes may have better parameters than linear codes. However, still very few cases are known and the explicit construction of such codes is a challenging problem. Here we show that a Griesmer type bound for the length of additive codes can always be attained with equality if the minimum distance is sufficiently large. This solves the problem for the optimal parameters of additive codes when the minimum distance is large and yields many infinite series of additive codes that outperform linear codes.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14615v1 Announce Type: new \nAbstract: Additive codes may have better parameters than linear codes. However, still very few cases are known and the explicit construction of such codes is a challenging problem. Here we show that a Griesmer type bound for the length of additive codes can always be attained with equality if the minimum distance is sufficiently large. This solves the problem for the optimal parameters of additive codes when the minimum distance is large and yields many infinite series of additive codes that outperform linear codes.'}",oai:arXiv.org:2412.14615v1,False,"[{'term': 'cs.IT', 'scheme': None, 'label': None}, {'term': 'math.CO', 'scheme': None, 'label': None}, {'term': 'math.IT', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}",[{'name': 'Sascha Kurz'}],Sascha Kurz,{'name': 'Sascha Kurz'},,
215,How good is GPT at writing political speeches for the White House?,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'How good is GPT at writing political speeches for the White House?'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14617'}]",https://arxiv.org/abs/2412.14617,"arXiv:2412.14617v1 Announce Type: new 
Abstract: Using large language models (LLMs), computers are able to generate a written text in response to a us er request. As this pervasive technology can be applied in numerous contexts, this study analyses the written style of one LLM called GPT by comparing its generated speeches with those of the recent US presidents. To achieve this objective, the State of the Union (SOTU) addresses written by Reagan to Biden are contrasted to those produced by both GPT-3.5 and GPT-4.o versions. Compared to US presidents, GPT tends to overuse the lemma ""we"" and produce shorter messages with, on average, longer sentences. Moreover, GPT opts for an optimistic tone, opting more often for political (e.g., president, Congress), symbolic (e.g., freedom), and abstract terms (e.g., freedom). Even when imposing an author's style to GPT, the resulting speech remains distinct from addresses written by the target author. Finally, the two GPT versions present distinct characteristics, but both appear overall dissimilar to true presidential messages.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14617v1 Announce Type: new \nAbstract: Using large language models (LLMs), computers are able to generate a written text in response to a us er request. As this pervasive technology can be applied in numerous contexts, this study analyses the written style of one LLM called GPT by comparing its generated speeches with those of the recent US presidents. To achieve this objective, the State of the Union (SOTU) addresses written by Reagan to Biden are contrasted to those produced by both GPT-3.5 and GPT-4.o versions. Compared to US presidents, GPT tends to overuse the lemma ""we"" and produce shorter messages with, on average, longer sentences. Moreover, GPT opts for an optimistic tone, opting more often for political (e.g., president, Congress), symbolic (e.g., freedom), and abstract terms (e.g., freedom). Even when imposing an author\'s style to GPT, the resulting speech remains distinct from addresses written by the target author. Finally, the two GPT versions present distinct characteristics, but both appear overall dissimilar to true presidential messages.'}",oai:arXiv.org:2412.14617v1,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}",[{'name': 'Jacques Savoy'}],Jacques Savoy,{'name': 'Jacques Savoy'},,
216,Pitfalls of topology-aware image segmentation,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Pitfalls of topology-aware image segmentation'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14619'}]",https://arxiv.org/abs/2412.14619,"arXiv:2412.14619v1 Announce Type: new 
Abstract: Topological correctness, i.e., the preservation of structural integrity and specific characteristics of shape, is a fundamental requirement for medical imaging tasks, such as neuron or vessel segmentation. Despite the recent surge in topology-aware methods addressing this challenge, their real-world applicability is hindered by flawed benchmarking practices. In this paper, we identify critical pitfalls in model evaluation that include inadequate connectivity choices, overlooked topological artifacts in ground truth annotations, and inappropriate use of evaluation metrics. Through detailed empirical analysis, we uncover these issues' profound impact on the evaluation and ranking of segmentation methods. Drawing from our findings, we propose a set of actionable recommendations to establish fair and robust evaluation standards for topology-aware medical image segmentation methods.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14619v1 Announce Type: new \nAbstract: Topological correctness, i.e., the preservation of structural integrity and specific characteristics of shape, is a fundamental requirement for medical imaging tasks, such as neuron or vessel segmentation. Despite the recent surge in topology-aware methods addressing this challenge, their real-world applicability is hindered by flawed benchmarking practices. In this paper, we identify critical pitfalls in model evaluation that include inadequate connectivity choices, overlooked topological artifacts in ground truth annotations, and inappropriate use of evaluation metrics. Through detailed empirical analysis, we uncover these issues' profound impact on the evaluation and ranking of segmentation methods. Drawing from our findings, we propose a set of actionable recommendations to establish fair and robust evaluation standards for topology-aware medical image segmentation methods.""}",oai:arXiv.org:2412.14619v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by-nc-sa/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-sa/4.0/'}","[{'name': 'Alexander H. Berger, Laurin Lux, Alexander Weers, Martin Menten, Daniel Rueckert, Johannes C. Paetzold'}]","Alexander H. Berger, Laurin Lux, Alexander Weers, Martin Menten, Daniel Rueckert, Johannes C. Paetzold","{'name': 'Alexander H. Berger, Laurin Lux, Alexander Weers, Martin Menten, Daniel Rueckert, Johannes C. Paetzold'}",,
217,Continuous latent representations for modeling precipitation with deep learning,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Continuous latent representations for modeling precipitation with deep learning'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14620'}]",https://arxiv.org/abs/2412.14620,"arXiv:2412.14620v1 Announce Type: new 
Abstract: The sparse and spatio-temporally discontinuous nature of precipitation data presents significant challenges for simulation and statistical processing for bias correction and downscaling. These include incorrect representation of intermittency and extreme values (critical for hydrology applications), Gibbs phenomenon upon regridding, and lack of fine scales details. To address these challenges, a common approach is to transform the precipitation variable nonlinearly into one that is more malleable. In this work, we explore how deep learning can be used to generate a smooth, spatio-temporally continuous variable as a proxy for simulation of precipitation data. We develop a normally distributed field called pseudo-precipitation (PP) as an alternative for simulating precipitation. The practical applicability of this variable is investigated by applying it for downscaling precipitation from \(1\degree\) (\(\sim\) 100 km) to \(0.25\degree\) (\(\sim\) 25 km).","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14620v1 Announce Type: new \nAbstract: The sparse and spatio-temporally discontinuous nature of precipitation data presents significant challenges for simulation and statistical processing for bias correction and downscaling. These include incorrect representation of intermittency and extreme values (critical for hydrology applications), Gibbs phenomenon upon regridding, and lack of fine scales details. To address these challenges, a common approach is to transform the precipitation variable nonlinearly into one that is more malleable. In this work, we explore how deep learning can be used to generate a smooth, spatio-temporally continuous variable as a proxy for simulation of precipitation data. We develop a normally distributed field called pseudo-precipitation (PP) as an alternative for simulating precipitation. The practical applicability of this variable is investigated by applying it for downscaling precipitation from \\(1\\degree\\) (\\(\\sim\\) 100 km) to \\(0.25\\degree\\) (\\(\\sim\\) 25 km).'}",oai:arXiv.org:2412.14620v1,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Gokul Radhakrishnan, Rahul Sundar, Nishant Parashar, Antoine Blanchard, Daiwei Wang, Boyko Dodov'}]","Gokul Radhakrishnan, Rahul Sundar, Nishant Parashar, Antoine Blanchard, Daiwei Wang, Boyko Dodov","{'name': 'Gokul Radhakrishnan, Rahul Sundar, Nishant Parashar, Antoine Blanchard, Daiwei Wang, Boyko Dodov'}",,
218,FRIDAY: Mitigating Unintentional Facial Identity in Deepfake Detectors Guided by Facial Recognizers,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'FRIDAY: Mitigating Unintentional Facial Identity in Deepfake Detectors Guided by Facial Recognizers'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14623'}]",https://arxiv.org/abs/2412.14623,"arXiv:2412.14623v1 Announce Type: new 
Abstract: Previous Deepfake detection methods perform well within their training domains, but their effectiveness diminishes significantly with new synthesis techniques. Recent studies have revealed that detection models often create decision boundaries based on facial identity rather than synthetic artifacts, resulting in poor performance on cross-domain datasets. To address this limitation, we propose Facial Recognition Identity Attenuation (FRIDAY), a novel training method that mitigates facial identity influence using a face recognizer. Specifically, we first train a face recognizer using the same backbone as the Deepfake detector. The recognizer is then frozen and employed during the detector's training to reduce facial identity information. This is achieved by feeding input images into both the recognizer and the detector, and minimizing the similarity of their feature embeddings through our Facial Identity Attenuating loss. This process encourages the detector to generate embeddings distinct from the recognizer, effectively reducing the impact of facial identity. Extensive experiments demonstrate that our approach significantly enhances detection performance on both in-domain and cross-domain datasets.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14623v1 Announce Type: new \nAbstract: Previous Deepfake detection methods perform well within their training domains, but their effectiveness diminishes significantly with new synthesis techniques. Recent studies have revealed that detection models often create decision boundaries based on facial identity rather than synthetic artifacts, resulting in poor performance on cross-domain datasets. To address this limitation, we propose Facial Recognition Identity Attenuation (FRIDAY), a novel training method that mitigates facial identity influence using a face recognizer. Specifically, we first train a face recognizer using the same backbone as the Deepfake detector. The recognizer is then frozen and employed during the detector's training to reduce facial identity information. This is achieved by feeding input images into both the recognizer and the detector, and minimizing the similarity of their feature embeddings through our Facial Identity Attenuating loss. This process encourages the detector to generate embeddings distinct from the recognizer, effectively reducing the impact of facial identity. Extensive experiments demonstrate that our approach significantly enhances detection performance on both in-domain and cross-domain datasets.""}",oai:arXiv.org:2412.14623v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.CR', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Younhun Kim, Myung-Joon Kwon, Wonjun Lee, Changick Kim'}]","Younhun Kim, Myung-Joon Kwon, Wonjun Lee, Changick Kim","{'name': 'Younhun Kim, Myung-Joon Kwon, Wonjun Lee, Changick Kim'}",,
219,Learning to Generate Research Idea with Dynamic Control,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Learning to Generate Research Idea with Dynamic Control'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14626'}]",https://arxiv.org/abs/2412.14626,"arXiv:2412.14626v1 Announce Type: new 
Abstract: The rapid advancements in large language models (LLMs) have demonstrated their potential to accelerate scientific discovery, particularly in automating the process of research ideation. LLM-based systems have shown promise in generating hypotheses and research ideas. However, current approaches predominantly rely on prompting-based pre-trained models, limiting their ability to optimize generated content effectively. Moreover, they also lack the capability to deal with the complex interdependence and inherent restrictions among novelty, feasibility, and effectiveness, which remains challenging due to the inherent trade-offs among these dimensions, such as the innovation-feasibility conflict. To address these limitations, we for the first time propose fine-tuning LLMs to be better idea proposers and introduce a novel framework that employs a two-stage approach combining Supervised Fine-Tuning (SFT) and controllable Reinforcement Learning (RL). In the SFT stage, the model learns foundational patterns from pairs of research papers and follow-up ideas. In the RL stage, multi-dimensional reward modeling, guided by fine-grained feedback, evaluates and optimizes the generated ideas across key metrics. Dimensional controllers enable dynamic adjustment of generation, while a sentence-level decoder ensures context-aware emphasis during inference. Our framework provides a balanced approach to research ideation, achieving high-quality outcomes by dynamically navigating the trade-offs among novelty, feasibility, and effectiveness.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14626v1 Announce Type: new \nAbstract: The rapid advancements in large language models (LLMs) have demonstrated their potential to accelerate scientific discovery, particularly in automating the process of research ideation. LLM-based systems have shown promise in generating hypotheses and research ideas. However, current approaches predominantly rely on prompting-based pre-trained models, limiting their ability to optimize generated content effectively. Moreover, they also lack the capability to deal with the complex interdependence and inherent restrictions among novelty, feasibility, and effectiveness, which remains challenging due to the inherent trade-offs among these dimensions, such as the innovation-feasibility conflict. To address these limitations, we for the first time propose fine-tuning LLMs to be better idea proposers and introduce a novel framework that employs a two-stage approach combining Supervised Fine-Tuning (SFT) and controllable Reinforcement Learning (RL). In the SFT stage, the model learns foundational patterns from pairs of research papers and follow-up ideas. In the RL stage, multi-dimensional reward modeling, guided by fine-grained feedback, evaluates and optimizes the generated ideas across key metrics. Dimensional controllers enable dynamic adjustment of generation, while a sentence-level decoder ensures context-aware emphasis during inference. Our framework provides a balanced approach to research ideation, achieving high-quality outcomes by dynamically navigating the trade-offs among novelty, feasibility, and effectiveness.'}",oai:arXiv.org:2412.14626v1,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Ruochen Li, Liqiang Jing, Chi Han, Jiawei Zhou, Xinya Du'}]","Ruochen Li, Liqiang Jing, Chi Han, Jiawei Zhou, Xinya Du","{'name': 'Ruochen Li, Liqiang Jing, Chi Han, Jiawei Zhou, Xinya Du'}",,
220,Qua$^2$SeDiMo: Quantifiable Quantization Sensitivity of Diffusion Models,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Qua$^2$SeDiMo: Quantifiable Quantization Sensitivity of Diffusion Models'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14628'}]",https://arxiv.org/abs/2412.14628,"arXiv:2412.14628v1 Announce Type: new 
Abstract: Diffusion Models (DM) have democratized AI image generation through an iterative denoising process. Quantization is a major technique to alleviate the inference cost and reduce the size of DM denoiser networks. However, as denoisers evolve from variants of convolutional U-Nets toward newer Transformer architectures, it is of growing importance to understand the quantization sensitivity of different weight layers, operations and architecture types to performance. In this work, we address this challenge with Qua$^2$SeDiMo, a mixed-precision Post-Training Quantization framework that generates explainable insights on the cost-effectiveness of various model weight quantization methods for different denoiser operation types and block structures. We leverage these insights to make high-quality mixed-precision quantization decisions for a myriad of diffusion models ranging from foundational U-Nets to state-of-the-art Transformers. As a result, Qua$^2$SeDiMo can construct 3.4-bit, 3.9-bit, 3.65-bit and 3.7-bit weight quantization on PixArt-${\alpha}$, PixArt-${\Sigma}$, Hunyuan-DiT and SDXL, respectively. We further pair our weight-quantization configurations with 6-bit activation quantization and outperform existing approaches in terms of quantitative metrics and generative image quality.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14628v1 Announce Type: new \nAbstract: Diffusion Models (DM) have democratized AI image generation through an iterative denoising process. Quantization is a major technique to alleviate the inference cost and reduce the size of DM denoiser networks. However, as denoisers evolve from variants of convolutional U-Nets toward newer Transformer architectures, it is of growing importance to understand the quantization sensitivity of different weight layers, operations and architecture types to performance. In this work, we address this challenge with Qua$^2$SeDiMo, a mixed-precision Post-Training Quantization framework that generates explainable insights on the cost-effectiveness of various model weight quantization methods for different denoiser operation types and block structures. We leverage these insights to make high-quality mixed-precision quantization decisions for a myriad of diffusion models ranging from foundational U-Nets to state-of-the-art Transformers. As a result, Qua$^2$SeDiMo can construct 3.4-bit, 3.9-bit, 3.65-bit and 3.7-bit weight quantization on PixArt-${\\alpha}$, PixArt-${\\Sigma}$, Hunyuan-DiT and SDXL, respectively. We further pair our weight-quantization configurations with 6-bit activation quantization and outperform existing approaches in terms of quantitative metrics and generative image quality.'}",oai:arXiv.org:2412.14628v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Keith G. Mills, Mohammad Salameh, Ruichen Chen, Negar Hassanpour, Wei Lu, Di Niu'}]","Keith G. Mills, Mohammad Salameh, Ruichen Chen, Negar Hassanpour, Wei Lu, Di Niu","{'name': 'Keith G. Mills, Mohammad Salameh, Ruichen Chen, Negar Hassanpour, Wei Lu, Di Niu'}",,
221,Robust PCA Based on Adaptive Weighted Least Squares and Low-Rank Matrix Factorization,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Robust PCA Based on Adaptive Weighted Least Squares and Low-Rank Matrix Factorization'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14629'}]",https://arxiv.org/abs/2412.14629,"arXiv:2412.14629v1 Announce Type: new 
Abstract: Robust Principal Component Analysis (RPCA) is a fundamental technique for decomposing data into low-rank and sparse components, which plays a critical role for applications such as image processing and anomaly detection. Traditional RPCA methods commonly use $\ell_1$ norm regularization to enforce sparsity, but this approach can introduce bias and result in suboptimal estimates, particularly in the presence of significant noise or outliers. Non-convex regularization methods have been proposed to mitigate these challenges, but they tend to be complex to optimize and sensitive to initial conditions, leading to potential instability in solutions. To overcome these challenges, in this paper, we propose a novel RPCA model that integrates adaptive weighted least squares (AWLS) and low-rank matrix factorization (LRMF). The model employs a {self-attention-inspired} mechanism in its weight update process, allowing the weight matrix to dynamically adjust and emphasize significant components during each iteration. By employing a weighted F-norm for the sparse component, our method effectively reduces bias while simplifying the computational process compared to traditional $\ell_1$-norm-based methods. We use an alternating minimization algorithm, where each subproblem has an explicit solution, thereby improving computational efficiency. Despite its simplicity, numerical experiments demonstrate that our method outperforms existing non-convex regularization approaches, offering superior performance and stability, as well as enhanced accuracy and robustness in practical applications.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14629v1 Announce Type: new \nAbstract: Robust Principal Component Analysis (RPCA) is a fundamental technique for decomposing data into low-rank and sparse components, which plays a critical role for applications such as image processing and anomaly detection. Traditional RPCA methods commonly use $\\ell_1$ norm regularization to enforce sparsity, but this approach can introduce bias and result in suboptimal estimates, particularly in the presence of significant noise or outliers. Non-convex regularization methods have been proposed to mitigate these challenges, but they tend to be complex to optimize and sensitive to initial conditions, leading to potential instability in solutions. To overcome these challenges, in this paper, we propose a novel RPCA model that integrates adaptive weighted least squares (AWLS) and low-rank matrix factorization (LRMF). The model employs a {self-attention-inspired} mechanism in its weight update process, allowing the weight matrix to dynamically adjust and emphasize significant components during each iteration. By employing a weighted F-norm for the sparse component, our method effectively reduces bias while simplifying the computational process compared to traditional $\\ell_1$-norm-based methods. We use an alternating minimization algorithm, where each subproblem has an explicit solution, thereby improving computational efficiency. Despite its simplicity, numerical experiments demonstrate that our method outperforms existing non-convex regularization approaches, offering superior performance and stability, as well as enhanced accuracy and robustness in practical applications.'}",oai:arXiv.org:2412.14629v1,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Kexin Li, You-wei Wen, Xu Xiao, Mingchao Zhao'}]","Kexin Li, You-wei Wen, Xu Xiao, Mingchao Zhao","{'name': 'Kexin Li, You-wei Wen, Xu Xiao, Mingchao Zhao'}",,
222,Unified Image Restoration and Enhancement: Degradation Calibrated Cycle Reconstruction Diffusion Model,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Unified Image Restoration and Enhancement: Degradation Calibrated Cycle Reconstruction Diffusion Model'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14630'}]",https://arxiv.org/abs/2412.14630,"arXiv:2412.14630v1 Announce Type: new 
Abstract: Image restoration and enhancement are pivotal for numerous computer vision applications, yet unifying these tasks efficiently remains a significant challenge. Inspired by the iterative refinement capabilities of diffusion models, we propose CycleRDM, a novel framework designed to unify restoration and enhancement tasks while achieving high-quality mapping. Specifically, CycleRDM first learns the mapping relationships among the degraded domain, the rough normal domain, and the normal domain through a two-stage diffusion inference process. Subsequently, we transfer the final calibration process to the wavelet low-frequency domain using discrete wavelet transform, performing fine-grained calibration from a frequency domain perspective by leveraging task-specific frequency spaces. To improve restoration quality, we design a feature gain module for the decomposed wavelet high-frequency domain to eliminate redundant features. Additionally, we employ multimodal textual prompts and Fourier transform to drive stable denoising and reduce randomness during the inference process. After extensive validation, CycleRDM can be effectively generalized to a wide range of image restoration and enhancement tasks while requiring only a small number of training samples to be significantly superior on various benchmarks of reconstruction quality and perceptual quality. The source code will be available at https://github.com/hejh8/CycleRDM.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14630v1 Announce Type: new \nAbstract: Image restoration and enhancement are pivotal for numerous computer vision applications, yet unifying these tasks efficiently remains a significant challenge. Inspired by the iterative refinement capabilities of diffusion models, we propose CycleRDM, a novel framework designed to unify restoration and enhancement tasks while achieving high-quality mapping. Specifically, CycleRDM first learns the mapping relationships among the degraded domain, the rough normal domain, and the normal domain through a two-stage diffusion inference process. Subsequently, we transfer the final calibration process to the wavelet low-frequency domain using discrete wavelet transform, performing fine-grained calibration from a frequency domain perspective by leveraging task-specific frequency spaces. To improve restoration quality, we design a feature gain module for the decomposed wavelet high-frequency domain to eliminate redundant features. Additionally, we employ multimodal textual prompts and Fourier transform to drive stable denoising and reduce randomness during the inference process. After extensive validation, CycleRDM can be effectively generalized to a wide range of image restoration and enhancement tasks while requiring only a small number of training samples to be significantly superior on various benchmarks of reconstruction quality and perceptual quality. The source code will be available at https://github.com/hejh8/CycleRDM.'}",oai:arXiv.org:2412.14630v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Minglong Xue, Jinhong He, Shivakumara Palaiahnakote, Mingliang Zhou'}]","Minglong Xue, Jinhong He, Shivakumara Palaiahnakote, Mingliang Zhou","{'name': 'Minglong Xue, Jinhong He, Shivakumara Palaiahnakote, Mingliang Zhou'}",,
223,Review of Fruit Tree Image Segmentation,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Review of Fruit Tree Image Segmentation'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14631'}]",https://arxiv.org/abs/2412.14631,"arXiv:2412.14631v1 Announce Type: new 
Abstract: Fruit tree image segmentation is an essential problem in automating a variety of agricultural tasks such as phenotyping, harvesting, spraying, and pruning. Many research papers have proposed a diverse spectrum of solutions suitable to specific tasks and environments. The review scope of this paper is confined to the front views of fruit trees and based on 158 relevant papers collected using a newly designed crawling review method. These papers are systematically reviewed based on a taxonomy that sequentially considers the method, image, task, and fruit. This taxonomy will assist readers to intuitively grasp the big picture of these research activities. Our review reveals that the most noticeable deficiency of the previous studies was the lack of a versatile dataset and segmentation model that could be applied to a variety of tasks and environments. Six important future research tasks are suggested, with the expectation that these will pave the way to building a versatile tree segmentation module.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14631v1 Announce Type: new \nAbstract: Fruit tree image segmentation is an essential problem in automating a variety of agricultural tasks such as phenotyping, harvesting, spraying, and pruning. Many research papers have proposed a diverse spectrum of solutions suitable to specific tasks and environments. The review scope of this paper is confined to the front views of fruit trees and based on 158 relevant papers collected using a newly designed crawling review method. These papers are systematically reviewed based on a taxonomy that sequentially considers the method, image, task, and fruit. This taxonomy will assist readers to intuitively grasp the big picture of these research activities. Our review reveals that the most noticeable deficiency of the previous studies was the lack of a versatile dataset and segmentation model that could be applied to a variety of tasks and environments. Six important future research tasks are suggested, with the expectation that these will pave the way to building a versatile tree segmentation module.'}",oai:arXiv.org:2412.14631v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}",[{'name': 'Il-Seok Oh'}],Il-Seok Oh,{'name': 'Il-Seok Oh'},,
224,Progressive Fine-to-Coarse Reconstruction for Accurate Low-Bit Post-Training Quantization in Vision Transformers,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Progressive Fine-to-Coarse Reconstruction for Accurate Low-Bit Post-Training Quantization in Vision Transformers'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14633'}]",https://arxiv.org/abs/2412.14633,"arXiv:2412.14633v1 Announce Type: new 
Abstract: Due to its efficiency, Post-Training Quantization (PTQ) has been widely adopted for compressing Vision Transformers (ViTs). However, when quantized into low-bit representations, there is often a significant performance drop compared to their full-precision counterparts. To address this issue, reconstruction methods have been incorporated into the PTQ framework to improve performance in low-bit quantization settings. Nevertheless, existing related methods predefine the reconstruction granularity and seldom explore the progressive relationships between different reconstruction granularities, which leads to sub-optimal quantization results in ViTs. To this end, in this paper, we propose a Progressive Fine-to-Coarse Reconstruction (PFCR) method for accurate PTQ, which significantly improves the performance of low-bit quantized vision transformers. Specifically, we define multi-head self-attention and multi-layer perceptron modules along with their shortcuts as the finest reconstruction units. After reconstructing these two fine-grained units, we combine them to form coarser blocks and reconstruct them at a coarser granularity level. We iteratively perform this combination and reconstruction process, achieving progressive fine-to-coarse reconstruction. Additionally, we introduce a Progressive Optimization Strategy (POS) for PFCR to alleviate the difficulty of training, thereby further enhancing model performance. Experimental results on the ImageNet dataset demonstrate that our proposed method achieves the best Top-1 accuracy among state-of-the-art methods, particularly attaining 75.61% for 3-bit quantized ViT-B in PTQ. Besides, quantization results on the COCO dataset reveal the effectiveness and generalization of our proposed method on other computer vision tasks like object detection and instance segmentation.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14633v1 Announce Type: new \nAbstract: Due to its efficiency, Post-Training Quantization (PTQ) has been widely adopted for compressing Vision Transformers (ViTs). However, when quantized into low-bit representations, there is often a significant performance drop compared to their full-precision counterparts. To address this issue, reconstruction methods have been incorporated into the PTQ framework to improve performance in low-bit quantization settings. Nevertheless, existing related methods predefine the reconstruction granularity and seldom explore the progressive relationships between different reconstruction granularities, which leads to sub-optimal quantization results in ViTs. To this end, in this paper, we propose a Progressive Fine-to-Coarse Reconstruction (PFCR) method for accurate PTQ, which significantly improves the performance of low-bit quantized vision transformers. Specifically, we define multi-head self-attention and multi-layer perceptron modules along with their shortcuts as the finest reconstruction units. After reconstructing these two fine-grained units, we combine them to form coarser blocks and reconstruct them at a coarser granularity level. We iteratively perform this combination and reconstruction process, achieving progressive fine-to-coarse reconstruction. Additionally, we introduce a Progressive Optimization Strategy (POS) for PFCR to alleviate the difficulty of training, thereby further enhancing model performance. Experimental results on the ImageNet dataset demonstrate that our proposed method achieves the best Top-1 accuracy among state-of-the-art methods, particularly attaining 75.61% for 3-bit quantized ViT-B in PTQ. Besides, quantization results on the COCO dataset reveal the effectiveness and generalization of our proposed method on other computer vision tasks like object detection and instance segmentation.'}",oai:arXiv.org:2412.14633v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Rui Ding, Liang Yong, Sihuan Zhao, Jing Nie, Lihui Chen, Haijun Liu, Xichuan Zhou'}]","Rui Ding, Liang Yong, Sihuan Zhao, Jing Nie, Lihui Chen, Haijun Liu, Xichuan Zhou","{'name': 'Rui Ding, Liang Yong, Sihuan Zhao, Jing Nie, Lihui Chen, Haijun Liu, Xichuan Zhou'}",,
225,TuneS: Patient-specific model-based optimization of contact configuration in deep brain stimulation,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'TuneS: Patient-specific model-based optimization of contact configuration in deep brain stimulation'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14638'}]",https://arxiv.org/abs/2412.14638,"arXiv:2412.14638v1 Announce Type: new 
Abstract: Objective: The objective of this study is to develop and evaluate a systematic approach to optimize Deep Brain Stimulation (DBS) parameters, addressing the challenge of identifying patient-specific settings and optimal stimulation targets for various neurological and mental disorders. Methods: TuneS, a novel pipeline to predict clinically optimal DBS contact configurations based on predefined targets and constraints, is introduced. The method relies upon patient-specific models of stimulation spread and extends optimization beyond traditional neural structures to include automated, model-based targeting of streamlines. Results: Initial findings demonstrate that STN motor streamlines consistently receive a significant portion of the allocated stimulation volume, suggesting that a consistent portion of the stimulation should ideally focus on the STN motor streamlines. At the example of a small cohort of Parkinson's disease patients, the value of model-based contact predictions for assessing stimulation targets while observing constraints is demonstrated. Conclusion: TuneS shows promise as a research tool, enabling systematic assessment of DBS target effectiveness and facilitating constraint-aware optimization of stimulation parameters. Significance: The presented pipeline offers a pathway to improve patient-specific DBS therapies and contributes to the broader understanding of effective DBS targeting strategies.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14638v1 Announce Type: new \nAbstract: Objective: The objective of this study is to develop and evaluate a systematic approach to optimize Deep Brain Stimulation (DBS) parameters, addressing the challenge of identifying patient-specific settings and optimal stimulation targets for various neurological and mental disorders. Methods: TuneS, a novel pipeline to predict clinically optimal DBS contact configurations based on predefined targets and constraints, is introduced. The method relies upon patient-specific models of stimulation spread and extends optimization beyond traditional neural structures to include automated, model-based targeting of streamlines. Results: Initial findings demonstrate that STN motor streamlines consistently receive a significant portion of the allocated stimulation volume, suggesting that a consistent portion of the stimulation should ideally focus on the STN motor streamlines. At the example of a small cohort of Parkinson's disease patients, the value of model-based contact predictions for assessing stimulation targets while observing constraints is demonstrated. Conclusion: TuneS shows promise as a research tool, enabling systematic assessment of DBS target effectiveness and facilitating constraint-aware optimization of stimulation parameters. Significance: The presented pipeline offers a pathway to improve patient-specific DBS therapies and contributes to the broader understanding of effective DBS targeting strategies.""}",oai:arXiv.org:2412.14638v1,False,"[{'term': 'eess.SY', 'scheme': None, 'label': None}, {'term': 'cs.SY', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by-nc-sa/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-sa/4.0/'}","[{'name': 'Anna Franziska Frigge, Lina Uggla, Elena Jiltsova, Markus Fahlstr\\""om, Dag Nyholm, Alexander Medvedev'}]","Anna Franziska Frigge, Lina Uggla, Elena Jiltsova, Markus Fahlstr\""om, Dag Nyholm, Alexander Medvedev","{'name': 'Anna Franziska Frigge, Lina Uggla, Elena Jiltsova, Markus Fahlstr\\""om, Dag Nyholm, Alexander Medvedev'}",,
226,A Shapley Value Estimation Speedup for Efficient Explainable Quantum AI,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'A Shapley Value Estimation Speedup for Efficient Explainable Quantum AI'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14639'}]",https://arxiv.org/abs/2412.14639,"arXiv:2412.14639v1 Announce Type: new 
Abstract: This work focuses on developing efficient post-hoc explanations for quantum AI algorithms. In classical contexts, the cooperative game theory concept of the Shapley value adapts naturally to post-hoc explanations, where it can be used to identify which factors are important in an AI's decision-making process. An interesting question is how to translate Shapley values to the quantum setting and whether quantum effects could be used to accelerate their calculation. We propose quantum algorithms that can extract Shapley values within some confidence interval. Our method is capable of quadratically outperforming classical Monte Carlo approaches to approximating Shapley values up to polylogarithmic factors in various circumstances. We demonstrate the validity of our approach empirically with specific voting games and provide rigorous proofs of performance for general cooperative games.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14639v1 Announce Type: new \nAbstract: This work focuses on developing efficient post-hoc explanations for quantum AI algorithms. In classical contexts, the cooperative game theory concept of the Shapley value adapts naturally to post-hoc explanations, where it can be used to identify which factors are important in an AI's decision-making process. An interesting question is how to translate Shapley values to the quantum setting and whether quantum effects could be used to accelerate their calculation. We propose quantum algorithms that can extract Shapley values within some confidence interval. Our method is capable of quadratically outperforming classical Monte Carlo approaches to approximating Shapley values up to polylogarithmic factors in various circumstances. We demonstrate the validity of our approach empirically with specific voting games and provide rigorous proofs of performance for general cooperative games.""}",oai:arXiv.org:2412.14639v1,False,"[{'term': 'cs.CR', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Iain Burge, Michel Barbeau, Joaquin Garcia-Alfaro'}]","Iain Burge, Michel Barbeau, Joaquin Garcia-Alfaro","{'name': 'Iain Burge, Michel Barbeau, Joaquin Garcia-Alfaro'}",,
227,Adaptive Prompt Tuning: Vision Guided Prompt Tuning with Cross-Attention for Fine-Grained Few-Shot Learning,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Adaptive Prompt Tuning: Vision Guided Prompt Tuning with Cross-Attention for Fine-Grained Few-Shot Learning'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14640'}]",https://arxiv.org/abs/2412.14640,"arXiv:2412.14640v1 Announce Type: new 
Abstract: Few-shot, fine-grained classification in computer vision poses significant challenges due to the need to differentiate subtle class distinctions with limited data. This paper presents a novel method that enhances the Contrastive Language-Image Pre-Training (CLIP) model through adaptive prompt tuning, guided by real-time visual inputs. Unlike existing techniques such as Context Optimization (CoOp) and Visual Prompt Tuning (VPT), which are constrained by static prompts or visual token reliance, the proposed approach leverages a cross-attention mechanism to dynamically refine text prompts for the image at hand. This enables an image-specific alignment of textual features with image patches extracted from the Vision Transformer, making the model more effective for datasets with high intra-class variance and low inter-class differences. The method is evaluated on several datasets, including CUBirds, Oxford Flowers, and FGVC Aircraft, showing significant performance gains over static prompt tuning approaches. To ensure these performance gains translate into trustworthy predictions, we integrate Monte-Carlo Dropout in our approach to improve the reliability of the model predictions and uncertainty estimates. This integration provides valuable insights into the model's predictive confidence, helping to identify when predictions can be trusted and when additional verification is necessary. This dynamic approach offers a robust solution, advancing the state-of-the-art for few-shot fine-grained classification.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14640v1 Announce Type: new \nAbstract: Few-shot, fine-grained classification in computer vision poses significant challenges due to the need to differentiate subtle class distinctions with limited data. This paper presents a novel method that enhances the Contrastive Language-Image Pre-Training (CLIP) model through adaptive prompt tuning, guided by real-time visual inputs. Unlike existing techniques such as Context Optimization (CoOp) and Visual Prompt Tuning (VPT), which are constrained by static prompts or visual token reliance, the proposed approach leverages a cross-attention mechanism to dynamically refine text prompts for the image at hand. This enables an image-specific alignment of textual features with image patches extracted from the Vision Transformer, making the model more effective for datasets with high intra-class variance and low inter-class differences. The method is evaluated on several datasets, including CUBirds, Oxford Flowers, and FGVC Aircraft, showing significant performance gains over static prompt tuning approaches. To ensure these performance gains translate into trustworthy predictions, we integrate Monte-Carlo Dropout in our approach to improve the reliability of the model predictions and uncertainty estimates. This integration provides valuable insights into the model's predictive confidence, helping to identify when predictions can be trusted and when additional verification is necessary. This dynamic approach offers a robust solution, advancing the state-of-the-art for few-shot fine-grained classification.""}",oai:arXiv.org:2412.14640v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by-nc-sa/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-sa/4.0/'}","[{'name': 'Eric Brouwer, Jan Erik van Woerden, Gertjan Burghouts, Matias Valedenegro-Toro, Marco Zullich'}]","Eric Brouwer, Jan Erik van Woerden, Gertjan Burghouts, Matias Valedenegro-Toro, Marco Zullich","{'name': 'Eric Brouwer, Jan Erik van Woerden, Gertjan Burghouts, Matias Valedenegro-Toro, Marco Zullich'}",,
228,TOMG-Bench: Evaluating LLMs on Text-based Open Molecule Generation,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'TOMG-Bench: Evaluating LLMs on Text-based Open Molecule Generation'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14642'}]",https://arxiv.org/abs/2412.14642,"arXiv:2412.14642v1 Announce Type: new 
Abstract: In this paper, we propose Text-based Open Molecule Generation Benchmark (TOMG-Bench), the first benchmark to evaluate the open-domain molecule generation capability of LLMs. TOMG-Bench encompasses a dataset of three major tasks: molecule editing (MolEdit), molecule optimization (MolOpt), and customized molecule generation (MolCustom). Each task further contains three subtasks, with each subtask comprising 5,000 test samples. Given the inherent complexity of open molecule generation, we have also developed an automated evaluation system that helps measure both the quality and the accuracy of the generated molecules. Our comprehensive benchmarking of 25 LLMs reveals the current limitations and potential areas for improvement in text-guided molecule discovery. Furthermore, with the assistance of OpenMolIns, a specialized instruction tuning dataset proposed for solving challenges raised by TOMG-Bench, Llama3.1-8B could outperform all the open-source general LLMs, even surpassing GPT-3.5-turbo by 46.5\% on TOMG-Bench. Our codes and datasets are available through https://github.com/phenixace/TOMG-Bench.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14642v1 Announce Type: new \nAbstract: In this paper, we propose Text-based Open Molecule Generation Benchmark (TOMG-Bench), the first benchmark to evaluate the open-domain molecule generation capability of LLMs. TOMG-Bench encompasses a dataset of three major tasks: molecule editing (MolEdit), molecule optimization (MolOpt), and customized molecule generation (MolCustom). Each task further contains three subtasks, with each subtask comprising 5,000 test samples. Given the inherent complexity of open molecule generation, we have also developed an automated evaluation system that helps measure both the quality and the accuracy of the generated molecules. Our comprehensive benchmarking of 25 LLMs reveals the current limitations and potential areas for improvement in text-guided molecule discovery. Furthermore, with the assistance of OpenMolIns, a specialized instruction tuning dataset proposed for solving challenges raised by TOMG-Bench, Llama3.1-8B could outperform all the open-source general LLMs, even surpassing GPT-3.5-turbo by 46.5\\% on TOMG-Bench. Our codes and datasets are available through https://github.com/phenixace/TOMG-Bench.'}",oai:arXiv.org:2412.14642v1,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by-nc-sa/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-sa/4.0/'}","[{'name': 'Jiatong Li, Junxian Li, Yunqing Liu, Dongzhan Zhou, Qing Li'}]","Jiatong Li, Junxian Li, Yunqing Liu, Dongzhan Zhou, Qing Li","{'name': 'Jiatong Li, Junxian Li, Yunqing Liu, Dongzhan Zhou, Qing Li'}",,
229,RefHCM: A Unified Model for Referring Perceptions in Human-Centric Scenarios,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'RefHCM: A Unified Model for Referring Perceptions in Human-Centric Scenarios'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14643'}]",https://arxiv.org/abs/2412.14643,"arXiv:2412.14643v1 Announce Type: new 
Abstract: Human-centric perceptions play a crucial role in real-world applications. While recent human-centric works have achieved impressive progress, these efforts are often constrained to the visual domain and lack interaction with human instructions, limiting their applicability in broader scenarios such as chatbots and sports analysis. This paper introduces Referring Human Perceptions, where a referring prompt specifies the person of interest in an image. To tackle the new task, we propose RefHCM (Referring Human-Centric Model), a unified framework to integrate a wide range of human-centric referring tasks. Specifically, RefHCM employs sequence mergers to convert raw multimodal data -- including images, text, coordinates, and parsing maps -- into semantic tokens. This standardized representation enables RefHCM to reformulate diverse human-centric referring tasks into a sequence-to-sequence paradigm, solved using a plain encoder-decoder transformer architecture. Benefiting from a unified learning strategy, RefHCM effectively facilitates knowledge transfer across tasks and exhibits unforeseen capabilities in handling complex reasoning. This work represents the first attempt to address referring human perceptions with a general-purpose framework, while simultaneously establishing a corresponding benchmark that sets new standards for the field. Extensive experiments showcase RefHCM's competitive and even superior performance across multiple human-centric referring tasks. The code and data are publicly at https://github.com/JJJYmmm/RefHCM.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14643v1 Announce Type: new \nAbstract: Human-centric perceptions play a crucial role in real-world applications. While recent human-centric works have achieved impressive progress, these efforts are often constrained to the visual domain and lack interaction with human instructions, limiting their applicability in broader scenarios such as chatbots and sports analysis. This paper introduces Referring Human Perceptions, where a referring prompt specifies the person of interest in an image. To tackle the new task, we propose RefHCM (Referring Human-Centric Model), a unified framework to integrate a wide range of human-centric referring tasks. Specifically, RefHCM employs sequence mergers to convert raw multimodal data -- including images, text, coordinates, and parsing maps -- into semantic tokens. This standardized representation enables RefHCM to reformulate diverse human-centric referring tasks into a sequence-to-sequence paradigm, solved using a plain encoder-decoder transformer architecture. Benefiting from a unified learning strategy, RefHCM effectively facilitates knowledge transfer across tasks and exhibits unforeseen capabilities in handling complex reasoning. This work represents the first attempt to address referring human perceptions with a general-purpose framework, while simultaneously establishing a corresponding benchmark that sets new standards for the field. Extensive experiments showcase RefHCM's competitive and even superior performance across multiple human-centric referring tasks. The code and data are publicly at https://github.com/JJJYmmm/RefHCM.""}",oai:arXiv.org:2412.14643v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Jie Huang, Ruibing Hou, Jiahe Zhao, Hong Chang, Shiguang Shan'}]","Jie Huang, Ruibing Hou, Jiahe Zhao, Hong Chang, Shiguang Shan","{'name': 'Jie Huang, Ruibing Hou, Jiahe Zhao, Hong Chang, Shiguang Shan'}",,
230,Computing rough solutions of the stochastic nonlinear wave equation,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Computing rough solutions of the stochastic nonlinear wave equation'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14644'}]",https://arxiv.org/abs/2412.14644,"arXiv:2412.14644v1 Announce Type: new 
Abstract: The regularity of solutions to the stochastic nonlinear wave equation plays a critical role in the accuracy and efficiency of numerical algorithms. Rough or discontinuous initial conditions pose significant challenges, often leading to a loss of accuracy and reduced computational efficiency in existing methods. In this study, we address these challenges by developing a novel and efficient numerical algorithm specifically designed for computing rough solutions of the stochastic nonlinear wave equation, while significantly relaxing the regularity requirements on the initial data. By leveraging the intrinsic structure of the stochastic nonlinear wave equation and employing advanced tools from harmonic analysis, we construct a time discretization method that achieves robust convergence for initial values \((u^{0}, v^{0}) \in H^{\gamma} \times H^{\gamma-1}\) for all \(\gamma > 0\). Notably, our method attains an improved error rate of \(O(\tau^{2\gamma-})\) in one and two dimensions for \(\gamma \in (0, \frac{1}{2}]\), and \(O(\tau^{\max(\gamma, 2\gamma - \frac{1}{2}-)})\) in three dimensions for \(\gamma \in (0, \frac{3}{4}]\), where \(\tau\) denotes the time step size. These convergence rates surpass those of existing numerical methods under the same regularity conditions, underscoring the advantage of our approach. To validate the performance of our method, we present extensive numerical experiments that demonstrate its superior accuracy and computational efficiency compared to state-of-the-art methods. These results highlight the potential of our approach to enable accurate and efficient simulations of stochastic wave phenomena even in the presence of challenging initial conditions.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14644v1 Announce Type: new \nAbstract: The regularity of solutions to the stochastic nonlinear wave equation plays a critical role in the accuracy and efficiency of numerical algorithms. Rough or discontinuous initial conditions pose significant challenges, often leading to a loss of accuracy and reduced computational efficiency in existing methods. In this study, we address these challenges by developing a novel and efficient numerical algorithm specifically designed for computing rough solutions of the stochastic nonlinear wave equation, while significantly relaxing the regularity requirements on the initial data. By leveraging the intrinsic structure of the stochastic nonlinear wave equation and employing advanced tools from harmonic analysis, we construct a time discretization method that achieves robust convergence for initial values \\((u^{0}, v^{0}) \\in H^{\\gamma} \\times H^{\\gamma-1}\\) for all \\(\\gamma > 0\\). Notably, our method attains an improved error rate of \\(O(\\tau^{2\\gamma-})\\) in one and two dimensions for \\(\\gamma \\in (0, \\frac{1}{2}]\\), and \\(O(\\tau^{\\max(\\gamma, 2\\gamma - \\frac{1}{2}-)})\\) in three dimensions for \\(\\gamma \\in (0, \\frac{3}{4}]\\), where \\(\\tau\\) denotes the time step size. These convergence rates surpass those of existing numerical methods under the same regularity conditions, underscoring the advantage of our approach. To validate the performance of our method, we present extensive numerical experiments that demonstrate its superior accuracy and computational efficiency compared to state-of-the-art methods. These results highlight the potential of our approach to enable accurate and efficient simulations of stochastic wave phenomena even in the presence of challenging initial conditions.'}",oai:arXiv.org:2412.14644v1,False,"[{'term': 'math.NA', 'scheme': None, 'label': None}, {'term': 'cs.NA', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Jiachuan Cao, Buyang Li, Katharina Schratz'}]","Jiachuan Cao, Buyang Li, Katharina Schratz","{'name': 'Jiachuan Cao, Buyang Li, Katharina Schratz'}",,
231,Optimization of Collective Bayesian Decision-Making in a Swarm of Miniaturized Vibration-Sensing Robots,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Optimization of Collective Bayesian Decision-Making in a Swarm of Miniaturized Vibration-Sensing Robots'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14646'}]",https://arxiv.org/abs/2412.14646,"arXiv:2412.14646v1 Announce Type: new 
Abstract: Inspection of infrastructure using static sensor nodes has become a well established approach in recent decades. In this work, we present an experimental setup to address a binary inspection task using mobile sensor nodes. The objective is to identify the predominant tile type in a 1mx1m tiled surface composed of vibrating and non-vibrating tiles. A swarm of miniaturized robots, equipped with onboard IMUs for sensing and IR sensors for collision avoidance, performs the inspection. The decision-making approach leverages a Bayesian algorithm, updating robots' belief using inference. The original algorithm uses one of two information sharing strategies. We introduce a novel information sharing strategy, aiming to accelerate the decision-making. To optimize the algorithm parameters, we develop a simulation framework calibrated to our real-world setup in the high-fidelity Webots robotic simulator. We evaluate the three information sharing strategies through simulations and real-world experiments. Moreover, we test the effectiveness of our optimization by placing swarms with optimized and non-optimized parameters in increasingly complex environments with varied spatial correlation and fill ratios. Results show that our proposed information sharing strategy consistently outperforms previously established information-sharing strategies in decision time. Additionally, optimized parameters yield robust performance across different environments. Conversely, non-optimized parameters perform well in simpler scenarios but show reduced accuracy in complex settings.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14646v1 Announce Type: new \nAbstract: Inspection of infrastructure using static sensor nodes has become a well established approach in recent decades. In this work, we present an experimental setup to address a binary inspection task using mobile sensor nodes. The objective is to identify the predominant tile type in a 1mx1m tiled surface composed of vibrating and non-vibrating tiles. A swarm of miniaturized robots, equipped with onboard IMUs for sensing and IR sensors for collision avoidance, performs the inspection. The decision-making approach leverages a Bayesian algorithm, updating robots' belief using inference. The original algorithm uses one of two information sharing strategies. We introduce a novel information sharing strategy, aiming to accelerate the decision-making. To optimize the algorithm parameters, we develop a simulation framework calibrated to our real-world setup in the high-fidelity Webots robotic simulator. We evaluate the three information sharing strategies through simulations and real-world experiments. Moreover, we test the effectiveness of our optimization by placing swarms with optimized and non-optimized parameters in increasingly complex environments with varied spatial correlation and fill ratios. Results show that our proposed information sharing strategy consistently outperforms previously established information-sharing strategies in decision time. Additionally, optimized parameters yield robust performance across different environments. Conversely, non-optimized parameters perform well in simpler scenarios but show reduced accuracy in complex settings.""}",oai:arXiv.org:2412.14646v1,False,"[{'term': 'cs.RO', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Thiemen Siemensma, Bahar Haghighat'}]","Thiemen Siemensma, Bahar Haghighat","{'name': 'Thiemen Siemensma, Bahar Haghighat'}",,
232,Trainable Adaptive Activation Function Structure (TAAFS) Enhances Neural Network Force Field Performance with Only Dozens of Additional Parameters,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Trainable Adaptive Activation Function Structure (TAAFS) Enhances Neural Network Force Field Performance with Only Dozens of Additional Parameters'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14655'}]",https://arxiv.org/abs/2412.14655,"arXiv:2412.14655v1 Announce Type: new 
Abstract: At the heart of neural network force fields (NNFFs) is the architecture of neural networks, where the capacity to model complex interactions is typically enhanced through widening or deepening multilayer perceptrons (MLPs) or by increasing layers of graph neural networks (GNNs). These enhancements, while improving the model's performance, often come at the cost of a substantial increase in the number of parameters. By applying the Trainable Adaptive Activation Function Structure (TAAFS), we introduce a method that selects distinct mathematical formulations for non-linear activations, thereby increasing the precision of NNFFs with an insignificant addition to the parameter count. In this study, we integrate TAAFS into a variety of neural network models, resulting in observed accuracy improvements, and further validate these enhancements through molecular dynamics (MD) simulations using DeepMD.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14655v1 Announce Type: new \nAbstract: At the heart of neural network force fields (NNFFs) is the architecture of neural networks, where the capacity to model complex interactions is typically enhanced through widening or deepening multilayer perceptrons (MLPs) or by increasing layers of graph neural networks (GNNs). These enhancements, while improving the model's performance, often come at the cost of a substantial increase in the number of parameters. By applying the Trainable Adaptive Activation Function Structure (TAAFS), we introduce a method that selects distinct mathematical formulations for non-linear activations, thereby increasing the precision of NNFFs with an insignificant addition to the parameter count. In this study, we integrate TAAFS into a variety of neural network models, resulting in observed accuracy improvements, and further validate these enhancements through molecular dynamics (MD) simulations using DeepMD.""}",oai:arXiv.org:2412.14655v1,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}",[{'name': 'Enji Li'}],Enji Li,{'name': 'Enji Li'},,
233,Length Controlled Generation for Black-box LLMs,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Length Controlled Generation for Black-box LLMs'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14656'}]",https://arxiv.org/abs/2412.14656,"arXiv:2412.14656v1 Announce Type: new 
Abstract: Large language models (LLMs) have demonstrated impressive instruction following capabilities, while still struggling to accurately manage the length of the generated text, which is a fundamental requirement in many real-world applications. Existing length control methods involve fine-tuning the parameters of LLMs, which is inefficient and suboptimal for practical use. In this paper, we propose a novel iterative sampling framework for text length control, integrating the Metropolis-Hastings algorithm with an importance sampling acceleration strategy. This framework efficiently and reliably regulates LLMs to generate length-constrained text without modifying the underlying parameters, thereby preserving the original capabilities of LLMs. Experimental results demonstrate that our framework achieves almost 100\% success rates of length control on Llama3.1 for tasks such as length-controlled abstractive summarization and length-constrained instruction following, with minimal additional computational overhead. This also highlights the significant potential of our method for precise length control across a broader range of applications, without compromising the versatility of LLMs.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14656v1 Announce Type: new \nAbstract: Large language models (LLMs) have demonstrated impressive instruction following capabilities, while still struggling to accurately manage the length of the generated text, which is a fundamental requirement in many real-world applications. Existing length control methods involve fine-tuning the parameters of LLMs, which is inefficient and suboptimal for practical use. In this paper, we propose a novel iterative sampling framework for text length control, integrating the Metropolis-Hastings algorithm with an importance sampling acceleration strategy. This framework efficiently and reliably regulates LLMs to generate length-constrained text without modifying the underlying parameters, thereby preserving the original capabilities of LLMs. Experimental results demonstrate that our framework achieves almost 100\\% success rates of length control on Llama3.1 for tasks such as length-controlled abstractive summarization and length-constrained instruction following, with minimal additional computational overhead. This also highlights the significant potential of our method for precise length control across a broader range of applications, without compromising the versatility of LLMs.'}",oai:arXiv.org:2412.14656v1,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Yuxuan Gu, Wenjie Wang, Xiaocheng Feng, Weihong Zhong, Kun Zhu, Lei Huang, Tat-Seng Chua, Bing Qin'}]","Yuxuan Gu, Wenjie Wang, Xiaocheng Feng, Weihong Zhong, Kun Zhu, Lei Huang, Tat-Seng Chua, Bing Qin","{'name': 'Yuxuan Gu, Wenjie Wang, Xiaocheng Feng, Weihong Zhong, Kun Zhu, Lei Huang, Tat-Seng Chua, Bing Qin'}",,
234,Directivity-Aware Degrees of Freedom Analysis for Extremely Large-Scale MIMO,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Directivity-Aware Degrees of Freedom Analysis for Extremely Large-Scale MIMO'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14657'}]",https://arxiv.org/abs/2412.14657,"arXiv:2412.14657v1 Announce Type: new 
Abstract: Extremely large-scale multiple-input multiple-output (XL-MIMO) communications, enabled by numerous antenna elements integrated into large antenna surfaces, can provide increased effective degree of freedom (EDoF) to achieve high diversity gain. However, it remains an open problem that how the EDoF is influenced by the directional radiation pattern of antenna elements. In this work, empowered by the wavenumber-domain channel representation, we analyze the EDoF in a general case where the directivity of antennas, determined by the antenna structure and element spacing, is considered. Specifically, we first reveal the uneven distribution of directivity-aware wavenumber-domain coupling coefficients, i.e., channel gain towards different directions, in the isotropic Rayleigh fading channel. EDoF is then calculated based on such distribution of coupling coefficients. A numerical method is also provided to obtain coupling coefficients via electromagnetic full-wave simulations. Due to the influence of antenna directivity, how EDoF and ergodic channel capacity vary with the element spacing are explored via simulations for different antenna types.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14657v1 Announce Type: new \nAbstract: Extremely large-scale multiple-input multiple-output (XL-MIMO) communications, enabled by numerous antenna elements integrated into large antenna surfaces, can provide increased effective degree of freedom (EDoF) to achieve high diversity gain. However, it remains an open problem that how the EDoF is influenced by the directional radiation pattern of antenna elements. In this work, empowered by the wavenumber-domain channel representation, we analyze the EDoF in a general case where the directivity of antennas, determined by the antenna structure and element spacing, is considered. Specifically, we first reveal the uneven distribution of directivity-aware wavenumber-domain coupling coefficients, i.e., channel gain towards different directions, in the isotropic Rayleigh fading channel. EDoF is then calculated based on such distribution of coupling coefficients. A numerical method is also provided to obtain coupling coefficients via electromagnetic full-wave simulations. Due to the influence of antenna directivity, how EDoF and ergodic channel capacity vary with the element spacing are explored via simulations for different antenna types.'}",oai:arXiv.org:2412.14657v1,False,"[{'term': 'cs.IT', 'scheme': None, 'label': None}, {'term': 'eess.SP', 'scheme': None, 'label': None}, {'term': 'math.IT', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Shaohua Yue, Liang Liu, Boya Di'}]","Shaohua Yue, Liang Liu, Boya Di","{'name': 'Shaohua Yue, Liang Liu, Boya Di'}",,
235,Robustness Evaluation of a Physical Internet-based Intermodal Logistic Network,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Robustness Evaluation of a Physical Internet-based Intermodal Logistic Network'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14658'}]",https://arxiv.org/abs/2412.14658,"arXiv:2412.14658v1 Announce Type: new 
Abstract: The Physical Internet (PI) paradigm, which has gained attention in research and academia in recent years, leverages advanced logistics and interconnected networks to revolutionize the way goods are transported and delivered, thereby enhancing efficiency, reducing costs and delays, and minimizing environmental impact. Within this system, PI-hubs function similarly to cross-docks enabling the splitting of PI-containers into smaller modules to be delivered through a network of interconnected hubs, allowing dynamic routing optimization and efficient consolidation of PI-containers. Nevertheless, the impact of the system parameters and of the relevant uncertainties on the performance of this innovative logistics framework is still unclear. For this reason, this work proposes a robustness analysis to understand how the PI logistic framework is affected by how PI-containers are handled, consolidated, and processed at the PI-hubs. To this end, the considered PI logistic system is represented via a mathematical programming model that determines the best allocation of PI-containers in an intermodal setting with different transportation modes. In doing so, four Key Performance Indicators (KPIs) are separately considered to investigate different aspects of the PI system's performance and the relevant robustness is assessed with respect to the PI-hubs' processing times and the number of modules per PI-container. In particular, a Global Sensitivity Analysis (GSA) is considered to evaluate, by means of a case study, the individual relevance of each input parameter on the resulting performance.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14658v1 Announce Type: new \nAbstract: The Physical Internet (PI) paradigm, which has gained attention in research and academia in recent years, leverages advanced logistics and interconnected networks to revolutionize the way goods are transported and delivered, thereby enhancing efficiency, reducing costs and delays, and minimizing environmental impact. Within this system, PI-hubs function similarly to cross-docks enabling the splitting of PI-containers into smaller modules to be delivered through a network of interconnected hubs, allowing dynamic routing optimization and efficient consolidation of PI-containers. Nevertheless, the impact of the system parameters and of the relevant uncertainties on the performance of this innovative logistics framework is still unclear. For this reason, this work proposes a robustness analysis to understand how the PI logistic framework is affected by how PI-containers are handled, consolidated, and processed at the PI-hubs. To this end, the considered PI logistic system is represented via a mathematical programming model that determines the best allocation of PI-containers in an intermodal setting with different transportation modes. In doing so, four Key Performance Indicators (KPIs) are separately considered to investigate different aspects of the PI system's performance and the relevant robustness is assessed with respect to the PI-hubs' processing times and the number of modules per PI-container. In particular, a Global Sensitivity Analysis (GSA) is considered to evaluate, by means of a case study, the individual relevance of each input parameter on the resulting performance.""}",oai:arXiv.org:2412.14658v1,False,"[{'term': 'eess.SY', 'scheme': None, 'label': None}, {'term': 'cs.NI', 'scheme': None, 'label': None}, {'term': 'cs.SY', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Federico Gallo, Alireza Shahedi, Angela Di Febbraro, Mahnam Saeednia, Nicola Sacco'}]","Federico Gallo, Alireza Shahedi, Angela Di Febbraro, Mahnam Saeednia, Nicola Sacco","{'name': 'Federico Gallo, Alireza Shahedi, Angela Di Febbraro, Mahnam Saeednia, Nicola Sacco'}",,
236,Unveiling Uncertainty: A Deep Dive into Calibration and Performance of Multimodal Large Language Models,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Unveiling Uncertainty: A Deep Dive into Calibration and Performance of Multimodal Large Language Models'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14660'}]",https://arxiv.org/abs/2412.14660,"arXiv:2412.14660v1 Announce Type: new 
Abstract: Multimodal large language models (MLLMs) combine visual and textual data for tasks such as image captioning and visual question answering. Proper uncertainty calibration is crucial, yet challenging, for reliable use in areas like healthcare and autonomous driving. This paper investigates representative MLLMs, focusing on their calibration across various scenarios, including before and after visual fine-tuning, as well as before and after multimodal training of the base LLMs. We observed miscalibration in their performance, and at the same time, no significant differences in calibration across these scenarios. We also highlight how uncertainty differs between text and images and how their integration affects overall uncertainty. To better understand MLLMs' miscalibration and their ability to self-assess uncertainty, we construct the IDK (I don't know) dataset, which is key to evaluating how they handle unknowns. Our findings reveal that MLLMs tend to give answers rather than admit uncertainty, but this self-assessment improves with proper prompt adjustments. Finally, to calibrate MLLMs and enhance model reliability, we propose techniques such as temperature scaling and iterative prompt optimization. Our results provide insights into improving MLLMs for effective and responsible deployment in multimodal applications. Code and IDK dataset: \href{https://github.com/hfutml/Calibration-MLLM}{https://github.com/hfutml/Calibration-MLLM}.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14660v1 Announce Type: new \nAbstract: Multimodal large language models (MLLMs) combine visual and textual data for tasks such as image captioning and visual question answering. Proper uncertainty calibration is crucial, yet challenging, for reliable use in areas like healthcare and autonomous driving. This paper investigates representative MLLMs, focusing on their calibration across various scenarios, including before and after visual fine-tuning, as well as before and after multimodal training of the base LLMs. We observed miscalibration in their performance, and at the same time, no significant differences in calibration across these scenarios. We also highlight how uncertainty differs between text and images and how their integration affects overall uncertainty. To better understand MLLMs' miscalibration and their ability to self-assess uncertainty, we construct the IDK (I don't know) dataset, which is key to evaluating how they handle unknowns. Our findings reveal that MLLMs tend to give answers rather than admit uncertainty, but this self-assessment improves with proper prompt adjustments. Finally, to calibrate MLLMs and enhance model reliability, we propose techniques such as temperature scaling and iterative prompt optimization. Our results provide insights into improving MLLMs for effective and responsible deployment in multimodal applications. Code and IDK dataset: \\href{https://github.com/hfutml/Calibration-MLLM}{https://github.com/hfutml/Calibration-MLLM}.""}",oai:arXiv.org:2412.14660v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.CL', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'stat.ML', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Zijun Chen, Wenbo Hu, Guande He, Zhijie Deng, Zheng Zhang, Richang Hong'}]","Zijun Chen, Wenbo Hu, Guande He, Zhijie Deng, Zheng Zhang, Richang Hong","{'name': 'Zijun Chen, Wenbo Hu, Guande He, Zhijie Deng, Zheng Zhang, Richang Hong'}",,
237,IOHunter: Graph Foundation Model to Uncover Online Information Operations,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'IOHunter: Graph Foundation Model to Uncover Online Information Operations'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14663'}]",https://arxiv.org/abs/2412.14663,"arXiv:2412.14663v1 Announce Type: new 
Abstract: Social media platforms have become vital spaces for public discourse, serving as modern agor\'as where a wide range of voices influence societal narratives. However, their open nature also makes them vulnerable to exploitation by malicious actors, including state-sponsored entities, who can conduct information operations (IOs) to manipulate public opinion. The spread of misinformation, false news, and misleading claims threatens democratic processes and societal cohesion, making it crucial to develop methods for the timely detection of inauthentic activity to protect the integrity of online discourse. In this work, we introduce a methodology designed to identify users orchestrating information operations, a.k.a. \textit{IO drivers}, across various influence campaigns. Our framework, named \texttt{IOHunter}, leverages the combined strengths of Language Models and Graph Neural Networks to improve generalization in \emph{supervised}, \emph{scarcely-supervised}, and \emph{cross-IO} contexts. Our approach achieves state-of-the-art performance across multiple sets of IOs originating from six countries, significantly surpassing existing approaches. This research marks a step toward developing Graph Foundation Models specifically tailored for the task of IO detection on social media platforms.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14663v1 Announce Type: new \nAbstract: Social media platforms have become vital spaces for public discourse, serving as modern agor\\'as where a wide range of voices influence societal narratives. However, their open nature also makes them vulnerable to exploitation by malicious actors, including state-sponsored entities, who can conduct information operations (IOs) to manipulate public opinion. The spread of misinformation, false news, and misleading claims threatens democratic processes and societal cohesion, making it crucial to develop methods for the timely detection of inauthentic activity to protect the integrity of online discourse. In this work, we introduce a methodology designed to identify users orchestrating information operations, a.k.a. \\textit{IO drivers}, across various influence campaigns. Our framework, named \\texttt{IOHunter}, leverages the combined strengths of Language Models and Graph Neural Networks to improve generalization in \\emph{supervised}, \\emph{scarcely-supervised}, and \\emph{cross-IO} contexts. Our approach achieves state-of-the-art performance across multiple sets of IOs originating from six countries, significantly surpassing existing approaches. This research marks a step toward developing Graph Foundation Models specifically tailored for the task of IO detection on social media platforms.""}",oai:arXiv.org:2412.14663v1,False,"[{'term': 'cs.SI', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by-nc-sa/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-sa/4.0/'}","[{'name': 'Marco Minici, Luca Luceri, Francesco Fabbri, Emilio Ferrara'}]","Marco Minici, Luca Luceri, Francesco Fabbri, Emilio Ferrara","{'name': 'Marco Minici, Luca Luceri, Francesco Fabbri, Emilio Ferrara'}",,
238,A preconditioned inverse iteration with an improved convergence guarantee,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'A preconditioned inverse iteration with an improved convergence guarantee'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14665'}]",https://arxiv.org/abs/2412.14665,"arXiv:2412.14665v1 Announce Type: new 
Abstract: Preconditioned eigenvalue solvers offer the possibility to incorporate preconditioners for the solution of large-scale eigenvalue problems, as they arise from the discretization of partial differential equations. The convergence analysis of such methods is intricate. Even for the relatively simple preconditioned inverse iteration (PINVIT), which targets the smallest eigenvalue of a symmetric positive definite matrix, the celebrated analysis by Neymeyr is highly nontrivial and only yields convergence if the starting vector is fairly close to the desired eigenvector. In this work, we prove a new non-asymptotic convergence result for a variant of PINVIT. Our proof proceeds by analyzing an equivalent Riemannian steepest descent method and leveraging convexity-like properties. We show a convergence rate that nearly matches the one of PINVIT. As a major benefit, we require a condition on the starting vector that tends to be less stringent. This improved global convergence property is demonstrated for two classes of preconditioners with theoretical bounds and a range of numerical experiments.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14665v1 Announce Type: new \nAbstract: Preconditioned eigenvalue solvers offer the possibility to incorporate preconditioners for the solution of large-scale eigenvalue problems, as they arise from the discretization of partial differential equations. The convergence analysis of such methods is intricate. Even for the relatively simple preconditioned inverse iteration (PINVIT), which targets the smallest eigenvalue of a symmetric positive definite matrix, the celebrated analysis by Neymeyr is highly nontrivial and only yields convergence if the starting vector is fairly close to the desired eigenvector. In this work, we prove a new non-asymptotic convergence result for a variant of PINVIT. Our proof proceeds by analyzing an equivalent Riemannian steepest descent method and leveraging convexity-like properties. We show a convergence rate that nearly matches the one of PINVIT. As a major benefit, we require a condition on the starting vector that tends to be less stringent. This improved global convergence property is demonstrated for two classes of preconditioners with theoretical bounds and a range of numerical experiments.'}",oai:arXiv.org:2412.14665v1,False,"[{'term': 'math.NA', 'scheme': None, 'label': None}, {'term': 'cs.NA', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Foivos Alimisis, Daniel Kressner, Nian Shao, Bart Vandereycken'}]","Foivos Alimisis, Daniel Kressner, Nian Shao, Bart Vandereycken","{'name': 'Foivos Alimisis, Daniel Kressner, Nian Shao, Bart Vandereycken'}",,
239,LoLaFL: Low-Latency Federated Learning via Forward-only Propagation,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'LoLaFL: Low-Latency Federated Learning via Forward-only Propagation'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14668'}]",https://arxiv.org/abs/2412.14668,"arXiv:2412.14668v1 Announce Type: new 
Abstract: Federated learning (FL) has emerged as a widely adopted paradigm for enabling edge learning with distributed data while ensuring data privacy. However, the traditional FL with deep neural networks trained via backpropagation can hardly meet the low-latency learning requirements in the sixth generation (6G) mobile networks. This challenge mainly arises from the high-dimensional model parameters to be transmitted and the numerous rounds of communication required for convergence due to the inherent randomness of the training process. To address this issue, we adopt the state-of-the-art principle of maximal coding rate reduction to learn linear discriminative features and extend the resultant white-box neural network into FL, yielding the novel framework of Low-Latency Federated Learning (LoLaFL) via forward-only propagation. LoLaFL enables layer-wise transmissions and aggregation with significantly fewer communication rounds, thereby considerably reducing latency. Additionally, we propose two \emph{nonlinear} aggregation schemes for LoLaFL. The first scheme is based on the proof that the optimal NN parameter aggregation in LoLaFL should be harmonic-mean-like. The second scheme further exploits the low-rank structures of the features and transmits the low-rank-approximated covariance matrices of features to achieve additional latency reduction. Theoretic analysis and experiments are conducted to evaluate the performance of LoLaFL. In comparison with traditional FL, the two nonlinear aggregation schemes for LoLaFL can achieve reductions in latency of over 91\% and 98\%, respectively, while maintaining comparable accuracies.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14668v1 Announce Type: new \nAbstract: Federated learning (FL) has emerged as a widely adopted paradigm for enabling edge learning with distributed data while ensuring data privacy. However, the traditional FL with deep neural networks trained via backpropagation can hardly meet the low-latency learning requirements in the sixth generation (6G) mobile networks. This challenge mainly arises from the high-dimensional model parameters to be transmitted and the numerous rounds of communication required for convergence due to the inherent randomness of the training process. To address this issue, we adopt the state-of-the-art principle of maximal coding rate reduction to learn linear discriminative features and extend the resultant white-box neural network into FL, yielding the novel framework of Low-Latency Federated Learning (LoLaFL) via forward-only propagation. LoLaFL enables layer-wise transmissions and aggregation with significantly fewer communication rounds, thereby considerably reducing latency. Additionally, we propose two \\emph{nonlinear} aggregation schemes for LoLaFL. The first scheme is based on the proof that the optimal NN parameter aggregation in LoLaFL should be harmonic-mean-like. The second scheme further exploits the low-rank structures of the features and transmits the low-rank-approximated covariance matrices of features to achieve additional latency reduction. Theoretic analysis and experiments are conducted to evaluate the performance of LoLaFL. In comparison with traditional FL, the two nonlinear aggregation schemes for LoLaFL can achieve reductions in latency of over 91\\% and 98\\%, respectively, while maintaining comparable accuracies.'}",oai:arXiv.org:2412.14668v1,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.NI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Jierui Zhang, Jianhao Huang, Kaibin Huang'}]","Jierui Zhang, Jianhao Huang, Kaibin Huang","{'name': 'Jierui Zhang, Jianhao Huang, Kaibin Huang'}",,
240,Simplicity over Complexity: An ARN-Based Intrusion Detection Method for Industrial Control Network,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Simplicity over Complexity: An ARN-Based Intrusion Detection Method for Industrial Control Network'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14669'}]",https://arxiv.org/abs/2412.14669,"arXiv:2412.14669v1 Announce Type: new 
Abstract: Industrial control network (ICN) is characterized by real-time responsiveness and reliability, which plays a key role in increasing production speed, rational and efficient processing, and managing the production process. Despite tremendous advantages, ICN inevitably struggles with some challenges, such as malicious user intrusion and hacker attack. To detect malicious intrusions in ICN, intrusion detection systems have been deployed. However, in ICN, network traffic data is equipped with characteristics of large scale, irregularity, multiple features, temporal correlation and high dimensionality, which greatly affect the efficiency and performance. To properly solve the above problems, we design a new intrusion detection method for ICN. Specifically, we first design a novel neural network model called associative recurrent network (ARN), which can properly handle the relationship between past moment hidden state and current moment information. Then, we adopt ARN to design a new intrusion detection method that can efficiently and accurately detect malicious intrusions in ICN. Subsequently, we demonstrate the high efficiency of our proposed method through theoretical computational complexity analysis. Finally, we develop a prototype implementation to evaluate the accuracy. The experimental results prove that our proposed method has sate-of-the-art performance on both the ICN dataset SWaT and the conventional network traffic dataset UNSW-NB15. The accuracies on the SWaT dataset and the UNSW-NB15 dataset reach 95.48% and 97.61%, respectively.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14669v1 Announce Type: new \nAbstract: Industrial control network (ICN) is characterized by real-time responsiveness and reliability, which plays a key role in increasing production speed, rational and efficient processing, and managing the production process. Despite tremendous advantages, ICN inevitably struggles with some challenges, such as malicious user intrusion and hacker attack. To detect malicious intrusions in ICN, intrusion detection systems have been deployed. However, in ICN, network traffic data is equipped with characteristics of large scale, irregularity, multiple features, temporal correlation and high dimensionality, which greatly affect the efficiency and performance. To properly solve the above problems, we design a new intrusion detection method for ICN. Specifically, we first design a novel neural network model called associative recurrent network (ARN), which can properly handle the relationship between past moment hidden state and current moment information. Then, we adopt ARN to design a new intrusion detection method that can efficiently and accurately detect malicious intrusions in ICN. Subsequently, we demonstrate the high efficiency of our proposed method through theoretical computational complexity analysis. Finally, we develop a prototype implementation to evaluate the accuracy. The experimental results prove that our proposed method has sate-of-the-art performance on both the ICN dataset SWaT and the conventional network traffic dataset UNSW-NB15. The accuracies on the SWaT dataset and the UNSW-NB15 dataset reach 95.48% and 97.61%, respectively.'}",oai:arXiv.org:2412.14669v1,False,"[{'term': 'cs.CR', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Ziyi Liu, Dengpan Ye, Changsong Yang, Yong Ding, Yueling Liu, Long Tang, Chuanxi Chen'}]","Ziyi Liu, Dengpan Ye, Changsong Yang, Yong Ding, Yueling Liu, Long Tang, Chuanxi Chen","{'name': 'Ziyi Liu, Dengpan Ye, Changsong Yang, Yong Ding, Yueling Liu, Long Tang, Chuanxi Chen'}",,
241,Analysis and Visualization of Linguistic Structures in Large Language Models: Neural Representations of Verb-Particle Constructions in BERT,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Analysis and Visualization of Linguistic Structures in Large Language Models: Neural Representations of Verb-Particle Constructions in BERT'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14670'}]",https://arxiv.org/abs/2412.14670,"arXiv:2412.14670v1 Announce Type: new 
Abstract: This study investigates the internal representations of verb-particle combinations within transformer-based large language models (LLMs), specifically examining how these models capture lexical and syntactic nuances at different neural network layers. Employing the BERT architecture, we analyse the representational efficacy of its layers for various verb-particle constructions such as 'agree on', 'come back', and 'give up'. Our methodology includes a detailed dataset preparation from the British National Corpus, followed by extensive model training and output analysis through techniques like multi-dimensional scaling (MDS) and generalized discrimination value (GDV) calculations. Results show that BERT's middle layers most effectively capture syntactic structures, with significant variability in representational accuracy across different verb categories. These findings challenge the conventional uniformity assumed in neural network processing of linguistic elements and suggest a complex interplay between network architecture and linguistic representation. Our research contributes to a better understanding of how deep learning models comprehend and process language, offering insights into the potential and limitations of current neural approaches to linguistic analysis. This study not only advances our knowledge in computational linguistics but also prompts further research into optimizing neural architectures for enhanced linguistic precision.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14670v1 Announce Type: new \nAbstract: This study investigates the internal representations of verb-particle combinations within transformer-based large language models (LLMs), specifically examining how these models capture lexical and syntactic nuances at different neural network layers. Employing the BERT architecture, we analyse the representational efficacy of its layers for various verb-particle constructions such as 'agree on', 'come back', and 'give up'. Our methodology includes a detailed dataset preparation from the British National Corpus, followed by extensive model training and output analysis through techniques like multi-dimensional scaling (MDS) and generalized discrimination value (GDV) calculations. Results show that BERT's middle layers most effectively capture syntactic structures, with significant variability in representational accuracy across different verb categories. These findings challenge the conventional uniformity assumed in neural network processing of linguistic elements and suggest a complex interplay between network architecture and linguistic representation. Our research contributes to a better understanding of how deep learning models comprehend and process language, offering insights into the potential and limitations of current neural approaches to linguistic analysis. This study not only advances our knowledge in computational linguistics but also prompts further research into optimizing neural architectures for enhanced linguistic precision.""}",oai:arXiv.org:2412.14670v1,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by-nc-nd/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-nd/4.0/'}","[{'name': 'Hassane Kissane, Achim Schilling, Patrick Krauss'}]","Hassane Kissane, Achim Schilling, Patrick Krauss","{'name': 'Hassane Kissane, Achim Schilling, Patrick Krauss'}",,
242,MUSTER: Longitudinal Deformable Registration by Composition of Consecutive Deformations,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'MUSTER: Longitudinal Deformable Registration by Composition of Consecutive Deformations'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14671'}]",https://arxiv.org/abs/2412.14671,"arXiv:2412.14671v1 Announce Type: new 
Abstract: Longitudinal imaging allows for the study of structural changes over time. One approach to detecting such changes is by non-linear image registration. This study introduces Multi-Session Temporal Registration (MUSTER), a novel method that facilitates longitudinal analysis of changes in extended series of medical images. MUSTER improves upon conventional pairwise registration by incorporating more than two imaging sessions to recover longitudinal deformations. Longitudinal analysis at a voxel-level is challenging due to effects of a changing image contrast as well as instrumental and environmental sources of bias between sessions. We show that local normalized cross-correlation as an image similarity metric leads to biased results and propose a robust alternative. We test the performance of MUSTER on a synthetic multi-site, multi-session neuroimaging dataset and show that, in various scenarios, using MUSTER significantly enhances the estimated deformations relative to pairwise registration. Additionally, we apply MUSTER on a sample of older adults from the Alzheimer's Disease Neuroimaging Initiative (ADNI) study. The results show that MUSTER can effectively identify patterns of neuro-degeneration from T1-weighted images and that these changes correlate with changes in cognition, matching the performance of state of the art segmentation methods. By leveraging GPU acceleration, MUSTER efficiently handles large datasets, making it feasible also in situations with limited computational resources.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14671v1 Announce Type: new \nAbstract: Longitudinal imaging allows for the study of structural changes over time. One approach to detecting such changes is by non-linear image registration. This study introduces Multi-Session Temporal Registration (MUSTER), a novel method that facilitates longitudinal analysis of changes in extended series of medical images. MUSTER improves upon conventional pairwise registration by incorporating more than two imaging sessions to recover longitudinal deformations. Longitudinal analysis at a voxel-level is challenging due to effects of a changing image contrast as well as instrumental and environmental sources of bias between sessions. We show that local normalized cross-correlation as an image similarity metric leads to biased results and propose a robust alternative. We test the performance of MUSTER on a synthetic multi-site, multi-session neuroimaging dataset and show that, in various scenarios, using MUSTER significantly enhances the estimated deformations relative to pairwise registration. Additionally, we apply MUSTER on a sample of older adults from the Alzheimer's Disease Neuroimaging Initiative (ADNI) study. The results show that MUSTER can effectively identify patterns of neuro-degeneration from T1-weighted images and that these changes correlate with changes in cognition, matching the performance of state of the art segmentation methods. By leveraging GPU acceleration, MUSTER efficiently handles large datasets, making it feasible also in situations with limited computational resources.""}",oai:arXiv.org:2412.14671v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.NA', 'scheme': None, 'label': None}, {'term': 'math.NA', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by-nc-sa/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-sa/4.0/'}","[{'name': 'Edvard O. S. Gr{\\o}dem, Donatas Sederevi\\v{c}ius, Esten H. Leonardsen, Bradley J. MacIntosh, Atle Bj{\\o}rnerud, Till Schellhorn, {\\O}ystein S{\\o}rensen, Inge Amlien, Pablo F. Garrido, Anders M. Fjell'}]","Edvard O. S. Gr{\o}dem, Donatas Sederevi\v{c}ius, Esten H. Leonardsen, Bradley J. MacIntosh, Atle Bj{\o}rnerud, Till Schellhorn, {\O}ystein S{\o}rensen, Inge Amlien, Pablo F. Garrido, Anders M. Fjell","{'name': 'Edvard O. S. Gr{\\o}dem, Donatas Sederevi\\v{c}ius, Esten H. Leonardsen, Bradley J. MacIntosh, Atle Bj{\\o}rnerud, Till Schellhorn, {\\O}ystein S{\\o}rensen, Inge Amlien, Pablo F. Garrido, Anders M. Fjell'}",,
243,FiVL: A Framework for Improved Vision-Language Alignment,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'FiVL: A Framework for Improved Vision-Language Alignment'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14672'}]",https://arxiv.org/abs/2412.14672,"arXiv:2412.14672v1 Announce Type: new 
Abstract: Large Vision Language Models (LVLMs) have achieved significant progress in integrating visual and textual inputs for multimodal reasoning. However, a recurring challenge is ensuring these models utilize visual information as effectively as linguistic content when both modalities are necessary to formulate an accurate answer. We hypothesize that hallucinations arise due to the lack of effective visual grounding in current LVLMs. This issue extends to vision-language benchmarks, where it is difficult to make the image indispensable for accurate answer generation, particularly in vision question-answering tasks. In this work, we introduce FiVL, a novel method for constructing datasets designed to train LVLMs for enhanced visual grounding and to evaluate their effectiveness in achieving it. These datasets can be utilized for both training and assessing an LVLM's ability to use image content as substantive evidence rather than relying solely on linguistic priors, providing insights into the model's reliance on visual information. To demonstrate the utility of our dataset, we introduce an innovative training task that outperforms baselines alongside a validation method and application for explainability. The code is available at https://github.com/IntelLabs/fivl.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14672v1 Announce Type: new \nAbstract: Large Vision Language Models (LVLMs) have achieved significant progress in integrating visual and textual inputs for multimodal reasoning. However, a recurring challenge is ensuring these models utilize visual information as effectively as linguistic content when both modalities are necessary to formulate an accurate answer. We hypothesize that hallucinations arise due to the lack of effective visual grounding in current LVLMs. This issue extends to vision-language benchmarks, where it is difficult to make the image indispensable for accurate answer generation, particularly in vision question-answering tasks. In this work, we introduce FiVL, a novel method for constructing datasets designed to train LVLMs for enhanced visual grounding and to evaluate their effectiveness in achieving it. These datasets can be utilized for both training and assessing an LVLM's ability to use image content as substantive evidence rather than relying solely on linguistic priors, providing insights into the model's reliance on visual information. To demonstrate the utility of our dataset, we introduce an innovative training task that outperforms baselines alongside a validation method and application for explainability. The code is available at https://github.com/IntelLabs/fivl.""}",oai:arXiv.org:2412.14672v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Estelle Aflalo, Gabriela Ben Melech Stan, Tiep Le, Man Luo, Shachar Rosenman, Sayak Paul, Shao-Yen Tseng, Vasudev Lal'}]","Estelle Aflalo, Gabriela Ben Melech Stan, Tiep Le, Man Luo, Shachar Rosenman, Sayak Paul, Shao-Yen Tseng, Vasudev Lal","{'name': 'Estelle Aflalo, Gabriela Ben Melech Stan, Tiep Le, Man Luo, Shachar Rosenman, Sayak Paul, Shao-Yen Tseng, Vasudev Lal'}",,
244,Classification of Linear Observed Systems on Multi-Frame Groups via Automorphisms,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Classification of Linear Observed Systems on Multi-Frame Groups via Automorphisms'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14673'}]",https://arxiv.org/abs/2412.14673,"arXiv:2412.14673v1 Announce Type: new 
Abstract: Many navigation problems can be formulated as observer design on linear observed systems with a two-frame group structure, on which an invariant filter can be implemented with guaranteed consistency and stability. It's still unclear how this could be generalized to simultaneous estimation of the poses of multiple frames and the general forms of the linear observed systems involving multiple frames remain unknown. In this letter, we propose a multi-frame group structure by semi-direct product using the two-frame group as building blocks, covering all natural extensions. More importantly, we give a systematic direct calculation to classify all possible forms of linear observed systems including process ODEs and algebraic observations on such multi-frame group through its automorphism structure, in comparison to the existing classification on two-frame groups relying on ingenious construction. Depth-camera inertial odometry with online extrinsics calibration is provided as an application.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14673v1 Announce Type: new \nAbstract: Many navigation problems can be formulated as observer design on linear observed systems with a two-frame group structure, on which an invariant filter can be implemented with guaranteed consistency and stability. It's still unclear how this could be generalized to simultaneous estimation of the poses of multiple frames and the general forms of the linear observed systems involving multiple frames remain unknown. In this letter, we propose a multi-frame group structure by semi-direct product using the two-frame group as building blocks, covering all natural extensions. More importantly, we give a systematic direct calculation to classify all possible forms of linear observed systems including process ODEs and algebraic observations on such multi-frame group through its automorphism structure, in comparison to the existing classification on two-frame groups relying on ingenious construction. Depth-camera inertial odometry with online extrinsics calibration is provided as an application.""}",oai:arXiv.org:2412.14673v1,False,"[{'term': 'eess.SY', 'scheme': None, 'label': None}, {'term': 'cs.SY', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Changwu Liu, Yuan Shen'}]","Changwu Liu, Yuan Shen","{'name': 'Changwu Liu, Yuan Shen'}",,
245,LLMs as mediators: Can they diagnose conflicts accurately?,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'LLMs as mediators: Can they diagnose conflicts accurately?'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14675'}]",https://arxiv.org/abs/2412.14675,"arXiv:2412.14675v1 Announce Type: new 
Abstract: Prior research indicates that to be able to mediate conflict, observers of disagreements between parties must be able to reliably distinguish the sources of their disagreement as stemming from differences in beliefs about what is true (causality) vs. differences in what they value (morality). In this paper, we test if OpenAI's Large Language Models GPT 3.5 and GPT 4 can perform this task and whether one or other type of disagreement proves particularly challenging for LLM's to diagnose. We replicate study 1 in Ko\c{c}ak et al. (2003), which employes a vignette design, with OpenAI's GPT 3.5 and GPT 4. We find that both LLMs have similar semantic understanding of the distinction between causal and moral codes as humans and can reliably distinguish between them. When asked to diagnose the source of disagreement in a conversation, both LLMs, compared to humans, exhibit a tendency to overestimate the extent of causal disagreement and underestimate the extent of moral disagreement in the moral misalignment condition. This tendency is especially pronounced for GPT 4 when using a proximate scale that relies on concrete language specific to an issue. GPT 3.5 does not perform as well as GPT4 or humans when using either the proximate or the distal scale. The study provides a first test of the potential for using LLMs to mediate conflict by diagnosing the root of disagreements in causal and evaluative codes.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14675v1 Announce Type: new \nAbstract: Prior research indicates that to be able to mediate conflict, observers of disagreements between parties must be able to reliably distinguish the sources of their disagreement as stemming from differences in beliefs about what is true (causality) vs. differences in what they value (morality). In this paper, we test if OpenAI's Large Language Models GPT 3.5 and GPT 4 can perform this task and whether one or other type of disagreement proves particularly challenging for LLM's to diagnose. We replicate study 1 in Ko\\c{c}ak et al. (2003), which employes a vignette design, with OpenAI's GPT 3.5 and GPT 4. We find that both LLMs have similar semantic understanding of the distinction between causal and moral codes as humans and can reliably distinguish between them. When asked to diagnose the source of disagreement in a conversation, both LLMs, compared to humans, exhibit a tendency to overestimate the extent of causal disagreement and underestimate the extent of moral disagreement in the moral misalignment condition. This tendency is especially pronounced for GPT 4 when using a proximate scale that relies on concrete language specific to an issue. GPT 3.5 does not perform as well as GPT4 or humans when using either the proximate or the distal scale. The study provides a first test of the potential for using LLMs to mediate conflict by diagnosing the root of disagreements in causal and evaluative codes.""}",oai:arXiv.org:2412.14675v1,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by-nc-nd/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-nd/4.0/'}","[{'name': '\\""Ozgecan Ko\\c{c}ak (Emory University), Phanish Puranam (INSEAD), Af\\c{s}ar Yegin (Kadir Has University)'}]","\""Ozgecan Ko\c{c}ak (Emory University), Phanish Puranam (INSEAD), Af\c{s}ar Yegin (Kadir Has University)","{'name': '\\""Ozgecan Ko\\c{c}ak (Emory University), Phanish Puranam (INSEAD), Af\\c{s}ar Yegin (Kadir Has University)'}",,
246,Efficient Few-Shot Neural Architecture Search by Counting the Number of Nonlinear Functions,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Efficient Few-Shot Neural Architecture Search by Counting the Number of Nonlinear Functions'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14678'}]",https://arxiv.org/abs/2412.14678,"arXiv:2412.14678v1 Announce Type: new 
Abstract: Neural architecture search (NAS) enables finding the best-performing architecture from a search space automatically. Most NAS methods exploit an over-parameterized network (i.e., a supernet) containing all possible architectures (i.e., subnets) in the search space. However, the subnets that share the same set of parameters are likely to have different characteristics, interfering with each other during training. To address this, few-shot NAS methods have been proposed that divide the space into a few subspaces and employ a separate supernet for each subspace to limit the extent of weight sharing. They achieve state-of-the-art performance, but the computational cost increases accordingly. We introduce in this paper a novel few-shot NAS method that exploits the number of nonlinear functions to split the search space. To be specific, our method divides the space such that each subspace consists of subnets with the same number of nonlinear functions. Our splitting criterion is efficient, since it does not require comparing gradients of a supernet to split the space. In addition, we have found that dividing the space allows us to reduce the channel dimensions required for each supernet, which enables training multiple supernets in an efficient manner. We also introduce a supernet-balanced sampling (SBS) technique, sampling several subnets at each training step, to train different supernets evenly within a limited number of training steps. Extensive experiments on standard NAS benchmarks demonstrate the effectiveness of our approach. Our code is available at https://cvlab.yonsei.ac.kr/projects/EFS-NAS.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14678v1 Announce Type: new \nAbstract: Neural architecture search (NAS) enables finding the best-performing architecture from a search space automatically. Most NAS methods exploit an over-parameterized network (i.e., a supernet) containing all possible architectures (i.e., subnets) in the search space. However, the subnets that share the same set of parameters are likely to have different characteristics, interfering with each other during training. To address this, few-shot NAS methods have been proposed that divide the space into a few subspaces and employ a separate supernet for each subspace to limit the extent of weight sharing. They achieve state-of-the-art performance, but the computational cost increases accordingly. We introduce in this paper a novel few-shot NAS method that exploits the number of nonlinear functions to split the search space. To be specific, our method divides the space such that each subspace consists of subnets with the same number of nonlinear functions. Our splitting criterion is efficient, since it does not require comparing gradients of a supernet to split the space. In addition, we have found that dividing the space allows us to reduce the channel dimensions required for each supernet, which enables training multiple supernets in an efficient manner. We also introduce a supernet-balanced sampling (SBS) technique, sampling several subnets at each training step, to train different supernets evenly within a limited number of training steps. Extensive experiments on standard NAS benchmarks demonstrate the effectiveness of our approach. Our code is available at https://cvlab.yonsei.ac.kr/projects/EFS-NAS.'}",oai:arXiv.org:2412.14678v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Youngmin Oh, Hyunju Lee, Bumsub Ham'}]","Youngmin Oh, Hyunju Lee, Bumsub Ham","{'name': 'Youngmin Oh, Hyunju Lee, Bumsub Ham'}",,
247,"On Enforcing Satisfiable, Coherent, and Minimal Sets of Self-Map Constraints in MatBase","{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'On Enforcing Satisfiable, Coherent, and Minimal Sets of Self-Map Constraints in MatBase'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14679'}]",https://arxiv.org/abs/2412.14679,"arXiv:2412.14679v1 Announce Type: new 
Abstract: This paper rigorously and concisely defines, in the context of our (Elementary) Mathematical Data Model ((E)MDM), the mathematical concepts of self-map, compound mapping, totality, one-to-oneness, non-primeness, ontoness, bijectivity, default value, (null-)reflexivity, irreflexivity, (null-)symmetry, asymmetry, (null-)idempotency, anti-idempotency, (null-)equivalence, acyclicity, (null-)representative system mapping, the properties that relate them, and the corresponding corollaries on the coherence and minimality of sets made of such mapping properties viewed as database constraints. Its main contribution is the pseudocode algorithm used by MatBase, our intelligent database management system prototype based on both (E)MDM, the relational, and the entity-relationship data models, for enforcing self-map, atomic, and compound mapping constraint sets. We prove that this algorithm guarantees the satisfiability, coherence, and minimality of such sets, while being very fast, solid, complete, and minimal. In the sequel, we also presented the relevant MatBase user interface as well as the tables of its metacatalog used by this algorithm.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14679v1 Announce Type: new \nAbstract: This paper rigorously and concisely defines, in the context of our (Elementary) Mathematical Data Model ((E)MDM), the mathematical concepts of self-map, compound mapping, totality, one-to-oneness, non-primeness, ontoness, bijectivity, default value, (null-)reflexivity, irreflexivity, (null-)symmetry, asymmetry, (null-)idempotency, anti-idempotency, (null-)equivalence, acyclicity, (null-)representative system mapping, the properties that relate them, and the corresponding corollaries on the coherence and minimality of sets made of such mapping properties viewed as database constraints. Its main contribution is the pseudocode algorithm used by MatBase, our intelligent database management system prototype based on both (E)MDM, the relational, and the entity-relationship data models, for enforcing self-map, atomic, and compound mapping constraint sets. We prove that this algorithm guarantees the satisfiability, coherence, and minimality of such sets, while being very fast, solid, complete, and minimal. In the sequel, we also presented the relevant MatBase user interface as well as the tables of its metacatalog used by this algorithm.'}",oai:arXiv.org:2412.14679v1,False,"[{'term': 'cs.DB', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}",[{'name': 'Christian Mancas'}],Christian Mancas,{'name': 'Christian Mancas'},,
248,A Light-Weight Framework for Open-Set Object Detection with Decoupled Feature Alignment in Joint Space,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'A Light-Weight Framework for Open-Set Object Detection with Decoupled Feature Alignment in Joint Space'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14680'}]",https://arxiv.org/abs/2412.14680,"arXiv:2412.14680v1 Announce Type: new 
Abstract: Open-set object detection (OSOD) is highly desirable for robotic manipulation in unstructured environments. However, existing OSOD methods often fail to meet the requirements of robotic applications due to their high computational burden and complex deployment. To address this issue, this paper proposes a light-weight framework called Decoupled OSOD (DOSOD), which is a practical and highly efficient solution to support real-time OSOD tasks in robotic systems. Specifically, DOSOD builds upon the YOLO-World pipeline by integrating a vision-language model (VLM) with a detector. A Multilayer Perceptron (MLP) adaptor is developed to transform text embeddings extracted by the VLM into a joint space, within which the detector learns the region representations of class-agnostic proposals. Cross-modality features are directly aligned in the joint space, avoiding the complex feature interactions and thereby improving computational efficiency. DOSOD operates like a traditional closed-set detector during the testing phase, effectively bridging the gap between closed-set and open-set detection. Compared to the baseline YOLO-World, the proposed DOSOD significantly enhances real-time performance while maintaining comparable accuracy. The slight DOSOD-S model achieves a Fixed AP of $26.7\%$, compared to $26.2\%$ for YOLO-World-v1-S and $22.7\%$ for YOLO-World-v2-S, using similar backbones on the LVIS minival dataset. Meanwhile, the FPS of DOSOD-S is $57.1\%$ higher than YOLO-World-v1-S and $29.6\%$ higher than YOLO-World-v2-S. Meanwhile, we demonstrate that the DOSOD model facilitates the deployment of edge devices. The codes and models are publicly available at https://github.com/D-Robotics-AI-Lab/DOSOD.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14680v1 Announce Type: new \nAbstract: Open-set object detection (OSOD) is highly desirable for robotic manipulation in unstructured environments. However, existing OSOD methods often fail to meet the requirements of robotic applications due to their high computational burden and complex deployment. To address this issue, this paper proposes a light-weight framework called Decoupled OSOD (DOSOD), which is a practical and highly efficient solution to support real-time OSOD tasks in robotic systems. Specifically, DOSOD builds upon the YOLO-World pipeline by integrating a vision-language model (VLM) with a detector. A Multilayer Perceptron (MLP) adaptor is developed to transform text embeddings extracted by the VLM into a joint space, within which the detector learns the region representations of class-agnostic proposals. Cross-modality features are directly aligned in the joint space, avoiding the complex feature interactions and thereby improving computational efficiency. DOSOD operates like a traditional closed-set detector during the testing phase, effectively bridging the gap between closed-set and open-set detection. Compared to the baseline YOLO-World, the proposed DOSOD significantly enhances real-time performance while maintaining comparable accuracy. The slight DOSOD-S model achieves a Fixed AP of $26.7\\%$, compared to $26.2\\%$ for YOLO-World-v1-S and $22.7\\%$ for YOLO-World-v2-S, using similar backbones on the LVIS minival dataset. Meanwhile, the FPS of DOSOD-S is $57.1\\%$ higher than YOLO-World-v1-S and $29.6\\%$ higher than YOLO-World-v2-S. Meanwhile, we demonstrate that the DOSOD model facilitates the deployment of edge devices. The codes and models are publicly available at https://github.com/D-Robotics-AI-Lab/DOSOD.'}",oai:arXiv.org:2412.14680v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.RO', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Yonghao He, Hu Su, Haiyong Yu, Cong Yang, Wei Sui, Cong Wang, Song Liu'}]","Yonghao He, Hu Su, Haiyong Yu, Cong Yang, Wei Sui, Cong Wang, Song Liu","{'name': 'Yonghao He, Hu Su, Haiyong Yu, Cong Yang, Wei Sui, Cong Wang, Song Liu'}",,
249,Numerical Robustness of PINNs for Multiscale Transport Equations,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Numerical Robustness of PINNs for Multiscale Transport Equations'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14683'}]",https://arxiv.org/abs/2412.14683,"arXiv:2412.14683v1 Announce Type: new 
Abstract: We investigate the numerical solution of multiscale transport equations using Physics Informed Neural Networks (PINNs) with ReLU activation functions. Therefore, we study the analogy between PINNs and Least-Squares Finite Elements (LSFE) which lies in the shared approach to reformulate the PDE solution as a minimization of a quadratic functional. We prove that in the diffusive regime, the correct limit is not reached, in agreement with known results for first-order LSFE. A diffusive scaling is introduced that can be applied to overcome this, again in full agreement with theoretical results for LSFE. We provide numerical results in the case of slab geometry that support our theoretical findings.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14683v1 Announce Type: new \nAbstract: We investigate the numerical solution of multiscale transport equations using Physics Informed Neural Networks (PINNs) with ReLU activation functions. Therefore, we study the analogy between PINNs and Least-Squares Finite Elements (LSFE) which lies in the shared approach to reformulate the PDE solution as a minimization of a quadratic functional. We prove that in the diffusive regime, the correct limit is not reached, in agreement with known results for first-order LSFE. A diffusive scaling is introduced that can be applied to overcome this, again in full agreement with theoretical results for LSFE. We provide numerical results in the case of slab geometry that support our theoretical findings.'}",oai:arXiv.org:2412.14683v1,False,"[{'term': 'math.NA', 'scheme': None, 'label': None}, {'term': 'cs.NA', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by-nc-nd/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-nd/4.0/'}","[{'name': 'Alexander Jesser, Kai Krycki, Ryan G. McClarren, Martin Frank'}]","Alexander Jesser, Kai Krycki, Ryan G. McClarren, Martin Frank","{'name': 'Alexander Jesser, Kai Krycki, Ryan G. McClarren, Martin Frank'}",,
250,Bel Esprit: Multi-Agent Framework for Building AI Model Pipelines,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Bel Esprit: Multi-Agent Framework for Building AI Model Pipelines'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14684'}]",https://arxiv.org/abs/2412.14684,"arXiv:2412.14684v1 Announce Type: new 
Abstract: As the demand for artificial intelligence (AI) grows to address complex real-world tasks, single models are often insufficient, requiring the integration of multiple models into pipelines. This paper introduces Bel Esprit, a conversational agent designed to construct AI model pipelines based on user-defined requirements. Bel Esprit employs a multi-agent framework where subagents collaborate to clarify requirements, build, validate, and populate pipelines with appropriate models. We demonstrate the effectiveness of this framework in generating pipelines from ambiguous user queries, using both human-curated and synthetic data. A detailed error analysis highlights ongoing challenges in pipeline construction. Bel Esprit is available for a free trial at https://belesprit.aixplain.com.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14684v1 Announce Type: new \nAbstract: As the demand for artificial intelligence (AI) grows to address complex real-world tasks, single models are often insufficient, requiring the integration of multiple models into pipelines. This paper introduces Bel Esprit, a conversational agent designed to construct AI model pipelines based on user-defined requirements. Bel Esprit employs a multi-agent framework where subagents collaborate to clarify requirements, build, validate, and populate pipelines with appropriate models. We demonstrate the effectiveness of this framework in generating pipelines from ambiguous user queries, using both human-curated and synthetic data. A detailed error analysis highlights ongoing challenges in pipeline construction. Bel Esprit is available for a free trial at https://belesprit.aixplain.com.'}",oai:arXiv.org:2412.14684v1,False,"[{'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.HC', 'scheme': None, 'label': None}, {'term': 'cs.MA', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by-nc-nd/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-nd/4.0/'}","[{'name': 'Yunsu Kim, AhmedElmogtaba Abdelaziz, Thiago Castro Ferreira, Mohamed Al-Badrashiny, Hassan Sawaf'}]","Yunsu Kim, AhmedElmogtaba Abdelaziz, Thiago Castro Ferreira, Mohamed Al-Badrashiny, Hassan Sawaf","{'name': 'Yunsu Kim, AhmedElmogtaba Abdelaziz, Thiago Castro Ferreira, Mohamed Al-Badrashiny, Hassan Sawaf'}",,
251,Each Fake News is Fake in its Own Way: An Attribution Multi-Granularity Benchmark for Multimodal Fake News Detection,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Each Fake News is Fake in its Own Way: An Attribution Multi-Granularity Benchmark for Multimodal Fake News Detection'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14686'}]",https://arxiv.org/abs/2412.14686,"arXiv:2412.14686v1 Announce Type: new 
Abstract: Social platforms, while facilitating access to information, have also become saturated with a plethora of fake news, resulting in negative consequences. Automatic multimodal fake news detection is a worthwhile pursuit. Existing multimodal fake news datasets only provide binary labels of real or fake. However, real news is alike, while each fake news is fake in its own way. These datasets fail to reflect the mixed nature of various types of multimodal fake news. To bridge the gap, we construct an attributing multi-granularity multimodal fake news detection dataset \amg, revealing the inherent fake pattern. Furthermore, we propose a multi-granularity clue alignment model \our to achieve multimodal fake news detection and attribution. Experimental results demonstrate that \amg is a challenging dataset, and its attribution setting opens up new avenues for future research.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14686v1 Announce Type: new \nAbstract: Social platforms, while facilitating access to information, have also become saturated with a plethora of fake news, resulting in negative consequences. Automatic multimodal fake news detection is a worthwhile pursuit. Existing multimodal fake news datasets only provide binary labels of real or fake. However, real news is alike, while each fake news is fake in its own way. These datasets fail to reflect the mixed nature of various types of multimodal fake news. To bridge the gap, we construct an attributing multi-granularity multimodal fake news detection dataset \\amg, revealing the inherent fake pattern. Furthermore, we propose a multi-granularity clue alignment model \\our to achieve multimodal fake news detection and attribution. Experimental results demonstrate that \\amg is a challenging dataset, and its attribution setting opens up new avenues for future research.'}",oai:arXiv.org:2412.14686v1,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by-nc-nd/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-nd/4.0/'}","[{'name': 'Hao Guo, Zihan Ma, Zhi Zeng, Minnan Luo, Weixin Zeng, Jiuyang Tang, Xiang Zhao'}]","Hao Guo, Zihan Ma, Zhi Zeng, Minnan Luo, Weixin Zeng, Jiuyang Tang, Xiang Zhao","{'name': 'Hao Guo, Zihan Ma, Zhi Zeng, Minnan Luo, Weixin Zeng, Jiuyang Tang, Xiang Zhao'}",,
252,Logic Induced High-Order Reasoning Network for Event-Event Relation Extraction,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Logic Induced High-Order Reasoning Network for Event-Event Relation Extraction'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14688'}]",https://arxiv.org/abs/2412.14688,"arXiv:2412.14688v1 Announce Type: new 
Abstract: To understand a document with multiple events, event-event relation extraction (ERE) emerges as a crucial task, aiming to discern how natural events temporally or structurally associate with each other. To achieve this goal, our work addresses the problems of temporal event relation extraction (TRE) and subevent relation extraction (SRE). The latest methods for such problems have commonly built document-level event graphs for global reasoning across sentences. However, the edges between events are usually derived from external tools heuristically, which are not always reliable and may introduce noise. Moreover, they are not capable of preserving logical constraints among event relations, e.g., coreference constraint, symmetry constraint and conjunction constraint. These constraints guarantee coherence between different relation types,enabling the generation of a uniffed event evolution graph. In this work, we propose a novel method named LogicERE, which performs high-order event relation reasoning through modeling logic constraints. Speciffcally, different from conventional event graphs, we design a logic constraint induced graph (LCG) without any external tools. LCG involves event nodes where the interactions among them can model the coreference constraint, and event pairs nodes where the interactions among them can retain the symmetry constraint and conjunction constraint. Then we perform high-order reasoning on LCG with relational graph transformer to obtain enhanced event and event pair embeddings. Finally, we further incorporate logic constraint information via a joint logic learning module. Extensive experiments demonstrate the effectiveness of the proposed method with state-of-the-art performance on benchmark datasets.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14688v1 Announce Type: new \nAbstract: To understand a document with multiple events, event-event relation extraction (ERE) emerges as a crucial task, aiming to discern how natural events temporally or structurally associate with each other. To achieve this goal, our work addresses the problems of temporal event relation extraction (TRE) and subevent relation extraction (SRE). The latest methods for such problems have commonly built document-level event graphs for global reasoning across sentences. However, the edges between events are usually derived from external tools heuristically, which are not always reliable and may introduce noise. Moreover, they are not capable of preserving logical constraints among event relations, e.g., coreference constraint, symmetry constraint and conjunction constraint. These constraints guarantee coherence between different relation types,enabling the generation of a uniffed event evolution graph. In this work, we propose a novel method named LogicERE, which performs high-order event relation reasoning through modeling logic constraints. Speciffcally, different from conventional event graphs, we design a logic constraint induced graph (LCG) without any external tools. LCG involves event nodes where the interactions among them can model the coreference constraint, and event pairs nodes where the interactions among them can retain the symmetry constraint and conjunction constraint. Then we perform high-order reasoning on LCG with relational graph transformer to obtain enhanced event and event pair embeddings. Finally, we further incorporate logic constraint information via a joint logic learning module. Extensive experiments demonstrate the effectiveness of the proposed method with state-of-the-art performance on benchmark datasets.'}",oai:arXiv.org:2412.14688v1,False,"[{'term': 'cs.IT', 'scheme': None, 'label': None}, {'term': 'math.IT', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Peixin Huang, Xiang Zhao, Minghao Hu, Zhen Tan, Weidong Xiao'}]","Peixin Huang, Xiang Zhao, Minghao Hu, Zhen Tan, Weidong Xiao","{'name': 'Peixin Huang, Xiang Zhao, Minghao Hu, Zhen Tan, Weidong Xiao'}",,
253,How to Synthesize Text Data without Model Collapse?,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'How to Synthesize Text Data without Model Collapse?'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14689'}]",https://arxiv.org/abs/2412.14689,"arXiv:2412.14689v1 Announce Type: new 
Abstract: Model collapse in synthetic data indicates that iterative training on self-generated data leads to a gradual decline in performance. With the proliferation of AI models, synthetic data will fundamentally reshape the web data ecosystem. Future GPT-$\{n\}$ models will inevitably be trained on a blend of synthetic and human-produced data. In this paper, we focus on two questions: what is the impact of synthetic data on language model training, and how to synthesize data without model collapse? We first pre-train language models across different proportions of synthetic data, revealing a negative correlation between the proportion of synthetic data and model performance. We further conduct statistical analysis on synthetic data to uncover distributional shift phenomenon and over-concentration of n-gram features. Inspired by the above findings, we propose token editing on human-produced data to obtain semi-synthetic data. As a proof of concept, we theoretically demonstrate that token-level editing can prevent model collapse, as the test error is constrained by a finite upper bound. We conduct extensive experiments on pre-training from scratch, continual pre-training, and supervised fine-tuning. The results validate our theoretical proof that token-level editing improves data quality and enhances model performance.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14689v1 Announce Type: new \nAbstract: Model collapse in synthetic data indicates that iterative training on self-generated data leads to a gradual decline in performance. With the proliferation of AI models, synthetic data will fundamentally reshape the web data ecosystem. Future GPT-$\\{n\\}$ models will inevitably be trained on a blend of synthetic and human-produced data. In this paper, we focus on two questions: what is the impact of synthetic data on language model training, and how to synthesize data without model collapse? We first pre-train language models across different proportions of synthetic data, revealing a negative correlation between the proportion of synthetic data and model performance. We further conduct statistical analysis on synthetic data to uncover distributional shift phenomenon and over-concentration of n-gram features. Inspired by the above findings, we propose token editing on human-produced data to obtain semi-synthetic data. As a proof of concept, we theoretically demonstrate that token-level editing can prevent model collapse, as the test error is constrained by a finite upper bound. We conduct extensive experiments on pre-training from scratch, continual pre-training, and supervised fine-tuning. The results validate our theoretical proof that token-level editing improves data quality and enhances model performance.'}",oai:arXiv.org:2412.14689v1,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Xuekai Zhu, Daixuan Cheng, Hengli Li, Kaiyan Zhang, Ermo Hua, Xingtai Lv, Ning Ding, Zhouhan Lin, Zilong Zheng, Bowen Zhou'}]","Xuekai Zhu, Daixuan Cheng, Hengli Li, Kaiyan Zhang, Ermo Hua, Xingtai Lv, Ning Ding, Zhouhan Lin, Zilong Zheng, Bowen Zhou","{'name': 'Xuekai Zhu, Daixuan Cheng, Hengli Li, Kaiyan Zhang, Ermo Hua, Xingtai Lv, Ning Ding, Zhouhan Lin, Zilong Zheng, Bowen Zhou'}",,
254,Accurate modeling of continuous-time SAT solvers in SPICE,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Accurate modeling of continuous-time SAT solvers in SPICE'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14690'}]",https://arxiv.org/abs/2412.14690,"arXiv:2412.14690v1 Announce Type: new 
Abstract: Recently, there has been an increasing interest in employing dynamical systems as solvers of NP-complete problems. In this paper, we present accurate implementations of two continuous-time dynamical solvers, known in the literature as analog SAT and digital memcomputing, using advanced numerical integration algorithms of SPICE circuit simulators. For this purpose, we have developed Python scripts that convert Boolean satisfiability (SAT) problems into electronic circuits representing the analog SAT and digital memcomputing dynamical systems. Our Python scripts process conjunctive normal form (CNF) files and create netlists that can be directly imported into LTspice. We explore the SPICE implementations of analog SAT and digital memcomputing solvers by applying these to a selected set of problems and present some interesting and potentially useful findings related to digital memcomputing and analog SAT.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14690v1 Announce Type: new \nAbstract: Recently, there has been an increasing interest in employing dynamical systems as solvers of NP-complete problems. In this paper, we present accurate implementations of two continuous-time dynamical solvers, known in the literature as analog SAT and digital memcomputing, using advanced numerical integration algorithms of SPICE circuit simulators. For this purpose, we have developed Python scripts that convert Boolean satisfiability (SAT) problems into electronic circuits representing the analog SAT and digital memcomputing dynamical systems. Our Python scripts process conjunctive normal form (CNF) files and create netlists that can be directly imported into LTspice. We explore the SPICE implementations of analog SAT and digital memcomputing solvers by applying these to a selected set of problems and present some interesting and potentially useful findings related to digital memcomputing and analog SAT.'}",oai:arXiv.org:2412.14690v1,False,"[{'term': 'cs.ET', 'scheme': None, 'label': None}, {'term': 'nlin.CD', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Yuriy V. Pershin, Dyk Chung Nguyen'}]","Yuriy V. Pershin, Dyk Chung Nguyen","{'name': 'Yuriy V. Pershin, Dyk Chung Nguyen'}",,
255,Explicit Relational Reasoning Network for Scene Text Detection,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Explicit Relational Reasoning Network for Scene Text Detection'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14692'}]",https://arxiv.org/abs/2412.14692,"arXiv:2412.14692v1 Announce Type: new 
Abstract: Connected component (CC) is a proper text shape representation that aligns with human reading intuition. However, CC-based text detection methods have recently faced a developmental bottleneck that their time-consuming post-processing is difficult to eliminate. To address this issue, we introduce an explicit relational reasoning network (ERRNet) to elegantly model the component relationships without post-processing. Concretely, we first represent each text instance as multiple ordered text components, and then treat these components as objects in sequential movement. In this way, scene text detection can be innovatively viewed as a tracking problem. From this perspective, we design an end-to-end tracking decoder to achieve a CC-based method dispensing with post-processing entirely. Additionally, we observe that there is an inconsistency between classification confidence and localization quality, so we propose a Polygon Monte-Carlo method to quickly and accurately evaluate the localization quality. Based on this, we introduce a position-supervised classification loss to guide the task-aligned learning of ERRNet. Experiments on challenging benchmarks demonstrate the effectiveness of our ERRNet. It consistently achieves state-of-the-art accuracy while holding highly competitive inference speed.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14692v1 Announce Type: new \nAbstract: Connected component (CC) is a proper text shape representation that aligns with human reading intuition. However, CC-based text detection methods have recently faced a developmental bottleneck that their time-consuming post-processing is difficult to eliminate. To address this issue, we introduce an explicit relational reasoning network (ERRNet) to elegantly model the component relationships without post-processing. Concretely, we first represent each text instance as multiple ordered text components, and then treat these components as objects in sequential movement. In this way, scene text detection can be innovatively viewed as a tracking problem. From this perspective, we design an end-to-end tracking decoder to achieve a CC-based method dispensing with post-processing entirely. Additionally, we observe that there is an inconsistency between classification confidence and localization quality, so we propose a Polygon Monte-Carlo method to quickly and accurately evaluate the localization quality. Based on this, we introduce a position-supervised classification loss to guide the task-aligned learning of ERRNet. Experiments on challenging benchmarks demonstrate the effectiveness of our ERRNet. It consistently achieves state-of-the-art accuracy while holding highly competitive inference speed.'}",oai:arXiv.org:2412.14692v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Yuchen Su, Zhineng Chen, Yongkun Du, Zhilong Ji, Kai Hu, Jinfeng Bai, Xieping Gao'}]","Yuchen Su, Zhineng Chen, Yongkun Du, Zhilong Ji, Kai Hu, Jinfeng Bai, Xieping Gao","{'name': 'Yuchen Su, Zhineng Chen, Yongkun Du, Zhilong Ji, Kai Hu, Jinfeng Bai, Xieping Gao'}",,
256,Lorentzian Residual Neural Networks,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Lorentzian Residual Neural Networks'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14695'}]",https://arxiv.org/abs/2412.14695,"arXiv:2412.14695v1 Announce Type: new 
Abstract: Hyperbolic neural networks have emerged as a powerful tool for modeling hierarchical data structures prevalent in real-world datasets. Notably, residual connections, which facilitate the direct flow of information across layers, have been instrumental in the success of deep neural networks. However, current methods for constructing hyperbolic residual networks suffer from limitations such as increased model complexity, numerical instability, and errors due to multiple mappings to and from the tangent space. To address these limitations, we introduce LResNet, a novel Lorentzian residual neural network based on the weighted Lorentzian centroid in the Lorentz model of hyperbolic geometry. Our method enables the efficient integration of residual connections in Lorentz hyperbolic neural networks while preserving their hierarchical representation capabilities. We demonstrate that our method can theoretically derive previous methods while offering improved stability, efficiency, and effectiveness. Extensive experiments on both graph and vision tasks showcase the superior performance and robustness of our method compared to state-of-the-art Euclidean and hyperbolic alternatives. Our findings highlight the potential of \method for building more expressive neural networks in hyperbolic embedding space as a generally applicable method to multiple architectures, including CNNs, GNNs, and graph Transformers.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14695v1 Announce Type: new \nAbstract: Hyperbolic neural networks have emerged as a powerful tool for modeling hierarchical data structures prevalent in real-world datasets. Notably, residual connections, which facilitate the direct flow of information across layers, have been instrumental in the success of deep neural networks. However, current methods for constructing hyperbolic residual networks suffer from limitations such as increased model complexity, numerical instability, and errors due to multiple mappings to and from the tangent space. To address these limitations, we introduce LResNet, a novel Lorentzian residual neural network based on the weighted Lorentzian centroid in the Lorentz model of hyperbolic geometry. Our method enables the efficient integration of residual connections in Lorentz hyperbolic neural networks while preserving their hierarchical representation capabilities. We demonstrate that our method can theoretically derive previous methods while offering improved stability, efficiency, and effectiveness. Extensive experiments on both graph and vision tasks showcase the superior performance and robustness of our method compared to state-of-the-art Euclidean and hyperbolic alternatives. Our findings highlight the potential of \\method for building more expressive neural networks in hyperbolic embedding space as a generally applicable method to multiple architectures, including CNNs, GNNs, and graph Transformers.'}",oai:arXiv.org:2412.14695v1,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by-nc-nd/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-nd/4.0/'}","[{'name': 'Neil He, Menglin Yang, Rex Ying'}]","Neil He, Menglin Yang, Rex Ying","{'name': 'Neil He, Menglin Yang, Rex Ying'}",,
257,Physics informed neural network for forward and inverse radiation heat transfer in graded-index medium,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Physics informed neural network for forward and inverse radiation heat transfer in graded-index medium'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14699'}]",https://arxiv.org/abs/2412.14699,"arXiv:2412.14699v1 Announce Type: new 
Abstract: Radiation heat transfer in a graded-index medium often suffers accuracy problems due to the gradual changes in the refractive index. The finite element method, meshfree, and other numerical methods often struggle to maintain accuracy when applied to this medium. To address this issue, we apply physics-informed neural networks (PINNs)-based machine learning algorithms to simulate forward and inverse problems for this medium. We also provide the theoretical upper bounds. This theoretical framework is validated through numerical experiments of predefined and newly developed models that demonstrate the accuracy and robustness of the algorithms in solving radiation transport problems in the medium. The simulations show that the novel algorithm goes on with numerical stability and effectively mitigates oscillatory errors, even in cases with more pronounced variations in the refractive index.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14699v1 Announce Type: new \nAbstract: Radiation heat transfer in a graded-index medium often suffers accuracy problems due to the gradual changes in the refractive index. The finite element method, meshfree, and other numerical methods often struggle to maintain accuracy when applied to this medium. To address this issue, we apply physics-informed neural networks (PINNs)-based machine learning algorithms to simulate forward and inverse problems for this medium. We also provide the theoretical upper bounds. This theoretical framework is validated through numerical experiments of predefined and newly developed models that demonstrate the accuracy and robustness of the algorithms in solving radiation transport problems in the medium. The simulations show that the novel algorithm goes on with numerical stability and effectively mitigates oscillatory errors, even in cases with more pronounced variations in the refractive index.'}",oai:arXiv.org:2412.14699v1,False,"[{'term': 'math.NA', 'scheme': None, 'label': None}, {'term': 'cs.NA', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'K. Murari, S. Sundar'}]","K. Murari, S. Sundar","{'name': 'K. Murari, S. Sundar'}",,
258,Taming the Memory Beast: Strategies for Reliable ML Training on Kubernetes,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Taming the Memory Beast: Strategies for Reliable ML Training on Kubernetes'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14701'}]",https://arxiv.org/abs/2412.14701,"arXiv:2412.14701v1 Announce Type: new 
Abstract: Kubernetes offers a powerful orchestration platform for machine learning training, but memory management can be challenging due to specialized needs and resource constraints. This paper outlines how Kubernetes handles memory requests, limits, Quality of Service classes, and eviction policies for ML workloads, with special focus on GPU memory and ephemeral storage. Common pitfalls such as overcommitment, memory leaks, and ephemeral volume exhaustion are examined. We then provide best practices for stable, scalable memory utilization to help ML practitioners prevent out-of-memory events and ensure high-performance ML training pipelines.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14701v1 Announce Type: new \nAbstract: Kubernetes offers a powerful orchestration platform for machine learning training, but memory management can be challenging due to specialized needs and resource constraints. This paper outlines how Kubernetes handles memory requests, limits, Quality of Service classes, and eviction policies for ML workloads, with special focus on GPU memory and ephemeral storage. Common pitfalls such as overcommitment, memory leaks, and ephemeral volume exhaustion are examined. We then provide best practices for stable, scalable memory utilization to help ML practitioners prevent out-of-memory events and ensure high-performance ML training pipelines.'}",oai:arXiv.org:2412.14701v1,False,"[{'term': 'cs.DC', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}",[{'name': 'Jaideep Ray'}],Jaideep Ray,{'name': 'Jaideep Ray'},,
259,Event-assisted 12-stop HDR Imaging of Dynamic Scene,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Event-assisted 12-stop HDR Imaging of Dynamic Scene'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14705'}]",https://arxiv.org/abs/2412.14705,"arXiv:2412.14705v1 Announce Type: new 
Abstract: High dynamic range (HDR) imaging is a crucial task in computational photography, which captures details across diverse lighting conditions. Traditional HDR fusion methods face limitations in dynamic scenes with extreme exposure differences, as aligning low dynamic range (LDR) frames becomes challenging due to motion and brightness variation. In this work, we propose a novel 12-stop HDR imaging approach for dynamic scenes, leveraging a dual-camera system with an event camera and an RGB camera. The event camera provides temporally dense, high dynamic range signals that improve alignment between LDR frames with large exposure differences, reducing ghosting artifacts caused by motion. Also, a real-world finetuning strategy is proposed to increase the generalization of alignment module on real-world events. Additionally, we introduce a diffusion-based fusion module that incorporates image priors from pre-trained diffusion models to address artifacts in high-contrast regions and minimize errors from the alignment process. To support this work, we developed the ESHDR dataset, the first dataset for 12-stop HDR imaging with synchronized event signals, and validated our approach on both simulated and real-world data. Extensive experiments demonstrate that our method achieves state-of-the-art performance, successfully extending HDR imaging to 12 stops in dynamic scenes.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14705v1 Announce Type: new \nAbstract: High dynamic range (HDR) imaging is a crucial task in computational photography, which captures details across diverse lighting conditions. Traditional HDR fusion methods face limitations in dynamic scenes with extreme exposure differences, as aligning low dynamic range (LDR) frames becomes challenging due to motion and brightness variation. In this work, we propose a novel 12-stop HDR imaging approach for dynamic scenes, leveraging a dual-camera system with an event camera and an RGB camera. The event camera provides temporally dense, high dynamic range signals that improve alignment between LDR frames with large exposure differences, reducing ghosting artifacts caused by motion. Also, a real-world finetuning strategy is proposed to increase the generalization of alignment module on real-world events. Additionally, we introduce a diffusion-based fusion module that incorporates image priors from pre-trained diffusion models to address artifacts in high-contrast regions and minimize errors from the alignment process. To support this work, we developed the ESHDR dataset, the first dataset for 12-stop HDR imaging with synchronized event signals, and validated our approach on both simulated and real-world data. Extensive experiments demonstrate that our method achieves state-of-the-art performance, successfully extending HDR imaging to 12 stops in dynamic scenes.'}",oai:arXiv.org:2412.14705v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Shi Guo, Zixuan Chen, Ziran Zhang, Yutian Chen, Gangwei Xu, Tianfan Xue'}]","Shi Guo, Zixuan Chen, Ziran Zhang, Yutian Chen, Gangwei Xu, Tianfan Xue","{'name': 'Shi Guo, Zixuan Chen, Ziran Zhang, Yutian Chen, Gangwei Xu, Tianfan Xue'}",,
260,EnergyMoGen: Compositional Human Motion Generation with Energy-Based Diffusion Model in Latent Space,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'EnergyMoGen: Compositional Human Motion Generation with Energy-Based Diffusion Model in Latent Space'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14706'}]",https://arxiv.org/abs/2412.14706,"arXiv:2412.14706v1 Announce Type: new 
Abstract: Diffusion models, particularly latent diffusion models, have demonstrated remarkable success in text-driven human motion generation. However, it remains challenging for latent diffusion models to effectively compose multiple semantic concepts into a single, coherent motion sequence. To address this issue, we propose EnergyMoGen, which includes two spectrums of Energy-Based Models: (1) We interpret the diffusion model as a latent-aware energy-based model that generates motions by composing a set of diffusion models in latent space; (2) We introduce a semantic-aware energy model based on cross-attention, which enables semantic composition and adaptive gradient descent for text embeddings. To overcome the challenges of semantic inconsistency and motion distortion across these two spectrums, we introduce Synergistic Energy Fusion. This design allows the motion latent diffusion model to synthesize high-quality, complex motions by combining multiple energy terms corresponding to textual descriptions. Experiments show that our approach outperforms existing state-of-the-art models on various motion generation tasks, including text-to-motion generation, compositional motion generation, and multi-concept motion generation. Additionally, we demonstrate that our method can be used to extend motion datasets and improve the text-to-motion task.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14706v1 Announce Type: new \nAbstract: Diffusion models, particularly latent diffusion models, have demonstrated remarkable success in text-driven human motion generation. However, it remains challenging for latent diffusion models to effectively compose multiple semantic concepts into a single, coherent motion sequence. To address this issue, we propose EnergyMoGen, which includes two spectrums of Energy-Based Models: (1) We interpret the diffusion model as a latent-aware energy-based model that generates motions by composing a set of diffusion models in latent space; (2) We introduce a semantic-aware energy model based on cross-attention, which enables semantic composition and adaptive gradient descent for text embeddings. To overcome the challenges of semantic inconsistency and motion distortion across these two spectrums, we introduce Synergistic Energy Fusion. This design allows the motion latent diffusion model to synthesize high-quality, complex motions by combining multiple energy terms corresponding to textual descriptions. Experiments show that our approach outperforms existing state-of-the-art models on various motion generation tasks, including text-to-motion generation, compositional motion generation, and multi-concept motion generation. Additionally, we demonstrate that our method can be used to extend motion datasets and improve the text-to-motion task.'}",oai:arXiv.org:2412.14706v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Jianrong Zhang, Hehe Fan, Yi Yang'}]","Jianrong Zhang, Hehe Fan, Yi Yang","{'name': 'Jianrong Zhang, Hehe Fan, Yi Yang'}",,
261,Creation of AI-driven Smart Spaces for Enhanced Indoor Environments -- A Survey,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Creation of AI-driven Smart Spaces for Enhanced Indoor Environments -- A Survey'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14708'}]",https://arxiv.org/abs/2412.14708,"arXiv:2412.14708v1 Announce Type: new 
Abstract: Smart spaces are ubiquitous computing environments that integrate diverse sensing and communication technologies to enhance space functionality, optimize energy utilization, and improve user comfort and well-being. The integration of emerging AI methodologies into these environments facilitates the formation of AI-driven smart spaces, which further enhance functionalities of the spaces by enabling advanced applications such as personalized comfort settings, interactive living spaces, and automatization of the space systems, all resulting in enhanced indoor experiences of the users. In this paper, we present a systematic survey of existing research on the foundational components of AI-driven smart spaces, including sensor technologies, data communication protocols, sensor network management and maintenance strategies, as well as the data collection, processing and analytics. Given the pivotal role of AI in establishing AI-powered smart spaces, we explore the opportunities and challenges associated with traditional machine learning (ML) approaches, such as deep learning (DL), and emerging methodologies including large language models (LLMs). Finally, we provide key insights necessary for the development of AI-driven smart spaces, propose future research directions, and sheds light on the path forward.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14708v1 Announce Type: new \nAbstract: Smart spaces are ubiquitous computing environments that integrate diverse sensing and communication technologies to enhance space functionality, optimize energy utilization, and improve user comfort and well-being. The integration of emerging AI methodologies into these environments facilitates the formation of AI-driven smart spaces, which further enhance functionalities of the spaces by enabling advanced applications such as personalized comfort settings, interactive living spaces, and automatization of the space systems, all resulting in enhanced indoor experiences of the users. In this paper, we present a systematic survey of existing research on the foundational components of AI-driven smart spaces, including sensor technologies, data communication protocols, sensor network management and maintenance strategies, as well as the data collection, processing and analytics. Given the pivotal role of AI in establishing AI-powered smart spaces, we explore the opportunities and challenges associated with traditional machine learning (ML) approaches, such as deep learning (DL), and emerging methodologies including large language models (LLMs). Finally, we provide key insights necessary for the development of AI-driven smart spaces, propose future research directions, and sheds light on the path forward.'}",oai:arXiv.org:2412.14708v1,False,"[{'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.DC', 'scheme': None, 'label': None}, {'term': 'cs.ET', 'scheme': None, 'label': None}, {'term': 'cs.HC', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Ayg\\""un Varol, Naser Hossein Motlagh, Mirka Leino, Sasu Tarkoma, Johanna Virkki'}]","Ayg\""un Varol, Naser Hossein Motlagh, Mirka Leino, Sasu Tarkoma, Johanna Virkki","{'name': 'Ayg\\""un Varol, Naser Hossein Motlagh, Mirka Leino, Sasu Tarkoma, Johanna Virkki'}",,
262,ReMoE: Fully Differentiable Mixture-of-Experts with ReLU Routing,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'ReMoE: Fully Differentiable Mixture-of-Experts with ReLU Routing'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14711'}]",https://arxiv.org/abs/2412.14711,"arXiv:2412.14711v1 Announce Type: new 
Abstract: Sparsely activated Mixture-of-Experts (MoE) models are widely adopted to scale up model capacity without increasing the computation budget. However, vanilla TopK routers are trained in a discontinuous, non-differentiable way, limiting their performance and scalability. To address this issue, we propose ReMoE, a fully differentiable MoE architecture that offers a simple yet effective drop-in replacement for the conventional TopK+Softmax routing, utilizing ReLU as the router instead. We further propose methods to regulate the router's sparsity while balancing the load among experts. ReMoE's continuous nature enables efficient dynamic allocation of computation across tokens and layers, while also exhibiting domain specialization. Our experiments demonstrate that ReMoE consistently outperforms vanilla TopK-routed MoE across various model sizes, expert counts, and levels of granularity. Furthermore, ReMoE exhibits superior scalability with respect to the number of experts, surpassing traditional MoE architectures. The implementation based on Megatron-LM is available at https://github.com/thu-ml/ReMoE.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14711v1 Announce Type: new \nAbstract: Sparsely activated Mixture-of-Experts (MoE) models are widely adopted to scale up model capacity without increasing the computation budget. However, vanilla TopK routers are trained in a discontinuous, non-differentiable way, limiting their performance and scalability. To address this issue, we propose ReMoE, a fully differentiable MoE architecture that offers a simple yet effective drop-in replacement for the conventional TopK+Softmax routing, utilizing ReLU as the router instead. We further propose methods to regulate the router's sparsity while balancing the load among experts. ReMoE's continuous nature enables efficient dynamic allocation of computation across tokens and layers, while also exhibiting domain specialization. Our experiments demonstrate that ReMoE consistently outperforms vanilla TopK-routed MoE across various model sizes, expert counts, and levels of granularity. Furthermore, ReMoE exhibits superior scalability with respect to the number of experts, surpassing traditional MoE architectures. The implementation based on Megatron-LM is available at https://github.com/thu-ml/ReMoE.""}",oai:arXiv.org:2412.14711v1,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Ziteng Wang, Jianfei Chen, Jun Zhu'}]","Ziteng Wang, Jianfei Chen, Jun Zhu","{'name': 'Ziteng Wang, Jianfei Chen, Jun Zhu'}",,
263,Holistic Adversarially Robust Pruning,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Holistic Adversarially Robust Pruning'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14714'}]",https://arxiv.org/abs/2412.14714,"arXiv:2412.14714v1 Announce Type: new 
Abstract: Neural networks can be drastically shrunk in size by removing redundant parameters. While crucial for the deployment on resource-constraint hardware, oftentimes, compression comes with a severe drop in accuracy and lack of adversarial robustness. Despite recent advances, counteracting both aspects has only succeeded for moderate compression rates so far. We propose a novel method, HARP, that copes with aggressive pruning significantly better than prior work. For this, we consider the network holistically. We learn a global compression strategy that optimizes how many parameters (compression rate) and which parameters (scoring connections) to prune specific to each layer individually. Our method fine-tunes an existing model with dynamic regularization, that follows a step-wise incremental function balancing the different objectives. It starts by favoring robustness before shifting focus on reaching the target compression rate and only then handles the objectives equally. The learned compression strategies allow us to maintain the pre-trained model natural accuracy and its adversarial robustness for a reduction by 99% of the network original size. Moreover, we observe a crucial influence of non-uniform compression across layers.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14714v1 Announce Type: new \nAbstract: Neural networks can be drastically shrunk in size by removing redundant parameters. While crucial for the deployment on resource-constraint hardware, oftentimes, compression comes with a severe drop in accuracy and lack of adversarial robustness. Despite recent advances, counteracting both aspects has only succeeded for moderate compression rates so far. We propose a novel method, HARP, that copes with aggressive pruning significantly better than prior work. For this, we consider the network holistically. We learn a global compression strategy that optimizes how many parameters (compression rate) and which parameters (scoring connections) to prune specific to each layer individually. Our method fine-tunes an existing model with dynamic regularization, that follows a step-wise incremental function balancing the different objectives. It starts by favoring robustness before shifting focus on reaching the target compression rate and only then handles the objectives equally. The learned compression strategies allow us to maintain the pre-trained model natural accuracy and its adversarial robustness for a reduction by 99% of the network original size. Moreover, we observe a crucial influence of non-uniform compression across layers.'}",oai:arXiv.org:2412.14714v1,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Qi Zhao, Christian Wressnegger'}]","Qi Zhao, Christian Wressnegger","{'name': 'Qi Zhao, Christian Wressnegger'}",,
264,Computing Gram Matrix for SMILES Strings using RDKFingerprint and Sinkhorn-Knopp Algorithm,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Computing Gram Matrix for SMILES Strings using RDKFingerprint and Sinkhorn-Knopp Algorithm'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14717'}]",https://arxiv.org/abs/2412.14717,"arXiv:2412.14717v1 Announce Type: new 
Abstract: In molecular structure data, SMILES (Simplified Molecular Input Line Entry System) strings are used to analyze molecular structure design. Numerical feature representation of SMILES strings is a challenging task. This work proposes a kernel-based approach for encoding and analyzing molecular structures from SMILES strings. The proposed approach involves computing a kernel matrix using the Sinkhorn-Knopp algorithm while using kernel principal component analysis (PCA) for dimensionality reduction. The resulting low-dimensional embeddings are then used for classification and regression analysis. The kernel matrix is computed by converting the SMILES strings into molecular structures using the Morgan Fingerprint, which computes a fingerprint for each molecule. The distance matrix is computed using the pairwise kernels function. The Sinkhorn-Knopp algorithm is used to compute the final kernel matrix that satisfies the constraints of a probability distribution. This is achieved by iteratively adjusting the kernel matrix until the marginal distributions of the rows and columns match the desired marginal distributions. We provided a comprehensive empirical analysis of the proposed kernel method to evaluate its goodness with greater depth. The suggested method is assessed for drug subcategory prediction (classification task) and solubility AlogPS ``Aqueous solubility and Octanol/Water partition coefficient"" (regression task) using the benchmark SMILES string dataset. The outcomes show the proposed method outperforms several baseline methods in terms of supervised analysis and has potential uses in molecular design and drug discovery. Overall, the suggested method is a promising avenue for kernel methods-based molecular structure analysis and design.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14717v1 Announce Type: new \nAbstract: In molecular structure data, SMILES (Simplified Molecular Input Line Entry System) strings are used to analyze molecular structure design. Numerical feature representation of SMILES strings is a challenging task. This work proposes a kernel-based approach for encoding and analyzing molecular structures from SMILES strings. The proposed approach involves computing a kernel matrix using the Sinkhorn-Knopp algorithm while using kernel principal component analysis (PCA) for dimensionality reduction. The resulting low-dimensional embeddings are then used for classification and regression analysis. The kernel matrix is computed by converting the SMILES strings into molecular structures using the Morgan Fingerprint, which computes a fingerprint for each molecule. The distance matrix is computed using the pairwise kernels function. The Sinkhorn-Knopp algorithm is used to compute the final kernel matrix that satisfies the constraints of a probability distribution. This is achieved by iteratively adjusting the kernel matrix until the marginal distributions of the rows and columns match the desired marginal distributions. We provided a comprehensive empirical analysis of the proposed kernel method to evaluate its goodness with greater depth. The suggested method is assessed for drug subcategory prediction (classification task) and solubility AlogPS ``Aqueous solubility and Octanol/Water partition coefficient"" (regression task) using the benchmark SMILES string dataset. The outcomes show the proposed method outperforms several baseline methods in terms of supervised analysis and has potential uses in molecular design and drug discovery. Overall, the suggested method is a promising avenue for kernel methods-based molecular structure analysis and design.'}",oai:arXiv.org:2412.14717v1,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by-nc-nd/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-nd/4.0/'}","[{'name': 'Sarwan Ali, Haris Mansoor, Prakash Chourasia, Imdad Ullah Khan, Murray Patterson'}]","Sarwan Ali, Haris Mansoor, Prakash Chourasia, Imdad Ullah Khan, Murray Patterson","{'name': 'Sarwan Ali, Haris Mansoor, Prakash Chourasia, Imdad Ullah Khan, Murray Patterson'}",,
265,A Comprehensive Forecasting Framework based on Multi-Stage Hierarchical Forecasting Reconciliation and Adjustment,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'A Comprehensive Forecasting Framework based on Multi-Stage Hierarchical Forecasting Reconciliation and Adjustment'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14718'}]",https://arxiv.org/abs/2412.14718,"arXiv:2412.14718v1 Announce Type: new 
Abstract: Ads demand forecasting for Walmart's ad products plays a critical role in enabling effective resource planning, allocation, and management of ads performance. In this paper, we introduce a comprehensive demand forecasting system that tackles hierarchical time series forecasting in business settings. Though traditional hierarchical reconciliation methods ensure forecasting coherence, they often trade off accuracy for coherence especially at lower levels and fail to capture the seasonality unique to each time-series in the hierarchy. Thus, we propose a novel framework ""Multi-Stage Hierarchical Forecasting Reconciliation and Adjustment (Multi-Stage HiFoReAd)"" to address the challenges of preserving seasonality, ensuring coherence, and improving accuracy. Our system first utilizes diverse models, ensembled through Bayesian Optimization (BO), achieving base forecasts. The generated base forecasts are then passed into the Multi-Stage HiFoReAd framework. The initial stage refines the hierarchy using Top-Down forecasts and ""harmonic alignment."" The second stage aligns the higher levels' forecasts using MinTrace algorithm, following which the last two levels undergo ""harmonic alignment"" and ""stratified scaling"", to eventually achieve accurate and coherent forecasts across the whole hierarchy. Our experiments on Walmart's internal Ads-demand dataset and 3 other public datasets, each with 4 hierarchical levels, demonstrate that the average Absolute Percentage Error from the cross-validation sets improve from 3% to 40% across levels against BO-ensemble of models (LGBM, MSTL+ETS, Prophet) as well as from 1.2% to 92.9% against State-Of-The-Art models. In addition, the forecasts at all hierarchical levels are proved to be coherent. The proposed framework has been deployed and leveraged by Walmart's ads, sales and operations teams to track future demands, make informed decisions and plan resources.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14718v1 Announce Type: new \nAbstract: Ads demand forecasting for Walmart\'s ad products plays a critical role in enabling effective resource planning, allocation, and management of ads performance. In this paper, we introduce a comprehensive demand forecasting system that tackles hierarchical time series forecasting in business settings. Though traditional hierarchical reconciliation methods ensure forecasting coherence, they often trade off accuracy for coherence especially at lower levels and fail to capture the seasonality unique to each time-series in the hierarchy. Thus, we propose a novel framework ""Multi-Stage Hierarchical Forecasting Reconciliation and Adjustment (Multi-Stage HiFoReAd)"" to address the challenges of preserving seasonality, ensuring coherence, and improving accuracy. Our system first utilizes diverse models, ensembled through Bayesian Optimization (BO), achieving base forecasts. The generated base forecasts are then passed into the Multi-Stage HiFoReAd framework. The initial stage refines the hierarchy using Top-Down forecasts and ""harmonic alignment."" The second stage aligns the higher levels\' forecasts using MinTrace algorithm, following which the last two levels undergo ""harmonic alignment"" and ""stratified scaling"", to eventually achieve accurate and coherent forecasts across the whole hierarchy. Our experiments on Walmart\'s internal Ads-demand dataset and 3 other public datasets, each with 4 hierarchical levels, demonstrate that the average Absolute Percentage Error from the cross-validation sets improve from 3% to 40% across levels against BO-ensemble of models (LGBM, MSTL+ETS, Prophet) as well as from 1.2% to 92.9% against State-Of-The-Art models. In addition, the forecasts at all hierarchical levels are proved to be coherent. The proposed framework has been deployed and leveraged by Walmart\'s ads, sales and operations teams to track future demands, make informed decisions and plan resources.'}",oai:arXiv.org:2412.14718v1,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'cs.DC', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Zhengchao Yang, Mithun Ghosh, Anish Saha, Dong Xu, Konstantin Shmakov, Kuang-chih Lee'}]","Zhengchao Yang, Mithun Ghosh, Anish Saha, Dong Xu, Konstantin Shmakov, Kuang-chih Lee","{'name': 'Zhengchao Yang, Mithun Ghosh, Anish Saha, Dong Xu, Konstantin Shmakov, Kuang-chih Lee'}",,
266,Prototypical Calibrating Ambiguous Samples for Micro-Action Recognition,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Prototypical Calibrating Ambiguous Samples for Micro-Action Recognition'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14719'}]",https://arxiv.org/abs/2412.14719,"arXiv:2412.14719v1 Announce Type: new 
Abstract: Micro-Action Recognition (MAR) has gained increasing attention due to its crucial role as a form of non-verbal communication in social interactions, with promising potential for applications in human communication and emotion analysis. However, current approaches often overlook the inherent ambiguity in micro-actions, which arises from the wide category range and subtle visual differences between categories. This oversight hampers the accuracy of micro-action recognition. In this paper, we propose a novel Prototypical Calibrating Ambiguous Network (\textbf{PCAN}) to unleash and mitigate the ambiguity of MAR. \textbf{Firstly}, we employ a hierarchical action-tree to identify the ambiguous sample, categorizing them into distinct sets of ambiguous samples of false negatives and false positives, considering both body- and action-level categories. \textbf{Secondly}, we implement an ambiguous contrastive refinement module to calibrate these ambiguous samples by regulating the distance between ambiguous samples and their corresponding prototypes. This calibration process aims to pull false negative ($\mathbb{FN}$) samples closer to their respective prototypes and push false positive ($\mathbb{FP}$) samples apart from their affiliated prototypes. In addition, we propose a new prototypical diversity amplification loss to strengthen the model's capacity by amplifying the differences between different prototypes. \textbf{Finally}, we propose a prototype-guided rectification to rectify prediction by incorporating the representability of prototypes. Extensive experiments conducted on the benchmark dataset demonstrate the superior performance of our method compared to existing approaches. The code is available at https://github.com/kunli-cs/PCAN.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14719v1 Announce Type: new \nAbstract: Micro-Action Recognition (MAR) has gained increasing attention due to its crucial role as a form of non-verbal communication in social interactions, with promising potential for applications in human communication and emotion analysis. However, current approaches often overlook the inherent ambiguity in micro-actions, which arises from the wide category range and subtle visual differences between categories. This oversight hampers the accuracy of micro-action recognition. In this paper, we propose a novel Prototypical Calibrating Ambiguous Network (\\textbf{PCAN}) to unleash and mitigate the ambiguity of MAR. \\textbf{Firstly}, we employ a hierarchical action-tree to identify the ambiguous sample, categorizing them into distinct sets of ambiguous samples of false negatives and false positives, considering both body- and action-level categories. \\textbf{Secondly}, we implement an ambiguous contrastive refinement module to calibrate these ambiguous samples by regulating the distance between ambiguous samples and their corresponding prototypes. This calibration process aims to pull false negative ($\\mathbb{FN}$) samples closer to their respective prototypes and push false positive ($\\mathbb{FP}$) samples apart from their affiliated prototypes. In addition, we propose a new prototypical diversity amplification loss to strengthen the model's capacity by amplifying the differences between different prototypes. \\textbf{Finally}, we propose a prototype-guided rectification to rectify prediction by incorporating the representability of prototypes. Extensive experiments conducted on the benchmark dataset demonstrate the superior performance of our method compared to existing approaches. The code is available at https://github.com/kunli-cs/PCAN.""}",oai:arXiv.org:2412.14719v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Kun Li, Dan Guo, Guoliang Chen, Chunxiao Fan, Jingyuan Xu, Zhiliang Wu, Hehe Fan, Meng Wang'}]","Kun Li, Dan Guo, Guoliang Chen, Chunxiao Fan, Jingyuan Xu, Zhiliang Wu, Hehe Fan, Meng Wang","{'name': 'Kun Li, Dan Guo, Guoliang Chen, Chunxiao Fan, Jingyuan Xu, Zhiliang Wu, Hehe Fan, Meng Wang'}",,
267,FROC: Building Fair ROC from a Trained Classifier,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'FROC: Building Fair ROC from a Trained Classifier'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14724'}]",https://arxiv.org/abs/2412.14724,"arXiv:2412.14724v1 Announce Type: new 
Abstract: This paper considers the problem of fair probabilistic binary classification with binary protected groups. The classifier assigns scores, and a practitioner predicts labels using a certain cut-off threshold based on the desired trade-off between false positives vs. false negatives. It derives these thresholds from the ROC of the classifier. The resultant classifier may be unfair to one of the two protected groups in the dataset. It is desirable that no matter what threshold the practitioner uses, the classifier should be fair to both the protected groups; that is, the $\mathcal{L}_p$ norm between FPRs and TPRs of both the protected groups should be at most $\varepsilon$. We call such fairness on ROCs of both the protected attributes $\varepsilon_p$-Equalized ROC. Given a classifier not satisfying $\varepsilon_1$-Equalized ROC, we aim to design a post-processing method to transform the given (potentially unfair) classifier's output (score) to a suitable randomized yet fair classifier. That is, the resultant classifier must satisfy $\varepsilon_1$-Equalized ROC. First, we introduce a threshold query model on the ROC curves for each protected group. The resulting classifier is bound to face a reduction in AUC. With the proposed query model, we provide a rigorous theoretical analysis of the minimal AUC loss to achieve $\varepsilon_1$-Equalized ROC. To achieve this, we design a linear time algorithm, namely \texttt{FROC}, to transform a given classifier's output to a probabilistic classifier that satisfies $\varepsilon_1$-Equalized ROC. We prove that under certain theoretical conditions, \texttt{FROC}\ achieves the theoretical optimal guarantees. We also study the performance of our \texttt{FROC}\ on multiple real-world datasets with many trained classifiers.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14724v1 Announce Type: new \nAbstract: This paper considers the problem of fair probabilistic binary classification with binary protected groups. The classifier assigns scores, and a practitioner predicts labels using a certain cut-off threshold based on the desired trade-off between false positives vs. false negatives. It derives these thresholds from the ROC of the classifier. The resultant classifier may be unfair to one of the two protected groups in the dataset. It is desirable that no matter what threshold the practitioner uses, the classifier should be fair to both the protected groups; that is, the $\\mathcal{L}_p$ norm between FPRs and TPRs of both the protected groups should be at most $\\varepsilon$. We call such fairness on ROCs of both the protected attributes $\\varepsilon_p$-Equalized ROC. Given a classifier not satisfying $\\varepsilon_1$-Equalized ROC, we aim to design a post-processing method to transform the given (potentially unfair) classifier's output (score) to a suitable randomized yet fair classifier. That is, the resultant classifier must satisfy $\\varepsilon_1$-Equalized ROC. First, we introduce a threshold query model on the ROC curves for each protected group. The resulting classifier is bound to face a reduction in AUC. With the proposed query model, we provide a rigorous theoretical analysis of the minimal AUC loss to achieve $\\varepsilon_1$-Equalized ROC. To achieve this, we design a linear time algorithm, namely \\texttt{FROC}, to transform a given classifier's output to a probabilistic classifier that satisfies $\\varepsilon_1$-Equalized ROC. We prove that under certain theoretical conditions, \\texttt{FROC}\\ achieves the theoretical optimal guarantees. We also study the performance of our \\texttt{FROC}\\ on multiple real-world datasets with many trained classifiers.""}",oai:arXiv.org:2412.14724v1,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Avyukta Manjunatha Vummintala, Shantanu Das, Sujit Gujar'}]","Avyukta Manjunatha Vummintala, Shantanu Das, Sujit Gujar","{'name': 'Avyukta Manjunatha Vummintala, Shantanu Das, Sujit Gujar'}",,
268,LTLf Synthesis Under Unreliable Input,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'LTLf Synthesis Under Unreliable Input'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14728'}]",https://arxiv.org/abs/2412.14728,"arXiv:2412.14728v1 Announce Type: new 
Abstract: We study the problem of realizing strategies for an LTLf goal specification while ensuring that at least an LTLf backup specification is satisfied in case of unreliability of certain input variables. We formally define the problem and characterize its worst-case complexity as 2EXPTIME-complete, like standard LTLf synthesis. Then we devise three different solution techniques: one based on direct automata manipulation, which is 2EXPTIME, one disregarding unreliable input variables by adopting a belief construction, which is 3EXPTIME, and one leveraging second-order quantified LTLf (QLTLf), which is 2EXPTIME and allows for a direct encoding into monadic second-order logic, which in turn is worst-case nonelementary. We prove their correctness and evaluate them against each other empirically. Interestingly, theoretical worst-case bounds do not translate into observed performance; the MSO technique performs best, followed by belief construction and direct automata manipulation. As a byproduct of our study, we provide a general synthesis procedure for arbitrary QLTLf specifications.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14728v1 Announce Type: new \nAbstract: We study the problem of realizing strategies for an LTLf goal specification while ensuring that at least an LTLf backup specification is satisfied in case of unreliability of certain input variables. We formally define the problem and characterize its worst-case complexity as 2EXPTIME-complete, like standard LTLf synthesis. Then we devise three different solution techniques: one based on direct automata manipulation, which is 2EXPTIME, one disregarding unreliable input variables by adopting a belief construction, which is 3EXPTIME, and one leveraging second-order quantified LTLf (QLTLf), which is 2EXPTIME and allows for a direct encoding into monadic second-order logic, which in turn is worst-case nonelementary. We prove their correctness and evaluate them against each other empirically. Interestingly, theoretical worst-case bounds do not translate into observed performance; the MSO technique performs best, followed by belief construction and direct automata manipulation. As a byproduct of our study, we provide a general synthesis procedure for arbitrary QLTLf specifications.'}",oai:arXiv.org:2412.14728v1,False,"[{'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.LO', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Christian Hagemeier, Giuseppe de Giacomo, Moshe Y. Vardi'}]","Christian Hagemeier, Giuseppe de Giacomo, Moshe Y. Vardi","{'name': 'Christian Hagemeier, Giuseppe de Giacomo, Moshe Y. Vardi'}",,
269,Generative AI for Banks: Benchmarks and Algorithms for Synthetic Financial Transaction Data,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Generative AI for Banks: Benchmarks and Algorithms for Synthetic Financial Transaction Data'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14730'}]",https://arxiv.org/abs/2412.14730,"arXiv:2412.14730v1 Announce Type: new 
Abstract: The banking sector faces challenges in using deep learning due to data sensitivity and regulatory constraints, but generative AI may offer a solution. Thus, this study identifies effective algorithms for generating synthetic financial transaction data and evaluates five leading models - Conditional Tabular Generative Adversarial Networks (CTGAN), DoppelGANger (DGAN), Wasserstein GAN, Financial Diffusion (FinDiff), and Tabular Variational AutoEncoders (TVAE) - across five criteria: fidelity, synthesis quality, efficiency, privacy, and graph structure. While none of the algorithms is able to replicate the real data's graph structure, each excels in specific areas: DGAN is ideal for privacy-sensitive tasks, FinDiff and TVAE excel in data replication and augmentation, and CTGAN achieves a balance across all five criteria, making it suitable for general applications with moderate privacy concerns. As a result, our findings offer valuable insights for choosing the most suitable algorithm.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14730v1 Announce Type: new \nAbstract: The banking sector faces challenges in using deep learning due to data sensitivity and regulatory constraints, but generative AI may offer a solution. Thus, this study identifies effective algorithms for generating synthetic financial transaction data and evaluates five leading models - Conditional Tabular Generative Adversarial Networks (CTGAN), DoppelGANger (DGAN), Wasserstein GAN, Financial Diffusion (FinDiff), and Tabular Variational AutoEncoders (TVAE) - across five criteria: fidelity, synthesis quality, efficiency, privacy, and graph structure. While none of the algorithms is able to replicate the real data's graph structure, each excels in specific areas: DGAN is ideal for privacy-sensitive tasks, FinDiff and TVAE excel in data replication and augmentation, and CTGAN achieves a balance across all five criteria, making it suitable for general applications with moderate privacy concerns. As a result, our findings offer valuable insights for choosing the most suitable algorithm.""}",oai:arXiv.org:2412.14730v1,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Fabian Sven Karst, Sook-Yee Chong, Abigail A. Antenor, Enyu Lin, Mahei Manhai Li, Jan Marco Leimeister'}]","Fabian Sven Karst, Sook-Yee Chong, Abigail A. Antenor, Enyu Lin, Mahei Manhai Li, Jan Marco Leimeister","{'name': 'Fabian Sven Karst, Sook-Yee Chong, Abigail A. Antenor, Enyu Lin, Mahei Manhai Li, Jan Marco Leimeister'}",,
270,"Beyond the Hype: A Comprehensive Review of Current Trends in Generative AI Research, Teaching Practices, and Tools","{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Beyond the Hype: A Comprehensive Review of Current Trends in Generative AI Research, Teaching Practices, and Tools'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14732'}]",https://arxiv.org/abs/2412.14732,"arXiv:2412.14732v1 Announce Type: new 
Abstract: Generative AI (GenAI) is advancing rapidly, and the literature in computing education is expanding almost as quickly. Initial responses to GenAI tools were mixed between panic and utopian optimism. Many were fast to point out the opportunities and challenges of GenAI. Researchers reported that these new tools are capable of solving most introductory programming tasks and are causing disruptions throughout the curriculum. These tools can write and explain code, enhance error messages, create resources for instructors, and even provide feedback and help for students like a traditional teaching assistant. In 2024, new research started to emerge on the effects of GenAI usage in the computing classroom. These new data involve the use of GenAI to support classroom instruction at scale and to teach students how to code with GenAI. In support of the former, a new class of tools is emerging that can provide personalized feedback to students on their programming assignments or teach both programming and prompting skills at the same time. With the literature expanding so rapidly, this report aims to summarize and explain what is happening on the ground in computing classrooms. We provide a systematic literature review; a survey of educators and industry professionals; and interviews with educators using GenAI in their courses, educators studying GenAI, and researchers who create GenAI tools to support computing education. The triangulation of these methods and data sources expands the understanding of GenAI usage and perceptions at this critical moment for our community.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14732v1 Announce Type: new \nAbstract: Generative AI (GenAI) is advancing rapidly, and the literature in computing education is expanding almost as quickly. Initial responses to GenAI tools were mixed between panic and utopian optimism. Many were fast to point out the opportunities and challenges of GenAI. Researchers reported that these new tools are capable of solving most introductory programming tasks and are causing disruptions throughout the curriculum. These tools can write and explain code, enhance error messages, create resources for instructors, and even provide feedback and help for students like a traditional teaching assistant. In 2024, new research started to emerge on the effects of GenAI usage in the computing classroom. These new data involve the use of GenAI to support classroom instruction at scale and to teach students how to code with GenAI. In support of the former, a new class of tools is emerging that can provide personalized feedback to students on their programming assignments or teach both programming and prompting skills at the same time. With the literature expanding so rapidly, this report aims to summarize and explain what is happening on the ground in computing classrooms. We provide a systematic literature review; a survey of educators and industry professionals; and interviews with educators using GenAI in their courses, educators studying GenAI, and researchers who create GenAI tools to support computing education. The triangulation of these methods and data sources expands the understanding of GenAI usage and perceptions at this critical moment for our community.'}",oai:arXiv.org:2412.14732v1,False,"[{'term': 'cs.CY', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.HC', 'scheme': None, 'label': None}, {'term': 'cs.SE', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'James Prather, Juho Leinonen, Natalie Kiesler, Jamie Gorson Benario, Sam Lau, Stephen MacNeil, Narges Norouzi, Simone Opel, Vee Pettit, Leo Porter, Brent N. Reeves, Jaromir Savelka, David H. Smith IV, Sven Strickroth, Daniel Zingaro'}]","James Prather, Juho Leinonen, Natalie Kiesler, Jamie Gorson Benario, Sam Lau, Stephen MacNeil, Narges Norouzi, Simone Opel, Vee Pettit, Leo Porter, Brent N. Reeves, Jaromir Savelka, David H. Smith IV, Sven Strickroth, Daniel Zingaro","{'name': 'James Prather, Juho Leinonen, Natalie Kiesler, Jamie Gorson Benario, Sam Lau, Stephen MacNeil, Narges Norouzi, Simone Opel, Vee Pettit, Leo Porter, Brent N. Reeves, Jaromir Savelka, David H. Smith IV, Sven Strickroth, Daniel Zingaro'}",,
271,Advances in Artificial Intelligence forDiabetes Prediction: Insights from a Systematic Literature Review,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Advances in Artificial Intelligence forDiabetes Prediction: Insights from a Systematic Literature Review'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14736'}]",https://arxiv.org/abs/2412.14736,"arXiv:2412.14736v1 Announce Type: new 
Abstract: This systematic review explores the use of machine learning (ML) in predicting diabetes, focusing on datasets, algorithms, training methods, and evaluation metrics. It examines datasets like the Singapore National Diabetic Retinopathy Screening program, REPLACE-BG, National Health and Nutrition Examination Survey, and Pima Indians Diabetes Database. The review assesses the performance of ML algorithms like CNN, SVM, Logistic Regression, and XGBoost in predicting diabetes outcomes. The study emphasizes the importance of interdisciplinary collaboration and ethical considerations in ML-based diabetes prediction models.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14736v1 Announce Type: new \nAbstract: This systematic review explores the use of machine learning (ML) in predicting diabetes, focusing on datasets, algorithms, training methods, and evaluation metrics. It examines datasets like the Singapore National Diabetic Retinopathy Screening program, REPLACE-BG, National Health and Nutrition Examination Survey, and Pima Indians Diabetes Database. The review assesses the performance of ML algorithms like CNN, SVM, Logistic Regression, and XGBoost in predicting diabetes outcomes. The study emphasizes the importance of interdisciplinary collaboration and ethical considerations in ML-based diabetes prediction models.'}",oai:arXiv.org:2412.14736v1,False,"[{'term': 'cs.SE', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Pir Bakhsh Khokhar, Carmine Gravino, Fabio Palomba'}]","Pir Bakhsh Khokhar, Carmine Gravino, Fabio Palomba","{'name': 'Pir Bakhsh Khokhar, Carmine Gravino, Fabio Palomba'}",,
272,On Verbalized Confidence Scores for LLMs,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'On Verbalized Confidence Scores for LLMs'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14737'}]",https://arxiv.org/abs/2412.14737,"arXiv:2412.14737v1 Announce Type: new 
Abstract: The rise of large language models (LLMs) and their tight integration into our daily life make it essential to dedicate efforts towards their trustworthiness. Uncertainty quantification for LLMs can establish more human trust into their responses, but also allows LLM agents to make more informed decisions based on each other's uncertainty. To estimate the uncertainty in a response, internal token logits, task-specific proxy models, or sampling of multiple responses are commonly used. This work focuses on asking the LLM itself to verbalize its uncertainty with a confidence score as part of its output tokens, which is a promising way for prompt- and model-agnostic uncertainty quantification with low overhead. Using an extensive benchmark, we assess the reliability of verbalized confidence scores with respect to different datasets, models, and prompt methods. Our results reveal that the reliability of these scores strongly depends on how the model is asked, but also that it is possible to extract well-calibrated confidence scores with certain prompt methods. We argue that verbalized confidence scores can become a simple but effective and versatile uncertainty quantification method in the future. Our code is available at https://github.com/danielyxyang/llm-verbalized-uq .","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14737v1 Announce Type: new \nAbstract: The rise of large language models (LLMs) and their tight integration into our daily life make it essential to dedicate efforts towards their trustworthiness. Uncertainty quantification for LLMs can establish more human trust into their responses, but also allows LLM agents to make more informed decisions based on each other's uncertainty. To estimate the uncertainty in a response, internal token logits, task-specific proxy models, or sampling of multiple responses are commonly used. This work focuses on asking the LLM itself to verbalize its uncertainty with a confidence score as part of its output tokens, which is a promising way for prompt- and model-agnostic uncertainty quantification with low overhead. Using an extensive benchmark, we assess the reliability of verbalized confidence scores with respect to different datasets, models, and prompt methods. Our results reveal that the reliability of these scores strongly depends on how the model is asked, but also that it is possible to extract well-calibrated confidence scores with certain prompt methods. We argue that verbalized confidence scores can become a simple but effective and versatile uncertainty quantification method in the future. Our code is available at https://github.com/danielyxyang/llm-verbalized-uq .""}",oai:arXiv.org:2412.14737v1,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Daniel Yang, Yao-Hung Hubert Tsai, Makoto Yamada'}]","Daniel Yang, Yao-Hung Hubert Tsai, Makoto Yamada","{'name': 'Daniel Yang, Yao-Hung Hubert Tsai, Makoto Yamada'}",,
273,Boosting GNN Performance via Training Sample Selection Based on Adversarial Robustness Evaluation,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Boosting GNN Performance via Training Sample Selection Based on Adversarial Robustness Evaluation'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14738'}]",https://arxiv.org/abs/2412.14738,"arXiv:2412.14738v1 Announce Type: new 
Abstract: Graph Neural Networks (GNNs) have established themselves as one of the most powerful neural network architectures, excelling in leveraging graph topology and node features for various tasks. However, GNNs are inherently vulnerable to noise in their inputs. Such noise can significantly degrade their performance. To address this challenge, we propose a novel approach that employs adversarial robustness evaluation techniques to identify nodes in the graph that are most susceptible to noise. By selecting and constructing a training set composed of these particularly noise-prone nodes, we then use them to train a Graph Convolutional Network (GCN). Our experimental results demonstrate that this strategy leads to substantial improvements in the GCN's performance.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14738v1 Announce Type: new \nAbstract: Graph Neural Networks (GNNs) have established themselves as one of the most powerful neural network architectures, excelling in leveraging graph topology and node features for various tasks. However, GNNs are inherently vulnerable to noise in their inputs. Such noise can significantly degrade their performance. To address this challenge, we propose a novel approach that employs adversarial robustness evaluation techniques to identify nodes in the graph that are most susceptible to noise. By selecting and constructing a training set composed of these particularly noise-prone nodes, we then use them to train a Graph Convolutional Network (GCN). Our experimental results demonstrate that this strategy leads to substantial improvements in the GCN's performance.""}",oai:arXiv.org:2412.14738v1,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}",[{'name': 'Yongyu Wang'}],Yongyu Wang,{'name': 'Yongyu Wang'},,
274,On the Use of Deep Learning Models for Semantic Clone Detection,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'On the Use of Deep Learning Models for Semantic Clone Detection'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14739'}]",https://arxiv.org/abs/2412.14739,"arXiv:2412.14739v1 Announce Type: new 
Abstract: Detecting and tracking code clones can ease various software development and maintenance tasks when changes in a code fragment should be propagated over all its copies. Several deep learning-based clone detection models have appeared in the literature for detecting syntactic and semantic clones, widely evaluated with the BigCloneBench dataset. However, class imbalance and the small number of semantic clones make BigCloneBench less ideal for interpreting model performance. Researchers also use other datasets such as GoogleCodeJam, OJClone, and SemanticCloneBench to understand model generalizability. To overcome the limitations of existing datasets, the GPT-assisted semantic and cross-language clone dataset GPTCloneBench has been released. However, how these models compare across datasets remains unclear. In this paper, we propose a multi-step evaluation approach for five state-of-the-art clone detection models leveraging existing benchmark datasets, including GPTCloneBench, and using mutation operators to study model ability. Specifically, we examine three highly-performing single-language models (ASTNN, GMN, CodeBERT) on BigCloneBench, SemanticCloneBench, and GPTCloneBench, testing their robustness with mutation operations. Additionally, we compare them against cross-language models (C4, CLCDSA) known for detecting semantic clones. While single-language models show high F1 scores for BigCloneBench, their performance on SemanticCloneBench varies (up to 20%). Interestingly, the cross-language model (C4) shows superior performance (around 7%) on SemanticCloneBench over other models and performs similarly on BigCloneBench and GPTCloneBench. On mutation-based datasets, C4 has more robust performance (less than 1% difference) compared to single-language models, which show high variability.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14739v1 Announce Type: new \nAbstract: Detecting and tracking code clones can ease various software development and maintenance tasks when changes in a code fragment should be propagated over all its copies. Several deep learning-based clone detection models have appeared in the literature for detecting syntactic and semantic clones, widely evaluated with the BigCloneBench dataset. However, class imbalance and the small number of semantic clones make BigCloneBench less ideal for interpreting model performance. Researchers also use other datasets such as GoogleCodeJam, OJClone, and SemanticCloneBench to understand model generalizability. To overcome the limitations of existing datasets, the GPT-assisted semantic and cross-language clone dataset GPTCloneBench has been released. However, how these models compare across datasets remains unclear. In this paper, we propose a multi-step evaluation approach for five state-of-the-art clone detection models leveraging existing benchmark datasets, including GPTCloneBench, and using mutation operators to study model ability. Specifically, we examine three highly-performing single-language models (ASTNN, GMN, CodeBERT) on BigCloneBench, SemanticCloneBench, and GPTCloneBench, testing their robustness with mutation operations. Additionally, we compare them against cross-language models (C4, CLCDSA) known for detecting semantic clones. While single-language models show high F1 scores for BigCloneBench, their performance on SemanticCloneBench varies (up to 20%). Interestingly, the cross-language model (C4) shows superior performance (around 7%) on SemanticCloneBench over other models and performs similarly on BigCloneBench and GPTCloneBench. On mutation-based datasets, C4 has more robust performance (less than 1% difference) compared to single-language models, which show high variability.'}",oai:arXiv.org:2412.14739v1,False,"[{'term': 'cs.SE', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Subroto Nag Pinku, Debajyoti Mondal, Chanchal K. Roy'}]","Subroto Nag Pinku, Debajyoti Mondal, Chanchal K. Roy","{'name': 'Subroto Nag Pinku, Debajyoti Mondal, Chanchal K. Roy'}",,
275,Active Inference and Human--Computer Interaction,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Active Inference and Human--Computer Interaction'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14741'}]",https://arxiv.org/abs/2412.14741,"arXiv:2412.14741v1 Announce Type: new 
Abstract: Active Inference is a closed-loop computational theoretical basis for understanding behaviour, based on agents with internal probabilistic generative models that encode their beliefs about how hidden states in their environment cause their sensations. We review Active Inference and how it could be applied to model the human-computer interaction loop. Active Inference provides a coherent framework for managing generative models of humans, their environments, sensors and interface components. It informs off-line design and supports real-time, online adaptation. It provides model-based explanations for behaviours observed in HCI, and new tools to measure important concepts such as agency and engagement. We discuss how Active Inference offers a new basis for a theory of interaction in HCI, tools for design of modern, complex sensor-based systems, and integration of artificial intelligence technologies, enabling it to cope with diversity in human users and contexts. We discuss the practical challenges in implementing such Active Inference-based systems.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14741v1 Announce Type: new \nAbstract: Active Inference is a closed-loop computational theoretical basis for understanding behaviour, based on agents with internal probabilistic generative models that encode their beliefs about how hidden states in their environment cause their sensations. We review Active Inference and how it could be applied to model the human-computer interaction loop. Active Inference provides a coherent framework for managing generative models of humans, their environments, sensors and interface components. It informs off-line design and supports real-time, online adaptation. It provides model-based explanations for behaviours observed in HCI, and new tools to measure important concepts such as agency and engagement. We discuss how Active Inference offers a new basis for a theory of interaction in HCI, tools for design of modern, complex sensor-based systems, and integration of artificial intelligence technologies, enabling it to cope with diversity in human users and contexts. We discuss the practical challenges in implementing such Active Inference-based systems.'}",oai:arXiv.org:2412.14741v1,False,"[{'term': 'cs.HC', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Roderick Murray-Smith, John H. Williamson, Sebastian Stein'}]","Roderick Murray-Smith, John H. Williamson, Sebastian Stein","{'name': 'Roderick Murray-Smith, John H. Williamson, Sebastian Stein'}",,
276,A parametric algorithm is optimal for non-parametric regression of smooth functions,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'A parametric algorithm is optimal for non-parametric regression of smooth functions'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14744'}]",https://arxiv.org/abs/2412.14744,"arXiv:2412.14744v1 Announce Type: new 
Abstract: We address the regression problem for a general function $f:[-1,1]^d\to \mathbb R$ when the learner selects the training points $\{x_i\}_{i=1}^n$ to achieve a uniform error bound across the entire domain. In this setting, known historically as nonparametric regression, we aim to establish a sample complexity bound that depends solely on the function's degree of smoothness. Assuming periodicity at the domain boundaries, we introduce PADUA, an algorithm that, with high probability, provides performance guarantees optimal up to constant or logarithmic factors across all problem parameters. Notably, PADUA is the first parametric algorithm with optimal sample complexity for this setting. Due to this feature, we prove that, differently from the non-parametric state of the art, PADUA enjoys optimal space complexity in the prediction phase. To validate these results, we perform numerical experiments over functions coming from real audio data, where PADUA shows comparable performance to state-of-the-art methods, while requiring only a fraction of the computational time.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14744v1 Announce Type: new \nAbstract: We address the regression problem for a general function $f:[-1,1]^d\\to \\mathbb R$ when the learner selects the training points $\\{x_i\\}_{i=1}^n$ to achieve a uniform error bound across the entire domain. In this setting, known historically as nonparametric regression, we aim to establish a sample complexity bound that depends solely on the function's degree of smoothness. Assuming periodicity at the domain boundaries, we introduce PADUA, an algorithm that, with high probability, provides performance guarantees optimal up to constant or logarithmic factors across all problem parameters. Notably, PADUA is the first parametric algorithm with optimal sample complexity for this setting. Due to this feature, we prove that, differently from the non-parametric state of the art, PADUA enjoys optimal space complexity in the prediction phase. To validate these results, we perform numerical experiments over functions coming from real audio data, where PADUA shows comparable performance to state-of-the-art methods, while requiring only a fraction of the computational time.""}",oai:arXiv.org:2412.14744v1,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Davide Maran, Marcello Restelli'}]","Davide Maran, Marcello Restelli","{'name': 'Davide Maran, Marcello Restelli'}",,
277,Query pipeline optimization for cancer patient question answering systems,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Query pipeline optimization for cancer patient question answering systems'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14751'}]",https://arxiv.org/abs/2412.14751,"arXiv:2412.14751v1 Announce Type: new 
Abstract: Retrieval-augmented generation (RAG) mitigates hallucination in Large Language Models (LLMs) by using query pipelines to retrieve relevant external information and grounding responses in retrieved knowledge. However, query pipeline optimization for cancer patient question-answering (CPQA) systems requires separately optimizing multiple components with domain-specific considerations. We propose a novel three-aspect optimization approach for the RAG query pipeline in CPQA systems, utilizing public biomedical databases like PubMed and PubMed Central. Our optimization includes: (1) document retrieval, utilizing a comparative analysis of NCBI resources and introducing Hybrid Semantic Real-time Document Retrieval (HSRDR); (2) passage retrieval, identifying optimal pairings of dense retrievers and rerankers; and (3) semantic representation, introducing Semantic Enhanced Overlap Segmentation (SEOS) for improved contextual understanding. On a custom-developed dataset tailored for cancer-related inquiries, our optimized RAG approach improved the answer accuracy of Claude-3-haiku by 5.24% over chain-of-thought prompting and about 3% over a naive RAG setup. This study highlights the importance of domain-specific query optimization in realizing the full potential of RAG and provides a robust framework for building more accurate and reliable CPQA systems, advancing the development of RAG-based biomedical systems.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14751v1 Announce Type: new \nAbstract: Retrieval-augmented generation (RAG) mitigates hallucination in Large Language Models (LLMs) by using query pipelines to retrieve relevant external information and grounding responses in retrieved knowledge. However, query pipeline optimization for cancer patient question-answering (CPQA) systems requires separately optimizing multiple components with domain-specific considerations. We propose a novel three-aspect optimization approach for the RAG query pipeline in CPQA systems, utilizing public biomedical databases like PubMed and PubMed Central. Our optimization includes: (1) document retrieval, utilizing a comparative analysis of NCBI resources and introducing Hybrid Semantic Real-time Document Retrieval (HSRDR); (2) passage retrieval, identifying optimal pairings of dense retrievers and rerankers; and (3) semantic representation, introducing Semantic Enhanced Overlap Segmentation (SEOS) for improved contextual understanding. On a custom-developed dataset tailored for cancer-related inquiries, our optimized RAG approach improved the answer accuracy of Claude-3-haiku by 5.24% over chain-of-thought prompting and about 3% over a naive RAG setup. This study highlights the importance of domain-specific query optimization in realizing the full potential of RAG and provides a robust framework for building more accurate and reliable CPQA systems, advancing the development of RAG-based biomedical systems.'}",oai:arXiv.org:2412.14751v1,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Maolin He, Rena Gao, Mike Conway, Brian E. Chapman'}]","Maolin He, Rena Gao, Mike Conway, Brian E. Chapman","{'name': 'Maolin He, Rena Gao, Mike Conway, Brian E. Chapman'}",,
278,Semantics Foundation of Reductive Reasoning,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Semantics Foundation of Reductive Reasoning'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14758'}]",https://arxiv.org/abs/2412.14758,"arXiv:2412.14758v1 Announce Type: new 
Abstract: The development of logic has largely been through the 'deductive' paradigm: conclusions are inferred from established premisses. However, the use of logic in the context of both human and machine reasoning is typically through the dual 'reductive' perspective: collections of sufficient premisses are generated from putative conclusions. We call this paradigm, 'reductive logic'. This expression of logic encompass as diverse reasoning activities as proving a formula in a formal system to seeking to meet a friend before noon on Saturday. This paper is a semantical analysis of reductive logic. In particular, we provide mathematical foundations for representing and reasoning about 'reduction operators'. Heuristically, reduction operators may be thought of as `backwards' inference rules. In this paper, we address their mathematical representation, how they are used in the context of reductive reasoning, and, crucially, what makes them 'valid'.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14758v1 Announce Type: new \nAbstract: The development of logic has largely been through the 'deductive' paradigm: conclusions are inferred from established premisses. However, the use of logic in the context of both human and machine reasoning is typically through the dual 'reductive' perspective: collections of sufficient premisses are generated from putative conclusions. We call this paradigm, 'reductive logic'. This expression of logic encompass as diverse reasoning activities as proving a formula in a formal system to seeking to meet a friend before noon on Saturday. This paper is a semantical analysis of reductive logic. In particular, we provide mathematical foundations for representing and reasoning about 'reduction operators'. Heuristically, reduction operators may be thought of as `backwards' inference rules. In this paper, we address their mathematical representation, how they are used in the context of reductive reasoning, and, crucially, what makes them 'valid'.""}",oai:arXiv.org:2412.14758v1,False,"[{'term': 'cs.LO', 'scheme': None, 'label': None}, {'term': 'math.LO', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Alexander V. Gheorghiu, David J. Pym'}]","Alexander V. Gheorghiu, David J. Pym","{'name': 'Alexander V. Gheorghiu, David J. Pym'}",,"Topoi 2024, Special issue 'Meaning and Understanding via Proofs'"
279,A Meshfree RBF-FD Constant along Normal Method for Solving PDEs on Surfaces,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'A Meshfree RBF-FD Constant along Normal Method for Solving PDEs on Surfaces'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14761'}]",https://arxiv.org/abs/2412.14761,"arXiv:2412.14761v1 Announce Type: new 
Abstract: This paper introduces a novel meshfree methodology based on Radial Basis Function-Finite Difference (RBF-FD) approximations for the numerical solution of partial differential equations (PDEs) on surfaces of codimension 1 embedded in $\mathbb{R}^3$. The method is built upon the principles of the closest point method, without the use of a grid or a closest point mapping. We show that the combination of local embedded stencils with these principles can be employed to approximate surface derivatives using polyharmonic spline kernels and polynomials (PHS+Poly) RBF-FD. Specifically, we show that it is enough to consider a constant extension along the normal direction only at a single node to overcome the rank deficiency of the polynomial basis. An extensive parameter analysis is presented to test the dependence of the approach. We demonstrate high-order convergence rates on problems involving surface advection and surface diffusion, and solve Turing pattern formations on surfaces defined either implicitly or by point clouds. Moreover, a simple coupling approach with a particle tracking method demonstrates the potential of the proposed method in solving PDEs on evolving surfaces in the normal direction. Our numerical results confirm the stability, flexibility, and high-order algebraic convergence of the approach.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14761v1 Announce Type: new \nAbstract: This paper introduces a novel meshfree methodology based on Radial Basis Function-Finite Difference (RBF-FD) approximations for the numerical solution of partial differential equations (PDEs) on surfaces of codimension 1 embedded in $\\mathbb{R}^3$. The method is built upon the principles of the closest point method, without the use of a grid or a closest point mapping. We show that the combination of local embedded stencils with these principles can be employed to approximate surface derivatives using polyharmonic spline kernels and polynomials (PHS+Poly) RBF-FD. Specifically, we show that it is enough to consider a constant extension along the normal direction only at a single node to overcome the rank deficiency of the polynomial basis. An extensive parameter analysis is presented to test the dependence of the approach. We demonstrate high-order convergence rates on problems involving surface advection and surface diffusion, and solve Turing pattern formations on surfaces defined either implicitly or by point clouds. Moreover, a simple coupling approach with a particle tracking method demonstrates the potential of the proposed method in solving PDEs on evolving surfaces in the normal direction. Our numerical results confirm the stability, flexibility, and high-order algebraic convergence of the approach.'}",oai:arXiv.org:2412.14761v1,False,"[{'term': 'math.NA', 'scheme': None, 'label': None}, {'term': 'cs.NA', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': ""V\\'ictor Bayona, Argyrios Petras, C\\'ecile Piret, Steven J. Ruuth""}]","V\'ictor Bayona, Argyrios Petras, C\'ecile Piret, Steven J. Ruuth","{'name': ""V\\'ictor Bayona, Argyrios Petras, C\\'ecile Piret, Steven J. Ruuth""}",10.1137/23M1621265,"SIAM Journal on Scientific Computing, 46(6), A3897-A3921, 2024"
280,A General Control Method for Human-Robot Integration,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'A General Control Method for Human-Robot Integration'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14762'}]",https://arxiv.org/abs/2412.14762,"arXiv:2412.14762v1 Announce Type: new 
Abstract: This paper introduces a new generalized control method designed for multi-degrees-of-freedom devices to help people with limited motion capabilities in their daily activities. The challenge lies in finding the most adapted strategy for the control interface to effectively map user's motions in a low-dimensional space to complex robotic assistive devices, such as prostheses, supernumerary limbs, up to remote robotic avatars. The goal is a system which integrates the human and the robotic parts into a unique system, moving so as to reach the targets decided by the human while autonomously reducing the user's effort and discomfort. We present a framework to control general multi DoFs assistive systems, which translates user-performed compensatory motions into the necessary robot commands for reaching targets while canceling or reducing compensation. The framework extends to prostheses of any number of DoF up to full robotic avatars, regarded here as a sort of whole-body prosthesis of the person who sees the robot as an artificial extension of their own body without a physical link but with a sensory-motor integration. We have validated and applied this control strategy through tests encompassing simulated scenarios and real-world trials involving a virtual twin of the robotic parts (prosthesis and robot) and a physical humanoid avatar.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14762v1 Announce Type: new \nAbstract: This paper introduces a new generalized control method designed for multi-degrees-of-freedom devices to help people with limited motion capabilities in their daily activities. The challenge lies in finding the most adapted strategy for the control interface to effectively map user's motions in a low-dimensional space to complex robotic assistive devices, such as prostheses, supernumerary limbs, up to remote robotic avatars. The goal is a system which integrates the human and the robotic parts into a unique system, moving so as to reach the targets decided by the human while autonomously reducing the user's effort and discomfort. We present a framework to control general multi DoFs assistive systems, which translates user-performed compensatory motions into the necessary robot commands for reaching targets while canceling or reducing compensation. The framework extends to prostheses of any number of DoF up to full robotic avatars, regarded here as a sort of whole-body prosthesis of the person who sees the robot as an artificial extension of their own body without a physical link but with a sensory-motor integration. We have validated and applied this control strategy through tests encompassing simulated scenarios and real-world trials involving a virtual twin of the robotic parts (prosthesis and robot) and a physical humanoid avatar.""}",oai:arXiv.org:2412.14762v1,False,"[{'term': 'cs.RO', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by-nc-nd/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-nd/4.0/'}","[{'name': 'Maddalena Feder, Giorgio Grioli, Manuel G. Catalano, Antonio Bicchi'}]","Maddalena Feder, Giorgio Grioli, Manuel G. Catalano, Antonio Bicchi","{'name': 'Maddalena Feder, Giorgio Grioli, Manuel G. Catalano, Antonio Bicchi'}",,
281,CodeRepoQA: A Large-scale Benchmark for Software Engineering Question Answering,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'CodeRepoQA: A Large-scale Benchmark for Software Engineering Question Answering'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14764'}]",https://arxiv.org/abs/2412.14764,"arXiv:2412.14764v1 Announce Type: new 
Abstract: In this work, we introduce CodeRepoQA, a large-scale benchmark specifically designed for evaluating repository-level question-answering capabilities in the field of software engineering. CodeRepoQA encompasses five programming languages and covers a wide range of scenarios, enabling comprehensive evaluation of language models. To construct this dataset, we crawl data from 30 well-known repositories in GitHub, the largest platform for hosting and collaborating on code, and carefully filter raw data. In total, CodeRepoQA is a multi-turn question-answering benchmark with 585,687 entries, covering a diverse array of software engineering scenarios, with an average of 6.62 dialogue turns per entry.
  We evaluate ten popular large language models on our dataset and provide in-depth analysis. We find that LLMs still have limitations in question-answering capabilities in the field of software engineering, and medium-length contexts are more conducive to LLMs' performance. The entire benchmark is publicly available at https://github.com/kinesiatricssxilm14/CodeRepoQA.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14764v1 Announce Type: new \nAbstract: In this work, we introduce CodeRepoQA, a large-scale benchmark specifically designed for evaluating repository-level question-answering capabilities in the field of software engineering. CodeRepoQA encompasses five programming languages and covers a wide range of scenarios, enabling comprehensive evaluation of language models. To construct this dataset, we crawl data from 30 well-known repositories in GitHub, the largest platform for hosting and collaborating on code, and carefully filter raw data. In total, CodeRepoQA is a multi-turn question-answering benchmark with 585,687 entries, covering a diverse array of software engineering scenarios, with an average of 6.62 dialogue turns per entry.\n  We evaluate ten popular large language models on our dataset and provide in-depth analysis. We find that LLMs still have limitations in question-answering capabilities in the field of software engineering, and medium-length contexts are more conducive to LLMs' performance. The entire benchmark is publicly available at https://github.com/kinesiatricssxilm14/CodeRepoQA.""}",oai:arXiv.org:2412.14764v1,False,"[{'term': 'cs.SE', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by-nc-nd/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-nd/4.0/'}","[{'name': 'Ruida Hu, Chao Peng, Jingyi Ren, Bo Jiang, Xiangxin Meng, Qinyun Wu, Pengfei Gao, Xinchen Wang, Cuiyun Gao'}]","Ruida Hu, Chao Peng, Jingyi Ren, Bo Jiang, Xiangxin Meng, Qinyun Wu, Pengfei Gao, Xinchen Wang, Cuiyun Gao","{'name': 'Ruida Hu, Chao Peng, Jingyi Ren, Bo Jiang, Xiangxin Meng, Qinyun Wu, Pengfei Gao, Xinchen Wang, Cuiyun Gao'}",,
282,FLAMe: Federated Learning with Attention Mechanism using Spatio-Temporal Keypoint Transformers for Pedestrian Fall Detection in Smart Cities,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'FLAMe: Federated Learning with Attention Mechanism using Spatio-Temporal Keypoint Transformers for Pedestrian Fall Detection in Smart Cities'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14768'}]",https://arxiv.org/abs/2412.14768,"arXiv:2412.14768v1 Announce Type: new 
Abstract: In smart cities, detecting pedestrian falls is a major challenge to ensure the safety and quality of life of citizens. In this study, we propose a novel fall detection system using FLAMe (Federated Learning with Attention Mechanism), a federated learning (FL) based algorithm. FLAMe trains around important keypoint information and only transmits the trained important weights to the server, reducing communication costs and preserving data privacy. Furthermore, the lightweight keypoint transformer model is integrated into the FL framework to effectively learn spatio-temporal features. We validated the experiment using 22,672 video samples from the ""Fall Accident Risk Behavior Video-Sensor Pair data"" dataset from AI-Hub. As a result of the experiment, the FLAMe-based system achieved an accuracy of 94.02% with about 190,000 transmission parameters, maintaining performance similar to that of existing centralized learning while maximizing efficiency by reducing communication costs by about 40% compared to the existing FL algorithm, FedAvg. Therefore, the FLAMe algorithm has demonstrated that it provides robust performance in the distributed environment of smart cities and is a practical and effective solution for public safety.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14768v1 Announce Type: new \nAbstract: In smart cities, detecting pedestrian falls is a major challenge to ensure the safety and quality of life of citizens. In this study, we propose a novel fall detection system using FLAMe (Federated Learning with Attention Mechanism), a federated learning (FL) based algorithm. FLAMe trains around important keypoint information and only transmits the trained important weights to the server, reducing communication costs and preserving data privacy. Furthermore, the lightweight keypoint transformer model is integrated into the FL framework to effectively learn spatio-temporal features. We validated the experiment using 22,672 video samples from the ""Fall Accident Risk Behavior Video-Sensor Pair data"" dataset from AI-Hub. As a result of the experiment, the FLAMe-based system achieved an accuracy of 94.02% with about 190,000 transmission parameters, maintaining performance similar to that of existing centralized learning while maximizing efficiency by reducing communication costs by about 40% compared to the existing FL algorithm, FedAvg. Therefore, the FLAMe algorithm has demonstrated that it provides robust performance in the distributed environment of smart cities and is a practical and effective solution for public safety.'}",oai:arXiv.org:2412.14768v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Byeonghun Kim, Byeongjoon Noh'}]","Byeonghun Kim, Byeongjoon Noh","{'name': 'Byeonghun Kim, Byeongjoon Noh'}",,
283,PsyDraw: A Multi-Agent Multimodal System for Mental Health Screening in Left-Behind Children,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'PsyDraw: A Multi-Agent Multimodal System for Mental Health Screening in Left-Behind Children'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14769'}]",https://arxiv.org/abs/2412.14769,"arXiv:2412.14769v1 Announce Type: new 
Abstract: Left-behind children (LBCs), numbering over 66 million in China, face severe mental health challenges due to parental migration for work. Early screening and identification of at-risk LBCs is crucial, yet challenging due to the severe shortage of mental health professionals, especially in rural areas. While the House-Tree-Person (HTP) test shows higher child participation rates, its requirement for expert interpretation limits its application in resource-scarce regions. To address this challenge, we propose PsyDraw, a multi-agent system based on Multimodal Large Language Models that assists mental health professionals in analyzing HTP drawings. The system employs specialized agents for feature extraction and psychological interpretation, operating in two stages: comprehensive feature analysis and professional report generation. Evaluation of HTP drawings from 290 primary school students reveals that 71.03% of the analyzes achieved High Consistency with professional evaluations, 26.21% Moderate Consistency and only 2.41% Low Consistency. The system identified 31.03% of cases requiring professional attention, demonstrating its effectiveness as a preliminary screening tool. Currently deployed in pilot schools, \method shows promise in supporting mental health professionals, particularly in resource-limited areas, while maintaining high professional standards in psychological assessment.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14769v1 Announce Type: new \nAbstract: Left-behind children (LBCs), numbering over 66 million in China, face severe mental health challenges due to parental migration for work. Early screening and identification of at-risk LBCs is crucial, yet challenging due to the severe shortage of mental health professionals, especially in rural areas. While the House-Tree-Person (HTP) test shows higher child participation rates, its requirement for expert interpretation limits its application in resource-scarce regions. To address this challenge, we propose PsyDraw, a multi-agent system based on Multimodal Large Language Models that assists mental health professionals in analyzing HTP drawings. The system employs specialized agents for feature extraction and psychological interpretation, operating in two stages: comprehensive feature analysis and professional report generation. Evaluation of HTP drawings from 290 primary school students reveals that 71.03% of the analyzes achieved High Consistency with professional evaluations, 26.21% Moderate Consistency and only 2.41% Low Consistency. The system identified 31.03% of cases requiring professional attention, demonstrating its effectiveness as a preliminary screening tool. Currently deployed in pilot schools, \\method shows promise in supporting mental health professionals, particularly in resource-limited areas, while maintaining high professional standards in psychological assessment.'}",oai:arXiv.org:2412.14769v1,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Yiqun Zhang, Xiaocui Yang, Xiaobai Li, Siyuan Yu, Yi Luan, Shi Feng, Daling Wang, Yifei Zhang'}]","Yiqun Zhang, Xiaocui Yang, Xiaobai Li, Siyuan Yu, Yi Luan, Shi Feng, Daling Wang, Yifei Zhang","{'name': 'Yiqun Zhang, Xiaocui Yang, Xiaobai Li, Siyuan Yu, Yi Luan, Shi Feng, Daling Wang, Yifei Zhang'}",,
284,ALKAFI-LLAMA3: Fine-Tuning LLMs for Precise Legal Understanding in Palestine,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'ALKAFI-LLAMA3: Fine-Tuning LLMs for Precise Legal Understanding in Palestine'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14771'}]",https://arxiv.org/abs/2412.14771,"arXiv:2412.14771v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated remarkable potential in diverse domains, yet their application in the legal sector, particularly in low-resource contexts, remains limited. This study addresses the challenges of adapting LLMs to the Palestinian legal domain, where political instability, fragmented legal frameworks, and limited AI resources hinder effective machine-learning applications. We present a fine-tuned model based on a quantized version of Llama-3.2-1B-Instruct, trained on a synthetic data set derived from Palestinian legal texts. Using smaller-scale models and strategically generated question-answer pairs, we achieve a cost-effective, locally sustainable solution that provides accurate and contextually relevant legal guidance. Our experiments demonstrate promising performance on various query types, ranging from yes/no questions and narrative explanations to complex legal differentiations, while highlighting areas for improvement, such as handling calculation-based inquiries and structured list formatting. This work provides a pathway for the deployment of AI-driven legal assistance tools tailored to the needs of resource-constrained environments.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14771v1 Announce Type: new \nAbstract: Large Language Models (LLMs) have demonstrated remarkable potential in diverse domains, yet their application in the legal sector, particularly in low-resource contexts, remains limited. This study addresses the challenges of adapting LLMs to the Palestinian legal domain, where political instability, fragmented legal frameworks, and limited AI resources hinder effective machine-learning applications. We present a fine-tuned model based on a quantized version of Llama-3.2-1B-Instruct, trained on a synthetic data set derived from Palestinian legal texts. Using smaller-scale models and strategically generated question-answer pairs, we achieve a cost-effective, locally sustainable solution that provides accurate and contextually relevant legal guidance. Our experiments demonstrate promising performance on various query types, ranging from yes/no questions and narrative explanations to complex legal differentiations, while highlighting areas for improvement, such as handling calculation-based inquiries and structured list formatting. This work provides a pathway for the deployment of AI-driven legal assistance tools tailored to the needs of resource-constrained environments.'}",oai:arXiv.org:2412.14771v1,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Rabee Qasem, Mohannad Hendi, Banan Tantour'}]","Rabee Qasem, Mohannad Hendi, Banan Tantour","{'name': 'Rabee Qasem, Mohannad Hendi, Banan Tantour'}",,
285,Collaborative Problem Solving in Mixed Reality: A Study on Visual Graph Analysis,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Collaborative Problem Solving in Mixed Reality: A Study on Visual Graph Analysis'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14776'}]",https://arxiv.org/abs/2412.14776,"arXiv:2412.14776v1 Announce Type: new 
Abstract: Problem solving is a composite cognitive process, invoking a number of systems and subsystems, such as perception and memory. Individuals may form collectives to solve a given problem together, in collaboration, especially when complexity is thought to be high. To determine if and when collaborative problem solving is desired, we must quantify collaboration first. For this, we investigate the practical virtue of collaborative problem solving. Using visual graph analysis, we perform a study with 72 participants in two countries and three languages. We compare ad hoc pairs to individuals and nominal pairs, solving two different tasks on graphs in visuospatial mixed reality. The average collaborating pair does not outdo its nominal counterpart, but it does have a significant trade-off against the individual: an ad hoc pair uses 1.46 more time to achieve 4.6 higher accuracy. We also use the concept of task instance complexity to quantify differences in complexity. As task instance complexity increases, these differences largely scale, though with two notable exceptions. With this study we show the importance of using nominal groups as benchmark in collaborative virtual environments research. We conclude that a mixed reality environment does not automatically imply superior collaboration.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14776v1 Announce Type: new \nAbstract: Problem solving is a composite cognitive process, invoking a number of systems and subsystems, such as perception and memory. Individuals may form collectives to solve a given problem together, in collaboration, especially when complexity is thought to be high. To determine if and when collaborative problem solving is desired, we must quantify collaboration first. For this, we investigate the practical virtue of collaborative problem solving. Using visual graph analysis, we perform a study with 72 participants in two countries and three languages. We compare ad hoc pairs to individuals and nominal pairs, solving two different tasks on graphs in visuospatial mixed reality. The average collaborating pair does not outdo its nominal counterpart, but it does have a significant trade-off against the individual: an ad hoc pair uses 1.46 more time to achieve 4.6 higher accuracy. We also use the concept of task instance complexity to quantify differences in complexity. As task instance complexity increases, these differences largely scale, though with two notable exceptions. With this study we show the importance of using nominal groups as benchmark in collaborative virtual environments research. We conclude that a mixed reality environment does not automatically imply superior collaboration.'}",oai:arXiv.org:2412.14776v1,False,"[{'term': 'cs.HC', 'scheme': None, 'label': None}, {'term': 'cs.ET', 'scheme': None, 'label': None}, {'term': 'cs.GR', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Dimitar Garkov, Tommaso Piselli, Emilio Di Giacomo, Karsten Klein, Giuseppe Liotta, Fabrizio Montecchiani, Falk Schreiber'}]","Dimitar Garkov, Tommaso Piselli, Emilio Di Giacomo, Karsten Klein, Giuseppe Liotta, Fabrizio Montecchiani, Falk Schreiber","{'name': 'Dimitar Garkov, Tommaso Piselli, Emilio Di Giacomo, Karsten Klein, Giuseppe Liotta, Fabrizio Montecchiani, Falk Schreiber'}",,
286,Agent-Temporal Credit Assignment for Optimal Policy Preservation in Sparse Multi-Agent Reinforcement Learning,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Agent-Temporal Credit Assignment for Optimal Policy Preservation in Sparse Multi-Agent Reinforcement Learning'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14779'}]",https://arxiv.org/abs/2412.14779,"arXiv:2412.14779v1 Announce Type: new 
Abstract: In multi-agent environments, agents often struggle to learn optimal policies due to sparse or delayed global rewards, particularly in long-horizon tasks where it is challenging to evaluate actions at intermediate time steps. We introduce Temporal-Agent Reward Redistribution (TAR$^2$), a novel approach designed to address the agent-temporal credit assignment problem by redistributing sparse rewards both temporally and across agents. TAR$^2$ decomposes sparse global rewards into time-step-specific rewards and calculates agent-specific contributions to these rewards. We theoretically prove that TAR$^2$ is equivalent to potential-based reward shaping, ensuring that the optimal policy remains unchanged. Empirical results demonstrate that TAR$^2$ stabilizes and accelerates the learning process. Additionally, we show that when TAR$^2$ is integrated with single-agent reinforcement learning algorithms, it performs as well as or better than traditional multi-agent reinforcement learning methods.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14779v1 Announce Type: new \nAbstract: In multi-agent environments, agents often struggle to learn optimal policies due to sparse or delayed global rewards, particularly in long-horizon tasks where it is challenging to evaluate actions at intermediate time steps. We introduce Temporal-Agent Reward Redistribution (TAR$^2$), a novel approach designed to address the agent-temporal credit assignment problem by redistributing sparse rewards both temporally and across agents. TAR$^2$ decomposes sparse global rewards into time-step-specific rewards and calculates agent-specific contributions to these rewards. We theoretically prove that TAR$^2$ is equivalent to potential-based reward shaping, ensuring that the optimal policy remains unchanged. Empirical results demonstrate that TAR$^2$ stabilizes and accelerates the learning process. Additionally, we show that when TAR$^2$ is integrated with single-agent reinforcement learning algorithms, it performs as well as or better than traditional multi-agent reinforcement learning methods.'}",oai:arXiv.org:2412.14779v1,False,"[{'term': 'cs.MA', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.GT', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'cs.RO', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Aditya Kapoor, Sushant Swamy, Kale-ab Tessera, Mayank Baranwal, Mingfei Sun, Harshad Khadilkar, Stefano V. Albrecht'}]","Aditya Kapoor, Sushant Swamy, Kale-ab Tessera, Mayank Baranwal, Mingfei Sun, Harshad Khadilkar, Stefano V. Albrecht","{'name': 'Aditya Kapoor, Sushant Swamy, Kale-ab Tessera, Mayank Baranwal, Mingfei Sun, Harshad Khadilkar, Stefano V. Albrecht'}",,
287,Disentangling Reasoning Tokens and Boilerplate Tokens For Language Model Fine-tuning,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Disentangling Reasoning Tokens and Boilerplate Tokens For Language Model Fine-tuning'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14780'}]",https://arxiv.org/abs/2412.14780,"arXiv:2412.14780v1 Announce Type: new 
Abstract: When using agent-task datasets to enhance agent capabilities for Large Language Models (LLMs), current methodologies often treat all tokens within a sample equally. However, we argue that tokens serving different roles - specifically, reasoning tokens versus boilerplate tokens (e.g., those governing output format) - differ significantly in importance and learning complexity, necessitating their disentanglement and distinct treatment. To address this, we propose a novel Shuffle-Aware Discriminator (SHAD) for adaptive token discrimination. SHAD classifies tokens by exploiting predictability differences observed after shuffling input-output combinations across samples: boilerplate tokens, due to their repetitive nature among samples, maintain predictability, whereas reasoning tokens do not. Using SHAD, we propose the Reasoning-highlighted Fine-Tuning (RFT) method, which adaptively emphasizes reasoning tokens during fine-tuning, yielding notable performance gains over common Supervised Fine-Tuning (SFT).","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14780v1 Announce Type: new \nAbstract: When using agent-task datasets to enhance agent capabilities for Large Language Models (LLMs), current methodologies often treat all tokens within a sample equally. However, we argue that tokens serving different roles - specifically, reasoning tokens versus boilerplate tokens (e.g., those governing output format) - differ significantly in importance and learning complexity, necessitating their disentanglement and distinct treatment. To address this, we propose a novel Shuffle-Aware Discriminator (SHAD) for adaptive token discrimination. SHAD classifies tokens by exploiting predictability differences observed after shuffling input-output combinations across samples: boilerplate tokens, due to their repetitive nature among samples, maintain predictability, whereas reasoning tokens do not. Using SHAD, we propose the Reasoning-highlighted Fine-Tuning (RFT) method, which adaptively emphasizes reasoning tokens during fine-tuning, yielding notable performance gains over common Supervised Fine-Tuning (SFT).'}",oai:arXiv.org:2412.14780v1,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Ziang Ye, Zhenru Zhang, Yang Zhang, Jianxin Ma, Junyang Lin, Fuli Feng'}]","Ziang Ye, Zhenru Zhang, Yang Zhang, Jianxin Ma, Junyang Lin, Fuli Feng","{'name': 'Ziang Ye, Zhenru Zhang, Yang Zhang, Jianxin Ma, Junyang Lin, Fuli Feng'}",,
288,YOLOv11 Optimization for Efficient Resource Utilization,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'YOLOv11 Optimization for Efficient Resource Utilization'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14790'}]",https://arxiv.org/abs/2412.14790,"arXiv:2412.14790v1 Announce Type: new 
Abstract: The objective of this research is to optimize the eleventh iteration of You Only Look Once (YOLOv11) by developing size-specific modified versions of the architecture. These modifications involve pruning unnecessary layers and reconfiguring the main architecture of YOLOv11. Each proposed version is tailored to detect objects of specific size ranges, from small to large. To ensure proper model selection based on dataset characteristics, we introduced an object classifier program. This program identifies the most suitable modified version for a given dataset. The proposed models were evaluated on various datasets and compared with the original YOLOv11 and YOLOv8 models. The experimental results highlight significant improvements in computational resource efficiency, with the proposed models maintaining the accuracy of the original YOLOv11. In some cases, the modified versions outperformed the original model regarding detection performance. Furthermore, the proposed models demonstrated reduced model sizes and faster inference times. Models weights and the object size classifier can be found in this repository","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14790v1 Announce Type: new \nAbstract: The objective of this research is to optimize the eleventh iteration of You Only Look Once (YOLOv11) by developing size-specific modified versions of the architecture. These modifications involve pruning unnecessary layers and reconfiguring the main architecture of YOLOv11. Each proposed version is tailored to detect objects of specific size ranges, from small to large. To ensure proper model selection based on dataset characteristics, we introduced an object classifier program. This program identifies the most suitable modified version for a given dataset. The proposed models were evaluated on various datasets and compared with the original YOLOv11 and YOLOv8 models. The experimental results highlight significant improvements in computational resource efficiency, with the proposed models maintaining the accuracy of the original YOLOv11. In some cases, the modified versions outperformed the original model regarding detection performance. Furthermore, the proposed models demonstrated reduced model sizes and faster inference times. Models weights and the object size classifier can be found in this repository'}",oai:arXiv.org:2412.14790v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Areeg Fagad Rasheed, M. Zarkoosh'}]","Areeg Fagad Rasheed, M. Zarkoosh","{'name': 'Areeg Fagad Rasheed, M. Zarkoosh'}",,
289,Average case tractability of multivariate approximation with Gevrey type kernels,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Average case tractability of multivariate approximation with Gevrey type kernels'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14791'}]",https://arxiv.org/abs/2412.14791,"arXiv:2412.14791v1 Announce Type: new 
Abstract: We consider multivariate approximation problems in the average case setting with a zero mean Gaussian measure whose covariance kernel is a periodic Gevrey kernel. We investigate various notions of algebraic tractability and exponential tractability, and obtain necessary and sufficient conditions in terms of the parameters of the problem.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14791v1 Announce Type: new \nAbstract: We consider multivariate approximation problems in the average case setting with a zero mean Gaussian measure whose covariance kernel is a periodic Gevrey kernel. We investigate various notions of algebraic tractability and exponential tractability, and obtain necessary and sufficient conditions in terms of the parameters of the problem.'}",oai:arXiv.org:2412.14791v1,False,"[{'term': 'math.NA', 'scheme': None, 'label': None}, {'term': 'cs.NA', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Wanting Lu, Heping Wang'}]","Wanting Lu, Heping Wang","{'name': 'Wanting Lu, Heping Wang'}",,
290,DCL-Sparse: Distributed Range-only Cooperative Localization of Multi-Robots in Noisy and Sparse Sensing Graphs,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'DCL-Sparse: Distributed Range-only Cooperative Localization of Multi-Robots in Noisy and Sparse Sensing Graphs'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14793'}]",https://arxiv.org/abs/2412.14793,"arXiv:2412.14793v1 Announce Type: new 
Abstract: This paper presents a novel approach to range-based cooperative localization for robot swarms in GPS-denied environments, addressing the limitations of current methods in noisy and sparse settings. We propose a robust multi-layered localization framework that combines shadow edge localization techniques with the strategic deployment of UAVs. This approach not only addresses the challenges associated with nonrigid and poorly connected graphs but also enhances the convergence rate of the localization process. We introduce two key concepts: the S1-Edge approach in our distributed protocol to address the rigidity problem of sparse graphs and the concept of a powerful UAV node to increase the sensing and localization capability of the multi-robot system. Our approach leverages the advantages of the distributed localization methods, enhancing scalability and adaptability in large robot networks. We establish theoretical conditions for the new S1-Edge that ensure solutions exist even in the presence of noise, thereby validating the effectiveness of shadow edge localization. Extensive simulation experiments confirm the superior performance of our method compared to state-of-the-art techniques, resulting in up to 95\% reduction in localization error, demonstrating substantial improvements in localization accuracy and robustness to sparse graphs. This work provides a decisive advancement in the field of multi-robot localization, offering a powerful tool for high-performance and reliable operations in challenging environments.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14793v1 Announce Type: new \nAbstract: This paper presents a novel approach to range-based cooperative localization for robot swarms in GPS-denied environments, addressing the limitations of current methods in noisy and sparse settings. We propose a robust multi-layered localization framework that combines shadow edge localization techniques with the strategic deployment of UAVs. This approach not only addresses the challenges associated with nonrigid and poorly connected graphs but also enhances the convergence rate of the localization process. We introduce two key concepts: the S1-Edge approach in our distributed protocol to address the rigidity problem of sparse graphs and the concept of a powerful UAV node to increase the sensing and localization capability of the multi-robot system. Our approach leverages the advantages of the distributed localization methods, enhancing scalability and adaptability in large robot networks. We establish theoretical conditions for the new S1-Edge that ensure solutions exist even in the presence of noise, thereby validating the effectiveness of shadow edge localization. Extensive simulation experiments confirm the superior performance of our method compared to state-of-the-art techniques, resulting in up to 95\\% reduction in localization error, demonstrating substantial improvements in localization accuracy and robustness to sparse graphs. This work provides a decisive advancement in the field of multi-robot localization, offering a powerful tool for high-performance and reliable operations in challenging environments.'}",oai:arXiv.org:2412.14793v1,False,"[{'term': 'cs.RO', 'scheme': None, 'label': None}, {'term': 'cs.MA', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Atharva Sagale, Tohid Kargar Tasooji, Ramviyas Parasuraman'}]","Atharva Sagale, Tohid Kargar Tasooji, Ramviyas Parasuraman","{'name': 'Atharva Sagale, Tohid Kargar Tasooji, Ramviyas Parasuraman'}",,
291,Extending TWIG: Zero-Shot Predictive Hyperparameter Selection for KGEs based on Graph Structure,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Extending TWIG: Zero-Shot Predictive Hyperparameter Selection for KGEs based on Graph Structure'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14801'}]",https://arxiv.org/abs/2412.14801,"arXiv:2412.14801v1 Announce Type: new 
Abstract: Knowledge Graphs (KGs) have seen increasing use across various domains -- from biomedicine and linguistics to general knowledge modelling. In order to facilitate the analysis of knowledge graphs, Knowledge Graph Embeddings (KGEs) have been developed to automatically analyse KGs and predict new facts based on the information in a KG, a task called ""link prediction"". Many existing studies have documented that the structure of a KG, KGE model components, and KGE hyperparameters can significantly change how well KGEs perform and what relationships they are able to learn. Recently, the Topologically-Weighted Intelligence Generation (TWIG) model has been proposed as a solution to modelling how each of these elements relate. In this work, we extend the previous research on TWIG and evaluate its ability to simulate the output of the KGE model ComplEx in the cross-KG setting. Our results are twofold. First, TWIG is able to summarise KGE performance on a wide range of hyperparameter settings and KGs being learned, suggesting that it represents a general knowledge of how to predict KGE performance from KG structure. Second, we show that TWIG can successfully predict hyperparameter performance on unseen KGs in the zero-shot setting. This second observation leads us to propose that, with additional research, optimal hyperparameter selection for KGE models could be determined in a pre-hoc manner using TWIG-like methods, rather than by using a full hyperparameter search.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14801v1 Announce Type: new \nAbstract: Knowledge Graphs (KGs) have seen increasing use across various domains -- from biomedicine and linguistics to general knowledge modelling. In order to facilitate the analysis of knowledge graphs, Knowledge Graph Embeddings (KGEs) have been developed to automatically analyse KGs and predict new facts based on the information in a KG, a task called ""link prediction"". Many existing studies have documented that the structure of a KG, KGE model components, and KGE hyperparameters can significantly change how well KGEs perform and what relationships they are able to learn. Recently, the Topologically-Weighted Intelligence Generation (TWIG) model has been proposed as a solution to modelling how each of these elements relate. In this work, we extend the previous research on TWIG and evaluate its ability to simulate the output of the KGE model ComplEx in the cross-KG setting. Our results are twofold. First, TWIG is able to summarise KGE performance on a wide range of hyperparameter settings and KGs being learned, suggesting that it represents a general knowledge of how to predict KGE performance from KG structure. Second, we show that TWIG can successfully predict hyperparameter performance on unseen KGs in the zero-shot setting. This second observation leads us to propose that, with additional research, optimal hyperparameter selection for KGE models could be determined in a pre-hoc manner using TWIG-like methods, rather than by using a full hyperparameter search.'}",oai:arXiv.org:2412.14801v1,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': ""Jeffrey Sardina, John D. Kelleher, Declan O'Sullivan""}]","Jeffrey Sardina, John D. Kelleher, Declan O'Sullivan","{'name': ""Jeffrey Sardina, John D. Kelleher, Declan O'Sullivan""}",,
292,"Stack Trace Deduplication: Faster, More Accurately, and in More Realistic Scenarios","{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Stack Trace Deduplication: Faster, More Accurately, and in More Realistic Scenarios'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14802'}]",https://arxiv.org/abs/2412.14802,"arXiv:2412.14802v1 Announce Type: new 
Abstract: In large-scale software systems, there are often no fully-fledged bug reports with human-written descriptions when an error occurs. In this case, developers rely on stack traces, i.e., series of function calls that led to the error. Since there can be tens and hundreds of thousands of them describing the same issue from different users, automatic deduplication into categories is necessary to allow for processing. Recent works have proposed powerful deep learning-based approaches for this, but they are evaluated and compared in isolation from real-life workflows, and it is not clear whether they will actually work well at scale.
  To overcome this gap, this work presents three main contributions: a novel model, an industry-based dataset, and a multi-faceted evaluation. Our model consists of two parts - (1) an embedding model with byte-pair encoding and approximate nearest neighbor search to quickly find the most relevant stack traces to the incoming one, and (2) a reranker that re-ranks the most fitting stack traces, taking into account the repeated frames between them. To complement the existing datasets collected from open-source projects, we share with the community SlowOps - a dataset of stack traces from IntelliJ-based products developed by JetBrains, which has an order of magnitude more stack traces per category. Finally, we carry out an evaluation that strives to be realistic: measuring not only the accuracy of categorization, but also the operation time and the ability to create new categories. The evaluation shows that our model strikes a good balance - it outperforms other models on both open-source datasets and SlowOps, while also being faster on time than most. We release all of our code and data, and hope that our work can pave the way to further practice-oriented research in the area.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14802v1 Announce Type: new \nAbstract: In large-scale software systems, there are often no fully-fledged bug reports with human-written descriptions when an error occurs. In this case, developers rely on stack traces, i.e., series of function calls that led to the error. Since there can be tens and hundreds of thousands of them describing the same issue from different users, automatic deduplication into categories is necessary to allow for processing. Recent works have proposed powerful deep learning-based approaches for this, but they are evaluated and compared in isolation from real-life workflows, and it is not clear whether they will actually work well at scale.\n  To overcome this gap, this work presents three main contributions: a novel model, an industry-based dataset, and a multi-faceted evaluation. Our model consists of two parts - (1) an embedding model with byte-pair encoding and approximate nearest neighbor search to quickly find the most relevant stack traces to the incoming one, and (2) a reranker that re-ranks the most fitting stack traces, taking into account the repeated frames between them. To complement the existing datasets collected from open-source projects, we share with the community SlowOps - a dataset of stack traces from IntelliJ-based products developed by JetBrains, which has an order of magnitude more stack traces per category. Finally, we carry out an evaluation that strives to be realistic: measuring not only the accuracy of categorization, but also the operation time and the ability to create new categories. The evaluation shows that our model strikes a good balance - it outperforms other models on both open-source datasets and SlowOps, while also being faster on time than most. We release all of our code and data, and hope that our work can pave the way to further practice-oriented research in the area.'}",oai:arXiv.org:2412.14802v1,False,"[{'term': 'cs.SE', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Egor Shibaev, Denis Sushentsev, Yaroslav Golubev, Aleksandr Khvorov'}]","Egor Shibaev, Denis Sushentsev, Yaroslav Golubev, Aleksandr Khvorov","{'name': 'Egor Shibaev, Denis Sushentsev, Yaroslav Golubev, Aleksandr Khvorov'}",,
293,Video Prediction Policy: A Generalist Robot Policy with Predictive Visual Representations,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Video Prediction Policy: A Generalist Robot Policy with Predictive Visual Representations'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14803'}]",https://arxiv.org/abs/2412.14803,"arXiv:2412.14803v1 Announce Type: new 
Abstract: Recent advancements in robotics have focused on developing generalist policies capable of performing multiple tasks. Typically, these policies utilize pre-trained vision encoders to capture crucial information from current observations. However, previous vision encoders, which trained on two-image contrastive learning or single-image reconstruction, can not perfectly capture the sequential information essential for embodied tasks. Recently, video diffusion models (VDMs) have demonstrated the capability to accurately predict future image sequences, exhibiting a good understanding of physical dynamics. Motivated by the strong visual prediction capabilities of VDMs, we hypothesize that they inherently possess visual representations that reflect the evolution of the physical world, which we term predictive visual representations. Building on this hypothesis, we propose the Video Prediction Policy (VPP), a generalist robotic policy conditioned on the predictive visual representations from VDMs. To further enhance these representations, we incorporate diverse human or robotic manipulation datasets, employing unified video-generation training objectives. VPP consistently outperforms existing methods across two simulated and two real-world benchmarks. Notably, it achieves a 28.1\% relative improvement in the Calvin ABC-D benchmark compared to the previous state-of-the-art and delivers a 28.8\% increase in success rates for complex real-world dexterous manipulation tasks.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14803v1 Announce Type: new \nAbstract: Recent advancements in robotics have focused on developing generalist policies capable of performing multiple tasks. Typically, these policies utilize pre-trained vision encoders to capture crucial information from current observations. However, previous vision encoders, which trained on two-image contrastive learning or single-image reconstruction, can not perfectly capture the sequential information essential for embodied tasks. Recently, video diffusion models (VDMs) have demonstrated the capability to accurately predict future image sequences, exhibiting a good understanding of physical dynamics. Motivated by the strong visual prediction capabilities of VDMs, we hypothesize that they inherently possess visual representations that reflect the evolution of the physical world, which we term predictive visual representations. Building on this hypothesis, we propose the Video Prediction Policy (VPP), a generalist robotic policy conditioned on the predictive visual representations from VDMs. To further enhance these representations, we incorporate diverse human or robotic manipulation datasets, employing unified video-generation training objectives. VPP consistently outperforms existing methods across two simulated and two real-world benchmarks. Notably, it achieves a 28.1\\% relative improvement in the Calvin ABC-D benchmark compared to the previous state-of-the-art and delivers a 28.8\\% increase in success rates for complex real-world dexterous manipulation tasks.'}",oai:arXiv.org:2412.14803v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.RO', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Yucheng Hu, Yanjiang Guo, Pengchao Wang, Xiaoyu Chen, Yen-Jen Wang, Jianke Zhang, Koushil Sreenath, Chaochao Lu, Jianyu Chen'}]","Yucheng Hu, Yanjiang Guo, Pengchao Wang, Xiaoyu Chen, Yen-Jen Wang, Jianke Zhang, Koushil Sreenath, Chaochao Lu, Jianyu Chen","{'name': 'Yucheng Hu, Yanjiang Guo, Pengchao Wang, Xiaoyu Chen, Yen-Jen Wang, Jianke Zhang, Koushil Sreenath, Chaochao Lu, Jianyu Chen'}",,
294,ResoFilter: Rine-grained Synthetic Data Filtering for Large Language Models through Data-Parameter Resonance Analysis,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'ResoFilter: Rine-grained Synthetic Data Filtering for Large Language Models through Data-Parameter Resonance Analysis'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14809'}]",https://arxiv.org/abs/2412.14809,"arXiv:2412.14809v1 Announce Type: new 
Abstract: Large language models (LLMs) have shown remarkable effectiveness across various domains, with data augmentation methods utilizing GPT for synthetic data generation becoming prevalent. However, the quality and utility of augmented data remain questionable, and current methods lack clear metrics for evaluating data characteristics. To address these challenges, we propose ResoFilter, a novel method that integrates models, data, and tasks to refine datasets. ResoFilter leverages the fine-tuning process to obtain Data-Parameter features for data selection, offering improved interpretability by representing data characteristics through model weights. Our experiments demonstrate that ResoFilter achieves comparable results to full-scale fine-tuning using only half the data in mathematical tasks and exhibits strong generalization across different models and domains. This method provides valuable insights for constructing synthetic datasets and evaluating high-quality data, offering a promising solution for enhancing data augmentation techniques and improving training dataset quality for LLMs. For reproducibility, we will release our code and data upon acceptance.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14809v1 Announce Type: new \nAbstract: Large language models (LLMs) have shown remarkable effectiveness across various domains, with data augmentation methods utilizing GPT for synthetic data generation becoming prevalent. However, the quality and utility of augmented data remain questionable, and current methods lack clear metrics for evaluating data characteristics. To address these challenges, we propose ResoFilter, a novel method that integrates models, data, and tasks to refine datasets. ResoFilter leverages the fine-tuning process to obtain Data-Parameter features for data selection, offering improved interpretability by representing data characteristics through model weights. Our experiments demonstrate that ResoFilter achieves comparable results to full-scale fine-tuning using only half the data in mathematical tasks and exhibits strong generalization across different models and domains. This method provides valuable insights for constructing synthetic datasets and evaluating high-quality data, offering a promising solution for enhancing data augmentation techniques and improving training dataset quality for LLMs. For reproducibility, we will release our code and data upon acceptance.'}",oai:arXiv.org:2412.14809v1,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Zeao Tu, Xiangdi Meng, Yu He, Zihan Yao, Tianyu Qi, Jun Liu, Ming Li'}]","Zeao Tu, Xiangdi Meng, Yu He, Zihan Yao, Tianyu Qi, Jun Liu, Ming Li","{'name': 'Zeao Tu, Xiangdi Meng, Yu He, Zihan Yao, Tianyu Qi, Jun Liu, Ming Li'}",,
295,MARIA: a Multimodal Transformer Model for Incomplete Healthcare Data,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'MARIA: a Multimodal Transformer Model for Incomplete Healthcare Data'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14810'}]",https://arxiv.org/abs/2412.14810,"arXiv:2412.14810v1 Announce Type: new 
Abstract: In healthcare, the integration of multimodal data is pivotal for developing comprehensive diagnostic and predictive models. However, managing missing data remains a significant challenge in real-world applications. We introduce MARIA (Multimodal Attention Resilient to Incomplete datA), a novel transformer-based deep learning model designed to address these challenges through an intermediate fusion strategy. Unlike conventional approaches that depend on imputation, MARIA utilizes a masked self-attention mechanism, which processes only the available data without generating synthetic values. This approach enables it to effectively handle incomplete datasets, enhancing robustness and minimizing biases introduced by imputation methods. We evaluated MARIA against 10 state-of-the-art machine learning and deep learning models across 8 diagnostic and prognostic tasks. The results demonstrate that MARIA outperforms existing methods in terms of performance and resilience to varying levels of data incompleteness, underscoring its potential for critical healthcare applications.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14810v1 Announce Type: new \nAbstract: In healthcare, the integration of multimodal data is pivotal for developing comprehensive diagnostic and predictive models. However, managing missing data remains a significant challenge in real-world applications. We introduce MARIA (Multimodal Attention Resilient to Incomplete datA), a novel transformer-based deep learning model designed to address these challenges through an intermediate fusion strategy. Unlike conventional approaches that depend on imputation, MARIA utilizes a masked self-attention mechanism, which processes only the available data without generating synthetic values. This approach enables it to effectively handle incomplete datasets, enhancing robustness and minimizing biases introduced by imputation methods. We evaluated MARIA against 10 state-of-the-art machine learning and deep learning models across 8 diagnostic and prognostic tasks. The results demonstrate that MARIA outperforms existing methods in terms of performance and resilience to varying levels of data incompleteness, underscoring its potential for critical healthcare applications.'}",oai:arXiv.org:2412.14810v1,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by-nc-nd/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-nd/4.0/'}","[{'name': 'Camillo Maria Caruso, Paolo Soda, Valerio Guarrasi'}]","Camillo Maria Caruso, Paolo Soda, Valerio Guarrasi","{'name': 'Camillo Maria Caruso, Paolo Soda, Valerio Guarrasi'}",,
296,Answer Set Networks: Casting Answer Set Programming into Deep Learning,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Answer Set Networks: Casting Answer Set Programming into Deep Learning'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14814'}]",https://arxiv.org/abs/2412.14814,"arXiv:2412.14814v1 Announce Type: new 
Abstract: Although Answer Set Programming (ASP) allows constraining neural-symbolic (NeSy) systems, its employment is hindered by the prohibitive costs of computing stable models and the CPU-bound nature of state-of-the-art solvers. To this end, we propose Answer Set Networks (ASN), a NeSy solver. Based on Graph Neural Networks (GNN), ASNs are a scalable approach to ASP-based Deep Probabilistic Logic Programming (DPPL). Specifically, we show how to translate ASPs into ASNs and demonstrate how ASNs can efficiently solve the encoded problem by leveraging GPU's batching and parallelization capabilities. Our experimental evaluations demonstrate that ASNs outperform state-of-the-art CPU-bound NeSy systems on multiple tasks. Simultaneously, we make the following two contributions based on the strengths of ASNs. Namely, we are the first to show the finetuning of Large Language Models (LLM) with DPPLs, employing ASNs to guide the training with logic. Further, we show the ""constitutional navigation"" of drones, i.e., encoding public aviation laws in an ASN for routing Unmanned Aerial Vehicles in uncertain environments.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14814v1 Announce Type: new \nAbstract: Although Answer Set Programming (ASP) allows constraining neural-symbolic (NeSy) systems, its employment is hindered by the prohibitive costs of computing stable models and the CPU-bound nature of state-of-the-art solvers. To this end, we propose Answer Set Networks (ASN), a NeSy solver. Based on Graph Neural Networks (GNN), ASNs are a scalable approach to ASP-based Deep Probabilistic Logic Programming (DPPL). Specifically, we show how to translate ASPs into ASNs and demonstrate how ASNs can efficiently solve the encoded problem by leveraging GPU\'s batching and parallelization capabilities. Our experimental evaluations demonstrate that ASNs outperform state-of-the-art CPU-bound NeSy systems on multiple tasks. Simultaneously, we make the following two contributions based on the strengths of ASNs. Namely, we are the first to show the finetuning of Large Language Models (LLM) with DPPLs, employing ASNs to guide the training with logic. Further, we show the ""constitutional navigation"" of drones, i.e., encoding public aviation laws in an ASN for routing Unmanned Aerial Vehicles in uncertain environments.'}",oai:arXiv.org:2412.14814v1,False,"[{'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'cs.SC', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Arseny Skryagin, Daniel Ochs, Phillip Deibert, Simon Kohaut, Devendra Singh Dhami, Kristian Kersting'}]","Arseny Skryagin, Daniel Ochs, Phillip Deibert, Simon Kohaut, Devendra Singh Dhami, Kristian Kersting","{'name': 'Arseny Skryagin, Daniel Ochs, Phillip Deibert, Simon Kohaut, Devendra Singh Dhami, Kristian Kersting'}",,
297,Non-intrusive and Unconstrained Keystroke Inference in VR Platforms via Infrared Side Channel,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Non-intrusive and Unconstrained Keystroke Inference in VR Platforms via Infrared Side Channel'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14815'}]",https://arxiv.org/abs/2412.14815,"arXiv:2412.14815v1 Announce Type: new 
Abstract: Virtual Reality (VR) technologies are increasingly employed in numerous applications across various areas. Therefore, it is essential to ensure the security of interactions between users and VR devices. In this paper, we disclose a new side-channel leakage in the constellation tracking system of mainstream VR platforms, where the infrared (IR) signals emitted from the VR controllers for controller-headset interactions can be maliciously exploited to reconstruct unconstrained input keystrokes on the virtual keyboard non-intrusively. We propose a novel keystroke inference attack named VRecKey to demonstrate the feasibility and practicality of this novel infrared side channel. Specifically, VRecKey leverages a customized 2D IR sensor array to intercept ambient IR signals emitted from VR controllers and subsequently infers (i) character-level key presses on the virtual keyboard and (ii) word-level keystrokes along with their typing trajectories. We extensively evaluate the effectiveness of VRecKey with two commercial VR devices, and the results indicate that it can achieve over 94.2% and 90.5% top-3 accuracy in inferring character-level and word-level keystrokes with varying lengths, respectively. In addition, empirical results show that VRecKey is resilient to several practical impact factors and presents effectiveness in various real-world scenarios, which provides a complementary and orthogonal attack surface for the exploration of keystroke inference attacks in VR platforms.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14815v1 Announce Type: new \nAbstract: Virtual Reality (VR) technologies are increasingly employed in numerous applications across various areas. Therefore, it is essential to ensure the security of interactions between users and VR devices. In this paper, we disclose a new side-channel leakage in the constellation tracking system of mainstream VR platforms, where the infrared (IR) signals emitted from the VR controllers for controller-headset interactions can be maliciously exploited to reconstruct unconstrained input keystrokes on the virtual keyboard non-intrusively. We propose a novel keystroke inference attack named VRecKey to demonstrate the feasibility and practicality of this novel infrared side channel. Specifically, VRecKey leverages a customized 2D IR sensor array to intercept ambient IR signals emitted from VR controllers and subsequently infers (i) character-level key presses on the virtual keyboard and (ii) word-level keystrokes along with their typing trajectories. We extensively evaluate the effectiveness of VRecKey with two commercial VR devices, and the results indicate that it can achieve over 94.2% and 90.5% top-3 accuracy in inferring character-level and word-level keystrokes with varying lengths, respectively. In addition, empirical results show that VRecKey is resilient to several practical impact factors and presents effectiveness in various real-world scenarios, which provides a complementary and orthogonal attack surface for the exploration of keystroke inference attacks in VR platforms.'}",oai:arXiv.org:2412.14815v1,False,"[{'term': 'cs.CR', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Tao Ni, Yuefeng Du, Qingchuan Zhao, Cong Wang'}]","Tao Ni, Yuefeng Du, Qingchuan Zhao, Cong Wang","{'name': 'Tao Ni, Yuefeng Du, Qingchuan Zhao, Cong Wang'}",,
298,Explainable Tampered Text Detection via Multimodal Large Models,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Explainable Tampered Text Detection via Multimodal Large Models'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14816'}]",https://arxiv.org/abs/2412.14816,"arXiv:2412.14816v1 Announce Type: new 
Abstract: Recently, tampered text detection has attracted increasing attention due to its essential role in information security. Although existing methods can detect the tampered text region, the interpretation of such detection remains unclear, making the prediction unreliable. To address this black-box problem, we propose to explain the basis of tampered text detection with natural language via large multimodal models. To fill the data gap for this task, we propose a large-scale, comprehensive dataset, ETTD, which contains both pixel-level annotations indicating the tampered text region and natural language annotations describing the anomaly of the tampered text. Multiple methods are employed to improve the quality of the proposed data. For example, a fused mask prompt is proposed to reduce confusion when querying GPT4o to generate anomaly descriptions. By weighting the input image with the mask annotation, the tampered region can be clearly indicated and the content in and around the tampered region can also be preserved. We also propose prompting GPT4o to recognize tampered texts and filtering out the responses with low OCR accuracy, which can effectively improve annotation quality in an automatic manner. To further improve explainable tampered text detection, we propose a simple yet effective model called TTD, which benefits from improved fine-grained perception by paying attention to the suspected region with auxiliary reference grounding query. Extensive experiments on both the ETTD dataset and the public dataset have verified the effectiveness of the proposed methods. In-depth analysis is also provided to inspire further research. The dataset and code will be made publicly available.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14816v1 Announce Type: new \nAbstract: Recently, tampered text detection has attracted increasing attention due to its essential role in information security. Although existing methods can detect the tampered text region, the interpretation of such detection remains unclear, making the prediction unreliable. To address this black-box problem, we propose to explain the basis of tampered text detection with natural language via large multimodal models. To fill the data gap for this task, we propose a large-scale, comprehensive dataset, ETTD, which contains both pixel-level annotations indicating the tampered text region and natural language annotations describing the anomaly of the tampered text. Multiple methods are employed to improve the quality of the proposed data. For example, a fused mask prompt is proposed to reduce confusion when querying GPT4o to generate anomaly descriptions. By weighting the input image with the mask annotation, the tampered region can be clearly indicated and the content in and around the tampered region can also be preserved. We also propose prompting GPT4o to recognize tampered texts and filtering out the responses with low OCR accuracy, which can effectively improve annotation quality in an automatic manner. To further improve explainable tampered text detection, we propose a simple yet effective model called TTD, which benefits from improved fine-grained perception by paying attention to the suspected region with auxiliary reference grounding query. Extensive experiments on both the ETTD dataset and the public dataset have verified the effectiveness of the proposed methods. In-depth analysis is also provided to inspire further research. The dataset and code will be made publicly available.'}",oai:arXiv.org:2412.14816v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Chenfan Qu, Jian Liu, Haoxing Chen, Baihan Yu, Jingjing Liu, Weiqiang Wang, Lianwen Jin'}]","Chenfan Qu, Jian Liu, Haoxing Chen, Baihan Yu, Jingjing Liu, Weiqiang Wang, Lianwen Jin","{'name': 'Chenfan Qu, Jian Liu, Haoxing Chen, Baihan Yu, Jingjing Liu, Weiqiang Wang, Lianwen Jin'}",,
299,Fair Division with Social Impact,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Fair Division with Social Impact'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14818'}]",https://arxiv.org/abs/2412.14818,"arXiv:2412.14818v1 Announce Type: new 
Abstract: In this paper, we consider the problem of fair division of indivisible goods when the allocation of goods impacts society. Specifically, we introduce a second valuation function for each agent, determining the social impact of allocating a good to the agent. Such impact is considered desirable for the society -- the higher, the better. Our goal is to understand how to allocate goods fairly from the agents' perspective while maintaining society as happy as possible. To this end, we measure the impact on society using the utilitarian social welfare and provide both possibility and impossibility results. Our findings reveal that achieving good approximations, better than linear in the number of agents, is not possible while ensuring fairness to the agents. These impossibility results can be attributed to the fact that agents are completely unconscious of their social impact. Consequently, we explore scenarios where agents are socially aware, by introducing related fairness notions, and demonstrate that an appropriate definition of fairness aligns with the goal of maximizing the social objective.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14818v1 Announce Type: new \nAbstract: In this paper, we consider the problem of fair division of indivisible goods when the allocation of goods impacts society. Specifically, we introduce a second valuation function for each agent, determining the social impact of allocating a good to the agent. Such impact is considered desirable for the society -- the higher, the better. Our goal is to understand how to allocate goods fairly from the agents' perspective while maintaining society as happy as possible. To this end, we measure the impact on society using the utilitarian social welfare and provide both possibility and impossibility results. Our findings reveal that achieving good approximations, better than linear in the number of agents, is not possible while ensuring fairness to the agents. These impossibility results can be attributed to the fact that agents are completely unconscious of their social impact. Consequently, we explore scenarios where agents are socially aware, by introducing related fairness notions, and demonstrate that an appropriate definition of fairness aligns with the goal of maximizing the social objective.""}",oai:arXiv.org:2412.14818v1,False,"[{'term': 'cs.GT', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by-sa/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-sa/4.0/'}","[{'name': 'Michele Flammini, Gianluigi Greco, Giovanna Varricchio'}]","Michele Flammini, Gianluigi Greco, Giovanna Varricchio","{'name': 'Michele Flammini, Gianluigi Greco, Giovanna Varricchio'}",,
300,Multi-Level Embedding and Alignment Network with Consistency and Invariance Learning for Cross-View Geo-Localization,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Multi-Level Embedding and Alignment Network with Consistency and Invariance Learning for Cross-View Geo-Localization'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14819'}]",https://arxiv.org/abs/2412.14819,"arXiv:2412.14819v1 Announce Type: new 
Abstract: Cross-View Geo-Localization (CVGL) involves determining the localization of drone images by retrieving the most similar GPS-tagged satellite images. However, the imaging gaps between platforms are often significant and the variations in viewpoints are substantial, which limits the ability of existing methods to effectively associate cross-view features and extract consistent and invariant characteristics. Moreover, existing methods often overlook the problem of increased computational and storage requirements when improving model performance. To handle these limitations, we propose a lightweight enhanced alignment network, called the Multi-Level Embedding and Alignment Network (MEAN). The MEAN network uses a progressive multi-level enhancement strategy, global-to-local associations, and cross-domain alignment, enabling feature communication across levels. This allows MEAN to effectively connect features at different levels and learn robust cross-view consistent mappings and modality-invariant features. Moreover, MEAN adopts a shallow backbone network combined with a lightweight branch design, effectively reducing parameter count and computational complexity. Experimental results on the University-1652 and SUES-200 datasets demonstrate that MEAN reduces parameter count by 62.17% and computational complexity by 70.99% compared to state-of-the-art models, while maintaining competitive or even superior performance. The codes will be released soon.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14819v1 Announce Type: new \nAbstract: Cross-View Geo-Localization (CVGL) involves determining the localization of drone images by retrieving the most similar GPS-tagged satellite images. However, the imaging gaps between platforms are often significant and the variations in viewpoints are substantial, which limits the ability of existing methods to effectively associate cross-view features and extract consistent and invariant characteristics. Moreover, existing methods often overlook the problem of increased computational and storage requirements when improving model performance. To handle these limitations, we propose a lightweight enhanced alignment network, called the Multi-Level Embedding and Alignment Network (MEAN). The MEAN network uses a progressive multi-level enhancement strategy, global-to-local associations, and cross-domain alignment, enabling feature communication across levels. This allows MEAN to effectively connect features at different levels and learn robust cross-view consistent mappings and modality-invariant features. Moreover, MEAN adopts a shallow backbone network combined with a lightweight branch design, effectively reducing parameter count and computational complexity. Experimental results on the University-1652 and SUES-200 datasets demonstrate that MEAN reduces parameter count by 62.17% and computational complexity by 70.99% compared to state-of-the-art models, while maintaining competitive or even superior performance. The codes will be released soon.'}",oai:arXiv.org:2412.14819v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Zhongwei Chen, Zhao-Xu Yang, Hai-Jun Rong'}]","Zhongwei Chen, Zhao-Xu Yang, Hai-Jun Rong","{'name': 'Zhongwei Chen, Zhao-Xu Yang, Hai-Jun Rong'}",,
301,PC-BEV: An Efficient Polar-Cartesian BEV Fusion Framework for LiDAR Semantic Segmentation,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'PC-BEV: An Efficient Polar-Cartesian BEV Fusion Framework for LiDAR Semantic Segmentation'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14821'}]",https://arxiv.org/abs/2412.14821,"arXiv:2412.14821v1 Announce Type: new 
Abstract: Although multiview fusion has demonstrated potential in LiDAR segmentation, its dependence on computationally intensive point-based interactions, arising from the lack of fixed correspondences between views such as range view and Bird's-Eye View (BEV), hinders its practical deployment. This paper challenges the prevailing notion that multiview fusion is essential for achieving high performance. We demonstrate that significant gains can be realized by directly fusing Polar and Cartesian partitioning strategies within the BEV space. Our proposed BEV-only segmentation model leverages the inherent fixed grid correspondences between these partitioning schemes, enabling a fusion process that is orders of magnitude faster (170$\times$ speedup) than conventional point-based methods. Furthermore, our approach facilitates dense feature fusion, preserving richer contextual information compared to sparse point-based alternatives. To enhance scene understanding while maintaining inference efficiency, we also introduce a hybrid Transformer-CNN architecture. Extensive evaluation on the SemanticKITTI and nuScenes datasets provides compelling evidence that our method outperforms previous multiview fusion approaches in terms of both performance and inference speed, highlighting the potential of BEV-based fusion for LiDAR segmentation. Code is available at \url{https://github.com/skyshoumeng/PC-BEV.}","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14821v1 Announce Type: new \nAbstract: Although multiview fusion has demonstrated potential in LiDAR segmentation, its dependence on computationally intensive point-based interactions, arising from the lack of fixed correspondences between views such as range view and Bird's-Eye View (BEV), hinders its practical deployment. This paper challenges the prevailing notion that multiview fusion is essential for achieving high performance. We demonstrate that significant gains can be realized by directly fusing Polar and Cartesian partitioning strategies within the BEV space. Our proposed BEV-only segmentation model leverages the inherent fixed grid correspondences between these partitioning schemes, enabling a fusion process that is orders of magnitude faster (170$\\times$ speedup) than conventional point-based methods. Furthermore, our approach facilitates dense feature fusion, preserving richer contextual information compared to sparse point-based alternatives. To enhance scene understanding while maintaining inference efficiency, we also introduce a hybrid Transformer-CNN architecture. Extensive evaluation on the SemanticKITTI and nuScenes datasets provides compelling evidence that our method outperforms previous multiview fusion approaches in terms of both performance and inference speed, highlighting the potential of BEV-based fusion for LiDAR segmentation. Code is available at \\url{https://github.com/skyshoumeng/PC-BEV.}""}",oai:arXiv.org:2412.14821v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Shoumeng Qiu, Xinrun Li, XiangYang Xue, Jian Pu'}]","Shoumeng Qiu, Xinrun Li, XiangYang Xue, Jian Pu","{'name': 'Shoumeng Qiu, Xinrun Li, XiangYang Xue, Jian Pu'}",,
302,Mention Attention for Pronoun Translation,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Mention Attention for Pronoun Translation'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14829'}]",https://arxiv.org/abs/2412.14829,"arXiv:2412.14829v1 Announce Type: new 
Abstract: Most pronouns are referring expressions, computers need to resolve what do the pronouns refer to, and there are divergences on pronoun usage across languages. Thus, dealing with these divergences and translating pronouns is a challenge in machine translation. Mentions are referring candidates of pronouns and have closer relations with pronouns compared to general tokens. We assume that extracting additional mention features can help pronoun translation. Therefore, we introduce an additional mention attention module in the decoder to pay extra attention to source mentions but not non-mention tokens. Our mention attention module not only extracts features from source mentions, but also considers target-side context which benefits pronoun translation. In addition, we also introduce two mention classifiers to train models to recognize mentions, whose outputs guide the mention attention. We conduct experiments on the WMT17 English-German translation task, and evaluate our models on general translation and pronoun translation, using BLEU, APT, and contrastive evaluation metrics. Our proposed model outperforms the baseline Transformer model in terms of APT and BLEU scores, this confirms our hypothesis that we can improve pronoun translation by paying additional attention to source mentions, and shows that our introduced additional modules do not have negative effect on the general translation quality.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14829v1 Announce Type: new \nAbstract: Most pronouns are referring expressions, computers need to resolve what do the pronouns refer to, and there are divergences on pronoun usage across languages. Thus, dealing with these divergences and translating pronouns is a challenge in machine translation. Mentions are referring candidates of pronouns and have closer relations with pronouns compared to general tokens. We assume that extracting additional mention features can help pronoun translation. Therefore, we introduce an additional mention attention module in the decoder to pay extra attention to source mentions but not non-mention tokens. Our mention attention module not only extracts features from source mentions, but also considers target-side context which benefits pronoun translation. In addition, we also introduce two mention classifiers to train models to recognize mentions, whose outputs guide the mention attention. We conduct experiments on the WMT17 English-German translation task, and evaluate our models on general translation and pronoun translation, using BLEU, APT, and contrastive evaluation metrics. Our proposed model outperforms the baseline Transformer model in terms of APT and BLEU scores, this confirms our hypothesis that we can improve pronoun translation by paying additional attention to source mentions, and shows that our introduced additional modules do not have negative effect on the general translation quality.'}",oai:arXiv.org:2412.14829v1,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Gongbo Tang, Christian Hardmeier'}]","Gongbo Tang, Christian Hardmeier","{'name': 'Gongbo Tang, Christian Hardmeier'}",10.1145/3632971.3632977,
303,Federated Heavy Hitter Analytics with Local Differential Privacy,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Federated Heavy Hitter Analytics with Local Differential Privacy'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14832'}]",https://arxiv.org/abs/2412.14832,"arXiv:2412.14832v1 Announce Type: new 
Abstract: Federated heavy hitter analytics enables service providers to better understand the preferences of cross-party users by analyzing the most frequent items. As with federated learning, it faces challenges of privacy concerns, statistical heterogeneity, and expensive communication. Local differential privacy (LDP), as the \textit{de facto} standard for privacy-preserving data collection, solves the privacy challenge by letting each user perturb her data locally and report the sanitized version. However, in federated settings, applying LDP complicates the other two challenges, due to the deteriorated utility by the injected LDP noise or increasing communication/computation costs by perturbation mechanism. To tackle these problems, we propose a novel target-aligning prefix tree mechanism satisfying $\epsilon$-LDP, for federated heavy hitter analytics. In particular, we propose an adaptive extension strategy to address the inconsistencies between covering necessary prefixes and estimating heavy hitters within a party to enhance the utility. We also present a consensus-based pruning strategy that utilizes noisy prior knowledge from other parties to further align the inconsistency between finding heavy hitters in each party and providing reasonable frequency information to identify the global ones. To the best of our knowledge, our study is the first solution to the federated heavy hitter analytics in a cross-party setting while satisfying the stringent $\epsilon$-LDP. Comprehensive experiments on both real-world and synthetic datasets confirm the effectiveness of our proposed mechanism.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14832v1 Announce Type: new \nAbstract: Federated heavy hitter analytics enables service providers to better understand the preferences of cross-party users by analyzing the most frequent items. As with federated learning, it faces challenges of privacy concerns, statistical heterogeneity, and expensive communication. Local differential privacy (LDP), as the \\textit{de facto} standard for privacy-preserving data collection, solves the privacy challenge by letting each user perturb her data locally and report the sanitized version. However, in federated settings, applying LDP complicates the other two challenges, due to the deteriorated utility by the injected LDP noise or increasing communication/computation costs by perturbation mechanism. To tackle these problems, we propose a novel target-aligning prefix tree mechanism satisfying $\\epsilon$-LDP, for federated heavy hitter analytics. In particular, we propose an adaptive extension strategy to address the inconsistencies between covering necessary prefixes and estimating heavy hitters within a party to enhance the utility. We also present a consensus-based pruning strategy that utilizes noisy prior knowledge from other parties to further align the inconsistency between finding heavy hitters in each party and providing reasonable frequency information to identify the global ones. To the best of our knowledge, our study is the first solution to the federated heavy hitter analytics in a cross-party setting while satisfying the stringent $\\epsilon$-LDP. Comprehensive experiments on both real-world and synthetic datasets confirm the effectiveness of our proposed mechanism.'}",oai:arXiv.org:2412.14832v1,False,"[{'term': 'cs.CR', 'scheme': None, 'label': None}, {'term': 'cs.DB', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Yuemin Zhang, Qingqing Ye, Haibo Hu'}]","Yuemin Zhang, Qingqing Ye, Haibo Hu","{'name': 'Yuemin Zhang, Qingqing Ye, Haibo Hu'}",,
304,Synchronized and Fine-Grained Head for Skeleton-Based Ambiguous Action Recognition,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Synchronized and Fine-Grained Head for Skeleton-Based Ambiguous Action Recognition'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14833'}]",https://arxiv.org/abs/2412.14833,"arXiv:2412.14833v1 Announce Type: new 
Abstract: Skeleton-based action recognition using GCNs has achieved remarkable performance, but recognizing ambiguous actions, such as ""waving"" and ""saluting"", remains a significant challenge. Existing methods typically rely on a serial combination of GCNs and TCNs, where spatial and temporal features are extracted independently, leading to an unbalanced spatial-temporal information, which hinders accurate action recognition. Moreover, existing methods for ambiguous actions often overemphasize local details, resulting in the loss of crucial global context, which further complicates the task of differentiating ambiguous actions. To address these challenges, we propose a lightweight plug-and-play module called Synchronized and Fine-grained Head (SF-Head), inserted between GCN and TCN layers. SF-Head first conducts Synchronized Spatial-Temporal Extraction (SSTE) with a Feature Redundancy Loss (F-RL), ensuring a balanced interaction between the two types of features. It then performs Adaptive Cross-dimensional Feature Aggregation (AC-FA), with a Feature Consistency Loss (F-CL), which aligns the aggregated feature with their original spatial-temporal feature. This aggregation step effectively combines both global context and local details. Experimental results on NTU RGB+D 60, NTU RGB+D 120, and NW-UCLA datasets demonstrate significant improvements in distinguishing ambiguous actions. Our code will be made available at https://github.com/HaoHuang2003/SFHead.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14833v1 Announce Type: new \nAbstract: Skeleton-based action recognition using GCNs has achieved remarkable performance, but recognizing ambiguous actions, such as ""waving"" and ""saluting"", remains a significant challenge. Existing methods typically rely on a serial combination of GCNs and TCNs, where spatial and temporal features are extracted independently, leading to an unbalanced spatial-temporal information, which hinders accurate action recognition. Moreover, existing methods for ambiguous actions often overemphasize local details, resulting in the loss of crucial global context, which further complicates the task of differentiating ambiguous actions. To address these challenges, we propose a lightweight plug-and-play module called Synchronized and Fine-grained Head (SF-Head), inserted between GCN and TCN layers. SF-Head first conducts Synchronized Spatial-Temporal Extraction (SSTE) with a Feature Redundancy Loss (F-RL), ensuring a balanced interaction between the two types of features. It then performs Adaptive Cross-dimensional Feature Aggregation (AC-FA), with a Feature Consistency Loss (F-CL), which aligns the aggregated feature with their original spatial-temporal feature. This aggregation step effectively combines both global context and local details. Experimental results on NTU RGB+D 60, NTU RGB+D 120, and NW-UCLA datasets demonstrate significant improvements in distinguishing ambiguous actions. Our code will be made available at https://github.com/HaoHuang2003/SFHead.'}",oai:arXiv.org:2412.14833v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Hao Huang, Yujie Lin, Siyu Chen, Haiyang Liu'}]","Hao Huang, Yujie Lin, Siyu Chen, Haiyang Liu","{'name': 'Hao Huang, Yujie Lin, Siyu Chen, Haiyang Liu'}",,
305,Entropy Regularized Task Representation Learning for Offline Meta-Reinforcement Learning,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Entropy Regularized Task Representation Learning for Offline Meta-Reinforcement Learning'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14834'}]",https://arxiv.org/abs/2412.14834,"arXiv:2412.14834v1 Announce Type: new 
Abstract: Offline meta-reinforcement learning aims to equip agents with the ability to rapidly adapt to new tasks by training on data from a set of different tasks. Context-based approaches utilize a history of state-action-reward transitions -- referred to as the context -- to infer representations of the current task, and then condition the agent, i.e., the policy and value function, on the task representations. Intuitively, the better the task representations capture the underlying tasks, the better the agent can generalize to new tasks. Unfortunately, context-based approaches suffer from distribution mismatch, as the context in the offline data does not match the context at test time, limiting their ability to generalize to the test tasks. This leads to the task representations overfitting to the offline training data. Intuitively, the task representations should be independent of the behavior policy used to collect the offline data. To address this issue, we approximately minimize the mutual information between the distribution over the task representations and behavior policy by maximizing the entropy of behavior policy conditioned on the task representations. We validate our approach in MuJoCo environments, showing that compared to baselines, our task representations more faithfully represent the underlying tasks, leading to outperforming prior methods in both in-distribution and out-of-distribution tasks.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14834v1 Announce Type: new \nAbstract: Offline meta-reinforcement learning aims to equip agents with the ability to rapidly adapt to new tasks by training on data from a set of different tasks. Context-based approaches utilize a history of state-action-reward transitions -- referred to as the context -- to infer representations of the current task, and then condition the agent, i.e., the policy and value function, on the task representations. Intuitively, the better the task representations capture the underlying tasks, the better the agent can generalize to new tasks. Unfortunately, context-based approaches suffer from distribution mismatch, as the context in the offline data does not match the context at test time, limiting their ability to generalize to the test tasks. This leads to the task representations overfitting to the offline training data. Intuitively, the task representations should be independent of the behavior policy used to collect the offline data. To address this issue, we approximately minimize the mutual information between the distribution over the task representations and behavior policy by maximizing the entropy of behavior policy conditioned on the task representations. We validate our approach in MuJoCo environments, showing that compared to baselines, our task representations more faithfully represent the underlying tasks, leading to outperforming prior methods in both in-distribution and out-of-distribution tasks.'}",oai:arXiv.org:2412.14834v1,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Mohammadreza nakhaei, Aidan Scannell, Joni Pajarinen'}]","Mohammadreza nakhaei, Aidan Scannell, Joni Pajarinen","{'name': 'Mohammadreza nakhaei, Aidan Scannell, Joni Pajarinen'}",,
306,Progressive Multimodal Reasoning via Active Retrieval,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Progressive Multimodal Reasoning via Active Retrieval'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14835'}]",https://arxiv.org/abs/2412.14835,"arXiv:2412.14835v1 Announce Type: new 
Abstract: Multi-step multimodal reasoning tasks pose significant challenges for multimodal large language models (MLLMs), and finding effective ways to enhance their performance in such scenarios remains an unresolved issue. In this paper, we propose AR-MCTS, a universal framework designed to progressively improve the reasoning capabilities of MLLMs through Active Retrieval (AR) and Monte Carlo Tree Search (MCTS). Our approach begins with the development of a unified retrieval module that retrieves key supporting insights for solving complex reasoning problems from a hybrid-modal retrieval corpus. To bridge the gap in automated multimodal reasoning verification, we employ the MCTS algorithm combined with an active retrieval mechanism, which enables the automatic generation of step-wise annotations. This strategy dynamically retrieves key insights for each reasoning step, moving beyond traditional beam search sampling to improve the diversity and reliability of the reasoning space. Additionally, we introduce a process reward model that aligns progressively to support the automatic verification of multimodal reasoning tasks. Experimental results across three complex multimodal reasoning benchmarks confirm the effectiveness of the AR-MCTS framework in enhancing the performance of various multimodal models. Further analysis demonstrates that AR-MCTS can optimize sampling diversity and accuracy, yielding reliable multimodal reasoning.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14835v1 Announce Type: new \nAbstract: Multi-step multimodal reasoning tasks pose significant challenges for multimodal large language models (MLLMs), and finding effective ways to enhance their performance in such scenarios remains an unresolved issue. In this paper, we propose AR-MCTS, a universal framework designed to progressively improve the reasoning capabilities of MLLMs through Active Retrieval (AR) and Monte Carlo Tree Search (MCTS). Our approach begins with the development of a unified retrieval module that retrieves key supporting insights for solving complex reasoning problems from a hybrid-modal retrieval corpus. To bridge the gap in automated multimodal reasoning verification, we employ the MCTS algorithm combined with an active retrieval mechanism, which enables the automatic generation of step-wise annotations. This strategy dynamically retrieves key insights for each reasoning step, moving beyond traditional beam search sampling to improve the diversity and reliability of the reasoning space. Additionally, we introduce a process reward model that aligns progressively to support the automatic verification of multimodal reasoning tasks. Experimental results across three complex multimodal reasoning benchmarks confirm the effectiveness of the AR-MCTS framework in enhancing the performance of various multimodal models. Further analysis demonstrates that AR-MCTS can optimize sampling diversity and accuracy, yielding reliable multimodal reasoning.'}",oai:arXiv.org:2412.14835v1,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.IR', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Guanting Dong, Chenghao Zhang, Mengjie Deng, Yutao Zhu, Zhicheng Dou, Ji-Rong Wen'}]","Guanting Dong, Chenghao Zhang, Mengjie Deng, Yutao Zhu, Zhicheng Dou, Ji-Rong Wen","{'name': 'Guanting Dong, Chenghao Zhang, Mengjie Deng, Yutao Zhu, Zhicheng Dou, Ji-Rong Wen'}",,
307,Sparse induced subgraphs in $P_7$-free graphs of bounded clique number,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Sparse induced subgraphs in $P_7$-free graphs of bounded clique number'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14836'}]",https://arxiv.org/abs/2412.14836,"arXiv:2412.14836v1 Announce Type: new 
Abstract: Many natural computational problems, including e.g. Max Weight Independent Set, Feedback Vertex Set, or Vertex Planarization, can be unified under an umbrella of finding the largest sparse induced subgraph, that satisfies some property definable in CMSO$_2$ logic.
  It is believed that each problem expressible with this formalism can be solved in polynomial time in graphs that exclude a fixed path as an induced subgraph.
  This belief is supported by the existence of a quasipolynomial-time algorithm by Gartland, Lokshtanov, Pilipczuk, Pilipczuk, and Rz\k{a}\.zewski [STOC 2021], and a recent polynomial-time algorithm for $P_6$-free graphs by Chudnovsky, McCarty, Pilipczuk, Pilipczuk, and Rz\k{a}\.zewski [SODA 2024].
  In this work we extend polynomial-time tractability of all such problems to $P_7$-free graphs of bounded clique number.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14836v1 Announce Type: new \nAbstract: Many natural computational problems, including e.g. Max Weight Independent Set, Feedback Vertex Set, or Vertex Planarization, can be unified under an umbrella of finding the largest sparse induced subgraph, that satisfies some property definable in CMSO$_2$ logic.\n  It is believed that each problem expressible with this formalism can be solved in polynomial time in graphs that exclude a fixed path as an induced subgraph.\n  This belief is supported by the existence of a quasipolynomial-time algorithm by Gartland, Lokshtanov, Pilipczuk, Pilipczuk, and Rz\\k{a}\\.zewski [STOC 2021], and a recent polynomial-time algorithm for $P_6$-free graphs by Chudnovsky, McCarty, Pilipczuk, Pilipczuk, and Rz\\k{a}\\.zewski [SODA 2024].\n  In this work we extend polynomial-time tractability of all such problems to $P_7$-free graphs of bounded clique number.'}",oai:arXiv.org:2412.14836v1,False,"[{'term': 'cs.DS', 'scheme': None, 'label': None}, {'term': 'math.CO', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Maria Chudnovsky, Jadwiga Czy\\.zewska, Kacper Kluk, Marcin Pilipczuk, Pawe{\\l} Rz\\k{a}\\.zewski'}]","Maria Chudnovsky, Jadwiga Czy\.zewska, Kacper Kluk, Marcin Pilipczuk, Pawe{\l} Rz\k{a}\.zewski","{'name': 'Maria Chudnovsky, Jadwiga Czy\\.zewska, Kacper Kluk, Marcin Pilipczuk, Pawe{\\l} Rz\\k{a}\\.zewski'}",,
308,ObjVariantEnsemble: Advancing Point Cloud LLM Evaluation in Challenging Scenes with Subtly Distinguished Objects,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'ObjVariantEnsemble: Advancing Point Cloud LLM Evaluation in Challenging Scenes with Subtly Distinguished Objects'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14837'}]",https://arxiv.org/abs/2412.14837,"arXiv:2412.14837v1 Announce Type: new 
Abstract: 3D scene understanding is an important task, and there has been a recent surge of research interest in aligning 3D representations of point clouds with text to empower embodied AI. However, due to the lack of comprehensive 3D benchmarks, the capabilities of 3D models in real-world scenes, particularly those that are challenging with subtly distinguished objects, remain insufficiently investigated. To facilitate a more thorough evaluation of 3D models' capabilities, we propose a scheme, ObjVariantEnsemble, to systematically introduce more scenes with specified object classes, colors, shapes, quantities, and spatial relationships to meet model evaluation needs. More importantly, we intentionally construct scenes with similar objects to a certain degree and design an LLM-VLM-cooperated annotator to capture key distinctions as annotations. The resultant benchmark can better challenge 3D models, reveal their shortcomings in understanding, and potentially aid in the further development of 3D models.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14837v1 Announce Type: new \nAbstract: 3D scene understanding is an important task, and there has been a recent surge of research interest in aligning 3D representations of point clouds with text to empower embodied AI. However, due to the lack of comprehensive 3D benchmarks, the capabilities of 3D models in real-world scenes, particularly those that are challenging with subtly distinguished objects, remain insufficiently investigated. To facilitate a more thorough evaluation of 3D models' capabilities, we propose a scheme, ObjVariantEnsemble, to systematically introduce more scenes with specified object classes, colors, shapes, quantities, and spatial relationships to meet model evaluation needs. More importantly, we intentionally construct scenes with similar objects to a certain degree and design an LLM-VLM-cooperated annotator to capture key distinctions as annotations. The resultant benchmark can better challenge 3D models, reveal their shortcomings in understanding, and potentially aid in the further development of 3D models.""}",oai:arXiv.org:2412.14837v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Qihang Cao, Huangxun Chen'}]","Qihang Cao, Huangxun Chen","{'name': 'Qihang Cao, Huangxun Chen'}",,
309,DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context LLMs,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context LLMs'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14838'}]",https://arxiv.org/abs/2412.14838,"arXiv:2412.14838v1 Announce Type: new 
Abstract: Efficient KV cache management in LLMs is crucial for long-context tasks like RAG and summarization. Existing KV cache compression methods enforce a fixed pattern, neglecting task-specific characteristics and reducing the retention of essential information. However, we observe distinct activation patterns across layers in various tasks, highlighting the need for adaptive strategies tailored to each task's unique demands. Based on this insight, we propose DynamicKV, a method that dynamically optimizes token retention by adjusting the number of tokens retained at each layer to adapt to the specific task. DynamicKV establishes global and per-layer maximum KV cache budgets, temporarily retaining the maximum budget for the current layer, and periodically updating the KV cache sizes of all preceding layers during inference. Our method retains only 1.7% of the KV cache size while achieving ~85% of the Full KV cache performance on LongBench. Notably, even under extreme compression (0.9%), DynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the Needle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be released.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14838v1 Announce Type: new \nAbstract: Efficient KV cache management in LLMs is crucial for long-context tasks like RAG and summarization. Existing KV cache compression methods enforce a fixed pattern, neglecting task-specific characteristics and reducing the retention of essential information. However, we observe distinct activation patterns across layers in various tasks, highlighting the need for adaptive strategies tailored to each task's unique demands. Based on this insight, we propose DynamicKV, a method that dynamically optimizes token retention by adjusting the number of tokens retained at each layer to adapt to the specific task. DynamicKV establishes global and per-layer maximum KV cache budgets, temporarily retaining the maximum budget for the current layer, and periodically updating the KV cache sizes of all preceding layers during inference. Our method retains only 1.7% of the KV cache size while achieving ~85% of the Full KV cache performance on LongBench. Notably, even under extreme compression (0.9%), DynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the Needle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be released.""}",oai:arXiv.org:2412.14838v1,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Xiabin Zhou, Wenbin Wang, Minyan Zeng, Jiaxian Guo, Xuebo Liu, Li Shen, Min Zhang, Liang Ding'}]","Xiabin Zhou, Wenbin Wang, Minyan Zeng, Jiaxian Guo, Xuebo Liu, Li Shen, Min Zhang, Liang Ding","{'name': 'Xiabin Zhou, Wenbin Wang, Minyan Zeng, Jiaxian Guo, Xuebo Liu, Li Shen, Min Zhang, Liang Ding'}",,
310,Helping LLMs Improve Code Generation Using Feedback from Testing and Static Analysis,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Helping LLMs Improve Code Generation Using Feedback from Testing and Static Analysis'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14841'}]",https://arxiv.org/abs/2412.14841,"arXiv:2412.14841v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are one of the most promising developments in the field of artificial intelligence, and the software engineering community has readily noticed their potential role in the software development life-cycle. Developers routinely ask LLMs to generate code snippets, increasing productivity but also potentially introducing ownership, privacy, correctness, and security issues. Previous work highlighted how code generated by mainstream commercial LLMs is often not safe, containing vulnerabilities, bugs, and code smells. In this paper, we present a framework that leverages testing and static analysis to assess the quality, and guide the self-improvement, of code generated by general-purpose, open-source LLMs.
  First, we ask LLMs to generate C code to solve a number of programming tasks. Then we employ ground-truth tests to assess the (in)correctness of the generated code, and a static analysis tool to detect potential safety vulnerabilities. Next, we assess the models ability to evaluate the generated code, by asking them to detect errors and vulnerabilities. Finally, we test the models ability to fix the generated code, providing the reports produced during the static analysis and incorrectness evaluation phases as feedback.
  Our results show that models often produce incorrect code, and that the generated code can include safety issues. Moreover, they perform very poorly at detecting either issue. On the positive side, we observe a substantial ability to fix flawed code when provided with information about failed tests or potential vulnerabilities, indicating a promising avenue for improving the safety of LLM-based code generation tools.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14841v1 Announce Type: new \nAbstract: Large Language Models (LLMs) are one of the most promising developments in the field of artificial intelligence, and the software engineering community has readily noticed their potential role in the software development life-cycle. Developers routinely ask LLMs to generate code snippets, increasing productivity but also potentially introducing ownership, privacy, correctness, and security issues. Previous work highlighted how code generated by mainstream commercial LLMs is often not safe, containing vulnerabilities, bugs, and code smells. In this paper, we present a framework that leverages testing and static analysis to assess the quality, and guide the self-improvement, of code generated by general-purpose, open-source LLMs.\n  First, we ask LLMs to generate C code to solve a number of programming tasks. Then we employ ground-truth tests to assess the (in)correctness of the generated code, and a static analysis tool to detect potential safety vulnerabilities. Next, we assess the models ability to evaluate the generated code, by asking them to detect errors and vulnerabilities. Finally, we test the models ability to fix the generated code, providing the reports produced during the static analysis and incorrectness evaluation phases as feedback.\n  Our results show that models often produce incorrect code, and that the generated code can include safety issues. Moreover, they perform very poorly at detecting either issue. On the positive side, we observe a substantial ability to fix flawed code when provided with information about failed tests or potential vulnerabilities, indicating a promising avenue for improving the safety of LLM-based code generation tools.'}",oai:arXiv.org:2412.14841v1,False,"[{'term': 'cs.SE', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Greta Dolcetti, Vincenzo Arceri, Eleonora Iotti, Sergio Maffeis, Agostino Cortesi, Enea Zaffanella'}]","Greta Dolcetti, Vincenzo Arceri, Eleonora Iotti, Sergio Maffeis, Agostino Cortesi, Enea Zaffanella","{'name': 'Greta Dolcetti, Vincenzo Arceri, Eleonora Iotti, Sergio Maffeis, Agostino Cortesi, Enea Zaffanella'}",,
311,Mapping and Influencing the Political Ideology of Large Language Models using Synthetic Personas,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Mapping and Influencing the Political Ideology of Large Language Models using Synthetic Personas'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14843'}]",https://arxiv.org/abs/2412.14843,"arXiv:2412.14843v1 Announce Type: new 
Abstract: The analysis of political biases in large language models (LLMs) has primarily examined these systems as single entities with fixed viewpoints. While various methods exist for measuring such biases, the impact of persona-based prompting on LLMs' political orientation remains unexplored. In this work we leverage PersonaHub, a collection of synthetic persona descriptions, to map the political distribution of persona-based prompted LLMs using the Political Compass Test (PCT). We then examine whether these initial compass distributions can be manipulated through explicit ideological prompting towards diametrically opposed political orientations: right-authoritarian and left-libertarian. Our experiments reveal that synthetic personas predominantly cluster in the left-libertarian quadrant, with models demonstrating varying degrees of responsiveness when prompted with explicit ideological descriptors. While all models demonstrate significant shifts towards right-authoritarian positions, they exhibit more limited shifts towards left-libertarian positions, suggesting an asymmetric response to ideological manipulation that may reflect inherent biases in model training.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14843v1 Announce Type: new \nAbstract: The analysis of political biases in large language models (LLMs) has primarily examined these systems as single entities with fixed viewpoints. While various methods exist for measuring such biases, the impact of persona-based prompting on LLMs' political orientation remains unexplored. In this work we leverage PersonaHub, a collection of synthetic persona descriptions, to map the political distribution of persona-based prompted LLMs using the Political Compass Test (PCT). We then examine whether these initial compass distributions can be manipulated through explicit ideological prompting towards diametrically opposed political orientations: right-authoritarian and left-libertarian. Our experiments reveal that synthetic personas predominantly cluster in the left-libertarian quadrant, with models demonstrating varying degrees of responsiveness when prompted with explicit ideological descriptors. While all models demonstrate significant shifts towards right-authoritarian positions, they exhibit more limited shifts towards left-libertarian positions, suggesting an asymmetric response to ideological manipulation that may reflect inherent biases in model training.""}",oai:arXiv.org:2412.14843v1,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Pietro Bernardelle, Leon Fr\\""ohling, Stefano Civelli, Riccardo Lunardi, Kevin Roiter, Gianluca Demartini'}]","Pietro Bernardelle, Leon Fr\""ohling, Stefano Civelli, Riccardo Lunardi, Kevin Roiter, Gianluca Demartini","{'name': 'Pietro Bernardelle, Leon Fr\\""ohling, Stefano Civelli, Riccardo Lunardi, Kevin Roiter, Gianluca Demartini'}",,
312,A Survey of RWKV,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'A Survey of RWKV'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14847'}]",https://arxiv.org/abs/2412.14847,"arXiv:2412.14847v1 Announce Type: new 
Abstract: The Receptance Weighted Key Value (RWKV) model offers a novel alternative to the Transformer architecture, merging the benefits of recurrent and attention-based systems. Unlike conventional Transformers, which depend heavily on self-attention, RWKV adeptly captures long-range dependencies with minimal computational demands. By utilizing a recurrent framework, RWKV addresses some computational inefficiencies found in Transformers, particularly in tasks with long sequences. RWKV has recently drawn considerable attention for its robust performance across multiple domains. Despite its growing popularity, no systematic review of the RWKV model exists. This paper seeks to fill this gap as the first comprehensive review of the RWKV architecture, its core principles, and its varied applications, such as natural language generation, natural language understanding, and computer vision. We assess how RWKV compares to traditional Transformer models, highlighting its capability to manage long sequences efficiently and lower computational costs. Furthermore, we explore the challenges RWKV encounters and propose potential directions for future research and advancement. We consistently maintain the related open-source materials at: https://github.com/MLGroupJLU/RWKV-Survey.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14847v1 Announce Type: new \nAbstract: The Receptance Weighted Key Value (RWKV) model offers a novel alternative to the Transformer architecture, merging the benefits of recurrent and attention-based systems. Unlike conventional Transformers, which depend heavily on self-attention, RWKV adeptly captures long-range dependencies with minimal computational demands. By utilizing a recurrent framework, RWKV addresses some computational inefficiencies found in Transformers, particularly in tasks with long sequences. RWKV has recently drawn considerable attention for its robust performance across multiple domains. Despite its growing popularity, no systematic review of the RWKV model exists. This paper seeks to fill this gap as the first comprehensive review of the RWKV architecture, its core principles, and its varied applications, such as natural language generation, natural language understanding, and computer vision. We assess how RWKV compares to traditional Transformer models, highlighting its capability to manage long sequences efficiently and lower computational costs. Furthermore, we explore the challenges RWKV encounters and propose potential directions for future research and advancement. We consistently maintain the related open-source materials at: https://github.com/MLGroupJLU/RWKV-Survey.'}",oai:arXiv.org:2412.14847v1,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Zhiyuan Li, Tingyu Xia, Yi Chang, Yuan Wu'}]","Zhiyuan Li, Tingyu Xia, Yi Chang, Yuan Wu","{'name': 'Zhiyuan Li, Tingyu Xia, Yi Chang, Yuan Wu'}",,
313,DS$^2$-ABSA: Dual-Stream Data Synthesis with Label Refinement for Few-Shot Aspect-Based Sentiment Analysis,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'DS$^2$-ABSA: Dual-Stream Data Synthesis with Label Refinement for Few-Shot Aspect-Based Sentiment Analysis'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14849'}]",https://arxiv.org/abs/2412.14849,"arXiv:2412.14849v1 Announce Type: new 
Abstract: Recently developed large language models (LLMs) have presented promising new avenues to address data scarcity in low-resource scenarios. In few-shot aspect-based sentiment analysis (ABSA), previous efforts have explored data augmentation techniques, which prompt LLMs to generate new samples by modifying existing ones. However, these methods fail to produce adequately diverse data, impairing their effectiveness. Besides, some studies apply in-context learning for ABSA by using specific instructions and a few selected examples as prompts. Though promising, LLMs often yield labels that deviate from task requirements. To overcome these limitations, we propose DS$^2$-ABSA, a dual-stream data synthesis framework targeted for few-shot ABSA. It leverages LLMs to synthesize data from two complementary perspectives: \textit{key-point-driven} and \textit{instance-driven}, which effectively generate diverse and high-quality ABSA samples in low-resource settings. Furthermore, a \textit{label refinement} module is integrated to improve the synthetic labels. Extensive experiments demonstrate that DS$^2$-ABSA significantly outperforms previous few-shot ABSA solutions and other LLM-oriented data generation methods.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14849v1 Announce Type: new \nAbstract: Recently developed large language models (LLMs) have presented promising new avenues to address data scarcity in low-resource scenarios. In few-shot aspect-based sentiment analysis (ABSA), previous efforts have explored data augmentation techniques, which prompt LLMs to generate new samples by modifying existing ones. However, these methods fail to produce adequately diverse data, impairing their effectiveness. Besides, some studies apply in-context learning for ABSA by using specific instructions and a few selected examples as prompts. Though promising, LLMs often yield labels that deviate from task requirements. To overcome these limitations, we propose DS$^2$-ABSA, a dual-stream data synthesis framework targeted for few-shot ABSA. It leverages LLMs to synthesize data from two complementary perspectives: \\textit{key-point-driven} and \\textit{instance-driven}, which effectively generate diverse and high-quality ABSA samples in low-resource settings. Furthermore, a \\textit{label refinement} module is integrated to improve the synthetic labels. Extensive experiments demonstrate that DS$^2$-ABSA significantly outperforms previous few-shot ABSA solutions and other LLM-oriented data generation methods.'}",oai:arXiv.org:2412.14849v1,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Hongling Xu, Yice Zhang, Qianlong Wang, Ruifeng Xu'}]","Hongling Xu, Yice Zhang, Qianlong Wang, Ruifeng Xu","{'name': 'Hongling Xu, Yice Zhang, Qianlong Wang, Ruifeng Xu'}",,
314,Position: A taxonomy for reporting and describing AI security incidents,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Position: A taxonomy for reporting and describing AI security incidents'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14855'}]",https://arxiv.org/abs/2412.14855,"arXiv:2412.14855v1 Announce Type: new 
Abstract: AI systems are vulnerable to attacks, and corresponding AI security incidents have been described. Although a collection of safety incidents around AI will become a regulatory requirement, there is no proposal to collect AI security incidents. In this position paper, we argue that a proposal should be made, taking into account the interests and needs of different stakeholders: industry, providers, users, and researchers. We thus attempt to close this gap and propose a taxonomy alongside its requirements like machine readability and link-ability with existing databases. We aim to spark discussions and enable discussion of which information is feasible, necessary, and possible to report and share within and outside organizations using AI.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14855v1 Announce Type: new \nAbstract: AI systems are vulnerable to attacks, and corresponding AI security incidents have been described. Although a collection of safety incidents around AI will become a regulatory requirement, there is no proposal to collect AI security incidents. In this position paper, we argue that a proposal should be made, taking into account the interests and needs of different stakeholders: industry, providers, users, and researchers. We thus attempt to close this gap and propose a taxonomy alongside its requirements like machine readability and link-ability with existing databases. We aim to spark discussions and enable discussion of which information is feasible, necessary, and possible to report and share within and outside organizations using AI.'}",oai:arXiv.org:2412.14855v1,False,"[{'term': 'cs.CR', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Lukas Bieringer, Kevin Paeth, Andreas Wespi, Kathrin Grosse'}]","Lukas Bieringer, Kevin Paeth, Andreas Wespi, Kathrin Grosse","{'name': 'Lukas Bieringer, Kevin Paeth, Andreas Wespi, Kathrin Grosse'}",,
315,Think&Cite: Improving Attributed Text Generation with Self-Guided Tree Search and Progress Reward Modeling,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Think&Cite: Improving Attributed Text Generation with Self-Guided Tree Search and Progress Reward Modeling'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14860'}]",https://arxiv.org/abs/2412.14860,"arXiv:2412.14860v1 Announce Type: new 
Abstract: Despite their outstanding capabilities, large language models (LLMs) are prone to hallucination and producing factually incorrect information. This challenge has spurred efforts in attributed text generation, which prompts LLMs to generate content with supporting evidence. In this paper, we propose a novel framework, called Think&amp;Cite, and formulate attributed text generation as a multi-step reasoning problem integrated with search. Specifically, we propose Self-Guided Monte Carlo Tree Search (SG-MCTS), which capitalizes on the self-reflection capability of LLMs to reflect on the intermediate states of MCTS for guiding the tree expansion process. To provide reliable and comprehensive feedback, we introduce Progress Reward Models to measure the progress of tree search from the root to the current state from two aspects, i.e., generation and attribution progress. We conduct extensive experiments on three datasets and the results show that our approach significantly outperforms baseline approaches.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14860v1 Announce Type: new \nAbstract: Despite their outstanding capabilities, large language models (LLMs) are prone to hallucination and producing factually incorrect information. This challenge has spurred efforts in attributed text generation, which prompts LLMs to generate content with supporting evidence. In this paper, we propose a novel framework, called Think&amp;Cite, and formulate attributed text generation as a multi-step reasoning problem integrated with search. Specifically, we propose Self-Guided Monte Carlo Tree Search (SG-MCTS), which capitalizes on the self-reflection capability of LLMs to reflect on the intermediate states of MCTS for guiding the tree expansion process. To provide reliable and comprehensive feedback, we introduce Progress Reward Models to measure the progress of tree search from the root to the current state from two aspects, i.e., generation and attribution progress. We conduct extensive experiments on three datasets and the results show that our approach significantly outperforms baseline approaches.'}",oai:arXiv.org:2412.14860v1,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Junyi Li, Hwee Tou Ng'}]","Junyi Li, Hwee Tou Ng","{'name': 'Junyi Li, Hwee Tou Ng'}",,
316,Long induced paths and forbidden patterns: Polylogarithmic bounds,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Long induced paths and forbidden patterns: Polylogarithmic bounds'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14863'}]",https://arxiv.org/abs/2412.14863,"arXiv:2412.14863v1 Announce Type: new 
Abstract: Consider a graph $G$ with a long path $P$. When is it the case that $G$ also contains a long induced path? This question has been investigated in general as well as within a number of different graph classes since the 80s. We have recently observed in a companion paper (Long induced paths in sparse graphs and graphs with forbidden patterns, arXiv:2411.08685, 2024) that most existing results can recovered in a simple way by considering forbidden ordered patterns of edges along the path $P$. In particular we proved that if we forbid some fixed ordered matching along a path of order $n$ in a graph $G$, then $G$ must contain an induced path of order $(\log n)^{\Omega(1)}$. Moreover, we completely characterized the forbidden ordered patterns forcing the existence of an induced path of polynomial size.
  The purpose of the present paper is to completely characterize the ordered patterns $H$ such that forbidding $H$ along a path $P$ of order $n$ implies the existence of an induced path of order $(\log n)^{\Omega(1)}$. These patterns are star forests with some specific ordering, which we called constellations.
  As a direct consequence of our result, we show that if a graph $G$ has a path of length $n$ and does not contain $K_t$ as a topological minor, then $G$ contains an induced path of order $(\log n)^{\Omega(1/t \log^2 t)}$. The previously best known bound was $(\log n)^{f(t)}$ for some unspecified function $f$ depending on the Topological Minor Structure Theorem of Grohe and Marx (2015).","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14863v1 Announce Type: new \nAbstract: Consider a graph $G$ with a long path $P$. When is it the case that $G$ also contains a long induced path? This question has been investigated in general as well as within a number of different graph classes since the 80s. We have recently observed in a companion paper (Long induced paths in sparse graphs and graphs with forbidden patterns, arXiv:2411.08685, 2024) that most existing results can recovered in a simple way by considering forbidden ordered patterns of edges along the path $P$. In particular we proved that if we forbid some fixed ordered matching along a path of order $n$ in a graph $G$, then $G$ must contain an induced path of order $(\\log n)^{\\Omega(1)}$. Moreover, we completely characterized the forbidden ordered patterns forcing the existence of an induced path of polynomial size.\n  The purpose of the present paper is to completely characterize the ordered patterns $H$ such that forbidding $H$ along a path $P$ of order $n$ implies the existence of an induced path of order $(\\log n)^{\\Omega(1)}$. These patterns are star forests with some specific ordering, which we called constellations.\n  As a direct consequence of our result, we show that if a graph $G$ has a path of length $n$ and does not contain $K_t$ as a topological minor, then $G$ contains an induced path of order $(\\log n)^{\\Omega(1/t \\log^2 t)}$. The previously best known bound was $(\\log n)^{f(t)}$ for some unspecified function $f$ depending on the Topological Minor Structure Theorem of Grohe and Marx (2015).'}",oai:arXiv.org:2412.14863v1,False,"[{'term': 'cs.DM', 'scheme': None, 'label': None}, {'term': 'math.CO', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Julien Duron, Louis Esperet, Jean-Florent Raymond'}]","Julien Duron, Louis Esperet, Jean-Florent Raymond","{'name': 'Julien Duron, Louis Esperet, Jean-Florent Raymond'}",,
317,Hierarchical Subspaces of Policies for Continual Offline Reinforcement Learning,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Hierarchical Subspaces of Policies for Continual Offline Reinforcement Learning'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14865'}]",https://arxiv.org/abs/2412.14865,"arXiv:2412.14865v1 Announce Type: new 
Abstract: In dynamic domains such as autonomous robotics and video game simulations, agents must continuously adapt to new tasks while retaining previously acquired skills. This ongoing process, known as Continual Reinforcement Learning, presents significant challenges, including the risk of forgetting past knowledge and the need for scalable solutions as the number of tasks increases. To address these issues, we introduce HIerarchical LOW-rank Subspaces of Policies (HILOW), a novel framework designed for continual learning in offline navigation settings. HILOW leverages hierarchical policy subspaces to enable flexible and efficient adaptation to new tasks while preserving existing knowledge. We demonstrate, through a careful experimental study, the effectiveness of our method in both classical MuJoCo maze environments and complex video game-like simulations, showcasing competitive performance and satisfying adaptability according to classical continual learning metrics, in particular regarding memory usage. Our work provides a promising framework for real-world applications where continuous learning from pre-collected data is essential.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14865v1 Announce Type: new \nAbstract: In dynamic domains such as autonomous robotics and video game simulations, agents must continuously adapt to new tasks while retaining previously acquired skills. This ongoing process, known as Continual Reinforcement Learning, presents significant challenges, including the risk of forgetting past knowledge and the need for scalable solutions as the number of tasks increases. To address these issues, we introduce HIerarchical LOW-rank Subspaces of Policies (HILOW), a novel framework designed for continual learning in offline navigation settings. HILOW leverages hierarchical policy subspaces to enable flexible and efficient adaptation to new tasks while preserving existing knowledge. We demonstrate, through a careful experimental study, the effectiveness of our method in both classical MuJoCo maze environments and complex video game-like simulations, showcasing competitive performance and satisfying adaptability according to classical continual learning metrics, in particular regarding memory usage. Our work provides a promising framework for real-world applications where continuous learning from pre-collected data is essential.'}",oai:arXiv.org:2412.14865v1,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': ""Anthony Kobanda, R\\'emy Portelas, Odalric-Ambrym Maillard, Ludovic Denoyer""}]","Anthony Kobanda, R\'emy Portelas, Odalric-Ambrym Maillard, Ludovic Denoyer","{'name': ""Anthony Kobanda, R\\'emy Portelas, Odalric-Ambrym Maillard, Ludovic Denoyer""}",,
318,Graph-Convolutional Networks: Named Entity Recognition and Large Language Model Embedding in Document Clustering,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Graph-Convolutional Networks: Named Entity Recognition and Large Language Model Embedding in Document Clustering'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14867'}]",https://arxiv.org/abs/2412.14867,"arXiv:2412.14867v1 Announce Type: new 
Abstract: Recent advances in machine learning, particularly Large Language Models (LLMs) such as BERT and GPT, provide rich contextual embeddings that improve text representation. However, current document clustering approaches often ignore the deeper relationships between named entities (NEs) and the potential of LLM embeddings. This paper proposes a novel approach that integrates Named Entity Recognition (NER) and LLM embeddings within a graph-based framework for document clustering. The method builds a graph with nodes representing documents and edges weighted by named entity similarity, optimized using a graph-convolutional network (GCN). This ensures a more effective grouping of semantically related documents. Experimental results indicate that our approach outperforms conventional co-occurrence-based methods in clustering, notably for documents rich in named entities.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14867v1 Announce Type: new \nAbstract: Recent advances in machine learning, particularly Large Language Models (LLMs) such as BERT and GPT, provide rich contextual embeddings that improve text representation. However, current document clustering approaches often ignore the deeper relationships between named entities (NEs) and the potential of LLM embeddings. This paper proposes a novel approach that integrates Named Entity Recognition (NER) and LLM embeddings within a graph-based framework for document clustering. The method builds a graph with nodes representing documents and edges weighted by named entity similarity, optimized using a graph-convolutional network (GCN). This ensures a more effective grouping of semantically related documents. Experimental results indicate that our approach outperforms conventional co-occurrence-based methods in clustering, notably for documents rich in named entities.'}",oai:arXiv.org:2412.14867v1,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Imed Keraghel, Mohamed Nadif'}]","Imed Keraghel, Mohamed Nadif","{'name': 'Imed Keraghel, Mohamed Nadif'}",,
319,AI-Powered Intracranial Hemorrhage Detection: A Co-Scale Convolutional Attention Model with Uncertainty-Based Fuzzy Integral Operator and Feature Screening,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'AI-Powered Intracranial Hemorrhage Detection: A Co-Scale Convolutional Attention Model with Uncertainty-Based Fuzzy Integral Operator and Feature Screening'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14869'}]",https://arxiv.org/abs/2412.14869,"arXiv:2412.14869v1 Announce Type: new 
Abstract: Intracranial hemorrhage (ICH) refers to the leakage or accumulation of blood within the skull, which occurs due to the rupture of blood vessels in or around the brain. If this condition is not diagnosed in a timely manner and appropriately treated, it can lead to serious complications such as decreased consciousness, permanent neurological disabilities, or even death.The primary aim of this study is to detect the occurrence or non-occurrence of ICH, followed by determining the type of subdural hemorrhage (SDH). These tasks are framed as two separate binary classification problems. By adding two layers to the co-scale convolutional attention (CCA) classifier architecture, we introduce a novel approach for ICH detection. In the first layer, after extracting features from different slices of computed tomography (CT) scan images, we combine these features and select the 50 components that capture the highest variance in the data, considering them as informative features. We then assess the discriminative power of these features using the bootstrap forest algorithm, discarding those that lack sufficient discriminative ability between different classes. This algorithm explicitly determines the contribution of each feature to the final prediction, assisting us in developing an explainable AI model. The features feed into a boosting neural network as a latent feature space. In the second layer, we introduce a novel uncertainty-based fuzzy integral operator to fuse information from different CT scan slices. This operator, by accounting for the dependencies between consecutive slices, significantly improves detection accuracy.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14869v1 Announce Type: new \nAbstract: Intracranial hemorrhage (ICH) refers to the leakage or accumulation of blood within the skull, which occurs due to the rupture of blood vessels in or around the brain. If this condition is not diagnosed in a timely manner and appropriately treated, it can lead to serious complications such as decreased consciousness, permanent neurological disabilities, or even death.The primary aim of this study is to detect the occurrence or non-occurrence of ICH, followed by determining the type of subdural hemorrhage (SDH). These tasks are framed as two separate binary classification problems. By adding two layers to the co-scale convolutional attention (CCA) classifier architecture, we introduce a novel approach for ICH detection. In the first layer, after extracting features from different slices of computed tomography (CT) scan images, we combine these features and select the 50 components that capture the highest variance in the data, considering them as informative features. We then assess the discriminative power of these features using the bootstrap forest algorithm, discarding those that lack sufficient discriminative ability between different classes. This algorithm explicitly determines the contribution of each feature to the final prediction, assisting us in developing an explainable AI model. The features feed into a boosting neural network as a latent feature space. In the second layer, we introduce a novel uncertainty-based fuzzy integral operator to fuse information from different CT scan slices. This operator, by accounting for the dependencies between consecutive slices, significantly improves detection accuracy.'}",oai:arXiv.org:2412.14869v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by-nc-nd/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-nd/4.0/'}","[{'name': 'Mehdi Hosseini Chagahi, Md. Jalil Piran, Niloufar Delfan, Behzad Moshiri, Jaber Hatam Parikhan'}]","Mehdi Hosseini Chagahi, Md. Jalil Piran, Niloufar Delfan, Behzad Moshiri, Jaber Hatam Parikhan","{'name': 'Mehdi Hosseini Chagahi, Md. Jalil Piran, Niloufar Delfan, Behzad Moshiri, Jaber Hatam Parikhan'}",,
320,Large-scale School Mapping using Weakly Supervised Deep Learning for Universal School Connectivity,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Large-scale School Mapping using Weakly Supervised Deep Learning for Universal School Connectivity'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14870'}]",https://arxiv.org/abs/2412.14870,"arXiv:2412.14870v1 Announce Type: new 
Abstract: Improving global school connectivity is critical for ensuring inclusive and equitable quality education. To reliably estimate the cost of connecting schools, governments and connectivity providers require complete and accurate school location data - a resource that is often scarce in many low- and middle-income countries. To address this challenge, we propose a cost-effective, scalable approach to locating schools in high-resolution satellite images using weakly supervised deep learning techniques. Our best models, which combine vision transformers and convolutional neural networks, achieve AUPRC values above 0.96 across 10 pilot African countries. Leveraging explainable AI techniques, our approach can approximate the precise geographical coordinates of the school locations using only low-cost, classification-level annotations. To demonstrate the scalability of our method, we generate nationwide maps of school location predictions in African countries and present a detailed analysis of our results, using Senegal as our case study. Finally, we demonstrate the immediate usability of our work by introducing an interactive web mapping tool to streamline human-in-the-loop model validation efforts by government partners. This work successfully showcases the real-world utility of deep learning and satellite images for planning regional infrastructure and accelerating universal school connectivity.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14870v1 Announce Type: new \nAbstract: Improving global school connectivity is critical for ensuring inclusive and equitable quality education. To reliably estimate the cost of connecting schools, governments and connectivity providers require complete and accurate school location data - a resource that is often scarce in many low- and middle-income countries. To address this challenge, we propose a cost-effective, scalable approach to locating schools in high-resolution satellite images using weakly supervised deep learning techniques. Our best models, which combine vision transformers and convolutional neural networks, achieve AUPRC values above 0.96 across 10 pilot African countries. Leveraging explainable AI techniques, our approach can approximate the precise geographical coordinates of the school locations using only low-cost, classification-level annotations. To demonstrate the scalability of our method, we generate nationwide maps of school location predictions in African countries and present a detailed analysis of our results, using Senegal as our case study. Finally, we demonstrate the immediate usability of our work by introducing an interactive web mapping tool to streamline human-in-the-loop model validation efforts by government partners. This work successfully showcases the real-world utility of deep learning and satellite images for planning regional infrastructure and accelerating universal school connectivity.'}",oai:arXiv.org:2412.14870v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Isabelle Tingzon, Utku Can Ozturk, Ivan Dotu'}]","Isabelle Tingzon, Utku Can Ozturk, Ivan Dotu","{'name': 'Isabelle Tingzon, Utku Can Ozturk, Ivan Dotu'}",,
321,Why language models collapse when trained on recursively generated text,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Why language models collapse when trained on recursively generated text'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14872'}]",https://arxiv.org/abs/2412.14872,"arXiv:2412.14872v1 Announce Type: new 
Abstract: Language models (LMs) have been widely used to generate text on the Internet. The generated text is often collected into the training corpus of the next generations of LMs. Previous work has experimentally found that LMs collapse when trained on recursively generated text. This paper contributes to existing knowledge from two aspects. We present a theoretical proof of LM collapse. Our proof reveals the cause of LM collapse and proves that all auto-regressive LMs will definitely collapse. We present a new finding: the performance of LMs gradually declines when trained on recursively generated text until they perform no better than a randomly initialized LM. The trained LMs produce large amounts of repetitive text and perform poorly across a wide range of natural language tasks. The above proof and new findings deepen our understanding of LM collapse and offer valuable insights that may inspire new training techniques to mitigate this threat.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14872v1 Announce Type: new \nAbstract: Language models (LMs) have been widely used to generate text on the Internet. The generated text is often collected into the training corpus of the next generations of LMs. Previous work has experimentally found that LMs collapse when trained on recursively generated text. This paper contributes to existing knowledge from two aspects. We present a theoretical proof of LM collapse. Our proof reveals the cause of LM collapse and proves that all auto-regressive LMs will definitely collapse. We present a new finding: the performance of LMs gradually declines when trained on recursively generated text until they perform no better than a randomly initialized LM. The trained LMs produce large amounts of repetitive text and perform poorly across a wide range of natural language tasks. The above proof and new findings deepen our understanding of LM collapse and offer valuable insights that may inspire new training techniques to mitigate this threat.'}",oai:arXiv.org:2412.14872v1,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by-nc-sa/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-sa/4.0/'}","[{'name': 'Lecheng Wang, Xianjie Shi, Ge Li, Jia Li, Yihong Dong, Xuanming Zhang, Wenpin Jiao, Hong Mei'}]","Lecheng Wang, Xianjie Shi, Ge Li, Jia Li, Yihong Dong, Xuanming Zhang, Wenpin Jiao, Hong Mei","{'name': 'Lecheng Wang, Xianjie Shi, Ge Li, Jia Li, Yihong Dong, Xuanming Zhang, Wenpin Jiao, Hong Mei'}",,
322,Zero-Shot Artifact2Artifact: Self-incentive artifact removal for photoacoustic imaging without any data,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Zero-Shot Artifact2Artifact: Self-incentive artifact removal for photoacoustic imaging without any data'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14873'}]",https://arxiv.org/abs/2412.14873,"arXiv:2412.14873v1 Announce Type: new 
Abstract: Photoacoustic imaging (PAI) uniquely combines optical contrast with the penetration depth of ultrasound, making it critical for clinical applications. However, the quality of 3D PAI is often degraded due to reconstruction artifacts caused by the sparse and angle-limited configuration of detector arrays. Existing iterative or deep learning-based methods are either time-consuming or require large training datasets, significantly limiting their practical application. Here, we propose Zero-Shot Artifact2Artifact (ZS-A2A), a zero-shot self-supervised artifact removal method based on a super-lightweight network, which leverages the fact that reconstruction artifacts are sensitive to irregularities caused by data loss. By introducing random perturbations to the acquired PA data, it spontaneously generates subset data, which in turn stimulates the network to learn the artifact patterns in the reconstruction results, thus enabling zero-shot artifact removal. This approach requires neither training data nor prior knowledge of the artifacts, and is capable of artifact removal for 3D PAI. For maximum amplitude projection (MAP) images or slice images in 3D PAI acquired with arbitrarily sparse or angle-limited detector arrays, ZS-A2A employs a self-incentive strategy to complete artifact removal and improves the Contrast-to-Noise Ratio (CNR). We validated ZS-A2A in both simulation study and $ in\ vivo $ animal experiments. Results demonstrate that ZS-A2A achieves state-of-the-art (SOTA) performance compared to existing zero-shot methods, and for the $ in\ vivo $ rat liver, ZS-A2A improves CNR from 17.48 to 43.46 in just 8 seconds. The project for ZS-A2A will be available in the following GitHub repository: https://github.com/JaegerCQ/ZS-A2A.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14873v1 Announce Type: new \nAbstract: Photoacoustic imaging (PAI) uniquely combines optical contrast with the penetration depth of ultrasound, making it critical for clinical applications. However, the quality of 3D PAI is often degraded due to reconstruction artifacts caused by the sparse and angle-limited configuration of detector arrays. Existing iterative or deep learning-based methods are either time-consuming or require large training datasets, significantly limiting their practical application. Here, we propose Zero-Shot Artifact2Artifact (ZS-A2A), a zero-shot self-supervised artifact removal method based on a super-lightweight network, which leverages the fact that reconstruction artifacts are sensitive to irregularities caused by data loss. By introducing random perturbations to the acquired PA data, it spontaneously generates subset data, which in turn stimulates the network to learn the artifact patterns in the reconstruction results, thus enabling zero-shot artifact removal. This approach requires neither training data nor prior knowledge of the artifacts, and is capable of artifact removal for 3D PAI. For maximum amplitude projection (MAP) images or slice images in 3D PAI acquired with arbitrarily sparse or angle-limited detector arrays, ZS-A2A employs a self-incentive strategy to complete artifact removal and improves the Contrast-to-Noise Ratio (CNR). We validated ZS-A2A in both simulation study and $ in\\ vivo $ animal experiments. Results demonstrate that ZS-A2A achieves state-of-the-art (SOTA) performance compared to existing zero-shot methods, and for the $ in\\ vivo $ rat liver, ZS-A2A improves CNR from 17.48 to 43.46 in just 8 seconds. The project for ZS-A2A will be available in the following GitHub repository: https://github.com/JaegerCQ/ZS-A2A.'}",oai:arXiv.org:2412.14873v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Shuang Li, Qian Chen, Chulhong Kim, Seongwook Choi, Yibing Wang, Yu Zhang, Changhui Li'}]","Shuang Li, Qian Chen, Chulhong Kim, Seongwook Choi, Yibing Wang, Yu Zhang, Changhui Li","{'name': 'Shuang Li, Qian Chen, Chulhong Kim, Seongwook Choi, Yibing Wang, Yu Zhang, Changhui Li'}",,
323,Investigating the Energy Consumption of C++ and Java Solutions Mined from a Programming Contest Site,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Investigating the Energy Consumption of C++ and Java Solutions Mined from a Programming Contest Site'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14877'}]",https://arxiv.org/abs/2412.14877,"arXiv:2412.14877v1 Announce Type: new 
Abstract: The concern about global warming increased the interest in the energy efficiency of computer applications. Assuming power is constant, the general trend is that faster programs consume less energy, thus optimizing a program for speed would also improve its energy efficiency.
  We investigate this tendency in a set of C++ and Java solutions mined from Code Submission Evaluation System (CSES), a popular programming competition site, where each solution must give the correct answer under a given time limit. In such context, we can consider that all correct solutions for a problem were written with a speed concern, but not with energy efficiency in mind.
  We selected 15 problems from CSES and for each of them we mined at least 30 C++ and Java solutions, evaluating time and energy efficiency of each solution in at least two different machines. In our scenario, where there is a great diversity of programming styles, execution speed, and memory usage, we could confirm the general trend: faster programs consume less energy. Moreover, we were able to use ordinary least squares to fit a linear function, with good precision, that relates energy consumption of a program based on its execution time, as well as to automatically identify programs with abnormal energy consumption. A manual analysis of these programs revealed that often they perform a different amount of allocation and deallocation operations when compared to programs with similar execution times.
  We also calculated the energy consumption profile of sets of random C++ solutions for these 15 CSES problems, and we tried to associate each set with its corresponding CSES problem by using the energy consumption profiles previously computed for each one of them. With this approach, we restricted, for each set of random C++ solutions, the classification task to a subset of 7 CSES problems, a reduction of more than 50% in the search space.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14877v1 Announce Type: new \nAbstract: The concern about global warming increased the interest in the energy efficiency of computer applications. Assuming power is constant, the general trend is that faster programs consume less energy, thus optimizing a program for speed would also improve its energy efficiency.\n  We investigate this tendency in a set of C++ and Java solutions mined from Code Submission Evaluation System (CSES), a popular programming competition site, where each solution must give the correct answer under a given time limit. In such context, we can consider that all correct solutions for a problem were written with a speed concern, but not with energy efficiency in mind.\n  We selected 15 problems from CSES and for each of them we mined at least 30 C++ and Java solutions, evaluating time and energy efficiency of each solution in at least two different machines. In our scenario, where there is a great diversity of programming styles, execution speed, and memory usage, we could confirm the general trend: faster programs consume less energy. Moreover, we were able to use ordinary least squares to fit a linear function, with good precision, that relates energy consumption of a program based on its execution time, as well as to automatically identify programs with abnormal energy consumption. A manual analysis of these programs revealed that often they perform a different amount of allocation and deallocation operations when compared to programs with similar execution times.\n  We also calculated the energy consumption profile of sets of random C++ solutions for these 15 CSES problems, and we tried to associate each set with its corresponding CSES problem by using the energy consumption profiles previously computed for each one of them. With this approach, we restricted, for each set of random C++ solutions, the classification task to a subset of 7 CSES problems, a reduction of more than 50% in the search space.'}",oai:arXiv.org:2412.14877v1,False,"[{'term': 'cs.PL', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': ""S\\'ergio Queiroz de Medeiros, Marcelo Borges Nogueira, Gustavo Quezado""}]","S\'ergio Queiroz de Medeiros, Marcelo Borges Nogueira, Gustavo Quezado","{'name': ""S\\'ergio Queiroz de Medeiros, Marcelo Borges Nogueira, Gustavo Quezado""}",,
324,Multimodal Hypothetical Summary for Retrieval-based Multi-image Question Answering,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Multimodal Hypothetical Summary for Retrieval-based Multi-image Question Answering'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14880'}]",https://arxiv.org/abs/2412.14880,"arXiv:2412.14880v1 Announce Type: new 
Abstract: Retrieval-based multi-image question answering (QA) task involves retrieving multiple question-related images and synthesizing these images to generate an answer. Conventional ""retrieve-then-answer"" pipelines often suffer from cascading errors because the training objective of QA fails to optimize the retrieval stage. To address this issue, we propose a novel method to effectively introduce and reference retrieved information into the QA. Given the image set to be retrieved, we employ a multimodal large language model (visual perspective) and a large language model (textual perspective) to obtain multimodal hypothetical summary in question-form and description-form. By combining visual and textual perspectives, MHyS captures image content more specifically and replaces real images in retrieval, which eliminates the modality gap by transforming into text-to-text retrieval and helps improve retrieval. To more advantageously introduce retrieval with QA, we employ contrastive learning to align queries (questions) with MHyS. Moreover, we propose a coarse-to-fine strategy for calculating both sentence-level and word-level similarity scores, to further enhance retrieval and filter out irrelevant details. Our approach achieves a 3.7% absolute improvement over state-of-the-art methods on RETVQA and a 14.5% improvement over CLIP. Comprehensive experiments and detailed ablation studies demonstrate the superiority of our method.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14880v1 Announce Type: new \nAbstract: Retrieval-based multi-image question answering (QA) task involves retrieving multiple question-related images and synthesizing these images to generate an answer. Conventional ""retrieve-then-answer"" pipelines often suffer from cascading errors because the training objective of QA fails to optimize the retrieval stage. To address this issue, we propose a novel method to effectively introduce and reference retrieved information into the QA. Given the image set to be retrieved, we employ a multimodal large language model (visual perspective) and a large language model (textual perspective) to obtain multimodal hypothetical summary in question-form and description-form. By combining visual and textual perspectives, MHyS captures image content more specifically and replaces real images in retrieval, which eliminates the modality gap by transforming into text-to-text retrieval and helps improve retrieval. To more advantageously introduce retrieval with QA, we employ contrastive learning to align queries (questions) with MHyS. Moreover, we propose a coarse-to-fine strategy for calculating both sentence-level and word-level similarity scores, to further enhance retrieval and filter out irrelevant details. Our approach achieves a 3.7% absolute improvement over state-of-the-art methods on RETVQA and a 14.5% improvement over CLIP. Comprehensive experiments and detailed ablation studies demonstrate the superiority of our method.'}",oai:arXiv.org:2412.14880v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Peize Li, Qingyi Si, Peng Fu, Zheng Lin, Yan Wang'}]","Peize Li, Qingyi Si, Peng Fu, Zheng Lin, Yan Wang","{'name': 'Peize Li, Qingyi Si, Peng Fu, Zheng Lin, Yan Wang'}",,
325,Le chameau et le serpent rentrent dans un bar : v\'erification quasi-automatique de code OCaml en logique de s\'eparation,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""Le chameau et le serpent rentrent dans un bar : v\\'erification quasi-automatique de code OCaml en logique de s\\'eparation""}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14894'}]",https://arxiv.org/abs/2412.14894,"arXiv:2412.14894v1 Announce Type: new 
Abstract: This paper presents a translation from Gospel-annotated OCaml programs into Viper, an intermediate verification language featuring Separation Logic. The practical goal is to extend Cameleer with a new back-end to prove heap-dependent OCaml programs. The logical specification of such OCaml programs is described using an extension of Gospel to support Separation Logic features, which we describe in the paper.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14894v1 Announce Type: new \nAbstract: This paper presents a translation from Gospel-annotated OCaml programs into Viper, an intermediate verification language featuring Separation Logic. The practical goal is to extend Cameleer with a new back-end to prove heap-dependent OCaml programs. The logical specification of such OCaml programs is described using an extension of Gospel to support Separation Logic features, which we describe in the paper.'}",oai:arXiv.org:2412.14894v1,False,"[{'term': 'cs.LO', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': ""Charl\\`ene Gros, M\\'ario Pereira""}]","Charl\`ene Gros, M\'ario Pereira","{'name': ""Charl\\`ene Gros, M\\'ario Pereira""}",,
326,Diffusion priors for Bayesian 3D reconstruction from incomplete measurements,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Diffusion priors for Bayesian 3D reconstruction from incomplete measurements'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14897'}]",https://arxiv.org/abs/2412.14897,"arXiv:2412.14897v1 Announce Type: new 
Abstract: Many inverse problems are ill-posed and need to be complemented by prior information that restricts the class of admissible models. Bayesian approaches encode this information as prior distributions that impose generic properties on the model such as sparsity, non-negativity or smoothness. However, in case of complex structured models such as images, graphs or three-dimensional (3D) objects,generic prior distributions tend to favor models that differ largely from those observed in the real world. Here we explore the use of diffusion models as priors that are combined with experimental data within a Bayesian framework. We use 3D point clouds to represent 3D objects such as household items or biomolecular complexes formed from proteins and nucleic acids. We train diffusion models that generate coarse-grained 3D structures at a medium resolution and integrate these with incomplete and noisy experimental data. To demonstrate the power of our approach, we focus on the reconstruction of biomolecular assemblies from cryo-electron microscopy (cryo-EM) images, which is an important inverse problem in structural biology. We find that posterior sampling with diffusion model priors allows for 3D reconstruction from very sparse, low-resolution and partial observations.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14897v1 Announce Type: new \nAbstract: Many inverse problems are ill-posed and need to be complemented by prior information that restricts the class of admissible models. Bayesian approaches encode this information as prior distributions that impose generic properties on the model such as sparsity, non-negativity or smoothness. However, in case of complex structured models such as images, graphs or three-dimensional (3D) objects,generic prior distributions tend to favor models that differ largely from those observed in the real world. Here we explore the use of diffusion models as priors that are combined with experimental data within a Bayesian framework. We use 3D point clouds to represent 3D objects such as household items or biomolecular complexes formed from proteins and nucleic acids. We train diffusion models that generate coarse-grained 3D structures at a medium resolution and integrate these with incomplete and noisy experimental data. To demonstrate the power of our approach, we focus on the reconstruction of biomolecular assemblies from cryo-electron microscopy (cryo-EM) images, which is an important inverse problem in structural biology. We find that posterior sampling with diffusion model priors allows for 3D reconstruction from very sparse, low-resolution and partial observations.'}",oai:arXiv.org:2412.14897v1,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Julian L. M\\""obius, Michael Habeck'}]","Julian L. M\""obius, Michael Habeck","{'name': 'Julian L. M\\""obius, Michael Habeck'}",,
327,Vibration-based Full State In-Hand Manipulation of Thin Objects,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Vibration-based Full State In-Hand Manipulation of Thin Objects'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14899'}]",https://arxiv.org/abs/2412.14899,"arXiv:2412.14899v1 Announce Type: new 
Abstract: Robotic hands offer advanced manipulation capabilities, while their complexity and cost often limit their real-world applications. In contrast, simple parallel grippers, though affordable, are restricted to basic tasks like pick-and-place. Recently, a vibration-based mechanism was proposed to augment parallel grippers and enable in-hand manipulation capabilities for thin objects. By utilizing the stick-slip phenomenon, a simple controller was able to drive a grasped object to a desired position. However, due to the underactuated nature of the mechanism, direct control of the object's orientation was not possible. In this letter, we address the challenge of manipulating the entire state of the object. Hence, we present the excitation of a cyclic phenomenon where the object's center-of-mass rotates in a constant radius about the grasping point. With this cyclic motion, we propose an algorithm for manipulating the object to desired states. In addition to a full analytical analysis of the cyclic phenomenon, we propose the use of duty cycle modulation in operating the vibration actuator to provide more accurate manipulation. Finite element analysis, experiments and task demonstrations validate the proposed algorithm.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14899v1 Announce Type: new \nAbstract: Robotic hands offer advanced manipulation capabilities, while their complexity and cost often limit their real-world applications. In contrast, simple parallel grippers, though affordable, are restricted to basic tasks like pick-and-place. Recently, a vibration-based mechanism was proposed to augment parallel grippers and enable in-hand manipulation capabilities for thin objects. By utilizing the stick-slip phenomenon, a simple controller was able to drive a grasped object to a desired position. However, due to the underactuated nature of the mechanism, direct control of the object's orientation was not possible. In this letter, we address the challenge of manipulating the entire state of the object. Hence, we present the excitation of a cyclic phenomenon where the object's center-of-mass rotates in a constant radius about the grasping point. With this cyclic motion, we propose an algorithm for manipulating the object to desired states. In addition to a full analytical analysis of the cyclic phenomenon, we propose the use of duty cycle modulation in operating the vibration actuator to provide more accurate manipulation. Finite element analysis, experiments and task demonstrations validate the proposed algorithm.""}",oai:arXiv.org:2412.14899v1,False,"[{'term': 'cs.RO', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Oron Binyamin, Guy Shapira, Noam Nahum, Avishai Sintov'}]","Oron Binyamin, Guy Shapira, Noam Nahum, Avishai Sintov","{'name': 'Oron Binyamin, Guy Shapira, Noam Nahum, Avishai Sintov'}",,
328,"MagicNaming: Consistent Identity Generation by Finding a ""Name Space"" in T2I Diffusion Models","{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'MagicNaming: Consistent Identity Generation by Finding a ""Name Space"" in T2I Diffusion Models'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14902'}]",https://arxiv.org/abs/2412.14902,"arXiv:2412.14902v1 Announce Type: new 
Abstract: Large-scale text-to-image diffusion models, (e.g., DALL-E, SDXL) are capable of generating famous persons by simply referring to their names. Is it possible to make such models generate generic identities as simple as the famous ones, e.g., just use a name? In this paper, we explore the existence of a ""Name Space"", where any point in the space corresponds to a specific identity. Fortunately, we find some clues in the feature space spanned by text embedding of celebrities' names. Specifically, we first extract the embeddings of celebrities' names in the Laion5B dataset with the text encoder of diffusion models. Such embeddings are used as supervision to learn an encoder that can predict the name (actually an embedding) of a given face image. We experimentally find that such name embeddings work well in promising the generated image with good identity consistency. Note that like the names of celebrities, our predicted name embeddings are disentangled from the semantics of text inputs, making the original generation capability of text-to-image models well-preserved. Moreover, by simply plugging such name embeddings, all variants (e.g., from Civitai) derived from the same base model (i.e., SDXL) readily become identity-aware text-to-image models. Project homepage: \url{https://magicfusion.github.io/MagicNaming/}.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14902v1 Announce Type: new \nAbstract: Large-scale text-to-image diffusion models, (e.g., DALL-E, SDXL) are capable of generating famous persons by simply referring to their names. Is it possible to make such models generate generic identities as simple as the famous ones, e.g., just use a name? In this paper, we explore the existence of a ""Name Space"", where any point in the space corresponds to a specific identity. Fortunately, we find some clues in the feature space spanned by text embedding of celebrities\' names. Specifically, we first extract the embeddings of celebrities\' names in the Laion5B dataset with the text encoder of diffusion models. Such embeddings are used as supervision to learn an encoder that can predict the name (actually an embedding) of a given face image. We experimentally find that such name embeddings work well in promising the generated image with good identity consistency. Note that like the names of celebrities, our predicted name embeddings are disentangled from the semantics of text inputs, making the original generation capability of text-to-image models well-preserved. Moreover, by simply plugging such name embeddings, all variants (e.g., from Civitai) derived from the same base model (i.e., SDXL) readily become identity-aware text-to-image models. Project homepage: \\url{https://magicfusion.github.io/MagicNaming/}.'}",oai:arXiv.org:2412.14902v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Jing Zhao, Heliang Zheng, Chaoyue Wang, Long Lan, Wanrong Hunag, Yuhua Tang'}]","Jing Zhao, Heliang Zheng, Chaoyue Wang, Long Lan, Wanrong Hunag, Yuhua Tang","{'name': 'Jing Zhao, Heliang Zheng, Chaoyue Wang, Long Lan, Wanrong Hunag, Yuhua Tang'}",,AAAI 2025
329,Dehallucinating Parallel Context Extension for Retrieval-Augmented Generation,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Dehallucinating Parallel Context Extension for Retrieval-Augmented Generation'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14905'}]",https://arxiv.org/abs/2412.14905,"arXiv:2412.14905v1 Announce Type: new 
Abstract: Large language models (LLMs) are susceptible to generating hallucinated information, despite the integration of retrieval-augmented generation (RAG). Parallel context extension (PCE) is a line of research attempting to effectively integrating parallel (unordered) contexts, while it still suffers from hallucinations when adapted to RAG scenarios. In this paper, we propose DePaC (Dehallucinating Parallel Context Extension), which alleviates the hallucination problem with context-aware negative training and information-calibrated aggregation. DePaC is designed to alleviate two types of in-context hallucination: fact fabrication (i.e., LLMs present claims that are not supported by the contexts) and fact omission (i.e., LLMs fail to present claims that can be supported by the contexts). Specifically, (1) for fact fabrication, we apply the context-aware negative training that fine-tunes the LLMs with negative supervisions, thus explicitly guiding the LLMs to refuse to answer when contexts are not related to questions; (2) for fact omission, we propose the information-calibrated aggregation which prioritizes context windows with higher information increment from their contexts. The experimental results on nine RAG tasks demonstrate that DePaC significantly alleviates the two types of hallucination and consistently achieves better performances on these tasks.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14905v1 Announce Type: new \nAbstract: Large language models (LLMs) are susceptible to generating hallucinated information, despite the integration of retrieval-augmented generation (RAG). Parallel context extension (PCE) is a line of research attempting to effectively integrating parallel (unordered) contexts, while it still suffers from hallucinations when adapted to RAG scenarios. In this paper, we propose DePaC (Dehallucinating Parallel Context Extension), which alleviates the hallucination problem with context-aware negative training and information-calibrated aggregation. DePaC is designed to alleviate two types of in-context hallucination: fact fabrication (i.e., LLMs present claims that are not supported by the contexts) and fact omission (i.e., LLMs fail to present claims that can be supported by the contexts). Specifically, (1) for fact fabrication, we apply the context-aware negative training that fine-tunes the LLMs with negative supervisions, thus explicitly guiding the LLMs to refuse to answer when contexts are not related to questions; (2) for fact omission, we propose the information-calibrated aggregation which prioritizes context windows with higher information increment from their contexts. The experimental results on nine RAG tasks demonstrate that DePaC significantly alleviates the two types of hallucination and consistently achieves better performances on these tasks.'}",oai:arXiv.org:2412.14905v1,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/publicdomain/zero/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/publicdomain/zero/1.0/'}","[{'name': 'Zexiong Ma, Shengnan An, Zeqi Lin, Yanzhen Zou, Jian-Guang Lou, Bing Xie'}]","Zexiong Ma, Shengnan An, Zeqi Lin, Yanzhen Zou, Jian-Guang Lou, Bing Xie","{'name': 'Zexiong Ma, Shengnan An, Zeqi Lin, Yanzhen Zou, Jian-Guang Lou, Bing Xie'}",,
330,RobustFT: Robust Supervised Fine-tuning for Large Language Models under Noisy Response,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'RobustFT: Robust Supervised Fine-tuning for Large Language Models under Noisy Response'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14922'}]",https://arxiv.org/abs/2412.14922,"arXiv:2412.14922v1 Announce Type: new 
Abstract: Supervised fine-tuning (SFT) plays a crucial role in adapting large language models (LLMs) to specific domains or tasks. However, as demonstrated by empirical experiments, the collected data inevitably contains noise in practical applications, which poses significant challenges to model performance on downstream tasks. Therefore, there is an urgent need for a noise-robust SFT framework to enhance model capabilities in downstream tasks. To address this challenge, we introduce a robust SFT framework (RobustFT) that performs noise detection and relabeling on downstream task data. For noise identification, our approach employs a multi-expert collaborative system with inference-enhanced models to achieve superior noise detection. In the denoising phase, we utilize a context-enhanced strategy, which incorporates the most relevant and confident knowledge followed by careful assessment to generate reliable annotations. Additionally, we introduce an effective data selection mechanism based on response entropy, ensuring only high-quality samples are retained for fine-tuning. Extensive experiments conducted on multiple LLMs across five datasets demonstrate RobustFT's exceptional performance in noisy scenarios.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14922v1 Announce Type: new \nAbstract: Supervised fine-tuning (SFT) plays a crucial role in adapting large language models (LLMs) to specific domains or tasks. However, as demonstrated by empirical experiments, the collected data inevitably contains noise in practical applications, which poses significant challenges to model performance on downstream tasks. Therefore, there is an urgent need for a noise-robust SFT framework to enhance model capabilities in downstream tasks. To address this challenge, we introduce a robust SFT framework (RobustFT) that performs noise detection and relabeling on downstream task data. For noise identification, our approach employs a multi-expert collaborative system with inference-enhanced models to achieve superior noise detection. In the denoising phase, we utilize a context-enhanced strategy, which incorporates the most relevant and confident knowledge followed by careful assessment to generate reliable annotations. Additionally, we introduce an effective data selection mechanism based on response entropy, ensuring only high-quality samples are retained for fine-tuning. Extensive experiments conducted on multiple LLMs across five datasets demonstrate RobustFT's exceptional performance in noisy scenarios.""}",oai:arXiv.org:2412.14922v1,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Junyu Luo, Xiao Luo, Kaize Ding, Jingyang Yuan, Zhiping Xiao, Ming Zhang'}]","Junyu Luo, Xiao Luo, Kaize Ding, Jingyang Yuan, Zhiping Xiao, Ming Zhang","{'name': 'Junyu Luo, Xiao Luo, Kaize Ding, Jingyang Yuan, Zhiping Xiao, Ming Zhang'}",,
331,"Automatic Spectral Calibration of Hyperspectral Images:Method, Dataset and Benchmark","{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Automatic Spectral Calibration of Hyperspectral Images:Method, Dataset and Benchmark'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14925'}]",https://arxiv.org/abs/2412.14925,"arXiv:2412.14925v1 Announce Type: new 
Abstract: Hyperspectral image (HSI) densely samples the world in both the space and frequency domain and therefore is more distinctive than RGB images. Usually, HSI needs to be calibrated to minimize the impact of various illumination conditions. The traditional way to calibrate HSI utilizes a physical reference, which involves manual operations, occlusions, and/or limits camera mobility. These limitations inspire this paper to automatically calibrate HSIs using a learning-based method. Towards this goal, a large-scale HSI calibration dataset is created, which has 765 high-quality HSI pairs covering diversified natural scenes and illuminations. The dataset is further expanded to 7650 pairs by combining with 10 different physically measured illuminations. A spectral illumination transformer (SIT) together with an illumination attention module is proposed. Extensive benchmarks demonstrate the SoTA performance of the proposed SIT. The benchmarks also indicate that low-light conditions are more challenging than normal conditions. The dataset and codes are available online:https://github.com/duranze/Automatic-spectral-calibration-of-HSI","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14925v1 Announce Type: new \nAbstract: Hyperspectral image (HSI) densely samples the world in both the space and frequency domain and therefore is more distinctive than RGB images. Usually, HSI needs to be calibrated to minimize the impact of various illumination conditions. The traditional way to calibrate HSI utilizes a physical reference, which involves manual operations, occlusions, and/or limits camera mobility. These limitations inspire this paper to automatically calibrate HSIs using a learning-based method. Towards this goal, a large-scale HSI calibration dataset is created, which has 765 high-quality HSI pairs covering diversified natural scenes and illuminations. The dataset is further expanded to 7650 pairs by combining with 10 different physically measured illuminations. A spectral illumination transformer (SIT) together with an illumination attention module is proposed. Extensive benchmarks demonstrate the SoTA performance of the proposed SIT. The benchmarks also indicate that low-light conditions are more challenging than normal conditions. The dataset and codes are available online:https://github.com/duranze/Automatic-spectral-calibration-of-HSI'}",oai:arXiv.org:2412.14925v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'eess.IV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Zhuoran Du, Shaodi You, Cheng Cheng, Shikui Wei'}]","Zhuoran Du, Shaodi You, Cheng Cheng, Shikui Wei","{'name': 'Zhuoran Du, Shaodi You, Cheng Cheng, Shikui Wei'}",,
332,Cirbo: A New Tool for Boolean Circuit Analysis and Synthesis,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Cirbo: A New Tool for Boolean Circuit Analysis and Synthesis'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14933'}]",https://arxiv.org/abs/2412.14933,"arXiv:2412.14933v1 Announce Type: new 
Abstract: We present an open-source tool for manipulating Boolean circuits. It implements efficient algorithms, both existing and novel, for a rich variety of frequently used circuit tasks such as satisfiability, synthesis, and minimization. We tested the tool on a wide range of practically relevant circuits (computing, in particular, symmetric and arithmetic functions) that have been optimized intensively by the community for the last three years. The tool helped us to win the IWLS 2024 Programming Contest. In 2023, it was Google DeepMind who took the first place in the competition. We were able to reduce the size of the best circuits from 2023 by 12\% on average, whereas for some individual circuits, our size reduction was as large as 83\%.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14933v1 Announce Type: new \nAbstract: We present an open-source tool for manipulating Boolean circuits. It implements efficient algorithms, both existing and novel, for a rich variety of frequently used circuit tasks such as satisfiability, synthesis, and minimization. We tested the tool on a wide range of practically relevant circuits (computing, in particular, symmetric and arithmetic functions) that have been optimized intensively by the community for the last three years. The tool helped us to win the IWLS 2024 Programming Contest. In 2023, it was Google DeepMind who took the first place in the competition. We were able to reduce the size of the best circuits from 2023 by 12\\% on average, whereas for some individual circuits, our size reduction was as large as 83\\%.'}",oai:arXiv.org:2412.14933v1,False,"[{'term': 'cs.LO', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Daniil Averkov, Tatiana Belova, Gregory Emdin, Mikhail Goncharov, Viktoriia Krivogornitsyna, Alexander S. Kulikov, Fedor Kurmazov, Daniil Levtsov, Georgie Levtsov, Vsevolod Vaskin, Aleksey Vorobiev'}]","Daniil Averkov, Tatiana Belova, Gregory Emdin, Mikhail Goncharov, Viktoriia Krivogornitsyna, Alexander S. Kulikov, Fedor Kurmazov, Daniil Levtsov, Georgie Levtsov, Vsevolod Vaskin, Aleksey Vorobiev","{'name': 'Daniil Averkov, Tatiana Belova, Gregory Emdin, Mikhail Goncharov, Viktoriia Krivogornitsyna, Alexander S. Kulikov, Fedor Kurmazov, Daniil Levtsov, Georgie Levtsov, Vsevolod Vaskin, Aleksey Vorobiev'}",,
333,Expressivity of AuDaLa: Turing Completeness and Possible Extensions,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Expressivity of AuDaLa: Turing Completeness and Possible Extensions'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14938'}]",https://arxiv.org/abs/2412.14938,"arXiv:2412.14938v1 Announce Type: new 
Abstract: AuDaLa is a recently introduced programming language that follows the new data autonomous paradigm. In this paradigm, small pieces of data execute functions autonomously. Considering the paradigm and the design choices of AuDaLa, it is interesting to determine the expressiveness of the language and to create verification methods for it. In this paper, we implement Turing machines in AuDaLa and prove that implementation correct. This proves that AuDaLa is Turing complete, giving an initial indication of AuDaLa's expressiveness, and by proving the implementation correct this contributes to the creation of verification methods for AuDaLa. Additionally, we give examples of how to add in possible extensions for AuDaLa to increase its expressivity to better match conventional parallel languages, allowing for a smoother and more performant implementation of algorithms.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14938v1 Announce Type: new \nAbstract: AuDaLa is a recently introduced programming language that follows the new data autonomous paradigm. In this paradigm, small pieces of data execute functions autonomously. Considering the paradigm and the design choices of AuDaLa, it is interesting to determine the expressiveness of the language and to create verification methods for it. In this paper, we implement Turing machines in AuDaLa and prove that implementation correct. This proves that AuDaLa is Turing complete, giving an initial indication of AuDaLa's expressiveness, and by proving the implementation correct this contributes to the creation of verification methods for AuDaLa. Additionally, we give examples of how to add in possible extensions for AuDaLa to increase its expressivity to better match conventional parallel languages, allowing for a smoother and more performant implementation of algorithms.""}",oai:arXiv.org:2412.14938v1,False,"[{'term': 'cs.LO', 'scheme': None, 'label': None}, {'term': 'cs.PL', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Tom T. P. Franken, Thomas Neele'}]","Tom T. P. Franken, Thomas Neele","{'name': 'Tom T. P. Franken, Thomas Neele'}",,
334,GURecon: Learning Detailed 3D Geometric Uncertainties for Neural Surface Reconstruction,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'GURecon: Learning Detailed 3D Geometric Uncertainties for Neural Surface Reconstruction'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14939'}]",https://arxiv.org/abs/2412.14939,"arXiv:2412.14939v1 Announce Type: new 
Abstract: Neural surface representation has demonstrated remarkable success in the areas of novel view synthesis and 3D reconstruction. However, assessing the geometric quality of 3D reconstructions in the absence of ground truth mesh remains a significant challenge, due to its rendering-based optimization process and entangled learning of appearance and geometry with photometric losses. In this paper, we present a novel framework, i.e, GURecon, which establishes a geometric uncertainty field for the neural surface based on geometric consistency. Different from existing methods that rely on rendering-based measurement, GURecon models a continuous 3D uncertainty field for the reconstructed surface, and is learned by an online distillation approach without introducing real geometric information for supervision. Moreover, in order to mitigate the interference of illumination on geometric consistency, a decoupled field is learned and exploited to finetune the uncertainty field. Experiments on various datasets demonstrate the superiority of GURecon in modeling 3D geometric uncertainty, as well as its plug-and-play extension to various neural surface representations and improvement on downstream tasks such as incremental reconstruction. The code and supplementary material are available on the project website: https://zju3dv.github.io/GURecon/.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14939v1 Announce Type: new \nAbstract: Neural surface representation has demonstrated remarkable success in the areas of novel view synthesis and 3D reconstruction. However, assessing the geometric quality of 3D reconstructions in the absence of ground truth mesh remains a significant challenge, due to its rendering-based optimization process and entangled learning of appearance and geometry with photometric losses. In this paper, we present a novel framework, i.e, GURecon, which establishes a geometric uncertainty field for the neural surface based on geometric consistency. Different from existing methods that rely on rendering-based measurement, GURecon models a continuous 3D uncertainty field for the reconstructed surface, and is learned by an online distillation approach without introducing real geometric information for supervision. Moreover, in order to mitigate the interference of illumination on geometric consistency, a decoupled field is learned and exploited to finetune the uncertainty field. Experiments on various datasets demonstrate the superiority of GURecon in modeling 3D geometric uncertainty, as well as its plug-and-play extension to various neural surface representations and improvement on downstream tasks such as incremental reconstruction. The code and supplementary material are available on the project website: https://zju3dv.github.io/GURecon/.'}",oai:arXiv.org:2412.14939v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Zesong Yang, Ru Zhang, Jiale Shi, Zixiang Ai, Boming Zhao, Hujun Bao, Luwei Yang, Zhaopeng Cui'}]","Zesong Yang, Ru Zhang, Jiale Shi, Zixiang Ai, Boming Zhao, Hujun Bao, Luwei Yang, Zhaopeng Cui","{'name': 'Zesong Yang, Ru Zhang, Jiale Shi, Zixiang Ai, Boming Zhao, Hujun Bao, Luwei Yang, Zhaopeng Cui'}",,
335,Unveiling social vibrancy in urban spaces with app usage,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Unveiling social vibrancy in urban spaces with app usage'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14943'}]",https://arxiv.org/abs/2412.14943,"arXiv:2412.14943v1 Announce Type: new 
Abstract: Urban vibrancy is an important measure of the energetic nature of a city that is related to why and how people use urban spaces, and it is inherently connected with our social behaviour. Increasingly, people use a wide range of mobile phone apps in their daily lives to connect socially, search for information, make decisions, and arrange travel, amongst many other reasons. However, the relationship between online app usage and urban vibrancy remains unclear, particularly regarding how sociospatial behaviours interact with urban features. Here, we use app-usage data as a digital signature to investigate this question. To do this, we use a high-resolution data source of mobile service-level traffic volumes across eighteen cities in France. We investigate the social component of cities using socially relevant urban features constructed from OpenStreetMap 'Points of Interest'. We developed a methodology for identifying and classifying multidimensional app usage time series based on similarity. We used these in predictive models to interpret the results for each city and across France. Across cities, there were spatial behavioural archetypes, characterised by multidimensional properties. We found patterns between the week and the weekend, and across cities, and the country. These archetypes correspond to changes in socially relevant urban features that impact urban vibrancy. Our results add further evidence for the importance of using computational approaches to understand urban environments, the use of sociological concepts in computational science, and urban vibrancy in cities.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14943v1 Announce Type: new \nAbstract: Urban vibrancy is an important measure of the energetic nature of a city that is related to why and how people use urban spaces, and it is inherently connected with our social behaviour. Increasingly, people use a wide range of mobile phone apps in their daily lives to connect socially, search for information, make decisions, and arrange travel, amongst many other reasons. However, the relationship between online app usage and urban vibrancy remains unclear, particularly regarding how sociospatial behaviours interact with urban features. Here, we use app-usage data as a digital signature to investigate this question. To do this, we use a high-resolution data source of mobile service-level traffic volumes across eighteen cities in France. We investigate the social component of cities using socially relevant urban features constructed from OpenStreetMap 'Points of Interest'. We developed a methodology for identifying and classifying multidimensional app usage time series based on similarity. We used these in predictive models to interpret the results for each city and across France. Across cities, there were spatial behavioural archetypes, characterised by multidimensional properties. We found patterns between the week and the weekend, and across cities, and the country. These archetypes correspond to changes in socially relevant urban features that impact urban vibrancy. Our results add further evidence for the importance of using computational approaches to understand urban environments, the use of sociological concepts in computational science, and urban vibrancy in cities.""}",oai:arXiv.org:2412.14943v1,False,"[{'term': 'cs.CY', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Thomas Collins, Diogo Pacheco, Riccardo Di Clemente, Federico Botta'}]","Thomas Collins, Diogo Pacheco, Riccardo Di Clemente, Federico Botta","{'name': 'Thomas Collins, Diogo Pacheco, Riccardo Di Clemente, Federico Botta'}",,
336,Generalizing Constraint Models in Constraint Acquisition,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Generalizing Constraint Models in Constraint Acquisition'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14950'}]",https://arxiv.org/abs/2412.14950,"arXiv:2412.14950v1 Announce Type: new 
Abstract: Constraint Acquisition (CA) aims to widen the use of constraint programming by assisting users in the modeling process. However, most CA methods suffer from a significant drawback: they learn a single set of individual constraints for a specific problem instance, but cannot generalize these constraints to the parameterized constraint specifications of the problem. In this paper, we address this limitation by proposing GenCon, a novel approach to learn parameterized constraint models capable of modeling varying instances of the same problem. To achieve this generalization, we make use of statistical learning techniques at the level of individual constraints. Specifically, we propose to train a classifier to predict, for any possible constraint and parameterization, whether the constraint belongs to the problem. We then show how, for some classes of classifiers, we can extract decision rules to construct interpretable constraint specifications. This enables the generation of ground constraints for any parameter instantiation. Additionally, we present a generate-and-test approach that can be used with any classifier, to generate the ground constraints on the fly. Our empirical results demonstrate that our approach achieves high accuracy and is robust to noise in the input instances.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14950v1 Announce Type: new \nAbstract: Constraint Acquisition (CA) aims to widen the use of constraint programming by assisting users in the modeling process. However, most CA methods suffer from a significant drawback: they learn a single set of individual constraints for a specific problem instance, but cannot generalize these constraints to the parameterized constraint specifications of the problem. In this paper, we address this limitation by proposing GenCon, a novel approach to learn parameterized constraint models capable of modeling varying instances of the same problem. To achieve this generalization, we make use of statistical learning techniques at the level of individual constraints. Specifically, we propose to train a classifier to predict, for any possible constraint and parameterization, whether the constraint belongs to the problem. We then show how, for some classes of classifiers, we can extract decision rules to construct interpretable constraint specifications. This enables the generation of ground constraints for any parameter instantiation. Additionally, we present a generate-and-test approach that can be used with any classifier, to generate the ground constraints on the fly. Our empirical results demonstrate that our approach achieves high accuracy and is robust to noise in the input instances.'}",oai:arXiv.org:2412.14950v1,False,"[{'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Dimos Tsouros, Senne Berden, Steven Prestwich, Tias Guns'}]","Dimos Tsouros, Senne Berden, Steven Prestwich, Tias Guns","{'name': 'Dimos Tsouros, Senne Berden, Steven Prestwich, Tias Guns'}",,
337,Corn Ear Detection and Orientation Estimation Using Deep Learning,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Corn Ear Detection and Orientation Estimation Using Deep Learning'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14954'}]",https://arxiv.org/abs/2412.14954,"arXiv:2412.14954v1 Announce Type: new 
Abstract: Monitoring growth behavior of maize plants such as the development of ears can give key insights into the plant's health and development. Traditionally, the measurement of the angle of ears is performed manually, which can be time-consuming and prone to human error. To address these challenges, this paper presents a computer vision-based system for detecting and tracking ears of corn in an image sequence. The proposed system could accurately detect, track, and predict the ear's orientation, which can be useful in monitoring their growth behavior. This can significantly save time compared to manual measurement and enables additional areas of ear orientation research and potential increase in efficiencies for maize production. Using an object detector with keypoint detection, the algorithm proposed could detect 90 percent of all ears. The cardinal estimation had a mean absolute error (MAE) of 18 degrees, compared to a mean 15 degree difference between two people measuring by hand. These results demonstrate the feasibility of using computer vision techniques for monitoring maize growth and can lead to further research in this area.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14954v1 Announce Type: new \nAbstract: Monitoring growth behavior of maize plants such as the development of ears can give key insights into the plant's health and development. Traditionally, the measurement of the angle of ears is performed manually, which can be time-consuming and prone to human error. To address these challenges, this paper presents a computer vision-based system for detecting and tracking ears of corn in an image sequence. The proposed system could accurately detect, track, and predict the ear's orientation, which can be useful in monitoring their growth behavior. This can significantly save time compared to manual measurement and enables additional areas of ear orientation research and potential increase in efficiencies for maize production. Using an object detector with keypoint detection, the algorithm proposed could detect 90 percent of all ears. The cardinal estimation had a mean absolute error (MAE) of 18 degrees, compared to a mean 15 degree difference between two people measuring by hand. These results demonstrate the feasibility of using computer vision techniques for monitoring maize growth and can lead to further research in this area.""}",oai:arXiv.org:2412.14954v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Nathan Sprague, John Evans, Michael Mardikes'}]","Nathan Sprague, John Evans, Michael Mardikes","{'name': 'Nathan Sprague, John Evans, Michael Mardikes'}",,
338,Dream to Manipulate: Compositional World Models Empowering Robot Imitation Learning with Imagination,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Dream to Manipulate: Compositional World Models Empowering Robot Imitation Learning with Imagination'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14957'}]",https://arxiv.org/abs/2412.14957,"arXiv:2412.14957v1 Announce Type: new 
Abstract: A world model provides an agent with a representation of its environment, enabling it to predict the causal consequences of its actions. Current world models typically cannot directly and explicitly imitate the actual environment in front of a robot, often resulting in unrealistic behaviors and hallucinations that make them unsuitable for real-world applications. In this paper, we introduce a new paradigm for constructing world models that are explicit representations of the real world and its dynamics. By integrating cutting-edge advances in real-time photorealism with Gaussian Splatting and physics simulators, we propose the first compositional manipulation world model, which we call DreMa. DreMa replicates the observed world and its dynamics, allowing it to imagine novel configurations of objects and predict the future consequences of robot actions. We leverage this capability to generate new data for imitation learning by applying equivariant transformations to a small set of demonstrations. Our evaluations across various settings demonstrate significant improvements in both accuracy and robustness by incrementing actions and object distributions, reducing the data needed to learn a policy and improving the generalization of the agents. As a highlight, we show that a real Franka Emika Panda robot, powered by DreMa's imagination, can successfully learn novel physical tasks from just a single example per task variation (one-shot policy learning). Our project page and source code can be found in https://leobarcellona.github.io/DreamToManipulate/","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14957v1 Announce Type: new \nAbstract: A world model provides an agent with a representation of its environment, enabling it to predict the causal consequences of its actions. Current world models typically cannot directly and explicitly imitate the actual environment in front of a robot, often resulting in unrealistic behaviors and hallucinations that make them unsuitable for real-world applications. In this paper, we introduce a new paradigm for constructing world models that are explicit representations of the real world and its dynamics. By integrating cutting-edge advances in real-time photorealism with Gaussian Splatting and physics simulators, we propose the first compositional manipulation world model, which we call DreMa. DreMa replicates the observed world and its dynamics, allowing it to imagine novel configurations of objects and predict the future consequences of robot actions. We leverage this capability to generate new data for imitation learning by applying equivariant transformations to a small set of demonstrations. Our evaluations across various settings demonstrate significant improvements in both accuracy and robustness by incrementing actions and object distributions, reducing the data needed to learn a policy and improving the generalization of the agents. As a highlight, we show that a real Franka Emika Panda robot, powered by DreMa's imagination, can successfully learn novel physical tasks from just a single example per task variation (one-shot policy learning). Our project page and source code can be found in https://leobarcellona.github.io/DreamToManipulate/""}",oai:arXiv.org:2412.14957v1,False,"[{'term': 'cs.RO', 'scheme': None, 'label': None}, {'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Leonardo Barcellona, Andrii Zadaianchuk, Davide Allegro, Samuele Papa, Stefano Ghidoni, Efstratios Gavves'}]","Leonardo Barcellona, Andrii Zadaianchuk, Davide Allegro, Samuele Papa, Stefano Ghidoni, Efstratios Gavves","{'name': 'Leonardo Barcellona, Andrii Zadaianchuk, Davide Allegro, Samuele Papa, Stefano Ghidoni, Efstratios Gavves'}",,
339,Understanding the Dark Side of LLMs' Intrinsic Self-Correction,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""Understanding the Dark Side of LLMs' Intrinsic Self-Correction""}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14959'}]",https://arxiv.org/abs/2412.14959,"arXiv:2412.14959v1 Announce Type: new 
Abstract: Intrinsic self-correction was proposed to improve LLMs' responses via feedback prompts solely based on their inherent capability. However, recent works show that LLMs' intrinsic self-correction fails without oracle labels as feedback prompts. In this paper, we aim to interpret LLMs' intrinsic self-correction for different tasks, especially for those failure cases. By including one simple task and three complex tasks with state-of-the-art (SOTA) LLMs like ChatGPT families (o1, 4o, 3.5-turbo) and Llama families (2-7B, 3-8B, and 3.1-8B), we design three interpretation methods to reveal the dark side of LLMs' intrinsic self-correction. We identify intrinsic self-correction can (1) cause LLMs to waver both intermedia and final answers and lead to prompt bias on simple factual questions; (2) introduce human-like cognitive bias on complex tasks. In light of our findings, we also provide two simple yet effective strategies for alleviation: question repeating and supervised fine-tuning with a few samples. We open-source our work at https://x-isc.info/.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14959v1 Announce Type: new \nAbstract: Intrinsic self-correction was proposed to improve LLMs' responses via feedback prompts solely based on their inherent capability. However, recent works show that LLMs' intrinsic self-correction fails without oracle labels as feedback prompts. In this paper, we aim to interpret LLMs' intrinsic self-correction for different tasks, especially for those failure cases. By including one simple task and three complex tasks with state-of-the-art (SOTA) LLMs like ChatGPT families (o1, 4o, 3.5-turbo) and Llama families (2-7B, 3-8B, and 3.1-8B), we design three interpretation methods to reveal the dark side of LLMs' intrinsic self-correction. We identify intrinsic self-correction can (1) cause LLMs to waver both intermedia and final answers and lead to prompt bias on simple factual questions; (2) introduce human-like cognitive bias on complex tasks. In light of our findings, we also provide two simple yet effective strategies for alleviation: question repeating and supervised fine-tuning with a few samples. We open-source our work at https://x-isc.info/.""}",oai:arXiv.org:2412.14959v1,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by-nc-nd/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-nd/4.0/'}","[{'name': 'Qingjie Zhang, Han Qiu, Di Wang, Haoting Qian, Yiming Li, Tianwei Zhang, Minlie Huang'}]","Qingjie Zhang, Han Qiu, Di Wang, Haoting Qian, Yiming Li, Tianwei Zhang, Minlie Huang","{'name': 'Qingjie Zhang, Han Qiu, Di Wang, Haoting Qian, Yiming Li, Tianwei Zhang, Minlie Huang'}",,
340,TDCNet: Transparent Objects Depth Completion with CNN-Transformer Dual-Branch Parallel Network,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'TDCNet: Transparent Objects Depth Completion with CNN-Transformer Dual-Branch Parallel Network'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14961'}]",https://arxiv.org/abs/2412.14961,"arXiv:2412.14961v1 Announce Type: new 
Abstract: The sensing and manipulation of transparent objects present a critical challenge in industrial and laboratory robotics. Conventional sensors face challenges in obtaining the full depth of transparent objects due to the refraction and reflection of light on their surfaces and their lack of visible texture. Previous research has attempted to obtain complete depth maps of transparent objects from RGB and damaged depth maps (collected by depth sensor) using deep learning models. However, existing methods fail to fully utilize the original depth map, resulting in limited accuracy for deep completion. To solve this problem, we propose TDCNet, a novel dual-branch CNN-Transformer parallel network for transparent object depth completion. The proposed framework consists of two different branches: one extracts features from partial depth maps, while the other processes RGB-D images. Experimental results demonstrate that our model achieves state-of-the-art performance across multiple public datasets. Our code and the pre-trained model are publicly available at https://github.com/XianghuiFan/TDCNet.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14961v1 Announce Type: new \nAbstract: The sensing and manipulation of transparent objects present a critical challenge in industrial and laboratory robotics. Conventional sensors face challenges in obtaining the full depth of transparent objects due to the refraction and reflection of light on their surfaces and their lack of visible texture. Previous research has attempted to obtain complete depth maps of transparent objects from RGB and damaged depth maps (collected by depth sensor) using deep learning models. However, existing methods fail to fully utilize the original depth map, resulting in limited accuracy for deep completion. To solve this problem, we propose TDCNet, a novel dual-branch CNN-Transformer parallel network for transparent object depth completion. The proposed framework consists of two different branches: one extracts features from partial depth maps, while the other processes RGB-D images. Experimental results demonstrate that our model achieves state-of-the-art performance across multiple public datasets. Our code and the pre-trained model are publicly available at https://github.com/XianghuiFan/TDCNet.'}",oai:arXiv.org:2412.14961v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Xianghui Fan, Chao Ye, Anping Deng, Xiaotian Wu, Mengyang Pan, Hang Yang'}]","Xianghui Fan, Chao Ye, Anping Deng, Xiaotian Wu, Mengyang Pan, Hang Yang","{'name': 'Xianghui Fan, Chao Ye, Anping Deng, Xiaotian Wu, Mengyang Pan, Hang Yang'}",,
341,IDOL: Instant Photorealistic 3D Human Creation from a Single Image,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'IDOL: Instant Photorealistic 3D Human Creation from a Single Image'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14963'}]",https://arxiv.org/abs/2412.14963,"arXiv:2412.14963v1 Announce Type: new 
Abstract: Creating a high-fidelity, animatable 3D full-body avatar from a single image is a challenging task due to the diverse appearance and poses of humans and the limited availability of high-quality training data. To achieve fast and high-quality human reconstruction, this work rethinks the task from the perspectives of dataset, model, and representation. First, we introduce a large-scale HUman-centric GEnerated dataset, HuGe100K, consisting of 100K diverse, photorealistic sets of human images. Each set contains 24-view frames in specific human poses, generated using a pose-controllable image-to-multi-view model. Next, leveraging the diversity in views, poses, and appearances within HuGe100K, we develop a scalable feed-forward transformer model to predict a 3D human Gaussian representation in a uniform space from a given human image. This model is trained to disentangle human pose, body shape, clothing geometry, and texture. The estimated Gaussians can be animated without post-processing. We conduct comprehensive experiments to validate the effectiveness of the proposed dataset and method. Our model demonstrates the ability to efficiently reconstruct photorealistic humans at 1K resolution from a single input image using a single GPU instantly. Additionally, it seamlessly supports various applications, as well as shape and texture editing tasks.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14963v1 Announce Type: new \nAbstract: Creating a high-fidelity, animatable 3D full-body avatar from a single image is a challenging task due to the diverse appearance and poses of humans and the limited availability of high-quality training data. To achieve fast and high-quality human reconstruction, this work rethinks the task from the perspectives of dataset, model, and representation. First, we introduce a large-scale HUman-centric GEnerated dataset, HuGe100K, consisting of 100K diverse, photorealistic sets of human images. Each set contains 24-view frames in specific human poses, generated using a pose-controllable image-to-multi-view model. Next, leveraging the diversity in views, poses, and appearances within HuGe100K, we develop a scalable feed-forward transformer model to predict a 3D human Gaussian representation in a uniform space from a given human image. This model is trained to disentangle human pose, body shape, clothing geometry, and texture. The estimated Gaussians can be animated without post-processing. We conduct comprehensive experiments to validate the effectiveness of the proposed dataset and method. Our model demonstrates the ability to efficiently reconstruct photorealistic humans at 1K resolution from a single input image using a single GPU instantly. Additionally, it seamlessly supports various applications, as well as shape and texture editing tasks.'}",oai:arXiv.org:2412.14963v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.GR', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by-nc-sa/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-sa/4.0/'}","[{'name': 'Yiyu Zhuang, Jiaxi Lv, Hao Wen, Qing Shuai, Ailing Zeng, Hao Zhu, Shifeng Chen, Yujiu Yang, Xun Cao, Wei Liu'}]","Yiyu Zhuang, Jiaxi Lv, Hao Wen, Qing Shuai, Ailing Zeng, Hao Zhu, Shifeng Chen, Yujiu Yang, Xun Cao, Wei Liu","{'name': 'Yiyu Zhuang, Jiaxi Lv, Hao Wen, Qing Shuai, Ailing Zeng, Hao Zhu, Shifeng Chen, Yujiu Yang, Xun Cao, Wei Liu'}",,
342,Knowledge Injection via Prompt Distillation,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Knowledge Injection via Prompt Distillation'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14964'}]",https://arxiv.org/abs/2412.14964,"arXiv:2412.14964v1 Announce Type: new 
Abstract: In many practical applications, large language models (LLMs) need to incorporate new knowledge not present in their pre-training data. The primary methods for this are fine-tuning and retrieval-augmented generation (RAG). Although RAG has emerged as the industry standard for knowledge injection, fine-tuning has not yet achieved comparable success. In this paper, we propose a new fine-tuning technique for learning new knowledge and show that it can reach the performance of RAG. The proposed method is based on the self-distillation approach, which we call prompt distillation. First, we generate question-answer pairs about the new knowledge. Then, we fine-tune a student model on the question-answer pairs to imitate the output distributions of a teacher model, which additionally receives the new knowledge in its prompt. The student model is identical to the teacher, except it is equipped with a LoRA adapter. This training procedure facilitates distilling the new knowledge from the teacher's prompt into the student's weights.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14964v1 Announce Type: new \nAbstract: In many practical applications, large language models (LLMs) need to incorporate new knowledge not present in their pre-training data. The primary methods for this are fine-tuning and retrieval-augmented generation (RAG). Although RAG has emerged as the industry standard for knowledge injection, fine-tuning has not yet achieved comparable success. In this paper, we propose a new fine-tuning technique for learning new knowledge and show that it can reach the performance of RAG. The proposed method is based on the self-distillation approach, which we call prompt distillation. First, we generate question-answer pairs about the new knowledge. Then, we fine-tune a student model on the question-answer pairs to imitate the output distributions of a teacher model, which additionally receives the new knowledge in its prompt. The student model is identical to the teacher, except it is equipped with a LoRA adapter. This training procedure facilitates distilling the new knowledge from the teacher's prompt into the student's weights.""}",oai:arXiv.org:2412.14964v1,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Kalle Kujanp\\""a\\""a, Harri Valpola, Alexander Ilin'}]","Kalle Kujanp\""a\""a, Harri Valpola, Alexander Ilin","{'name': 'Kalle Kujanp\\""a\\""a, Harri Valpola, Alexander Ilin'}",,
343,Movie2Story: A framework for understanding videos and telling stories in the form of novel text,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Movie2Story: A framework for understanding videos and telling stories in the form of novel text'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14965'}]",https://arxiv.org/abs/2412.14965,"arXiv:2412.14965v1 Announce Type: new 
Abstract: Multimodal video-to-text models have made considerable progress, primarily in generating brief descriptions of video content. However, there is still a deficiency in generating rich long-form text descriptions that integrate both video and audio. In this paper, we introduce a framework called M2S, designed to generate novel-length text by combining audio, video, and character recognition. M2S includes modules for video long-form text description and comprehension, audio-based analysis of emotion, speech rate, and character alignment, and visual-based character recognition alignment. By integrating multimodal information using the large language model GPT4o, M2S stands out in the field of multimodal text generation. We demonstrate the effectiveness and accuracy of M2S through comparative experiments and human evaluation. Additionally, the model framework has good scalability and significant potential for future research.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14965v1 Announce Type: new \nAbstract: Multimodal video-to-text models have made considerable progress, primarily in generating brief descriptions of video content. However, there is still a deficiency in generating rich long-form text descriptions that integrate both video and audio. In this paper, we introduce a framework called M2S, designed to generate novel-length text by combining audio, video, and character recognition. M2S includes modules for video long-form text description and comprehension, audio-based analysis of emotion, speech rate, and character alignment, and visual-based character recognition alignment. By integrating multimodal information using the large language model GPT4o, M2S stands out in the field of multimodal text generation. We demonstrate the effectiveness and accuracy of M2S through comparative experiments and human evaluation. Additionally, the model framework has good scalability and significant potential for future research.'}",oai:arXiv.org:2412.14965v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.CL', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Kangning Li, Zheyang Jia, Anyu Ying'}]","Kangning Li, Zheyang Jia, Anyu Ying","{'name': 'Kangning Li, Zheyang Jia, Anyu Ying'}",,
344,ECLIPSE: Contrastive Dimension Importance Estimation with Pseudo-Irrelevance Feedback for Dense Retrieval,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'ECLIPSE: Contrastive Dimension Importance Estimation with Pseudo-Irrelevance Feedback for Dense Retrieval'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14967'}]",https://arxiv.org/abs/2412.14967,"arXiv:2412.14967v1 Announce Type: new 
Abstract: Recent advances in Information Retrieval have leveraged high-dimensional embedding spaces to improve the retrieval of relevant documents. Moreover, the Manifold Clustering Hypothesis suggests that despite these high-dimensional representations, documents relevant to a query reside on a lower-dimensional, query-dependent manifold. While this hypothesis has inspired new retrieval methods, existing approaches still face challenges in effectively separating non-relevant information from relevant signals. We propose a novel methodology that addresses these limitations by leveraging information from both relevant and non-relevant documents. Our method, ECLIPSE, computes a centroid based on irrelevant documents as a reference to estimate noisy dimensions present in relevant ones, enhancing retrieval performance. Extensive experiments on three in-domain and one out-of-domain benchmarks demonstrate an average improvement of up to 19.50% (resp. 22.35%) in mAP(AP) and 11.42% (resp. 13.10%) in nDCG@10 w.r.t. the DIME-based baseline (resp. the baseline using all dimensions). Our results pave the way for more robust, pseudo-irrelevance-based retrieval systems in future IR research.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14967v1 Announce Type: new \nAbstract: Recent advances in Information Retrieval have leveraged high-dimensional embedding spaces to improve the retrieval of relevant documents. Moreover, the Manifold Clustering Hypothesis suggests that despite these high-dimensional representations, documents relevant to a query reside on a lower-dimensional, query-dependent manifold. While this hypothesis has inspired new retrieval methods, existing approaches still face challenges in effectively separating non-relevant information from relevant signals. We propose a novel methodology that addresses these limitations by leveraging information from both relevant and non-relevant documents. Our method, ECLIPSE, computes a centroid based on irrelevant documents as a reference to estimate noisy dimensions present in relevant ones, enhancing retrieval performance. Extensive experiments on three in-domain and one out-of-domain benchmarks demonstrate an average improvement of up to 19.50% (resp. 22.35%) in mAP(AP) and 11.42% (resp. 13.10%) in nDCG@10 w.r.t. the DIME-based baseline (resp. the baseline using all dimensions). Our results pave the way for more robust, pseudo-irrelevance-based retrieval systems in future IR research.'}",oai:arXiv.org:2412.14967v1,False,"[{'term': 'cs.IR', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': ""Giulio D'Erasmo, Giovanni Trappolini, Nicola Tonellotto, Fabrizio Silvestri""}]","Giulio D'Erasmo, Giovanni Trappolini, Nicola Tonellotto, Fabrizio Silvestri","{'name': ""Giulio D'Erasmo, Giovanni Trappolini, Nicola Tonellotto, Fabrizio Silvestri""}",,
345,PhotoHolmes: a Python library for forgery detection in digital images,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'PhotoHolmes: a Python library for forgery detection in digital images'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14969'}]",https://arxiv.org/abs/2412.14969,"arXiv:2412.14969v1 Announce Type: new 
Abstract: In this paper, we introduce PhotoHolmes, an open-source Python library designed to easily run and benchmark forgery detection methods on digital images. The library includes implementations of popular and state-of-the-art methods, dataset integration tools, and evaluation metrics. Utilizing the Benchmark tool in PhotoHolmes, users can effortlessly compare various methods. This facilitates an accurate and reproducible comparison between their own methods and those in the existing literature. Furthermore, PhotoHolmes includes a command-line interface (CLI) to easily run the methods implemented in the library on any suspicious image. As such, image forgery methods become more accessible to the community. The library has been built with extensibility and modularity in mind, which makes adding new methods, datasets and metrics to the library a straightforward process. The source code is available at https://github.com/photoholmes/photoholmes.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14969v1 Announce Type: new \nAbstract: In this paper, we introduce PhotoHolmes, an open-source Python library designed to easily run and benchmark forgery detection methods on digital images. The library includes implementations of popular and state-of-the-art methods, dataset integration tools, and evaluation metrics. Utilizing the Benchmark tool in PhotoHolmes, users can effortlessly compare various methods. This facilitates an accurate and reproducible comparison between their own methods and those in the existing literature. Furthermore, PhotoHolmes includes a command-line interface (CLI) to easily run the methods implemented in the library on any suspicious image. As such, image forgery methods become more accessible to the community. The library has been built with extensibility and modularity in mind, which makes adding new methods, datasets and metrics to the library a straightforward process. The source code is available at https://github.com/photoholmes/photoholmes.'}",oai:arXiv.org:2412.14969v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': ""Juli\\'an O'Flaherty, Rodrigo Paganini, Juan Pablo Sotelo, Julieta Umpi\\'errez, Marina Gardella, Mat\\'ias Tailanian, Pablo Mus\\'e""}]","Juli\'an O'Flaherty, Rodrigo Paganini, Juan Pablo Sotelo, Julieta Umpi\'errez, Marina Gardella, Mat\'ias Tailanian, Pablo Mus\'e","{'name': ""Juli\\'an O'Flaherty, Rodrigo Paganini, Juan Pablo Sotelo, Julieta Umpi\\'errez, Marina Gardella, Mat\\'ias Tailanian, Pablo Mus\\'e""}",,
346,AI and Cultural Context: An Empirical Investigation of Large Language Models' Performance on Chinese Social Work Professional Standards,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""AI and Cultural Context: An Empirical Investigation of Large Language Models' Performance on Chinese Social Work Professional Standards""}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14971'}]",https://arxiv.org/abs/2412.14971,"arXiv:2412.14971v1 Announce Type: new 
Abstract: Objective: This study examines how well leading Chinese and Western large language models understand and apply Chinese social work principles, focusing on their foundational knowledge within a non-Western professional setting. We test whether the cultural context in the developing country influences model reasoning and accuracy.
  Method: Using a published self-study version of the Chinese National Social Work Examination (160 questions) covering jurisprudence and applied knowledge, we administered three testing conditions to eight cloud-based large language models - four Chinese and four Western. We examined their responses following official guidelines and evaluated their explanations' reasoning quality.
  Results: Seven models exceeded the 60-point passing threshold in both sections. Chinese models performed better in jurisprudence (median = 77.0 vs. 70.3) but slightly lower in applied knowledge (median = 65.5 vs. 67.0). Both groups showed cultural biases, particularly regarding gender equality and family dynamics. Models demonstrated strong professional terminology knowledge but struggled with culturally specific interventions. Valid reasoning in incorrect answers ranged from 16.4% to 45.0%.
  Conclusions: While both Chinese and Western models show foundational knowledge of Chinese social work principles, technical language proficiency does not ensure cultural competence. Chinese models demonstrate advantages in regulatory content, yet both Chinese and Western models struggle with culturally nuanced practice scenarios. These findings contribute to informing responsible AI integration into cross-cultural social work practice.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14971v1 Announce Type: new \nAbstract: Objective: This study examines how well leading Chinese and Western large language models understand and apply Chinese social work principles, focusing on their foundational knowledge within a non-Western professional setting. We test whether the cultural context in the developing country influences model reasoning and accuracy.\n  Method: Using a published self-study version of the Chinese National Social Work Examination (160 questions) covering jurisprudence and applied knowledge, we administered three testing conditions to eight cloud-based large language models - four Chinese and four Western. We examined their responses following official guidelines and evaluated their explanations' reasoning quality.\n  Results: Seven models exceeded the 60-point passing threshold in both sections. Chinese models performed better in jurisprudence (median = 77.0 vs. 70.3) but slightly lower in applied knowledge (median = 65.5 vs. 67.0). Both groups showed cultural biases, particularly regarding gender equality and family dynamics. Models demonstrated strong professional terminology knowledge but struggled with culturally specific interventions. Valid reasoning in incorrect answers ranged from 16.4% to 45.0%.\n  Conclusions: While both Chinese and Western models show foundational knowledge of Chinese social work principles, technical language proficiency does not ensure cultural competence. Chinese models demonstrate advantages in regulatory content, yet both Chinese and Western models struggle with culturally nuanced practice scenarios. These findings contribute to informing responsible AI integration into cross-cultural social work practice.""}",oai:arXiv.org:2412.14971v1,False,"[{'term': 'cs.CY', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by-nc-sa/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-sa/4.0/'}","[{'name': 'Zia Qi, Brian E. Perron, Miao Wang, Cao Fang, Sitao Chen, Bryan G. Victor'}]","Zia Qi, Brian E. Perron, Miao Wang, Cao Fang, Sitao Chen, Bryan G. Victor","{'name': 'Zia Qi, Brian E. Perron, Miao Wang, Cao Fang, Sitao Chen, Bryan G. Victor'}",,
347,Arti-PG: A Toolbox for Procedurally Synthesizing Large-Scale and Diverse Articulated Objects with Rich Annotations,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Arti-PG: A Toolbox for Procedurally Synthesizing Large-Scale and Diverse Articulated Objects with Rich Annotations'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14974'}]",https://arxiv.org/abs/2412.14974,"arXiv:2412.14974v1 Announce Type: new 
Abstract: The acquisition of substantial volumes of 3D articulated object data is expensive and time-consuming, and consequently the scarcity of 3D articulated object data becomes an obstacle for deep learning methods to achieve remarkable performance in various articulated object understanding tasks. Meanwhile, pairing these object data with detailed annotations to enable training for various tasks is also difficult and labor-intensive to achieve. In order to expeditiously gather a significant number of 3D articulated objects with comprehensive and detailed annotations for training, we propose Articulated Object Procedural Generation toolbox, a.k.a. Arti-PG toolbox. Arti-PG toolbox consists of i) descriptions of articulated objects by means of a generalized structure program along with their analytic correspondence to the objects' point cloud, ii) procedural rules about manipulations on the structure program to synthesize large-scale and diverse new articulated objects, and iii) mathematical descriptions of knowledge (e.g. affordance, semantics, etc.) to provide annotations to the synthesized object. Arti-PG has two appealing properties for providing training data for articulated object understanding tasks: i) objects are created with unlimited variations in shape through program-oriented structure manipulation, ii) Arti-PG is widely applicable to diverse tasks by easily providing comprehensive and detailed annotations. Arti-PG now supports the procedural generation of 26 categories of articulate objects and provides annotations across a wide range of both vision and manipulation tasks, and we provide exhaustive experiments which fully demonstrate its advantages. We will make Arti-PG toolbox publicly available for the community to use.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14974v1 Announce Type: new \nAbstract: The acquisition of substantial volumes of 3D articulated object data is expensive and time-consuming, and consequently the scarcity of 3D articulated object data becomes an obstacle for deep learning methods to achieve remarkable performance in various articulated object understanding tasks. Meanwhile, pairing these object data with detailed annotations to enable training for various tasks is also difficult and labor-intensive to achieve. In order to expeditiously gather a significant number of 3D articulated objects with comprehensive and detailed annotations for training, we propose Articulated Object Procedural Generation toolbox, a.k.a. Arti-PG toolbox. Arti-PG toolbox consists of i) descriptions of articulated objects by means of a generalized structure program along with their analytic correspondence to the objects' point cloud, ii) procedural rules about manipulations on the structure program to synthesize large-scale and diverse new articulated objects, and iii) mathematical descriptions of knowledge (e.g. affordance, semantics, etc.) to provide annotations to the synthesized object. Arti-PG has two appealing properties for providing training data for articulated object understanding tasks: i) objects are created with unlimited variations in shape through program-oriented structure manipulation, ii) Arti-PG is widely applicable to diverse tasks by easily providing comprehensive and detailed annotations. Arti-PG now supports the procedural generation of 26 categories of articulate objects and provides annotations across a wide range of both vision and manipulation tasks, and we provide exhaustive experiments which fully demonstrate its advantages. We will make Arti-PG toolbox publicly available for the community to use.""}",oai:arXiv.org:2412.14974v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.RO', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Jianhua Sun, Yuxuan Li, Jiude Wei, Longfei Xu, Nange Wang, Yining Zhang, Cewu Lu'}]","Jianhua Sun, Yuxuan Li, Jiude Wei, Longfei Xu, Nange Wang, Yining Zhang, Cewu Lu","{'name': 'Jianhua Sun, Yuxuan Li, Jiude Wei, Longfei Xu, Nange Wang, Yining Zhang, Cewu Lu'}",,
348,Minimizing speculation overhead in a parallel recognizer for regular texts,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Minimizing speculation overhead in a parallel recognizer for regular texts'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14975'}]",https://arxiv.org/abs/2412.14975,"arXiv:2412.14975v1 Announce Type: new 
Abstract: Speculative data-parallel algorithms for language recognition have been widely experimented for various types of FA (DFA and NFA) automata, often derived from regular expressions. Such an algorithm cuts the input string into chunks, independently recognizes each chunk in parallel by means of identical FAs, and at last joins the chunk results and checks overall consistency. In chunk recognition, it is necessary to speculatively start the FAs in any state, thus causing an overhead that reduces the speedup over a serial algorithm. Existing data-parallel DFA-based recognizers suffer from the excessive number of starting states, and the NFA-based ones suffer from the number of nondeterministic transitions. Our data-parallel algorithm is based on the new FA type called reduced interface DFA (RI-DFA), which minimizes the speculation overhead without incurring in the penalty of nondeterministic transitions or of impractically enlarged DFA machines. The algorithm is proved to be correct and theoretically efficient, because it combines the state-reduction of an NFA with the speed of deterministic transitions, thus improving on both DFA-based and NFA-based existing implementations. The practical applicability of the RI-DFA approach is confirmed by a quantitative comparison of the number of starting states for a large public benchmark of complex FAs. On multi-core computing architectures, the RI-DFA recognizer is much faster than the NFA-based one on all benchmarks, while it matches the DFA-based one on some benchmarks and performs much better on some others. The extra time cost to construct RI-DFA vs DFA is moderate and is compatible with a practical use.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14975v1 Announce Type: new \nAbstract: Speculative data-parallel algorithms for language recognition have been widely experimented for various types of FA (DFA and NFA) automata, often derived from regular expressions. Such an algorithm cuts the input string into chunks, independently recognizes each chunk in parallel by means of identical FAs, and at last joins the chunk results and checks overall consistency. In chunk recognition, it is necessary to speculatively start the FAs in any state, thus causing an overhead that reduces the speedup over a serial algorithm. Existing data-parallel DFA-based recognizers suffer from the excessive number of starting states, and the NFA-based ones suffer from the number of nondeterministic transitions. Our data-parallel algorithm is based on the new FA type called reduced interface DFA (RI-DFA), which minimizes the speculation overhead without incurring in the penalty of nondeterministic transitions or of impractically enlarged DFA machines. The algorithm is proved to be correct and theoretically efficient, because it combines the state-reduction of an NFA with the speed of deterministic transitions, thus improving on both DFA-based and NFA-based existing implementations. The practical applicability of the RI-DFA approach is confirmed by a quantitative comparison of the number of starting states for a large public benchmark of complex FAs. On multi-core computing architectures, the RI-DFA recognizer is much faster than the NFA-based one on all benchmarks, while it matches the DFA-based one on some benchmarks and performs much better on some others. The extra time cost to construct RI-DFA vs DFA is moderate and is compatible with a practical use.'}",oai:arXiv.org:2412.14975v1,False,"[{'term': 'cs.DC', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by-nc-sa/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-sa/4.0/'}","[{'name': 'Angelo Borsotti, Luca Breveglieri, Stefano Crespi Reghizzi, Angelo Morzenti'}]","Angelo Borsotti, Luca Breveglieri, Stefano Crespi Reghizzi, Angelo Morzenti","{'name': 'Angelo Borsotti, Luca Breveglieri, Stefano Crespi Reghizzi, Angelo Morzenti'}",,
349,6GENABLERS-DLT: DLT-based Marketplace for Decentralized Trading of 6G Telco resources,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': '6GENABLERS-DLT: DLT-based Marketplace for Decentralized Trading of 6G Telco resources'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14977'}]",https://arxiv.org/abs/2412.14977,"arXiv:2412.14977v1 Announce Type: new 
Abstract: The 6GENABLERS-DLT project addresses critical challenges in fostering multi-party collaboration within dynamic 6G environments. As operators and service providers increasingly depend on third-party resources to meet their contractual and operational needs, the project introduces an innovative, Distributed Ledger Technology (DLT)-anchored Marketplace designed to streamline decentralized telco resource trading. This 6GENABLERS Marketplace serves as a collaborative platform where operators, resource providers, and service providers can seamlessly discover, advertise, and trade telco assets within a transparent, secure, and efficient permissioned environment. Distinguished from public DLT-Blockchain solutions, the Marketplace's permissioned nature ensures robust governance, privacy, and control, making it particularly suited to enterprise and consortium-based use cases in the Information and Communication Technology (ICT) sector. The adoption of a decentralized architecture eliminates reliance on a central operator, thereby mitigating risks associated with single points of failure and enhancing system trustworthiness, resilience, and fault tolerance. The Marketplace encompasses a wide range of resources integral to 6G networks, including virtualized mobile core components, Radio Access Network (RAN) assets, edge and cloud infrastructure, and vertical applications tailored to specific industry needs. This diversity enables stakeholders to dynamically access and scale resources, fostering operational efficiency and innovation across 6G ecosystems. Through the 6GENABLERS-DLT project, the vision of a collaborative, resource-rich 6G environment becomes a reality, laying the foundation for a next-generation telco ecosystem where decentralization empowers stakeholders to meet the demands of an interconnected, flexible, and scalable future.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14977v1 Announce Type: new \nAbstract: The 6GENABLERS-DLT project addresses critical challenges in fostering multi-party collaboration within dynamic 6G environments. As operators and service providers increasingly depend on third-party resources to meet their contractual and operational needs, the project introduces an innovative, Distributed Ledger Technology (DLT)-anchored Marketplace designed to streamline decentralized telco resource trading. This 6GENABLERS Marketplace serves as a collaborative platform where operators, resource providers, and service providers can seamlessly discover, advertise, and trade telco assets within a transparent, secure, and efficient permissioned environment. Distinguished from public DLT-Blockchain solutions, the Marketplace's permissioned nature ensures robust governance, privacy, and control, making it particularly suited to enterprise and consortium-based use cases in the Information and Communication Technology (ICT) sector. The adoption of a decentralized architecture eliminates reliance on a central operator, thereby mitigating risks associated with single points of failure and enhancing system trustworthiness, resilience, and fault tolerance. The Marketplace encompasses a wide range of resources integral to 6G networks, including virtualized mobile core components, Radio Access Network (RAN) assets, edge and cloud infrastructure, and vertical applications tailored to specific industry needs. This diversity enables stakeholders to dynamically access and scale resources, fostering operational efficiency and innovation across 6G ecosystems. Through the 6GENABLERS-DLT project, the vision of a collaborative, resource-rich 6G environment becomes a reality, laying the foundation for a next-generation telco ecosystem where decentralization empowers stakeholders to meet the demands of an interconnected, flexible, and scalable future.""}",oai:arXiv.org:2412.14977v1,False,"[{'term': 'cs.NI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by-sa/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-sa/4.0/'}","[{'name': ""Adriana Fern\\'andez-Fern\\'andez, Angel Martin, Guillermo Gomez""}]","Adriana Fern\'andez-Fern\'andez, Angel Martin, Guillermo Gomez","{'name': ""Adriana Fern\\'andez-Fern\\'andez, Angel Martin, Guillermo Gomez""}",,
350,Spectrum-based Modality Representation Fusion Graph Convolutional Network for Multimodal Recommendation,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Spectrum-based Modality Representation Fusion Graph Convolutional Network for Multimodal Recommendation'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14978'}]",https://arxiv.org/abs/2412.14978,"arXiv:2412.14978v1 Announce Type: new 
Abstract: Incorporating multi-modal features as side information has recently become a trend in recommender systems. To elucidate user-item preferences, recent studies focus on fusing modalities via concatenation, element-wise sum, or attention mechanisms. Despite having notable success, existing approaches do not account for the modality-specific noise encapsulated within each modality. As a result, direct fusion of modalities will lead to the amplification of cross-modality noise. Moreover, the variation of noise that is unique within each modality results in noise alleviation and fusion being more challenging. In this work, we propose a new Spectrum-based Modality Representation (SMORE) fusion graph recommender that aims to capture both uni-modal and fusion preferences while simultaneously suppressing modality noise. Specifically, SMORE projects the multi-modal features into the frequency domain and leverages the spectral space for fusion. To reduce dynamic contamination that is unique to each modality, we introduce a filter to attenuate and suppress the modality noise adaptively while capturing the universal modality patterns effectively. Furthermore, we explore the item latent structures by designing a new multi-modal graph learning module to capture associative semantic correlations and universal fusion patterns among similar items. Finally, we formulate a new modality-aware preference module, which infuses behavioral features and balances the uni- and multi-modal features for precise preference modeling. This empowers SMORE with the ability to infer both user modality-specific and fusion preferences more accurately. Experiments on three real-world datasets show the efficacy of our proposed model. The source code for this work has been made publicly available at https://github.com/kennethorq/SMORE.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14978v1 Announce Type: new \nAbstract: Incorporating multi-modal features as side information has recently become a trend in recommender systems. To elucidate user-item preferences, recent studies focus on fusing modalities via concatenation, element-wise sum, or attention mechanisms. Despite having notable success, existing approaches do not account for the modality-specific noise encapsulated within each modality. As a result, direct fusion of modalities will lead to the amplification of cross-modality noise. Moreover, the variation of noise that is unique within each modality results in noise alleviation and fusion being more challenging. In this work, we propose a new Spectrum-based Modality Representation (SMORE) fusion graph recommender that aims to capture both uni-modal and fusion preferences while simultaneously suppressing modality noise. Specifically, SMORE projects the multi-modal features into the frequency domain and leverages the spectral space for fusion. To reduce dynamic contamination that is unique to each modality, we introduce a filter to attenuate and suppress the modality noise adaptively while capturing the universal modality patterns effectively. Furthermore, we explore the item latent structures by designing a new multi-modal graph learning module to capture associative semantic correlations and universal fusion patterns among similar items. Finally, we formulate a new modality-aware preference module, which infuses behavioral features and balances the uni- and multi-modal features for precise preference modeling. This empowers SMORE with the ability to infer both user modality-specific and fusion preferences more accurately. Experiments on three real-world datasets show the efficacy of our proposed model. The source code for this work has been made publicly available at https://github.com/kennethorq/SMORE.'}",oai:arXiv.org:2412.14978v1,False,"[{'term': 'cs.IR', 'scheme': None, 'label': None}, {'term': 'cs.MM', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Rongqing Kenneth Ong, Andy W. H. Khong'}]","Rongqing Kenneth Ong, Andy W. H. Khong","{'name': 'Rongqing Kenneth Ong, Andy W. H. Khong'}",10.1145/3701551.3703561,
351,Efficient Motion Sickness Assessment: Recreation of On-Road Driving on a Compact Test Track,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Efficient Motion Sickness Assessment: Recreation of On-Road Driving on a Compact Test Track'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14982'}]",https://arxiv.org/abs/2412.14982,"arXiv:2412.14982v1 Announce Type: new 
Abstract: The ability to engage in other activities during the ride is considered by consumers as one of the key reasons for the adoption of automated vehicles. However, engagement in non-driving activities will provoke occupants' motion sickness, deteriorating their overall comfort and thereby risking acceptance of automated driving. Therefore, it is critical to extend our understanding of motion sickness and unravel the modulating factors that affect it through experiments with participants. Currently, most experiments are conducted on public roads (realistic but not reproducible) or test tracks (feasible with prototype automated vehicles). This research study develops a method to design an optimal path and speed reference to efficiently replicate on-road motion sickness exposure on a small test track. The method uses model predictive control to replicate the longitudinal and lateral accelerations collected from on-road drives on a test track of 70 m by 175 m. A within-subject experiment (47 participants) was conducted comparing the occupants' motion sickness occurrence in test-track and on-road conditions, with the conditions being cross-randomized. The results illustrate no difference and no effect of the condition on the occurrence of the average motion sickness across the participants. Meanwhile, there is an overall correspondence of individual sickness levels between on-road and test-track. This paves the path for the employment of our method for a simpler, safer and more replicable assessment of motion sickness.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14982v1 Announce Type: new \nAbstract: The ability to engage in other activities during the ride is considered by consumers as one of the key reasons for the adoption of automated vehicles. However, engagement in non-driving activities will provoke occupants' motion sickness, deteriorating their overall comfort and thereby risking acceptance of automated driving. Therefore, it is critical to extend our understanding of motion sickness and unravel the modulating factors that affect it through experiments with participants. Currently, most experiments are conducted on public roads (realistic but not reproducible) or test tracks (feasible with prototype automated vehicles). This research study develops a method to design an optimal path and speed reference to efficiently replicate on-road motion sickness exposure on a small test track. The method uses model predictive control to replicate the longitudinal and lateral accelerations collected from on-road drives on a test track of 70 m by 175 m. A within-subject experiment (47 participants) was conducted comparing the occupants' motion sickness occurrence in test-track and on-road conditions, with the conditions being cross-randomized. The results illustrate no difference and no effect of the condition on the occurrence of the average motion sickness across the participants. Meanwhile, there is an overall correspondence of individual sickness levels between on-road and test-track. This paves the path for the employment of our method for a simpler, safer and more replicable assessment of motion sickness.""}",oai:arXiv.org:2412.14982v1,False,"[{'term': 'cs.RO', 'scheme': None, 'label': None}, {'term': 'cs.ET', 'scheme': None, 'label': None}, {'term': 'cs.HC', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by-nc-nd/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-nd/4.0/'}","[{'name': 'Huseyin Harmankaya, Adrian Brietzke, Rebecca Pham Xuan, Barys Shyrokau, Riender Happee, Georgios Papaioannou'}]","Huseyin Harmankaya, Adrian Brietzke, Rebecca Pham Xuan, Barys Shyrokau, Riender Happee, Georgios Papaioannou","{'name': 'Huseyin Harmankaya, Adrian Brietzke, Rebecca Pham Xuan, Barys Shyrokau, Riender Happee, Georgios Papaioannou'}",,
352,Co-optimization of Vehicle Dynamics and Powertrain Management for Connected and Automated Electric Vehicles,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Co-optimization of Vehicle Dynamics and Powertrain Management for Connected and Automated Electric Vehicles'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14984'}]",https://arxiv.org/abs/2412.14984,"arXiv:2412.14984v1 Announce Type: new 
Abstract: Connected and automated vehicles (CAVs) represent the future of transportation, utilizing detailed traffic information to enhance control and decision-making. Eco-driving of CAVs has the potential to significantly improve energy efficiency, and the benefits are maximized when both vehicle speed and powertrain operation are optimized. In this paper, we studied the co-optimization of vehicle speed and powertrain management for energy savings in a dual-motor electric vehicle. Control-oriented vehicle dynamics and electric powertrain models were developed to transform the problem into an optimal control problem specifically designed to facilitate real-time computation. Simulation validation was conducted using real-world data calibrated traffic simulation scenarios in Chattanooga, TN. Evaluation results demonstrated a 12.80-24.52% reduction in the vehicle's power consumption under ideal predicted traffic conditions, while maintaining benefits with various prediction uncertainties, such as Gaussian process uncertainties on acceleration and time-shift effects on predicted speed. The energy savings of the proposed eco-driving strategy are achieved through effective speed control and optimized torque allocation. The proposed model can be extended to various CAV and electric vehicle applications, with potential adaptability to diverse traffic scenarios.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14984v1 Announce Type: new \nAbstract: Connected and automated vehicles (CAVs) represent the future of transportation, utilizing detailed traffic information to enhance control and decision-making. Eco-driving of CAVs has the potential to significantly improve energy efficiency, and the benefits are maximized when both vehicle speed and powertrain operation are optimized. In this paper, we studied the co-optimization of vehicle speed and powertrain management for energy savings in a dual-motor electric vehicle. Control-oriented vehicle dynamics and electric powertrain models were developed to transform the problem into an optimal control problem specifically designed to facilitate real-time computation. Simulation validation was conducted using real-world data calibrated traffic simulation scenarios in Chattanooga, TN. Evaluation results demonstrated a 12.80-24.52% reduction in the vehicle's power consumption under ideal predicted traffic conditions, while maintaining benefits with various prediction uncertainties, such as Gaussian process uncertainties on acceleration and time-shift effects on predicted speed. The energy savings of the proposed eco-driving strategy are achieved through effective speed control and optimized torque allocation. The proposed model can be extended to various CAV and electric vehicle applications, with potential adaptability to diverse traffic scenarios.""}",oai:arXiv.org:2412.14984v1,False,"[{'term': 'eess.SY', 'scheme': None, 'label': None}, {'term': 'cs.SY', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Zongtan Li, Yunli Shao'}]","Zongtan Li, Yunli Shao","{'name': 'Zongtan Li, Yunli Shao'}",,
353,Exploration of the Dynamics of Buy and Sale of Social Media Accounts,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Exploration of the Dynamics of Buy and Sale of Social Media Accounts'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14985'}]",https://arxiv.org/abs/2412.14985,"arXiv:2412.14985v1 Announce Type: new 
Abstract: There has been a rise in online platforms facilitating the buying and selling of social media accounts. While the trade of social media profiles is not inherently illegal, social media platforms view such transactions as violations of their policies. They often take action against accounts involved in the misuse of platforms for financial gain. This research conducts a comprehensive analysis of marketplaces that enable the buying and selling of social media accounts.
  We investigate the economic scale of account trading across five major platforms: X, Instagram, Facebook, TikTok, and YouTube. From February to June 2024, we identified 38,253 accounts advertising account sales across 11 online marketplaces, covering 211 distinct categories. The total value of marketed social media accounts exceeded \$64 million, with a median price of \$157 per account. Additionally, we analyzed the profiles of 11,457 visible advertised accounts, collecting their metadata and over 200,000 profile posts. By examining their engagement patterns and account creation methods, we evaluated the fraudulent activities commonly associated with these sold accounts. Our research reveals these marketplaces foster fraudulent activities such as bot farming, harvesting accounts for future fraud, and fraudulent engagement. Such practices pose significant risks to social media users, who are often targeted by fraudulent accounts resembling legitimate profiles and employing social engineering tactics. We highlight social media platform weaknesses in the ability to detect and mitigate such fraudulent accounts, thereby endangering users. Alongside this, we conducted thorough disclosures with the respective platforms and proposed actionable recommendations, including indicators to identify and track these accounts. These measures aim to enhance proactive detection and safeguard users from potential threats.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14985v1 Announce Type: new \nAbstract: There has been a rise in online platforms facilitating the buying and selling of social media accounts. While the trade of social media profiles is not inherently illegal, social media platforms view such transactions as violations of their policies. They often take action against accounts involved in the misuse of platforms for financial gain. This research conducts a comprehensive analysis of marketplaces that enable the buying and selling of social media accounts.\n  We investigate the economic scale of account trading across five major platforms: X, Instagram, Facebook, TikTok, and YouTube. From February to June 2024, we identified 38,253 accounts advertising account sales across 11 online marketplaces, covering 211 distinct categories. The total value of marketed social media accounts exceeded \\$64 million, with a median price of \\$157 per account. Additionally, we analyzed the profiles of 11,457 visible advertised accounts, collecting their metadata and over 200,000 profile posts. By examining their engagement patterns and account creation methods, we evaluated the fraudulent activities commonly associated with these sold accounts. Our research reveals these marketplaces foster fraudulent activities such as bot farming, harvesting accounts for future fraud, and fraudulent engagement. Such practices pose significant risks to social media users, who are often targeted by fraudulent accounts resembling legitimate profiles and employing social engineering tactics. We highlight social media platform weaknesses in the ability to detect and mitigate such fraudulent accounts, thereby endangering users. Alongside this, we conducted thorough disclosures with the respective platforms and proposed actionable recommendations, including indicators to identify and track these accounts. These measures aim to enhance proactive detection and safeguard users from potential threats.'}",oai:arXiv.org:2412.14985v1,False,"[{'term': 'cs.CR', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Mario Beluri, Bhupendra Acharya, Soheil Khodayari, Giada Stivala, Giancarlo Pellegrino, Thorsten Holz'}]","Mario Beluri, Bhupendra Acharya, Soheil Khodayari, Giada Stivala, Giancarlo Pellegrino, Thorsten Holz","{'name': 'Mario Beluri, Bhupendra Acharya, Soheil Khodayari, Giada Stivala, Giancarlo Pellegrino, Thorsten Holz'}",,
354,Chain-of-MetaWriting: Linguistic and Textual Analysis of How Small Language Models Write Young Students Texts,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Chain-of-MetaWriting: Linguistic and Textual Analysis of How Small Language Models Write Young Students Texts'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14986'}]",https://arxiv.org/abs/2412.14986,"arXiv:2412.14986v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have been used to generate texts in response to different writing tasks: reports, essays, story telling. However, language models do not have a meta-representation of the text writing process, nor inherent communication learning needs, comparable to those of young human students. This paper introduces a fine-grained linguistic and textual analysis of multilingual Small Language Models' (SLMs) writing. With our method, Chain-of-MetaWriting, SLMs can imitate some steps of the human writing process, such as planning and evaluation. We mainly focused on short story and essay writing tasks in French for schoolchildren and undergraduate students respectively. Our results show that SLMs encounter difficulties in assisting young students on sensitive topics such as violence in the schoolyard, and they sometimes use words too complex for the target audience. In particular, the output is quite different from the human produced texts in term of text cohesion and coherence regarding temporal connectors, topic progression, reference.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14986v1 Announce Type: new \nAbstract: Large Language Models (LLMs) have been used to generate texts in response to different writing tasks: reports, essays, story telling. However, language models do not have a meta-representation of the text writing process, nor inherent communication learning needs, comparable to those of young human students. This paper introduces a fine-grained linguistic and textual analysis of multilingual Small Language Models' (SLMs) writing. With our method, Chain-of-MetaWriting, SLMs can imitate some steps of the human writing process, such as planning and evaluation. We mainly focused on short story and essay writing tasks in French for schoolchildren and undergraduate students respectively. Our results show that SLMs encounter difficulties in assisting young students on sensitive topics such as violence in the schoolyard, and they sometimes use words too complex for the target audience. In particular, the output is quite different from the human produced texts in term of text cohesion and coherence regarding temporal connectors, topic progression, reference.""}",oai:arXiv.org:2412.14986v1,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by-nc-sa/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-sa/4.0/'}","[{'name': 'Ioana Buhnila, Georgeta Cislaru, Amalia Todirascu'}]","Ioana Buhnila, Georgeta Cislaru, Amalia Todirascu","{'name': 'Ioana Buhnila, Georgeta Cislaru, Amalia Todirascu'}",,
355,Stitch Contrast and Segment_Learning a Human Action Segmentation Model Using Trimmed Skeleton Videos,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Stitch Contrast and Segment_Learning a Human Action Segmentation Model Using Trimmed Skeleton Videos'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14988'}]",https://arxiv.org/abs/2412.14988,"arXiv:2412.14988v1 Announce Type: new 
Abstract: Existing skeleton-based human action classification models rely on well-trimmed action-specific skeleton videos for both training and testing, precluding their scalability to real-world applications where untrimmed videos exhibiting concatenated actions are predominant. To overcome this limitation, recently introduced skeleton action segmentation models involve un-trimmed skeleton videos into end-to-end training. The model is optimized to provide frame-wise predictions for any length of testing videos, simultaneously realizing action localization and classification. Yet, achieving such an improvement im-poses frame-wise annotated skeleton videos, which remains time-consuming in practice. This paper features a novel framework for skeleton-based action segmentation trained on short trimmed skeleton videos, but that can run on longer un-trimmed videos. The approach is implemented in three steps: Stitch, Contrast, and Segment. First, Stitch proposes a tem-poral skeleton stitching scheme that treats trimmed skeleton videos as elementary human motions that compose a semantic space and can be sampled to generate multi-action stitched se-quences. Contrast learns contrastive representations from stitched sequences with a novel discrimination pretext task that enables a skeleton encoder to learn meaningful action-temporal contexts to improve action segmentation. Finally, Segment relates the proposed method to action segmentation by learning a segmentation layer while handling particular da-ta availability. Experiments involve a trimmed source dataset and an untrimmed target dataset in an adaptation formulation for real-world skeleton-based human action segmentation to evaluate the effectiveness of the proposed method.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14988v1 Announce Type: new \nAbstract: Existing skeleton-based human action classification models rely on well-trimmed action-specific skeleton videos for both training and testing, precluding their scalability to real-world applications where untrimmed videos exhibiting concatenated actions are predominant. To overcome this limitation, recently introduced skeleton action segmentation models involve un-trimmed skeleton videos into end-to-end training. The model is optimized to provide frame-wise predictions for any length of testing videos, simultaneously realizing action localization and classification. Yet, achieving such an improvement im-poses frame-wise annotated skeleton videos, which remains time-consuming in practice. This paper features a novel framework for skeleton-based action segmentation trained on short trimmed skeleton videos, but that can run on longer un-trimmed videos. The approach is implemented in three steps: Stitch, Contrast, and Segment. First, Stitch proposes a tem-poral skeleton stitching scheme that treats trimmed skeleton videos as elementary human motions that compose a semantic space and can be sampled to generate multi-action stitched se-quences. Contrast learns contrastive representations from stitched sequences with a novel discrimination pretext task that enables a skeleton encoder to learn meaningful action-temporal contexts to improve action segmentation. Finally, Segment relates the proposed method to action segmentation by learning a segmentation layer while handling particular da-ta availability. Experiments involve a trimmed source dataset and an untrimmed target dataset in an adaptation formulation for real-world skeleton-based human action segmentation to evaluate the effectiveness of the proposed method.'}",oai:arXiv.org:2412.14988v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Haitao Tian, Pierre Payeur'}]","Haitao Tian, Pierre Payeur","{'name': 'Haitao Tian, Pierre Payeur'}",,
356,RoboCup@Home 2024 OPL Winner NimbRo: Anthropomorphic Service Robots using Foundation Models for Perception and Planning,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'RoboCup@Home 2024 OPL Winner NimbRo: Anthropomorphic Service Robots using Foundation Models for Perception and Planning'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14989'}]",https://arxiv.org/abs/2412.14989,"arXiv:2412.14989v1 Announce Type: new 
Abstract: We present the approaches and contributions of the winning team NimbRo@Home at the RoboCup@Home 2024 competition in the Open Platform League held in Eindhoven, NL. Further, we describe our hardware setup and give an overview of the results for the task stages and the final demonstration. For this year's competition, we put a special emphasis on open-vocabulary object segmentation and grasping approaches that overcome the labeling overhead of supervised vision approaches, commonly used in RoboCup@Home. We successfully demonstrated that we can segment and grasp non-labeled objects by text descriptions. Further, we extensively employed LLMs for natural language understanding and task planning. Throughout the competition, our approaches showed robustness and generalization capabilities. A video of our performance can be found online.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14989v1 Announce Type: new \nAbstract: We present the approaches and contributions of the winning team NimbRo@Home at the RoboCup@Home 2024 competition in the Open Platform League held in Eindhoven, NL. Further, we describe our hardware setup and give an overview of the results for the task stages and the final demonstration. For this year's competition, we put a special emphasis on open-vocabulary object segmentation and grasping approaches that overcome the labeling overhead of supervised vision approaches, commonly used in RoboCup@Home. We successfully demonstrated that we can segment and grasp non-labeled objects by text descriptions. Further, we extensively employed LLMs for natural language understanding and task planning. Throughout the competition, our approaches showed robustness and generalization capabilities. A video of our performance can be found online.""}",oai:arXiv.org:2412.14989v1,False,"[{'term': 'cs.RO', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Raphael Memmesheimer, Jan Nogga, Bastian P\\""atzold, Evgenii Kruzhkov, Simon Bultmann, Michael Schreiber, Jonas Bode, Bertan Karacora, Juhui Park, Alena Savinykh, Sven Behnke'}]","Raphael Memmesheimer, Jan Nogga, Bastian P\""atzold, Evgenii Kruzhkov, Simon Bultmann, Michael Schreiber, Jonas Bode, Bertan Karacora, Juhui Park, Alena Savinykh, Sven Behnke","{'name': 'Raphael Memmesheimer, Jan Nogga, Bastian P\\""atzold, Evgenii Kruzhkov, Simon Bultmann, Michael Schreiber, Jonas Bode, Bertan Karacora, Juhui Park, Alena Savinykh, Sven Behnke'}",,
357,HSEvo: Elevating Automatic Heuristic Design with Diversity-Driven Harmony Search and Genetic Algorithm Using LLMs,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'HSEvo: Elevating Automatic Heuristic Design with Diversity-Driven Harmony Search and Genetic Algorithm Using LLMs'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14995'}]",https://arxiv.org/abs/2412.14995,"arXiv:2412.14995v1 Announce Type: new 
Abstract: Automatic Heuristic Design (AHD) is an active research area due to its utility in solving complex search and NP-hard combinatorial optimization problems in the real world. The recent advancements in Large Language Models (LLMs) introduce new possibilities by coupling LLMs with evolutionary computation to automatically generate heuristics, known as LLM-based Evolutionary Program Search (LLM-EPS). While previous LLM-EPS studies obtained great performance on various tasks, there is still a gap in understanding the properties of heuristic search spaces and achieving a balance between exploration and exploitation, which is a critical factor in large heuristic search spaces. In this study, we address this gap by proposing two diversity measurement metrics and perform an analysis on previous LLM-EPS approaches, including FunSearch, EoH, and ReEvo. Results on black-box AHD problems reveal that while EoH demonstrates higher diversity than FunSearch and ReEvo, its objective score is unstable. Conversely, ReEvo's reflection mechanism yields good objective scores but fails to optimize diversity effectively. With this finding in mind, we introduce HSEvo, an adaptive LLM-EPS framework that maintains a balance between diversity and convergence with a harmony search algorithm. Through experimentation, we find that HSEvo achieved high diversity indices and good objective scores while remaining cost-effective. These results underscore the importance of balancing exploration and exploitation and understanding heuristic search spaces in designing frameworks in LLM-EPS.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14995v1 Announce Type: new \nAbstract: Automatic Heuristic Design (AHD) is an active research area due to its utility in solving complex search and NP-hard combinatorial optimization problems in the real world. The recent advancements in Large Language Models (LLMs) introduce new possibilities by coupling LLMs with evolutionary computation to automatically generate heuristics, known as LLM-based Evolutionary Program Search (LLM-EPS). While previous LLM-EPS studies obtained great performance on various tasks, there is still a gap in understanding the properties of heuristic search spaces and achieving a balance between exploration and exploitation, which is a critical factor in large heuristic search spaces. In this study, we address this gap by proposing two diversity measurement metrics and perform an analysis on previous LLM-EPS approaches, including FunSearch, EoH, and ReEvo. Results on black-box AHD problems reveal that while EoH demonstrates higher diversity than FunSearch and ReEvo, its objective score is unstable. Conversely, ReEvo's reflection mechanism yields good objective scores but fails to optimize diversity effectively. With this finding in mind, we introduce HSEvo, an adaptive LLM-EPS framework that maintains a balance between diversity and convergence with a harmony search algorithm. Through experimentation, we find that HSEvo achieved high diversity indices and good objective scores while remaining cost-effective. These results underscore the importance of balancing exploration and exploitation and understanding heuristic search spaces in designing frameworks in LLM-EPS.""}",oai:arXiv.org:2412.14995v1,False,"[{'term': 'cs.NE', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Pham Vu Tuan Dat, Long Doan, Huynh Thi Thanh Binh'}]","Pham Vu Tuan Dat, Long Doan, Huynh Thi Thanh Binh","{'name': 'Pham Vu Tuan Dat, Long Doan, Huynh Thi Thanh Binh'}",,
358,Autonomous Navigation in Dynamic Human Environments with an Embedded 2D LiDAR-based Person Tracker,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Autonomous Navigation in Dynamic Human Environments with an Embedded 2D LiDAR-based Person Tracker'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.15000'}]",https://arxiv.org/abs/2412.15000,"arXiv:2412.15000v1 Announce Type: new 
Abstract: In the rapidly evolving landscape of autonomous mobile robots, the emphasis on seamless human-robot interactions has shifted towards autonomous decision-making. This paper delves into the intricate challenges associated with robotic autonomy, focusing on navigation in dynamic environments shared with humans. It introduces an embedded real-time tracking pipeline, integrated into a navigation planning framework for effective person tracking and avoidance, adapting a state-of-the-art 2D LiDAR-based human detection network and an efficient multi-object tracker. By addressing the key components of detection, tracking, and planning separately, the proposed approach highlights the modularity and transferability of each component to other applications. Our tracking approach is validated on a quadruped robot equipped with 270{\deg} 2D-LiDAR against motion capture system data, with the preferred configuration achieving an average MOTA of 85.45% in three newly recorded datasets, while reliably running in real-time at 20 Hz on the NVIDIA Jetson Xavier NX embedded GPU-accelerated platform. Furthermore, the integrated tracking and avoidance system is evaluated in real-world navigation experiments, demonstrating how accurate person tracking benefits the planner in optimizing the generated trajectories, enhancing its collision avoidance capabilities. This paper contributes to safer human-robot cohabitation, blending recent advances in human detection with responsive planning to navigate shared spaces effectively and securely.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.15000v1 Announce Type: new \nAbstract: In the rapidly evolving landscape of autonomous mobile robots, the emphasis on seamless human-robot interactions has shifted towards autonomous decision-making. This paper delves into the intricate challenges associated with robotic autonomy, focusing on navigation in dynamic environments shared with humans. It introduces an embedded real-time tracking pipeline, integrated into a navigation planning framework for effective person tracking and avoidance, adapting a state-of-the-art 2D LiDAR-based human detection network and an efficient multi-object tracker. By addressing the key components of detection, tracking, and planning separately, the proposed approach highlights the modularity and transferability of each component to other applications. Our tracking approach is validated on a quadruped robot equipped with 270{\\deg} 2D-LiDAR against motion capture system data, with the preferred configuration achieving an average MOTA of 85.45% in three newly recorded datasets, while reliably running in real-time at 20 Hz on the NVIDIA Jetson Xavier NX embedded GPU-accelerated platform. Furthermore, the integrated tracking and avoidance system is evaluated in real-world navigation experiments, demonstrating how accurate person tracking benefits the planner in optimizing the generated trajectories, enhancing its collision avoidance capabilities. This paper contributes to safer human-robot cohabitation, blending recent advances in human detection with responsive planning to navigate shared spaces effectively and securely.'}",oai:arXiv.org:2412.15000v1,False,"[{'term': 'cs.RO', 'scheme': None, 'label': None}, {'term': 'cs.SY', 'scheme': None, 'label': None}, {'term': 'eess.SY', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Davide Plozza, Steven Marty, Cyril Scherrer, Simon Schwartz, Stefan Zihlmann, Michele Magno'}]","Davide Plozza, Steven Marty, Cyril Scherrer, Simon Schwartz, Stefan Zihlmann, Michele Magno","{'name': 'Davide Plozza, Steven Marty, Cyril Scherrer, Simon Schwartz, Stefan Zihlmann, Michele Magno'}",10.1109/SAS60918.2024.10636369,"IEEE Sensors Applications Symposium (SAS), 2024, pp. 1-6"
359,Large Language Models and Code Security: A Systematic Literature Review,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Large Language Models and Code Security: A Systematic Literature Review'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.15004'}]",https://arxiv.org/abs/2412.15004,"arXiv:2412.15004v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have emerged as powerful tools for automating various programming tasks, including security-related ones, such as detecting and fixing vulnerabilities. Despite their promising capabilities, when required to produce or modify pre-existing code, LLMs could introduce vulnerabilities unbeknown to the programmer. When analyzing code, they could miss clear vulnerabilities or signal nonexistent ones. In this Systematic Literature Review (SLR), we aim to investigate both the security benefits and potential drawbacks of using LLMs for a variety of code-related tasks. In particular, first we focus on the types of vulnerabilities that could be introduced by LLMs, when used for producing code. Second, we analyze the capabilities of LLMs to detect and fix vulnerabilities, in any given code, and how the prompting strategy of choice impacts their performance in these two tasks. Last, we provide an in-depth analysis on how data poisoning attacks on LLMs can impact performance in the aforementioned tasks.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.15004v1 Announce Type: new \nAbstract: Large Language Models (LLMs) have emerged as powerful tools for automating various programming tasks, including security-related ones, such as detecting and fixing vulnerabilities. Despite their promising capabilities, when required to produce or modify pre-existing code, LLMs could introduce vulnerabilities unbeknown to the programmer. When analyzing code, they could miss clear vulnerabilities or signal nonexistent ones. In this Systematic Literature Review (SLR), we aim to investigate both the security benefits and potential drawbacks of using LLMs for a variety of code-related tasks. In particular, first we focus on the types of vulnerabilities that could be introduced by LLMs, when used for producing code. Second, we analyze the capabilities of LLMs to detect and fix vulnerabilities, in any given code, and how the prompting strategy of choice impacts their performance in these two tasks. Last, we provide an in-depth analysis on how data poisoning attacks on LLMs can impact performance in the aforementioned tasks.'}",oai:arXiv.org:2412.15004v1,False,"[{'term': 'cs.CR', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.CL', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Enna Basic, Alberto Giaretta'}]","Enna Basic, Alberto Giaretta","{'name': 'Enna Basic, Alberto Giaretta'}",,
360,DisCo: Graph-Based Disentangled Contrastive Learning for Cold-Start Cross-Domain Recommendation,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'DisCo: Graph-Based Disentangled Contrastive Learning for Cold-Start Cross-Domain Recommendation'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.15005'}]",https://arxiv.org/abs/2412.15005,"arXiv:2412.15005v1 Announce Type: new 
Abstract: Recommender systems are widely used in various real-world applications, but they often encounter the persistent challenge of the user cold-start problem. Cross-domain recommendation (CDR), which leverages user interactions from one domain to improve prediction performance in another, has emerged as a promising solution. However, users with similar preferences in the source domain may exhibit different interests in the target domain. Therefore, directly transferring embeddings may introduce irrelevant source-domain collaborative information. In this paper, we propose a novel graph-based disentangled contrastive learning framework to capture fine-grained user intent and filter out irrelevant collaborative information, thereby avoiding negative transfer. Specifically, for each domain, we use a multi-channel graph encoder to capture diverse user intents. We then construct the affinity graph in the embedding space and perform multi-step random walks to capture high-order user similarity relationships. Treating one domain as the target, we propose a disentangled intent-wise contrastive learning approach, guided by user similarity, to refine the bridging of user intents across domains. Extensive experiments on four benchmark CDR datasets demonstrate that DisCo consistently outperforms existing state-of-the-art baselines, thereby validating the effectiveness of both DisCo and its components.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.15005v1 Announce Type: new \nAbstract: Recommender systems are widely used in various real-world applications, but they often encounter the persistent challenge of the user cold-start problem. Cross-domain recommendation (CDR), which leverages user interactions from one domain to improve prediction performance in another, has emerged as a promising solution. However, users with similar preferences in the source domain may exhibit different interests in the target domain. Therefore, directly transferring embeddings may introduce irrelevant source-domain collaborative information. In this paper, we propose a novel graph-based disentangled contrastive learning framework to capture fine-grained user intent and filter out irrelevant collaborative information, thereby avoiding negative transfer. Specifically, for each domain, we use a multi-channel graph encoder to capture diverse user intents. We then construct the affinity graph in the embedding space and perform multi-step random walks to capture high-order user similarity relationships. Treating one domain as the target, we propose a disentangled intent-wise contrastive learning approach, guided by user similarity, to refine the bridging of user intents across domains. Extensive experiments on four benchmark CDR datasets demonstrate that DisCo consistently outperforms existing state-of-the-art baselines, thereby validating the effectiveness of both DisCo and its components.'}",oai:arXiv.org:2412.15005v1,False,"[{'term': 'cs.IR', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Hourun Li, Yifan Wang, Zhiping Xiao, Jia Yang, Changling Zhou, Ming Zhang, Wei Ju'}]","Hourun Li, Yifan Wang, Zhiping Xiao, Jia Yang, Changling Zhou, Ming Zhang, Wei Ju","{'name': 'Hourun Li, Yifan Wang, Zhiping Xiao, Jia Yang, Changling Zhou, Ming Zhang, Wei Ju'}",,
361,Projection-based preprocessing for electrical impedance tomography to reduce the effect of electrode contacts,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Projection-based preprocessing for electrical impedance tomography to reduce the effect of electrode contacts'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.15009'}]",https://arxiv.org/abs/2412.15009,"arXiv:2412.15009v1 Announce Type: new 
Abstract: This work introduces a method for preprocessing measurements of electrical impedance tomography to considerably reduce the effect uncertainties in the electrode contacts have on the reconstruction quality, without a need to explicitly estimate the contacts. The idea is to compute the Jacobian matrix of the forward map with respect to the contact strengths and project the electrode measurements and the forward map onto the orthogonal complement of the range of this Jacobian. Using the smoothened complete electrode model as the forward model, it is demonstrated that inverting the resulting projected equation with respect to only the internal conductivity of the examined body results in good quality reconstructions both when resorting to a single step linearization with a smoothness prior and when combining lagged diffusivity iteration with total variation regularization. The quality of the reconstructions is further improved if the range of the employed projection is also orthogonal to that of the Jacobian with respect to the electrode positions. These results hold even if the projections are formed at internal and contact conductivities that significantly differ from the true ones; it is numerically demonstrated that the orthogonal complement of the range of the contact Jacobian is almost independent of the conductivity parameters at which it is evaluated. In particular, our observations introduce a numerical technique for inferring whether a change in the electrode measurements is caused by a change in the internal conductivity or alterations in the electrode contacts, which has potential applications, e.g., in bedside monitoring of stroke patients. The ideas are tested both on simulated data and on real-world water tank measurements with adjustable contact resistances.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.15009v1 Announce Type: new \nAbstract: This work introduces a method for preprocessing measurements of electrical impedance tomography to considerably reduce the effect uncertainties in the electrode contacts have on the reconstruction quality, without a need to explicitly estimate the contacts. The idea is to compute the Jacobian matrix of the forward map with respect to the contact strengths and project the electrode measurements and the forward map onto the orthogonal complement of the range of this Jacobian. Using the smoothened complete electrode model as the forward model, it is demonstrated that inverting the resulting projected equation with respect to only the internal conductivity of the examined body results in good quality reconstructions both when resorting to a single step linearization with a smoothness prior and when combining lagged diffusivity iteration with total variation regularization. The quality of the reconstructions is further improved if the range of the employed projection is also orthogonal to that of the Jacobian with respect to the electrode positions. These results hold even if the projections are formed at internal and contact conductivities that significantly differ from the true ones; it is numerically demonstrated that the orthogonal complement of the range of the contact Jacobian is almost independent of the conductivity parameters at which it is evaluated. In particular, our observations introduce a numerical technique for inferring whether a change in the electrode measurements is caused by a change in the internal conductivity or alterations in the electrode contacts, which has potential applications, e.g., in bedside monitoring of stroke patients. The ideas are tested both on simulated data and on real-world water tank measurements with adjustable contact resistances.'}",oai:arXiv.org:2412.15009v1,False,"[{'term': 'math.NA', 'scheme': None, 'label': None}, {'term': 'cs.NA', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Altti J\\""a\\""askel\\""ainen, Jussi Toivanen, Asko H\\""anninen, Ville Kolehmainen, Nuutti Hyv\\""onen'}]","Altti J\""a\""askel\""ainen, Jussi Toivanen, Asko H\""anninen, Ville Kolehmainen, Nuutti Hyv\""onen","{'name': 'Altti J\\""a\\""askel\\""ainen, Jussi Toivanen, Asko H\\""anninen, Ville Kolehmainen, Nuutti Hyv\\""onen'}",,
362,Robust Federated Learning in the Face of Covariate Shift: A Magnitude Pruning with Hybrid Regularization Framework for Enhanced Model Aggregation,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Robust Federated Learning in the Face of Covariate Shift: A Magnitude Pruning with Hybrid Regularization Framework for Enhanced Model Aggregation'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.15010'}]",https://arxiv.org/abs/2412.15010,"arXiv:2412.15010v1 Announce Type: new 
Abstract: The development of highly sophisticated neural networks has allowed for fast progress in every field of computer vision, however, applications where annotated data is prohibited due to privacy or security concerns remain challenging. Federated Learning (FL) offers a promising framework for individuals aiming to collaboratively develop a shared model while preserving data privacy. Nevertheless, our findings reveal that variations in data distribution among clients can profoundly affect FL methodologies, primarily due to instabilities in the aggregation process. We also propose a novel FL framework to mitigate the adverse effects of covariate shifts among federated clients by combining individual parameter pruning and regularization techniques to improve the robustness of individual clients' models to aggregate. Each client's model is optimized through magnitude-based pruning and the addition of dropout and noise injection layers to build more resilient decision pathways in the networks and improve the robustness of the model's parameter aggregation step. The proposed framework is capable of extracting robust representations even in the presence of very large covariate shifts among client data distributions and in the federation of a small number of clients. Empirical findings substantiate the effectiveness of our proposed methodology across common benchmark datasets, including CIFAR10, MNIST, SVHN, and Fashion MNIST. Furthermore, we introduce the CelebA-Gender dataset, specifically designed to evaluate performance on a more realistic domain. The proposed method is capable of extracting robust representations even in the presence of both high and low covariate shifts among client data distributions.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.15010v1 Announce Type: new \nAbstract: The development of highly sophisticated neural networks has allowed for fast progress in every field of computer vision, however, applications where annotated data is prohibited due to privacy or security concerns remain challenging. Federated Learning (FL) offers a promising framework for individuals aiming to collaboratively develop a shared model while preserving data privacy. Nevertheless, our findings reveal that variations in data distribution among clients can profoundly affect FL methodologies, primarily due to instabilities in the aggregation process. We also propose a novel FL framework to mitigate the adverse effects of covariate shifts among federated clients by combining individual parameter pruning and regularization techniques to improve the robustness of individual clients' models to aggregate. Each client's model is optimized through magnitude-based pruning and the addition of dropout and noise injection layers to build more resilient decision pathways in the networks and improve the robustness of the model's parameter aggregation step. The proposed framework is capable of extracting robust representations even in the presence of very large covariate shifts among client data distributions and in the federation of a small number of clients. Empirical findings substantiate the effectiveness of our proposed methodology across common benchmark datasets, including CIFAR10, MNIST, SVHN, and Fashion MNIST. Furthermore, we introduce the CelebA-Gender dataset, specifically designed to evaluate performance on a more realistic domain. The proposed method is capable of extracting robust representations even in the presence of both high and low covariate shifts among client data distributions.""}",oai:arXiv.org:2412.15010v1,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Ozgu Goksu, Nicolas Pugeault'}]","Ozgu Goksu, Nicolas Pugeault","{'name': 'Ozgu Goksu, Nicolas Pugeault'}",,
363,Event-based backpropagation on the neuromorphic platform SpiNNaker2,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Event-based backpropagation on the neuromorphic platform SpiNNaker2'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.15021'}]",https://arxiv.org/abs/2412.15021,"arXiv:2412.15021v1 Announce Type: new 
Abstract: Neuromorphic computing aims to replicate the brain's capabilities for energy efficient and parallel information processing, promising a solution to the increasing demand for faster and more efficient computational systems. Efficient training of neural networks on neuromorphic hardware requires the development of training algorithms that retain the sparsity of spike-based communication during training. Here, we report on the first implementation of event-based backpropagation on the SpiNNaker2 neuromorphic hardware platform. We use EventProp, an algorithm for event-based backpropagation in spiking neural networks (SNNs), to compute exact gradients using sparse communication of error signals between neurons. Our implementation computes multi-layer networks of leaky integrate-and-fire neurons using discretized versions of the differential equations and their adjoints, and uses event packets to transmit spikes and error signals between network layers. We demonstrate a proof-of-concept of batch-parallelized, on-chip training of SNNs using the Yin Yang dataset, and provide an off-chip implementation for efficient prototyping, hyper-parameter search, and hybrid training methods.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.15021v1 Announce Type: new \nAbstract: Neuromorphic computing aims to replicate the brain's capabilities for energy efficient and parallel information processing, promising a solution to the increasing demand for faster and more efficient computational systems. Efficient training of neural networks on neuromorphic hardware requires the development of training algorithms that retain the sparsity of spike-based communication during training. Here, we report on the first implementation of event-based backpropagation on the SpiNNaker2 neuromorphic hardware platform. We use EventProp, an algorithm for event-based backpropagation in spiking neural networks (SNNs), to compute exact gradients using sparse communication of error signals between neurons. Our implementation computes multi-layer networks of leaky integrate-and-fire neurons using discretized versions of the differential equations and their adjoints, and uses event packets to transmit spikes and error signals between network layers. We demonstrate a proof-of-concept of batch-parallelized, on-chip training of SNNs using the Yin Yang dataset, and provide an off-chip implementation for efficient prototyping, hyper-parameter search, and hybrid training methods.""}",oai:arXiv.org:2412.15021v1,False,"[{'term': 'cs.NE', 'scheme': None, 'label': None}, {'term': 'cs.AR', 'scheme': None, 'label': None}, {'term': 'cs.ET', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': ""B\\'ena Gabriel, Wunderlich Timo, Akl Mahmoud, Vogginger Bernhard, Mayr Christian, Andres Gonzales Hector""}]","B\'ena Gabriel, Wunderlich Timo, Akl Mahmoud, Vogginger Bernhard, Mayr Christian, Andres Gonzales Hector","{'name': ""B\\'ena Gabriel, Wunderlich Timo, Akl Mahmoud, Vogginger Bernhard, Mayr Christian, Andres Gonzales Hector""}",,
364,Stable-V2A: Synthesis of Synchronized Sound Effects with Temporal and Semantic Controls,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Stable-V2A: Synthesis of Synchronized Sound Effects with Temporal and Semantic Controls'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.15023'}]",https://arxiv.org/abs/2412.15023,"arXiv:2412.15023v1 Announce Type: new 
Abstract: Sound designers and Foley artists usually sonorize a scene, such as from a movie or video game, by manually annotating and sonorizing each action of interest in the video. In our case, the intent is to leave full creative control to sound designers with a tool that allows them to bypass the more repetitive parts of their work, thus being able to focus on the creative aspects of sound production. We achieve this presenting Stable-V2A, a two-stage model consisting of: an RMS-Mapper that estimates an envelope representative of the audio characteristics associated with the input video; and Stable-Foley, a diffusion model based on Stable Audio Open that generates audio semantically and temporally aligned with the target video. Temporal alignment is guaranteed by the use of the envelope as a ControlNet input, while semantic alignment is achieved through the use of sound representations chosen by the designer as cross-attention conditioning of the diffusion process. We train and test our model on Greatest Hits, a dataset commonly used to evaluate V2A models. In addition, to test our model on a case study of interest, we introduce Walking The Maps, a dataset of videos extracted from video games depicting animated characters walking in different locations. Samples and code available on our demo page at https://ispamm.github.io/Stable-V2A.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.15023v1 Announce Type: new \nAbstract: Sound designers and Foley artists usually sonorize a scene, such as from a movie or video game, by manually annotating and sonorizing each action of interest in the video. In our case, the intent is to leave full creative control to sound designers with a tool that allows them to bypass the more repetitive parts of their work, thus being able to focus on the creative aspects of sound production. We achieve this presenting Stable-V2A, a two-stage model consisting of: an RMS-Mapper that estimates an envelope representative of the audio characteristics associated with the input video; and Stable-Foley, a diffusion model based on Stable Audio Open that generates audio semantically and temporally aligned with the target video. Temporal alignment is guaranteed by the use of the envelope as a ControlNet input, while semantic alignment is achieved through the use of sound representations chosen by the designer as cross-attention conditioning of the diffusion process. We train and test our model on Greatest Hits, a dataset commonly used to evaluate V2A models. In addition, to test our model on a case study of interest, we introduce Walking The Maps, a dataset of videos extracted from video games depicting animated characters walking in different locations. Samples and code available on our demo page at https://ispamm.github.io/Stable-V2A.'}",oai:arXiv.org:2412.15023v1,False,"[{'term': 'cs.SD', 'scheme': None, 'label': None}, {'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'cs.MM', 'scheme': None, 'label': None}, {'term': 'eess.AS', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Riccardo Fosco Gramaccioni, Christian Marinoni, Emilian Postolache, Marco Comunit\\`a, Luca Cosmo, Joshua D. Reiss, Danilo Comminiello'}]","Riccardo Fosco Gramaccioni, Christian Marinoni, Emilian Postolache, Marco Comunit\`a, Luca Cosmo, Joshua D. Reiss, Danilo Comminiello","{'name': 'Riccardo Fosco Gramaccioni, Christian Marinoni, Emilian Postolache, Marco Comunit\\`a, Luca Cosmo, Joshua D. Reiss, Danilo Comminiello'}",,
365,When Copilot Becomes Autopilot: Generative AI's Critical Risk to Knowledge Work and a Critical Solution,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""When Copilot Becomes Autopilot: Generative AI's Critical Risk to Knowledge Work and a Critical Solution""}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.15030'}]",https://arxiv.org/abs/2412.15030,"arXiv:2412.15030v1 Announce Type: new 
Abstract: Generative AI, with its tendency to ""hallucinate"" incorrect results, may pose a risk to knowledge work by introducing errors. On the other hand, it may also provide unprecedented opportunities for users, particularly non-experts, to learn and apply advanced software features and greatly increase the scope and complexity of tasks they can successfully achieve.
  As an example of a complex knowledge workflow that is subject to risks and opportunities from generative AI, we consider the spreadsheet. AI hallucinations are an important challenge, but they are not the greatest risk posed by generative AI to spreadsheet workflows. Rather, as more work can be safely delegated to AI, the risk is that human critical thinking -- the ability to holistically and rigorously evaluate a problem and its solutions -- is degraded in the process. The solution is to design the interfaces of generative AI systems deliberately to foster and encourage critical thinking in knowledge work, building primarily on a long history of research on critical thinking tools for education.
  We discuss a prototype system for the activity of critical shortlisting in spreadsheets. The system uses generative AI to suggest shortlisting criteria and applies these criteria to sort rows in a spreadsheet. It also generates ""provocations"": short text snippets that critique the AI-generated criteria, highlighting risks, shortcomings, and alternatives. Our prototype opens up a rich and completely unexplored design space of critical thinking tools for modern AI-assisted knowledge work. We outline a research agenda for AI as a critic or provocateur, including questions about where and when provocations should appear, their form and content, and potential design trade-offs.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.15030v1 Announce Type: new \nAbstract: Generative AI, with its tendency to ""hallucinate"" incorrect results, may pose a risk to knowledge work by introducing errors. On the other hand, it may also provide unprecedented opportunities for users, particularly non-experts, to learn and apply advanced software features and greatly increase the scope and complexity of tasks they can successfully achieve.\n  As an example of a complex knowledge workflow that is subject to risks and opportunities from generative AI, we consider the spreadsheet. AI hallucinations are an important challenge, but they are not the greatest risk posed by generative AI to spreadsheet workflows. Rather, as more work can be safely delegated to AI, the risk is that human critical thinking -- the ability to holistically and rigorously evaluate a problem and its solutions -- is degraded in the process. The solution is to design the interfaces of generative AI systems deliberately to foster and encourage critical thinking in knowledge work, building primarily on a long history of research on critical thinking tools for education.\n  We discuss a prototype system for the activity of critical shortlisting in spreadsheets. The system uses generative AI to suggest shortlisting criteria and applies these criteria to sort rows in a spreadsheet. It also generates ""provocations"": short text snippets that critique the AI-generated criteria, highlighting risks, shortcomings, and alternatives. Our prototype opens up a rich and completely unexplored design space of critical thinking tools for modern AI-assisted knowledge work. We outline a research agenda for AI as a critic or provocateur, including questions about where and when provocations should appear, their form and content, and potential design trade-offs.'}",oai:arXiv.org:2412.15030v1,False,"[{'term': 'cs.HC', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Advait Sarkar (Tone),  Xiaotong (Tone),  Xu, Neil Toronto, Ian Drosos, Christian Poelitz'}]","Advait Sarkar (Tone),  Xiaotong (Tone),  Xu, Neil Toronto, Ian Drosos, Christian Poelitz","{'name': 'Advait Sarkar (Tone),  Xiaotong (Tone),  Xu, Neil Toronto, Ian Drosos, Christian Poelitz'}",,"Proceedings of the EuSpRIG 2024 Conference ""Spreadsheet Productivity & Risks"" ISBN : 978-1-905404-59-9"
366,DCTdiff: Intriguing Properties of Image Generative Modeling in the DCT Space,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'DCTdiff: Intriguing Properties of Image Generative Modeling in the DCT Space'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.15032'}]",https://arxiv.org/abs/2412.15032,"arXiv:2412.15032v1 Announce Type: new 
Abstract: This paper explores image modeling from the frequency space and introduces DCTdiff, an end-to-end diffusion generative paradigm that efficiently models images in the discrete cosine transform (DCT) space. We investigate the design space of DCTdiff and reveal the key design factors. Experiments on different frameworks (UViT, DiT), generation tasks, and various diffusion samplers demonstrate that DCTdiff outperforms pixel-based diffusion models regarding generative quality and training efficiency. Remarkably, DCTdiff can seamlessly scale up to high-resolution generation without using the latent diffusion paradigm. Finally, we illustrate several intriguing properties of DCT image modeling. For example, we provide a theoretical proof of why `image diffusion can be seen as spectral autoregression', bridging the gap between diffusion and autoregressive models. The effectiveness of DCTdiff and the introduced properties suggest a promising direction for image modeling in the frequency space. The code is at \url{https://github.com/forever208/DCTdiff}.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.15032v1 Announce Type: new \nAbstract: This paper explores image modeling from the frequency space and introduces DCTdiff, an end-to-end diffusion generative paradigm that efficiently models images in the discrete cosine transform (DCT) space. We investigate the design space of DCTdiff and reveal the key design factors. Experiments on different frameworks (UViT, DiT), generation tasks, and various diffusion samplers demonstrate that DCTdiff outperforms pixel-based diffusion models regarding generative quality and training efficiency. Remarkably, DCTdiff can seamlessly scale up to high-resolution generation without using the latent diffusion paradigm. Finally, we illustrate several intriguing properties of DCT image modeling. For example, we provide a theoretical proof of why `image diffusion can be seen as spectral autoregression', bridging the gap between diffusion and autoregressive models. The effectiveness of DCTdiff and the introduced properties suggest a promising direction for image modeling in the frequency space. The code is at \\url{https://github.com/forever208/DCTdiff}.""}",oai:arXiv.org:2412.15032v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'eess.IV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Mang Ning, Mingxiao Li, Jianlin Su, Haozhe Jia, Lanmiao Liu, Martin Bene\\v{s}, Albert Ali Salah, Itir Onal Ertugrul'}]","Mang Ning, Mingxiao Li, Jianlin Su, Haozhe Jia, Lanmiao Liu, Martin Bene\v{s}, Albert Ali Salah, Itir Onal Ertugrul","{'name': 'Mang Ning, Mingxiao Li, Jianlin Su, Haozhe Jia, Lanmiao Liu, Martin Bene\\v{s}, Albert Ali Salah, Itir Onal Ertugrul'}",,
367,LLMs Lost in Translation: M-ALERT uncovers Cross-Linguistic Safety Gaps,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'LLMs Lost in Translation: M-ALERT uncovers Cross-Linguistic Safety Gaps'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.15035'}]",https://arxiv.org/abs/2412.15035,"arXiv:2412.15035v1 Announce Type: new 
Abstract: Building safe Large Language Models (LLMs) across multiple languages is essential in ensuring both safe access and linguistic diversity. To this end, we introduce M-ALERT, a multilingual benchmark that evaluates the safety of LLMs in five languages: English, French, German, Italian, and Spanish. M-ALERT includes 15k high-quality prompts per language, totaling 75k, following the detailed ALERT taxonomy. Our extensive experiments on 10 state-of-the-art LLMs highlight the importance of language-specific safety analysis, revealing that models often exhibit significant inconsistencies in safety across languages and categories. For instance, Llama3.2 shows high unsafety in the category crime_tax for Italian but remains safe in other languages. Similar differences can be observed across all models. In contrast, certain categories, such as substance_cannabis and crime_propaganda, consistently trigger unsafe responses across models and languages. These findings underscore the need for robust multilingual safety practices in LLMs to ensure safe and responsible usage across diverse user communities.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.15035v1 Announce Type: new \nAbstract: Building safe Large Language Models (LLMs) across multiple languages is essential in ensuring both safe access and linguistic diversity. To this end, we introduce M-ALERT, a multilingual benchmark that evaluates the safety of LLMs in five languages: English, French, German, Italian, and Spanish. M-ALERT includes 15k high-quality prompts per language, totaling 75k, following the detailed ALERT taxonomy. Our extensive experiments on 10 state-of-the-art LLMs highlight the importance of language-specific safety analysis, revealing that models often exhibit significant inconsistencies in safety across languages and categories. For instance, Llama3.2 shows high unsafety in the category crime_tax for Italian but remains safe in other languages. Similar differences can be observed across all models. In contrast, certain categories, such as substance_cannabis and crime_propaganda, consistently trigger unsafe responses across models and languages. These findings underscore the need for robust multilingual safety practices in LLMs to ensure safe and responsible usage across diverse user communities.'}",oai:arXiv.org:2412.15035v1,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Felix Friedrich, Simone Tedeschi, Patrick Schramowski, Manuel Brack, Roberto Navigli, Huu Nguyen, Bo Li, Kristian Kersting'}]","Felix Friedrich, Simone Tedeschi, Patrick Schramowski, Manuel Brack, Roberto Navigli, Huu Nguyen, Bo Li, Kristian Kersting","{'name': 'Felix Friedrich, Simone Tedeschi, Patrick Schramowski, Manuel Brack, Roberto Navigli, Huu Nguyen, Bo Li, Kristian Kersting'}",,
368,"Compiling C to Safe Rust, Formalized","{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Compiling C to Safe Rust, Formalized'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.15042'}]",https://arxiv.org/abs/2412.15042,"arXiv:2412.15042v1 Announce Type: new 
Abstract: The popularity of the Rust language continues to explode; yet, many critical codebases remain authored in C, and cannot be realistically rewritten by hand. Automatically translating C to Rust is thus an appealing course of action. Several works have gone down this path, handling an ever-increasing subset of C through a variety of Rust features, such as unsafe. While the prospect of automation is appealing, producing code that relies on unsafe negates the memory safety guarantees offered by Rust, and therefore the main advantages of porting existing codebases to memory-safe languages.
  We instead explore a different path, and explore what it would take to translate C to safe Rust; that is, to produce code that is trivially memory safe, because it abides by Rust's type system without caveats. Our work sports several original contributions: a type-directed translation from (a subset of) C to safe Rust; a novel static analysis based on ""split trees"" that allows expressing C's pointer arithmetic using Rust's slices and splitting operations; an analysis that infers exactly which borrows need to be mutable; and a compilation strategy for C's struct types that is compatible with Rust's distinction between non-owned and owned allocations.
  We apply our methodology to existing formally verified C codebases: the HACL* cryptographic library, and binary parsers and serializers from EverParse, and show that the subset of C we support is sufficient to translate both applications to safe Rust. Our evaluation shows that for the few places that do violate Rust's aliasing discipline, automated, surgical rewrites suffice; and that the few strategic copies we insert have a negligible performance impact. Of particular note, the application of our approach to HACL* results in a 80,000 line verified cryptographic library, written in pure Rust, that implements all modern algorithms - the first of its kind.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.15042v1 Announce Type: new \nAbstract: The popularity of the Rust language continues to explode; yet, many critical codebases remain authored in C, and cannot be realistically rewritten by hand. Automatically translating C to Rust is thus an appealing course of action. Several works have gone down this path, handling an ever-increasing subset of C through a variety of Rust features, such as unsafe. While the prospect of automation is appealing, producing code that relies on unsafe negates the memory safety guarantees offered by Rust, and therefore the main advantages of porting existing codebases to memory-safe languages.\n  We instead explore a different path, and explore what it would take to translate C to safe Rust; that is, to produce code that is trivially memory safe, because it abides by Rust\'s type system without caveats. Our work sports several original contributions: a type-directed translation from (a subset of) C to safe Rust; a novel static analysis based on ""split trees"" that allows expressing C\'s pointer arithmetic using Rust\'s slices and splitting operations; an analysis that infers exactly which borrows need to be mutable; and a compilation strategy for C\'s struct types that is compatible with Rust\'s distinction between non-owned and owned allocations.\n  We apply our methodology to existing formally verified C codebases: the HACL* cryptographic library, and binary parsers and serializers from EverParse, and show that the subset of C we support is sufficient to translate both applications to safe Rust. Our evaluation shows that for the few places that do violate Rust\'s aliasing discipline, automated, surgical rewrites suffice; and that the few strategic copies we insert have a negligible performance impact. Of particular note, the application of our approach to HACL* results in a 80,000 line verified cryptographic library, written in pure Rust, that implements all modern algorithms - the first of its kind.'}",oai:arXiv.org:2412.15042v1,False,"[{'term': 'cs.PL', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by-nc-sa/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-sa/4.0/'}","[{'name': 'Aymeric Fromherz, Jonathan Protzenko'}]","Aymeric Fromherz, Jonathan Protzenko","{'name': 'Aymeric Fromherz, Jonathan Protzenko'}",,
369,"Measuring, Modeling, and Helping People Account for Privacy Risks in Online Self-Disclosures with AI","{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Measuring, Modeling, and Helping People Account for Privacy Risks in Online Self-Disclosures with AI'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.15047'}]",https://arxiv.org/abs/2412.15047,"arXiv:2412.15047v1 Announce Type: new 
Abstract: In pseudonymous online fora like Reddit, the benefits of self-disclosure are often apparent to users (e.g., I can vent about my in-laws to understanding strangers), but the privacy risks are more abstract (e.g., will my partner be able to tell that this is me?). Prior work has sought to develop natural language processing (NLP) tools that help users identify potentially risky self-disclosures in their text, but none have been designed for or evaluated with the users they hope to protect. Absent this assessment, these tools will be limited by the social-technical gap: users need assistive tools that help them make informed decisions, not paternalistic tools that tell them to avoid self-disclosure altogether. To bridge this gap, we conducted a study with N = 21 Reddit users; we had them use a state-of-the-art NLP disclosure detection model on two of their authored posts and asked them questions to understand if and how the model helped, where it fell short, and how it could be improved to help them make more informed decisions. Despite its imperfections, users responded positively to the model and highlighted its use as a tool that can help them catch mistakes, inform them of risks they were unaware of, and encourage self-reflection. However, our work also shows how, to be useful and usable, AI for supporting privacy decision-making must account for posting context, disclosure norms, and users' lived threat models, and provide explanations that help contextualize detected risks.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.15047v1 Announce Type: new \nAbstract: In pseudonymous online fora like Reddit, the benefits of self-disclosure are often apparent to users (e.g., I can vent about my in-laws to understanding strangers), but the privacy risks are more abstract (e.g., will my partner be able to tell that this is me?). Prior work has sought to develop natural language processing (NLP) tools that help users identify potentially risky self-disclosures in their text, but none have been designed for or evaluated with the users they hope to protect. Absent this assessment, these tools will be limited by the social-technical gap: users need assistive tools that help them make informed decisions, not paternalistic tools that tell them to avoid self-disclosure altogether. To bridge this gap, we conducted a study with N = 21 Reddit users; we had them use a state-of-the-art NLP disclosure detection model on two of their authored posts and asked them questions to understand if and how the model helped, where it fell short, and how it could be improved to help them make more informed decisions. Despite its imperfections, users responded positively to the model and highlighted its use as a tool that can help them catch mistakes, inform them of risks they were unaware of, and encourage self-reflection. However, our work also shows how, to be useful and usable, AI for supporting privacy decision-making must account for posting context, disclosure norms, and users' lived threat models, and provide explanations that help contextualize detected risks.""}",oai:arXiv.org:2412.15047v1,False,"[{'term': 'cs.HC', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Isadora Krsek, Anubha Kabra, Yao Dou, Tarek Naous, Laura A. Dabbish, Alan Ritter, Wei Xu, Sauvik Das'}]","Isadora Krsek, Anubha Kabra, Yao Dou, Tarek Naous, Laura A. Dabbish, Alan Ritter, Wei Xu, Sauvik Das","{'name': 'Isadora Krsek, Anubha Kabra, Yao Dou, Tarek Naous, Laura A. Dabbish, Alan Ritter, Wei Xu, Sauvik Das'}",,
370,Uni-Renderer: Unifying Rendering and Inverse Rendering Via Dual Stream Diffusion,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Uni-Renderer: Unifying Rendering and Inverse Rendering Via Dual Stream Diffusion'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.15050'}]",https://arxiv.org/abs/2412.15050,"arXiv:2412.15050v1 Announce Type: new 
Abstract: Rendering and inverse rendering are pivotal tasks in both computer vision and graphics. The rendering equation is the core of the two tasks, as an ideal conditional distribution transfer function from intrinsic properties to RGB images. Despite achieving promising results of existing rendering methods, they merely approximate the ideal estimation for a specific scene and come with a high computational cost. Additionally, the inverse conditional distribution transfer is intractable due to the inherent ambiguity. To address these challenges, we propose a data-driven method that jointly models rendering and inverse rendering as two conditional generation tasks within a single diffusion framework. Inspired by UniDiffuser, we utilize two distinct time schedules to model both tasks, and with a tailored dual streaming module, we achieve cross-conditioning of two pre-trained diffusion models. This unified approach, named Uni-Renderer, allows the two processes to facilitate each other through a cycle-consistent constrain, mitigating ambiguity by enforcing consistency between intrinsic properties and rendered images. Combined with a meticulously prepared dataset, our method effectively decomposition of intrinsic properties and demonstrates a strong capability to recognize changes during rendering. We will open-source our training and inference code to the public, fostering further research and development in this area.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.15050v1 Announce Type: new \nAbstract: Rendering and inverse rendering are pivotal tasks in both computer vision and graphics. The rendering equation is the core of the two tasks, as an ideal conditional distribution transfer function from intrinsic properties to RGB images. Despite achieving promising results of existing rendering methods, they merely approximate the ideal estimation for a specific scene and come with a high computational cost. Additionally, the inverse conditional distribution transfer is intractable due to the inherent ambiguity. To address these challenges, we propose a data-driven method that jointly models rendering and inverse rendering as two conditional generation tasks within a single diffusion framework. Inspired by UniDiffuser, we utilize two distinct time schedules to model both tasks, and with a tailored dual streaming module, we achieve cross-conditioning of two pre-trained diffusion models. This unified approach, named Uni-Renderer, allows the two processes to facilitate each other through a cycle-consistent constrain, mitigating ambiguity by enforcing consistency between intrinsic properties and rendered images. Combined with a meticulously prepared dataset, our method effectively decomposition of intrinsic properties and demonstrates a strong capability to recognize changes during rendering. We will open-source our training and inference code to the public, fostering further research and development in this area.'}",oai:arXiv.org:2412.15050v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by-nc-sa/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-sa/4.0/'}","[{'name': 'Zhifei Chen, Tianshuo Xu, Wenhang Ge, Leyi Wu, Dongyu Yan, Jing He, Luozhou Wang, Lu Zeng, Shunsi Zhang, Yingcong Chen'}]","Zhifei Chen, Tianshuo Xu, Wenhang Ge, Leyi Wu, Dongyu Yan, Jing He, Luozhou Wang, Lu Zeng, Shunsi Zhang, Yingcong Chen","{'name': 'Zhifei Chen, Tianshuo Xu, Wenhang Ge, Leyi Wu, Dongyu Yan, Jing He, Luozhou Wang, Lu Zeng, Shunsi Zhang, Yingcong Chen'}",,
371,Contiguous Boundary Guarding,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Contiguous Boundary Guarding'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.15053'}]",https://arxiv.org/abs/2412.15053,"arXiv:2412.15053v1 Announce Type: new 
Abstract: We study the problem of guarding the boundary of a simple polygon with a minimum number of guards such that each guard covers a contiguous portion of the boundary. First, we present a simple greedy algorithm for this problem that returns a guard set of size at most OPT + 1, where OPT is the number of guards in an optimal solution. Then, we present a polynomial-time exact algorithm. While the algorithm is not complicated, its correctness proof is rather involved. This result is interesting in the sense that guarding problems are typically NP-hard and, in particular, it is NP-hard to minimize the number of guards to see the boundary of a simple polygon, without the contiguous boundary guarding constraint.
  From the combinatorial point of view, we show that any $n$-vertex polygon can be guarded by at most $\lfloor \frac{n-2}{2}\rfloor$ guards. This bound is tight because there are polygons that require this many guards.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.15053v1 Announce Type: new \nAbstract: We study the problem of guarding the boundary of a simple polygon with a minimum number of guards such that each guard covers a contiguous portion of the boundary. First, we present a simple greedy algorithm for this problem that returns a guard set of size at most OPT + 1, where OPT is the number of guards in an optimal solution. Then, we present a polynomial-time exact algorithm. While the algorithm is not complicated, its correctness proof is rather involved. This result is interesting in the sense that guarding problems are typically NP-hard and, in particular, it is NP-hard to minimize the number of guards to see the boundary of a simple polygon, without the contiguous boundary guarding constraint.\n  From the combinatorial point of view, we show that any $n$-vertex polygon can be guarded by at most $\\lfloor \\frac{n-2}{2}\\rfloor$ guards. This bound is tight because there are polygons that require this many guards.'}",oai:arXiv.org:2412.15053v1,False,"[{'term': 'cs.CG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Ahmad Biniaz, Anil Maheshwari, Joseph S. B. Mitchell, Saeed Odak, Valentin Polishchuk, Thomas Shermer'}]","Ahmad Biniaz, Anil Maheshwari, Joseph S. B. Mitchell, Saeed Odak, Valentin Polishchuk, Thomas Shermer","{'name': 'Ahmad Biniaz, Anil Maheshwari, Joseph S. B. Mitchell, Saeed Odak, Valentin Polishchuk, Thomas Shermer'}",,
372,"GIRAFE: Glottal Imaging Dataset for Advanced Segmentation, Analysis, and Facilitative Playbacks Evaluation","{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'GIRAFE: Glottal Imaging Dataset for Advanced Segmentation, Analysis, and Facilitative Playbacks Evaluation'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.15054'}]",https://arxiv.org/abs/2412.15054,"arXiv:2412.15054v1 Announce Type: new 
Abstract: The advances in the development of Facilitative Playbacks extracted from High-Speed videoendoscopic sequences of the vocal folds are hindered by a notable lack of publicly available datasets annotated with the semantic segmentations corresponding to the area of the glottal gap. This fact also limits the reproducibility and further exploration of existing research in this field.
  To address this gap, GIRAFE is a data repository designed to facilitate the development of advanced techniques for the semantic segmentation, analysis, and fast evaluation of High-Speed videoendoscopic sequences of the vocal folds. The repository includes 65 high-speed videoendoscopic recordings from a cohort of 50 patients (30 female, 20 male). The dataset comprises 15 recordings from healthy controls, 26 from patients with diagnosed voice disorders, and 24 with an unknown health condition. All of them were manually annotated by an expert, including the masks corresponding to the semantic segmentation of the glottal gap. The repository is also complemented with the automatic segmentation of the glottal area using different state-of-the-art approaches.
  This data set has already supported several studies, which demonstrates its usefulness for the development of new glottal gap segmentation algorithms from High-Speed-Videoendoscopic sequences to improve or create new Facilitative Playbacks. Despite these advances and others in the field, the broader challenge of performing an accurate and completely automatic semantic segmentation method of the glottal area remains open.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.15054v1 Announce Type: new \nAbstract: The advances in the development of Facilitative Playbacks extracted from High-Speed videoendoscopic sequences of the vocal folds are hindered by a notable lack of publicly available datasets annotated with the semantic segmentations corresponding to the area of the glottal gap. This fact also limits the reproducibility and further exploration of existing research in this field.\n  To address this gap, GIRAFE is a data repository designed to facilitate the development of advanced techniques for the semantic segmentation, analysis, and fast evaluation of High-Speed videoendoscopic sequences of the vocal folds. The repository includes 65 high-speed videoendoscopic recordings from a cohort of 50 patients (30 female, 20 male). The dataset comprises 15 recordings from healthy controls, 26 from patients with diagnosed voice disorders, and 24 with an unknown health condition. All of them were manually annotated by an expert, including the masks corresponding to the semantic segmentation of the glottal gap. The repository is also complemented with the automatic segmentation of the glottal area using different state-of-the-art approaches.\n  This data set has already supported several studies, which demonstrates its usefulness for the development of new glottal gap segmentation algorithms from High-Speed-Videoendoscopic sequences to improve or create new Facilitative Playbacks. Despite these advances and others in the field, the broader challenge of performing an accurate and completely automatic semantic segmentation method of the glottal area remains open.'}",oai:arXiv.org:2412.15054v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.SD', 'scheme': None, 'label': None}, {'term': 'eess.AS', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by-nc-nd/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-nd/4.0/'}","[{'name': 'G. Andrade-Miranda, K. Chatzipapas, J. D. Arias-Londo\\~no, J. I. Godino-Llorente'}]","G. Andrade-Miranda, K. Chatzipapas, J. D. Arias-Londo\~no, J. I. Godino-Llorente","{'name': 'G. Andrade-Miranda, K. Chatzipapas, J. D. Arias-Londo\\~no, J. I. Godino-Llorente'}",,
373,MultiverSeg: Scalable Interactive Segmentation of Biomedical Imaging Datasets with In-Context Guidance,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'MultiverSeg: Scalable Interactive Segmentation of Biomedical Imaging Datasets with In-Context Guidance'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.15058'}]",https://arxiv.org/abs/2412.15058,"arXiv:2412.15058v1 Announce Type: new 
Abstract: Medical researchers and clinicians often need to perform novel segmentation tasks on a set of related images. Existing methods for segmenting a new dataset are either interactive, requiring substantial human effort for each image, or require an existing set of manually labeled images. We introduce a system, MultiverSeg, that enables practitioners to rapidly segment an entire new dataset without requiring access to any existing labeled data from that task or domain. Along with the image to segment, the model takes user interactions such as clicks, bounding boxes or scribbles as input, and predicts a segmentation. As the user segments more images, those images and segmentations become additional inputs to the model, providing context. As the context set of labeled images grows, the number of interactions required to segment each new image decreases. We demonstrate that MultiverSeg enables users to interactively segment new datasets efficiently, by amortizing the number of interactions per image to achieve an accurate segmentation. Compared to using a state-of-the-art interactive segmentation method, using MultiverSeg reduced the total number of scribble steps by 53% and clicks by 36% to achieve 90% Dice on sets of images from unseen tasks. We release code and model weights at https://multiverseg.csail.mit.edu","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.15058v1 Announce Type: new \nAbstract: Medical researchers and clinicians often need to perform novel segmentation tasks on a set of related images. Existing methods for segmenting a new dataset are either interactive, requiring substantial human effort for each image, or require an existing set of manually labeled images. We introduce a system, MultiverSeg, that enables practitioners to rapidly segment an entire new dataset without requiring access to any existing labeled data from that task or domain. Along with the image to segment, the model takes user interactions such as clicks, bounding boxes or scribbles as input, and predicts a segmentation. As the user segments more images, those images and segmentations become additional inputs to the model, providing context. As the context set of labeled images grows, the number of interactions required to segment each new image decreases. We demonstrate that MultiverSeg enables users to interactively segment new datasets efficiently, by amortizing the number of interactions per image to achieve an accurate segmentation. Compared to using a state-of-the-art interactive segmentation method, using MultiverSeg reduced the total number of scribble steps by 53% and clicks by 36% to achieve 90% Dice on sets of images from unseen tasks. We release code and model weights at https://multiverseg.csail.mit.edu'}",oai:arXiv.org:2412.15058v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'eess.IV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Hallee E. Wong, Jose Javier Gonzalez Ortiz, John Guttag, Adrian V. Dalca'}]","Hallee E. Wong, Jose Javier Gonzalez Ortiz, John Guttag, Adrian V. Dalca","{'name': 'Hallee E. Wong, Jose Javier Gonzalez Ortiz, John Guttag, Adrian V. Dalca'}",,
374,ConfliBERT: A Language Model for Political Conflict,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'ConfliBERT: A Language Model for Political Conflict'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.15060'}]",https://arxiv.org/abs/2412.15060,"arXiv:2412.15060v1 Announce Type: new 
Abstract: Conflict scholars have used rule-based approaches to extract information about political violence from news reports and texts. Recent Natural Language Processing developments move beyond rigid rule-based approaches. We review our recent ConfliBERT language model (Hu et al. 2022) to process political and violence related texts. The model can be used to extract actor and action classifications from texts about political conflict. When fine-tuned, results show that ConfliBERT has superior performance in accuracy, precision and recall over other large language models (LLM) like Google's Gemma 2 (9B), Meta's Llama 3.1 (7B), and Alibaba's Qwen 2.5 (14B) within its relevant domains. It is also hundreds of times faster than these more generalist LLMs. These results are illustrated using texts from the BBC, re3d, and the Global Terrorism Dataset (GTD).","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.15060v1 Announce Type: new \nAbstract: Conflict scholars have used rule-based approaches to extract information about political violence from news reports and texts. Recent Natural Language Processing developments move beyond rigid rule-based approaches. We review our recent ConfliBERT language model (Hu et al. 2022) to process political and violence related texts. The model can be used to extract actor and action classifications from texts about political conflict. When fine-tuned, results show that ConfliBERT has superior performance in accuracy, precision and recall over other large language models (LLM) like Google's Gemma 2 (9B), Meta's Llama 3.1 (7B), and Alibaba's Qwen 2.5 (14B) within its relevant domains. It is also hundreds of times faster than these more generalist LLMs. These results are illustrated using texts from the BBC, re3d, and the Global Terrorism Dataset (GTD).""}",oai:arXiv.org:2412.15060v1,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Patrick T. Brandt, Sultan Alsarra, Vito J. D`Orazio, Dagmar Heintze, Latifur Khan, Shreyas Meher, Javier Osorio, Marcus Sianan'}]","Patrick T. Brandt, Sultan Alsarra, Vito J. D`Orazio, Dagmar Heintze, Latifur Khan, Shreyas Meher, Javier Osorio, Marcus Sianan","{'name': 'Patrick T. Brandt, Sultan Alsarra, Vito J. D`Orazio, Dagmar Heintze, Latifur Khan, Shreyas Meher, Javier Osorio, Marcus Sianan'}",,
375,"Numerical analysis and simulation of lateral memristive devices: Schottky, ohmic, and multi-dimensional electrode models","{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Numerical analysis and simulation of lateral memristive devices: Schottky, ohmic, and multi-dimensional electrode models'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.15065'}]",https://arxiv.org/abs/2412.15065,"arXiv:2412.15065v1 Announce Type: new 
Abstract: In this paper, we present the numerical analysis and simulations of a multi-dimensional memristive device model. Memristive devices and memtransistors based on two-dimensional (2D) materials have demonstrated promising potential as components for next-generation artificial intelligence (AI) hardware and information technology. Our charge transport model describes the drift-diffusion of electrons, holes, and ionic defects self-consistently in an electric field. We incorporate two types of boundary models: ohmic and Schottky contacts. The coupled drift-diffusion partial differential equations are discretized using a physics-preserving Voronoi finite volume method. It relies on an implicit time-stepping scheme and the excess chemical potential flux approximation. We demonstrate that the fully discrete nonlinear scheme is unconditionally stable, preserving the free-energy structure of the continuous system and ensuring the non-negativity of carrier densities. Novel discrete entropy-dissipation inequalities for both boundary condition types in multiple dimensions allow us to prove the existence of discrete solutions. We perform multi-dimensional simulations to understand the impact of electrode configurations and device geometries, focusing on the hysteresis behavior in lateral 2D memristive devices. Three electrode configurations -- side, top, and mixed contacts -- are compared numerically for different geometries and boundary conditions. These simulations reveal the conditions under which a simplified one-dimensional electrode geometry can well represent the three electrode configurations. This work lays the foundations for developing accurate, efficient simulation tools for 2D memristive devices and memtransistors, offering tools and guidelines for their design and optimization in future applications.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.15065v1 Announce Type: new \nAbstract: In this paper, we present the numerical analysis and simulations of a multi-dimensional memristive device model. Memristive devices and memtransistors based on two-dimensional (2D) materials have demonstrated promising potential as components for next-generation artificial intelligence (AI) hardware and information technology. Our charge transport model describes the drift-diffusion of electrons, holes, and ionic defects self-consistently in an electric field. We incorporate two types of boundary models: ohmic and Schottky contacts. The coupled drift-diffusion partial differential equations are discretized using a physics-preserving Voronoi finite volume method. It relies on an implicit time-stepping scheme and the excess chemical potential flux approximation. We demonstrate that the fully discrete nonlinear scheme is unconditionally stable, preserving the free-energy structure of the continuous system and ensuring the non-negativity of carrier densities. Novel discrete entropy-dissipation inequalities for both boundary condition types in multiple dimensions allow us to prove the existence of discrete solutions. We perform multi-dimensional simulations to understand the impact of electrode configurations and device geometries, focusing on the hysteresis behavior in lateral 2D memristive devices. Three electrode configurations -- side, top, and mixed contacts -- are compared numerically for different geometries and boundary conditions. These simulations reveal the conditions under which a simplified one-dimensional electrode geometry can well represent the three electrode configurations. This work lays the foundations for developing accurate, efficient simulation tools for 2D memristive devices and memtransistors, offering tools and guidelines for their design and optimization in future applications.'}",oai:arXiv.org:2412.15065v1,False,"[{'term': 'math.NA', 'scheme': None, 'label': None}, {'term': 'cs.NA', 'scheme': None, 'label': None}, {'term': 'physics.app-ph', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Dilara Abdel, Maxime Herda, Martin Ziegler, Claire Chainais-Hillairet, Benjamin Spetzler, Patricio Farrell'}]","Dilara Abdel, Maxime Herda, Martin Ziegler, Claire Chainais-Hillairet, Benjamin Spetzler, Patricio Farrell","{'name': 'Dilara Abdel, Maxime Herda, Martin Ziegler, Claire Chainais-Hillairet, Benjamin Spetzler, Patricio Farrell'}",,
376,Fully Dynamic Approximate Minimum Cut in Subpolynomial Time per Operation,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Fully Dynamic Approximate Minimum Cut in Subpolynomial Time per Operation'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.15069'}]",https://arxiv.org/abs/2412.15069,"arXiv:2412.15069v1 Announce Type: new 
Abstract: Dynamically maintaining the minimum cut in a graph $G$ under edge insertions and deletions is a fundamental problem in dynamic graph algorithms for which no conditional lower bound on the time per operation exists. In an $n$-node graph the best known $(1+o(1))$-approximate algorithm takes $\tilde O(\sqrt{n})$ update time [Thorup 2007]. If the minimum cut is guaranteed to be $(\log n)^{o(1)}$, a deterministic exact algorithm with $n^{o(1)}$ update time exists [Jin, Sun, Thorup 2024]. We present the first fully dynamic algorithm for $(1+o(1))$-approximate minimum cut with $n^{o(1)}$ update time. Our main technical contribution is to show that it suffices to consider small-volume cuts in suitably contracted graphs.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.15069v1 Announce Type: new \nAbstract: Dynamically maintaining the minimum cut in a graph $G$ under edge insertions and deletions is a fundamental problem in dynamic graph algorithms for which no conditional lower bound on the time per operation exists. In an $n$-node graph the best known $(1+o(1))$-approximate algorithm takes $\\tilde O(\\sqrt{n})$ update time [Thorup 2007]. If the minimum cut is guaranteed to be $(\\log n)^{o(1)}$, a deterministic exact algorithm with $n^{o(1)}$ update time exists [Jin, Sun, Thorup 2024]. We present the first fully dynamic algorithm for $(1+o(1))$-approximate minimum cut with $n^{o(1)}$ update time. Our main technical contribution is to show that it suffices to consider small-volume cuts in suitably contracted graphs.'}",oai:arXiv.org:2412.15069v1,False,"[{'term': 'cs.DS', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Antoine El-Hayek, Monika Henzinger, Jason Li'}]","Antoine El-Hayek, Monika Henzinger, Jason Li","{'name': 'Antoine El-Hayek, Monika Henzinger, Jason Li'}",,
377,ScamChatBot: An End-to-End Analysis of Fake Account Recovery on Social Media via Chatbots,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'ScamChatBot: An End-to-End Analysis of Fake Account Recovery on Social Media via Chatbots'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.15072'}]",https://arxiv.org/abs/2412.15072,"arXiv:2412.15072v1 Announce Type: new 
Abstract: Social media platforms have become the hubs for various user interactions covering a wide range of needs, including technical support and services related to brands, products, or user accounts. Unfortunately, there has been a recent surge in scammers impersonating official services and providing fake technical support to users through these platforms. In this study, we focus on scammers engaging in such fake technical support to target users who are having problems recovering their accounts. More specifically, we focus on users encountering access problems with social media profiles (e.g., on platforms such as Facebook, Instagram, Gmail, and X) and cryptocurrency wallets. The main contribution of our work is the development of an automated system that interacts with scammers via a chatbot that mimics different personas. By initiating decoy interactions (e.g., through deceptive tweets), we have enticed scammers to interact with our system so that we can analyze their modus operandi. Our results show that scammers employ many social media profiles asking users to contact them via a few communication channels. Using a large language model (LLM), our chatbot had conversations with 450 scammers and provided valuable insights into their tactics and, most importantly, their payment profiles. This automated approach highlights how scammers use a variety of strategies, including role-playing, to trick victims into disclosing personal or financial information. With this study, we lay the foundation for using automated chat-based interactions with scammers to detect and study fraudulent activities at scale in an automated way.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.15072v1 Announce Type: new \nAbstract: Social media platforms have become the hubs for various user interactions covering a wide range of needs, including technical support and services related to brands, products, or user accounts. Unfortunately, there has been a recent surge in scammers impersonating official services and providing fake technical support to users through these platforms. In this study, we focus on scammers engaging in such fake technical support to target users who are having problems recovering their accounts. More specifically, we focus on users encountering access problems with social media profiles (e.g., on platforms such as Facebook, Instagram, Gmail, and X) and cryptocurrency wallets. The main contribution of our work is the development of an automated system that interacts with scammers via a chatbot that mimics different personas. By initiating decoy interactions (e.g., through deceptive tweets), we have enticed scammers to interact with our system so that we can analyze their modus operandi. Our results show that scammers employ many social media profiles asking users to contact them via a few communication channels. Using a large language model (LLM), our chatbot had conversations with 450 scammers and provided valuable insights into their tactics and, most importantly, their payment profiles. This automated approach highlights how scammers use a variety of strategies, including role-playing, to trick victims into disclosing personal or financial information. With this study, we lay the foundation for using automated chat-based interactions with scammers to detect and study fraudulent activities at scale in an automated way.'}",oai:arXiv.org:2412.15072v1,False,"[{'term': 'cs.CR', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Bhupendra Acharya, Dominik Sautter, Muhammad Saad, Thorsten Holz'}]","Bhupendra Acharya, Dominik Sautter, Muhammad Saad, Thorsten Holz","{'name': 'Bhupendra Acharya, Dominik Sautter, Muhammad Saad, Thorsten Holz'}",,
378,DroughtSet: Understanding Drought Through Spatial-Temporal Learning,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'DroughtSet: Understanding Drought Through Spatial-Temporal Learning'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.15075'}]",https://arxiv.org/abs/2412.15075,"arXiv:2412.15075v1 Announce Type: new 
Abstract: Drought is one of the most destructive and expensive natural disasters, severely impacting natural resources and risks by depleting water resources and diminishing agricultural yields. Under climate change, accurately predicting drought is critical for mitigating drought-induced risks. However, the intricate interplay among the physical and biological drivers that regulate droughts limits the predictability and understanding of drought, particularly at a subseasonal to seasonal (S2S) time scale. While deep learning has been demonstrated with potential in addressing climate forecasting challenges, its application to drought prediction has received relatively less attention. In this work, we propose a new dataset, DroughtSet, which integrates relevant predictive features and three drought indices from multiple remote sensing and reanalysis datasets across the contiguous United States (CONUS). DroughtSet specifically provides the machine learning community with a new real-world dataset to benchmark drought prediction models and more generally, time-series forecasting methods. Furthermore, we propose a spatial-temporal model SPDrought to predict and interpret S2S droughts. Our model learns from the spatial and temporal information of physical and biological features to predict three types of droughts simultaneously. Multiple strategies are employed to quantify the importance of physical and biological features for drought prediction. Our results provide insights for researchers to better understand the predictability and sensitivity of drought to biological and physical conditions. We aim to contribute to the climate field by proposing a new tool to predict and understand the occurrence of droughts and provide the AI community with a new benchmark to study deep learning applications in climate science.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.15075v1 Announce Type: new \nAbstract: Drought is one of the most destructive and expensive natural disasters, severely impacting natural resources and risks by depleting water resources and diminishing agricultural yields. Under climate change, accurately predicting drought is critical for mitigating drought-induced risks. However, the intricate interplay among the physical and biological drivers that regulate droughts limits the predictability and understanding of drought, particularly at a subseasonal to seasonal (S2S) time scale. While deep learning has been demonstrated with potential in addressing climate forecasting challenges, its application to drought prediction has received relatively less attention. In this work, we propose a new dataset, DroughtSet, which integrates relevant predictive features and three drought indices from multiple remote sensing and reanalysis datasets across the contiguous United States (CONUS). DroughtSet specifically provides the machine learning community with a new real-world dataset to benchmark drought prediction models and more generally, time-series forecasting methods. Furthermore, we propose a spatial-temporal model SPDrought to predict and interpret S2S droughts. Our model learns from the spatial and temporal information of physical and biological features to predict three types of droughts simultaneously. Multiple strategies are employed to quantify the importance of physical and biological features for drought prediction. Our results provide insights for researchers to better understand the predictability and sensitivity of drought to biological and physical conditions. We aim to contribute to the climate field by proposing a new tool to predict and understand the occurrence of droughts and provide the AI community with a new benchmark to study deep learning applications in climate science.'}",oai:arXiv.org:2412.15075v1,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Xuwei Tan, Qian Zhao, Yanlan Liu, Xueru Zhang'}]","Xuwei Tan, Qian Zhao, Yanlan Liu, Xueru Zhang","{'name': 'Xuwei Tan, Qian Zhao, Yanlan Liu, Xueru Zhang'}",,
379,Till the Layers Collapse: Compressing a Deep Neural Network through the Lenses of Batch Normalization Layers,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Till the Layers Collapse: Compressing a Deep Neural Network through the Lenses of Batch Normalization Layers'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.15077'}]",https://arxiv.org/abs/2412.15077,"arXiv:2412.15077v1 Announce Type: new 
Abstract: Today, deep neural networks are widely used since they can handle a variety of complex tasks. Their generality makes them very powerful tools in modern technology. However, deep neural networks are often overparameterized. The usage of these large models consumes a lot of computation resources. In this paper, we introduce a method called \textbf{T}ill the \textbf{L}ayers \textbf{C}ollapse (TLC), which compresses deep neural networks through the lenses of batch normalization layers. By reducing the depth of these networks, our method decreases deep neural networks' computational requirements and overall latency. We validate our method on popular models such as Swin-T, MobileNet-V2, and RoBERTa, across both image classification and natural language processing (NLP) tasks.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.15077v1 Announce Type: new \nAbstract: Today, deep neural networks are widely used since they can handle a variety of complex tasks. Their generality makes them very powerful tools in modern technology. However, deep neural networks are often overparameterized. The usage of these large models consumes a lot of computation resources. In this paper, we introduce a method called \\textbf{T}ill the \\textbf{L}ayers \\textbf{C}ollapse (TLC), which compresses deep neural networks through the lenses of batch normalization layers. By reducing the depth of these networks, our method decreases deep neural networks' computational requirements and overall latency. We validate our method on popular models such as Swin-T, MobileNet-V2, and RoBERTa, across both image classification and natural language processing (NLP) tasks.""}",oai:arXiv.org:2412.15077v1,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'cs.CL', 'scheme': None, 'label': None}, {'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': ""Zhu Liao, Nour Hezbri, Victor Qu\\'etu, Van-Tam Nguyen, Enzo Tartaglione""}]","Zhu Liao, Nour Hezbri, Victor Qu\'etu, Van-Tam Nguyen, Enzo Tartaglione","{'name': ""Zhu Liao, Nour Hezbri, Victor Qu\\'etu, Van-Tam Nguyen, Enzo Tartaglione""}",,
380,Novel Conditions for the Finite-Region Stability of 2D-Systems with Application to Iterative Learning Control,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Novel Conditions for the Finite-Region Stability of 2D-Systems with Application to Iterative Learning Control'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.15078'}]",https://arxiv.org/abs/2412.15078,"arXiv:2412.15078v1 Announce Type: new 
Abstract: Some recent papers have extended the concept of finite-time stability (FTS) to the context of 2D linear systems, where it has been referred to as finite-region stability (FRS). FRS methodologies make even more sense than the classical FTS approach developed for 1D-systems, since, typically, at least one of the state variables of 2D-systems is a space coordinate, rather than a time variable. Since space coordinates clearly belong to finite intervals, FRS techniques are much more effective than the classical Lyapunov approach, which looks to the asymptotic behavior of the system over an infinite interval. To this regard, the novel contribution of this paper goes in several directions. First, we provide a novel sufficient condition for the FRS of linear time-varying (LTV) discrete-time 2D-systems, which turns out to be less conservative than those ones provided in the existing literature. Then, an interesting application of FRS to the context of iterative learning control (ILC) is investigated, by exploiting the previously developed theory. In particular, a new procedure is proposed so that the tracking errors of the ILC law converges within the desired bound in a finite number of iterations. Finally, a sufficient condition to solve the finite-region stabilization problem is proposed. All the results provided in the paper lead to optimization problems constrained by linear matrix inequalities (LMIs), that can be solved via widely available software. Numerical examples illustrate and validate the effectiveness of the proposed technique.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.15078v1 Announce Type: new \nAbstract: Some recent papers have extended the concept of finite-time stability (FTS) to the context of 2D linear systems, where it has been referred to as finite-region stability (FRS). FRS methodologies make even more sense than the classical FTS approach developed for 1D-systems, since, typically, at least one of the state variables of 2D-systems is a space coordinate, rather than a time variable. Since space coordinates clearly belong to finite intervals, FRS techniques are much more effective than the classical Lyapunov approach, which looks to the asymptotic behavior of the system over an infinite interval. To this regard, the novel contribution of this paper goes in several directions. First, we provide a novel sufficient condition for the FRS of linear time-varying (LTV) discrete-time 2D-systems, which turns out to be less conservative than those ones provided in the existing literature. Then, an interesting application of FRS to the context of iterative learning control (ILC) is investigated, by exploiting the previously developed theory. In particular, a new procedure is proposed so that the tracking errors of the ILC law converges within the desired bound in a finite number of iterations. Finally, a sufficient condition to solve the finite-region stabilization problem is proposed. All the results provided in the paper lead to optimization problems constrained by linear matrix inequalities (LMIs), that can be solved via widely available software. Numerical examples illustrate and validate the effectiveness of the proposed technique.'}",oai:arXiv.org:2412.15078v1,False,"[{'term': 'eess.SY', 'scheme': None, 'label': None}, {'term': 'cs.SY', 'scheme': None, 'label': None}, {'term': 'math.OC', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by-nc-nd/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-nd/4.0/'}","[{'name': 'Chao Liang, Carlo Cosentino, Alessio Merola, Maria Romano, Francesco Amato'}]","Chao Liang, Carlo Cosentino, Alessio Merola, Maria Romano, Francesco Amato","{'name': 'Chao Liang, Carlo Cosentino, Alessio Merola, Maria Romano, Francesco Amato'}",,
381,A Traffic Adapative Physics-informed Learning Control for Energy Savings of Connected and Automated Vehicles,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'A Traffic Adapative Physics-informed Learning Control for Energy Savings of Connected and Automated Vehicles'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.15079'}]",https://arxiv.org/abs/2412.15079,"arXiv:2412.15079v1 Announce Type: new 
Abstract: Model predictive control has emerged as an effective approach for real-time optimal control of connected and automated vehicles. However, nonlinear dynamics of vehicle and traffic systems make accurate modeling and real-time optimization challenging. Learning-based control offer a promising alternative, as they adapt to environment without requiring an explicit model. For learning control framework, an augmented state space system design is necessary since optimal control depends on both the ego vehicle's state and predicted states of other vehicles. This work develops a traffic adaptive augmented state space system that allows the control strategy to intelligently adapt to varying traffic conditions. This design ensures that while different vehicle trajectories alter initial conditions, the system dynamics remain independent of specific trajectories. Additionally, a physics-informed learning control framework is presented that combines value function from Bellman's equation with derivative of value functions from Pontryagin's Maximum Principle into a unified loss function. This method aims to reduce required training data and time while enhancing robustness and efficiency. The proposed control framework is applied to car-following scenarios in real-world data calibrated simulation environments. The results show that this learning control approach alleviates real-time computational requirements while achieving car-following behaviors comparable to model-based methods, resulting in 9% energy savings in scenarios not previously seen in training dataset.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.15079v1 Announce Type: new \nAbstract: Model predictive control has emerged as an effective approach for real-time optimal control of connected and automated vehicles. However, nonlinear dynamics of vehicle and traffic systems make accurate modeling and real-time optimization challenging. Learning-based control offer a promising alternative, as they adapt to environment without requiring an explicit model. For learning control framework, an augmented state space system design is necessary since optimal control depends on both the ego vehicle's state and predicted states of other vehicles. This work develops a traffic adaptive augmented state space system that allows the control strategy to intelligently adapt to varying traffic conditions. This design ensures that while different vehicle trajectories alter initial conditions, the system dynamics remain independent of specific trajectories. Additionally, a physics-informed learning control framework is presented that combines value function from Bellman's equation with derivative of value functions from Pontryagin's Maximum Principle into a unified loss function. This method aims to reduce required training data and time while enhancing robustness and efficiency. The proposed control framework is applied to car-following scenarios in real-world data calibrated simulation environments. The results show that this learning control approach alleviates real-time computational requirements while achieving car-following behaviors comparable to model-based methods, resulting in 9% energy savings in scenarios not previously seen in training dataset.""}",oai:arXiv.org:2412.15079v1,False,"[{'term': 'eess.SY', 'scheme': None, 'label': None}, {'term': 'cs.SY', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}",[{'name': 'Yunli Shao'}],Yunli Shao,{'name': 'Yunli Shao'},,
382,AceMath: Advancing Frontier Math Reasoning with Post-Training and Reward Modeling,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'AceMath: Advancing Frontier Math Reasoning with Post-Training and Reward Modeling'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.15084'}]",https://arxiv.org/abs/2412.15084,"arXiv:2412.15084v1 Announce Type: new 
Abstract: In this paper, we introduce AceMath, a suite of frontier math models that excel in solving complex math problems, along with highly effective reward models capable of evaluating generated solutions and reliably identifying the correct ones. To develop the instruction-tuned math models, we propose a supervised fine-tuning (SFT) process that first achieves competitive performance across general domains, followed by targeted fine-tuning for the math domain using a carefully curated set of prompts and synthetically generated responses. The resulting model, AceMath-72B-Instruct greatly outperforms Qwen2.5-Math-72B-Instruct, GPT-4o and Claude-3.5 Sonnet. To develop math-specialized reward model, we first construct AceMath-RewardBench, a comprehensive and robust benchmark for evaluating math reward models across diverse problems and difficulty levels. After that, we present a systematic approach to build our math reward models. The resulting model, AceMath-72B-RM, consistently outperforms state-of-the-art reward models. Furthermore, when combining AceMath-72B-Instruct with AceMath-72B-RM, we achieve the highest average rm@8 score across the math reasoning benchmarks. We will release model weights, training data, and evaluation benchmarks at: https://research.nvidia.com/labs/adlr/acemath","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.15084v1 Announce Type: new \nAbstract: In this paper, we introduce AceMath, a suite of frontier math models that excel in solving complex math problems, along with highly effective reward models capable of evaluating generated solutions and reliably identifying the correct ones. To develop the instruction-tuned math models, we propose a supervised fine-tuning (SFT) process that first achieves competitive performance across general domains, followed by targeted fine-tuning for the math domain using a carefully curated set of prompts and synthetically generated responses. The resulting model, AceMath-72B-Instruct greatly outperforms Qwen2.5-Math-72B-Instruct, GPT-4o and Claude-3.5 Sonnet. To develop math-specialized reward model, we first construct AceMath-RewardBench, a comprehensive and robust benchmark for evaluating math reward models across diverse problems and difficulty levels. After that, we present a systematic approach to build our math reward models. The resulting model, AceMath-72B-RM, consistently outperforms state-of-the-art reward models. Furthermore, when combining AceMath-72B-Instruct with AceMath-72B-RM, we achieve the highest average rm@8 score across the math reasoning benchmarks. We will release model weights, training data, and evaluation benchmarks at: https://research.nvidia.com/labs/adlr/acemath'}",oai:arXiv.org:2412.15084v1,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Zihan Liu, Yang Chen, Mohammad Shoeybi, Bryan Catanzaro, Wei Ping'}]","Zihan Liu, Yang Chen, Mohammad Shoeybi, Bryan Catanzaro, Wei Ping","{'name': 'Zihan Liu, Yang Chen, Mohammad Shoeybi, Bryan Catanzaro, Wei Ping'}",,
383,Learning Disentangled Equivariant Representation for Explicitly Controllable 3D Molecule Generation,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Learning Disentangled Equivariant Representation for Explicitly Controllable 3D Molecule Generation'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.15086'}]",https://arxiv.org/abs/2412.15086,"arXiv:2412.15086v1 Announce Type: new 
Abstract: We consider the conditional generation of 3D drug-like molecules with \textit{explicit control} over molecular properties such as drug-like properties (e.g., Quantitative Estimate of Druglikeness or Synthetic Accessibility score) and effectively binding to specific protein sites. To tackle this problem, we propose an E(3)-equivariant Wasserstein autoencoder and factorize the latent space of our generative model into two disentangled aspects: molecular properties and the remaining structural context of 3D molecules. Our model ensures explicit control over these molecular attributes while maintaining equivariance of coordinate representation and invariance of data likelihood. Furthermore, we introduce a novel alignment-based coordinate loss to adapt equivariant networks for auto-regressive de-novo 3D molecule generation from scratch. Extensive experiments validate our model's effectiveness on property-guided and context-guided molecule generation, both for de-novo 3D molecule design and structure-based drug discovery against protein targets.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.15086v1 Announce Type: new \nAbstract: We consider the conditional generation of 3D drug-like molecules with \\textit{explicit control} over molecular properties such as drug-like properties (e.g., Quantitative Estimate of Druglikeness or Synthetic Accessibility score) and effectively binding to specific protein sites. To tackle this problem, we propose an E(3)-equivariant Wasserstein autoencoder and factorize the latent space of our generative model into two disentangled aspects: molecular properties and the remaining structural context of 3D molecules. Our model ensures explicit control over these molecular attributes while maintaining equivariance of coordinate representation and invariance of data likelihood. Furthermore, we introduce a novel alignment-based coordinate loss to adapt equivariant networks for auto-regressive de-novo 3D molecule generation from scratch. Extensive experiments validate our model's effectiveness on property-guided and context-guided molecule generation, both for de-novo 3D molecule design and structure-based drug discovery against protein targets.""}",oai:arXiv.org:2412.15086v1,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Haoran Liu, Youzhi Luo, Tianxiao Li, James Caverlee, Martin Renqiang Min'}]","Haoran Liu, Youzhi Luo, Tianxiao Li, James Caverlee, Martin Renqiang Min","{'name': 'Haoran Liu, Youzhi Luo, Tianxiao Li, James Caverlee, Martin Renqiang Min'}",,
384,Nano-ESG: Extracting Corporate Sustainability Information from News Articles,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Nano-ESG: Extracting Corporate Sustainability Information from News Articles'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.15093'}]",https://arxiv.org/abs/2412.15093,"arXiv:2412.15093v1 Announce Type: new 
Abstract: Determining the sustainability impact of companies is a highly complex subject which has garnered more and more attention over the past few years. Today, investors largely rely on sustainability-ratings from established rating-providers in order to analyze how responsibly a company acts. However, those ratings have recently been criticized for being hard to understand and nearly impossible to reproduce.
  An independent way to find out about the sustainability practices of companies lies in the rich landscape of news article data. In this paper, we explore a different approach to identify key opportunities and challenges of companies in the sustainability domain. We present a novel dataset of more than 840,000 news articles which were gathered for major German companies between January 2023 and September 2024. By applying a mixture of Natural Language Processing techniques, we first identify relevant articles, before summarizing them and extracting their sustainability-related sentiment and aspect using Large Language Models (LLMs). Furthermore, we conduct an evaluation of the obtained data and determine that the LLM-produced answers are accurate. We release both datasets at https://github.com/Bailefan/Nano-ESG.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.15093v1 Announce Type: new \nAbstract: Determining the sustainability impact of companies is a highly complex subject which has garnered more and more attention over the past few years. Today, investors largely rely on sustainability-ratings from established rating-providers in order to analyze how responsibly a company acts. However, those ratings have recently been criticized for being hard to understand and nearly impossible to reproduce.\n  An independent way to find out about the sustainability practices of companies lies in the rich landscape of news article data. In this paper, we explore a different approach to identify key opportunities and challenges of companies in the sustainability domain. We present a novel dataset of more than 840,000 news articles which were gathered for major German companies between January 2023 and September 2024. By applying a mixture of Natural Language Processing techniques, we first identify relevant articles, before summarizing them and extracting their sustainability-related sentiment and aspect using Large Language Models (LLMs). Furthermore, we conduct an evaluation of the obtained data and determine that the LLM-produced answers are accurate. We release both datasets at https://github.com/Bailefan/Nano-ESG.'}",oai:arXiv.org:2412.15093v1,False,"[{'term': 'cs.IR', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Fabian Billert, Stefan Conrad'}]","Fabian Billert, Stefan Conrad","{'name': 'Fabian Billert, Stefan Conrad'}",,
385,A Full Transformer-based Framework for Automatic Pain Estimation using Videos,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'A Full Transformer-based Framework for Automatic Pain Estimation using Videos'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.15095'}]",https://arxiv.org/abs/2412.15095,"arXiv:2412.15095v1 Announce Type: new 
Abstract: The automatic estimation of pain is essential in designing an optimal pain management system offering reliable assessment and reducing the suffering of patients. In this study, we present a novel full transformer-based framework consisting of a Transformer in Transformer (TNT) model and a Transformer leveraging cross-attention and self-attention blocks. Elaborating on videos from the BioVid database, we demonstrate state-of-the-art performances, showing the efficacy, efficiency, and generalization capability across all the primary pain estimation tasks.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.15095v1 Announce Type: new \nAbstract: The automatic estimation of pain is essential in designing an optimal pain management system offering reliable assessment and reducing the suffering of patients. In this study, we present a novel full transformer-based framework consisting of a Transformer in Transformer (TNT) model and a Transformer leveraging cross-attention and self-attention blocks. Elaborating on videos from the BioVid database, we demonstrate state-of-the-art performances, showing the efficacy, efficiency, and generalization capability across all the primary pain estimation tasks.'}",oai:arXiv.org:2412.15095v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Stefanos Gkikas, Manolis Tsiknakis'}]","Stefanos Gkikas, Manolis Tsiknakis","{'name': 'Stefanos Gkikas, Manolis Tsiknakis'}",10.1109/EMBC40787.2023.10340872,2023 45th Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC)
386,A Cross-Domain Study of the Use of Persuasion Techniques in Online Disinformation,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'A Cross-Domain Study of the Use of Persuasion Techniques in Online Disinformation'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.15098'}]",https://arxiv.org/abs/2412.15098,"arXiv:2412.15098v1 Announce Type: new 
Abstract: Disinformation, irrespective of domain or language, aims to deceive or manipulate public opinion, typically through employing advanced persuasion techniques. Qualitative and quantitative research on the weaponisation of persuasion techniques in disinformation has been mostly topic-specific (e.g., COVID-19) with limited cross-domain studies, resulting in a lack of comprehensive understanding of these strategies. This study employs a state-of-the-art persuasion technique classifier to conduct a large-scale, multi-domain analysis of the role of 16 persuasion techniques in disinformation narratives. It shows how different persuasion techniques are employed disproportionately in different disinformation domains. We also include a detailed case study on climate change disinformation, highlighting how linguistic, psychological, and cultural factors shape the adaptation of persuasion strategies to fit unique thematic contexts.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.15098v1 Announce Type: new \nAbstract: Disinformation, irrespective of domain or language, aims to deceive or manipulate public opinion, typically through employing advanced persuasion techniques. Qualitative and quantitative research on the weaponisation of persuasion techniques in disinformation has been mostly topic-specific (e.g., COVID-19) with limited cross-domain studies, resulting in a lack of comprehensive understanding of these strategies. This study employs a state-of-the-art persuasion technique classifier to conduct a large-scale, multi-domain analysis of the role of 16 persuasion techniques in disinformation narratives. It shows how different persuasion techniques are employed disproportionately in different disinformation domains. We also include a detailed case study on climate change disinformation, highlighting how linguistic, psychological, and cultural factors shape the adaptation of persuasion strategies to fit unique thematic contexts.'}",oai:arXiv.org:2412.15098v1,False,"[{'term': 'cs.CY', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.CL', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Jo\\~ao A. Leite, Olesya Razuvayevskaya, Carolina Scarton, Kalina Bontcheva'}]","Jo\~ao A. Leite, Olesya Razuvayevskaya, Carolina Scarton, Kalina Bontcheva","{'name': 'Jo\\~ao A. Leite, Olesya Razuvayevskaya, Carolina Scarton, Kalina Bontcheva'}",,
387,Review-Then-Refine: A Dynamic Framework for Multi-Hop Question Answering with Temporal Adaptability,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Review-Then-Refine: A Dynamic Framework for Multi-Hop Question Answering with Temporal Adaptability'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.15101'}]",https://arxiv.org/abs/2412.15101,"arXiv:2412.15101v1 Announce Type: new 
Abstract: Retrieve-augmented generation (RAG) frameworks have emerged as a promising solution to multi-hop question answering(QA) tasks since it enables large language models (LLMs) to incorporate external knowledge and mitigate their inherent knowledge deficiencies. Despite this progress, existing RAG frameworks, which usually follows the retrieve-then-read paradigm, often struggle with multi-hop QA with temporal information since it has difficulty retrieving and synthesizing accurate time-related information. To address the challenge, this paper proposes a novel framework called review-then-refine, which aims to enhance LLM performance in multi-hop QA scenarios with temporal information. Our approach begins with a review phase, where decomposed sub-queries are dynamically rewritten with temporal information, allowing for subsequent adaptive retrieval and reasoning process. In addition, we implement adaptive retrieval mechanism to minimize unnecessary retrievals, thus reducing the potential for hallucinations. In the subsequent refine phase, the LLM synthesizes the retrieved information from each sub-query along with its internal knowledge to formulate a coherent answer. Extensive experimental results across multiple datasets demonstrate the effectiveness of our proposed framework, highlighting its potential to significantly improve multi-hop QA capabilities in LLMs.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.15101v1 Announce Type: new \nAbstract: Retrieve-augmented generation (RAG) frameworks have emerged as a promising solution to multi-hop question answering(QA) tasks since it enables large language models (LLMs) to incorporate external knowledge and mitigate their inherent knowledge deficiencies. Despite this progress, existing RAG frameworks, which usually follows the retrieve-then-read paradigm, often struggle with multi-hop QA with temporal information since it has difficulty retrieving and synthesizing accurate time-related information. To address the challenge, this paper proposes a novel framework called review-then-refine, which aims to enhance LLM performance in multi-hop QA scenarios with temporal information. Our approach begins with a review phase, where decomposed sub-queries are dynamically rewritten with temporal information, allowing for subsequent adaptive retrieval and reasoning process. In addition, we implement adaptive retrieval mechanism to minimize unnecessary retrievals, thus reducing the potential for hallucinations. In the subsequent refine phase, the LLM synthesizes the retrieved information from each sub-query along with its internal knowledge to formulate a coherent answer. Extensive experimental results across multiple datasets demonstrate the effectiveness of our proposed framework, highlighting its potential to significantly improve multi-hop QA capabilities in LLMs.'}",oai:arXiv.org:2412.15101v1,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Xiangsen Chen, Xuming Hu, Nan Tang'}]","Xiangsen Chen, Xuming Hu, Nan Tang","{'name': 'Xiangsen Chen, Xuming Hu, Nan Tang'}",,
388,Knowing Where to Focus: Attention-Guided Alignment for Text-based Person Search,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Knowing Where to Focus: Attention-Guided Alignment for Text-based Person Search'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.15106'}]",https://arxiv.org/abs/2412.15106,"arXiv:2412.15106v1 Announce Type: new 
Abstract: In the realm of Text-Based Person Search (TBPS), mainstream methods aim to explore more efficient interaction frameworks between text descriptions and visual data. However, recent approaches encounter two principal challenges. Firstly, the widely used random-based Masked Language Modeling (MLM) considers all the words in the text equally during training. However, massive semantically vacuous words ('with', 'the', etc.) be masked fail to contribute efficient interaction in the cross-modal MLM and hampers the representation alignment. Secondly, manual descriptions in TBPS datasets are tedious and inevitably contain several inaccuracies. To address these issues, we introduce an Attention-Guided Alignment (AGA) framework featuring two innovative components: Attention-Guided Mask (AGM) Modeling and Text Enrichment Module (TEM). AGM dynamically masks semantically meaningful words by aggregating the attention weight derived from the text encoding process, thereby cross-modal MLM can capture information related to the masked word from text context and images and align their representations. Meanwhile, TEM alleviates low-quality representations caused by repetitive and erroneous text descriptions by replacing those semantically meaningful words with MLM's prediction. It not only enriches text descriptions but also prevents overfitting. Extensive experiments across three challenging benchmarks demonstrate the effectiveness of our AGA, achieving new state-of-the-art results with Rank-1 accuracy reaching 78.36%, 67.31%, and 67.4% on CUHK-PEDES, ICFG-PEDES, and RSTPReid, respectively.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.15106v1 Announce Type: new \nAbstract: In the realm of Text-Based Person Search (TBPS), mainstream methods aim to explore more efficient interaction frameworks between text descriptions and visual data. However, recent approaches encounter two principal challenges. Firstly, the widely used random-based Masked Language Modeling (MLM) considers all the words in the text equally during training. However, massive semantically vacuous words ('with', 'the', etc.) be masked fail to contribute efficient interaction in the cross-modal MLM and hampers the representation alignment. Secondly, manual descriptions in TBPS datasets are tedious and inevitably contain several inaccuracies. To address these issues, we introduce an Attention-Guided Alignment (AGA) framework featuring two innovative components: Attention-Guided Mask (AGM) Modeling and Text Enrichment Module (TEM). AGM dynamically masks semantically meaningful words by aggregating the attention weight derived from the text encoding process, thereby cross-modal MLM can capture information related to the masked word from text context and images and align their representations. Meanwhile, TEM alleviates low-quality representations caused by repetitive and erroneous text descriptions by replacing those semantically meaningful words with MLM's prediction. It not only enriches text descriptions but also prevents overfitting. Extensive experiments across three challenging benchmarks demonstrate the effectiveness of our AGA, achieving new state-of-the-art results with Rank-1 accuracy reaching 78.36%, 67.31%, and 67.4% on CUHK-PEDES, ICFG-PEDES, and RSTPReid, respectively.""}",oai:arXiv.org:2412.15106v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Lei Tan, Weihao Li, Pingyang Dai, Jie Chen, Liujuan Cao, Rongrong Ji'}]","Lei Tan, Weihao Li, Pingyang Dai, Jie Chen, Liujuan Cao, Rongrong Ji","{'name': 'Lei Tan, Weihao Li, Pingyang Dai, Jie Chen, Liujuan Cao, Rongrong Ji'}",,
389,Predictive Inverse Dynamics Models are Scalable Learners for Robotic Manipulation,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Predictive Inverse Dynamics Models are Scalable Learners for Robotic Manipulation'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.15109'}]",https://arxiv.org/abs/2412.15109,"arXiv:2412.15109v1 Announce Type: new 
Abstract: Current efforts to learn scalable policies in robotic manipulation primarily fall into two categories: one focuses on ""action,"" which involves behavior cloning from extensive collections of robotic data, while the other emphasizes ""vision,"" enhancing model generalization by pre-training representations or generative models, also referred to as world models, using large-scale visual datasets. This paper presents an end-to-end paradigm that predicts actions using inverse dynamics models conditioned on the robot's forecasted visual states, named Predictive Inverse Dynamics Models (PIDM). By closing the loop between vision and action, the end-to-end PIDM can be a better scalable action learner. In practice, we use Transformers to process both visual states and actions, naming the model Seer. It is initially pre-trained on large-scale robotic datasets, such as DROID, and can be adapted to realworld scenarios with a little fine-tuning data. Thanks to large-scale, end-to-end training and the synergy between vision and action, Seer significantly outperforms previous methods across both simulation and real-world experiments. It achieves improvements of 13% on the LIBERO-LONG benchmark, 21% on CALVIN ABC-D, and 43% in real-world tasks. Notably, Seer sets a new state-of-the-art on CALVIN ABC-D benchmark, achieving an average length of 4.28, and exhibits superior generalization for novel objects, lighting conditions, and environments under high-intensity disturbances on real-world scenarios. Code and models are publicly available at https://github.com/OpenRobotLab/Seer/.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.15109v1 Announce Type: new \nAbstract: Current efforts to learn scalable policies in robotic manipulation primarily fall into two categories: one focuses on ""action,"" which involves behavior cloning from extensive collections of robotic data, while the other emphasizes ""vision,"" enhancing model generalization by pre-training representations or generative models, also referred to as world models, using large-scale visual datasets. This paper presents an end-to-end paradigm that predicts actions using inverse dynamics models conditioned on the robot\'s forecasted visual states, named Predictive Inverse Dynamics Models (PIDM). By closing the loop between vision and action, the end-to-end PIDM can be a better scalable action learner. In practice, we use Transformers to process both visual states and actions, naming the model Seer. It is initially pre-trained on large-scale robotic datasets, such as DROID, and can be adapted to realworld scenarios with a little fine-tuning data. Thanks to large-scale, end-to-end training and the synergy between vision and action, Seer significantly outperforms previous methods across both simulation and real-world experiments. It achieves improvements of 13% on the LIBERO-LONG benchmark, 21% on CALVIN ABC-D, and 43% in real-world tasks. Notably, Seer sets a new state-of-the-art on CALVIN ABC-D benchmark, achieving an average length of 4.28, and exhibits superior generalization for novel objects, lighting conditions, and environments under high-intensity disturbances on real-world scenarios. Code and models are publicly available at https://github.com/OpenRobotLab/Seer/.'}",oai:arXiv.org:2412.15109v1,False,"[{'term': 'cs.RO', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by-nc-sa/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-sa/4.0/'}","[{'name': 'Yang Tian, Sizhe Yang, Jia Zeng, Ping Wang, Dahua Lin, Hao Dong, Jiangmiao Pang'}]","Yang Tian, Sizhe Yang, Jia Zeng, Ping Wang, Dahua Lin, Hao Dong, Jiangmiao Pang","{'name': 'Yang Tian, Sizhe Yang, Jia Zeng, Ping Wang, Dahua Lin, Hao Dong, Jiangmiao Pang'}",,
390,Associative memory inspires improvements for in-context learning using a novel attention residual stream architecture,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Associative memory inspires improvements for in-context learning using a novel attention residual stream architecture'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.15113'}]",https://arxiv.org/abs/2412.15113,"arXiv:2412.15113v1 Announce Type: new 
Abstract: Large language models (LLMs) demonstrate an impressive ability to utilise information within the context of their input sequences to appropriately respond to data unseen by the LLM during its training procedure. This ability is known as in-context learning (ICL). Humans and non-human animals demonstrate similar abilities, however their neural architectures differ substantially from LLMs. Despite this, a critical component within LLMs, the attention mechanism, resembles modern associative memory models, widely used in and influenced by the computational neuroscience community to model biological memory systems. Using this connection, we introduce an associative memory model capable of performing ICL. We use this as inspiration for a novel residual stream architecture which allows information to directly flow between attention heads. We test this architecture during training within a two-layer Transformer and show its ICL abilities manifest more quickly than without this modification. We then apply our architecture in small language models with 8 million parameters, focusing on attention head values, with results also indicating improved ICL performance at this larger and more naturalistic scale.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.15113v1 Announce Type: new \nAbstract: Large language models (LLMs) demonstrate an impressive ability to utilise information within the context of their input sequences to appropriately respond to data unseen by the LLM during its training procedure. This ability is known as in-context learning (ICL). Humans and non-human animals demonstrate similar abilities, however their neural architectures differ substantially from LLMs. Despite this, a critical component within LLMs, the attention mechanism, resembles modern associative memory models, widely used in and influenced by the computational neuroscience community to model biological memory systems. Using this connection, we introduce an associative memory model capable of performing ICL. We use this as inspiration for a novel residual stream architecture which allows information to directly flow between attention heads. We test this architecture during training within a two-layer Transformer and show its ICL abilities manifest more quickly than without this modification. We then apply our architecture in small language models with 8 million parameters, focusing on attention head values, with results also indicating improved ICL performance at this larger and more naturalistic scale.'}",oai:arXiv.org:2412.15113v1,False,"[{'term': 'cs.NE', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.CL', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Thomas F Burns, Tomoki Fukai, Christopher J Earls'}]","Thomas F Burns, Tomoki Fukai, Christopher J Earls","{'name': 'Thomas F Burns, Tomoki Fukai, Christopher J Earls'}",,
391,Towards Friendly AI: A Comprehensive Review and New Perspectives on Human-AI Alignment,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Towards Friendly AI: A Comprehensive Review and New Perspectives on Human-AI Alignment'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.15114'}]",https://arxiv.org/abs/2412.15114,"arXiv:2412.15114v1 Announce Type: new 
Abstract: As Artificial Intelligence (AI) continues to advance rapidly, Friendly AI (FAI) has been proposed to advocate for more equitable and fair development of AI. Despite its importance, there is a lack of comprehensive reviews examining FAI from an ethical perspective, as well as limited discussion on its potential applications and future directions. This paper addresses these gaps by providing a thorough review of FAI, focusing on theoretical perspectives both for and against its development, and presenting a formal definition in a clear and accessible format. Key applications are discussed from the perspectives of eXplainable AI (XAI), privacy, fairness and affective computing (AC). Additionally, the paper identifies challenges in current technological advancements and explores future research avenues. The findings emphasise the significance of developing FAI and advocate for its continued advancement to ensure ethical and beneficial AI development.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.15114v1 Announce Type: new \nAbstract: As Artificial Intelligence (AI) continues to advance rapidly, Friendly AI (FAI) has been proposed to advocate for more equitable and fair development of AI. Despite its importance, there is a lack of comprehensive reviews examining FAI from an ethical perspective, as well as limited discussion on its potential applications and future directions. This paper addresses these gaps by providing a thorough review of FAI, focusing on theoretical perspectives both for and against its development, and presenting a formal definition in a clear and accessible format. Key applications are discussed from the perspectives of eXplainable AI (XAI), privacy, fairness and affective computing (AC). Additionally, the paper identifies challenges in current technological advancements and explores future research avenues. The findings emphasise the significance of developing FAI and advocate for its continued advancement to ensure ethical and beneficial AI development.'}",oai:arXiv.org:2412.15114v1,False,"[{'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.CY', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by-nc-sa/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-sa/4.0/'}","[{'name': 'Qiyang Sun, Yupei Li, Emran Alturki, Sunil Munthumoduku Krishna Murthy, Bj\\""orn W. Schuller'}]","Qiyang Sun, Yupei Li, Emran Alturki, Sunil Munthumoduku Krishna Murthy, Bj\""orn W. Schuller","{'name': 'Qiyang Sun, Yupei Li, Emran Alturki, Sunil Munthumoduku Krishna Murthy, Bj\\""orn W. Schuller'}",,
392,Qwen2.5 Technical Report,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Qwen2.5 Technical Report'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.15115'}]",https://arxiv.org/abs/2412.15115,"arXiv:2412.15115v1 Announce Type: new 
Abstract: In this report, we introduce Qwen2.5, a comprehensive series of large language models (LLMs) designed to meet diverse needs. Compared to previous iterations, Qwen 2.5 has been significantly improved during both the pre-training and post-training stages. In terms of pre-training, we have scaled the high-quality pre-training datasets from the previous 7 trillion tokens to 18 trillion tokens. This provides a strong foundation for common sense, expert knowledge, and reasoning capabilities. In terms of post-training, we implement intricate supervised finetuning with over 1 million samples, as well as multistage reinforcement learning. Post-training techniques enhance human preference, and notably improve long text generation, structural data analysis, and instruction following. To handle diverse and varied use cases effectively, we present Qwen2.5 LLM series in rich sizes. Open-weight offerings include base and instruction-tuned models, with quantized versions available. In addition, for hosted solutions, the proprietary models currently include two mixture-of-experts (MoE) variants: Qwen2.5-Turbo and Qwen2.5-Plus, both available from Alibaba Cloud Model Studio. Qwen2.5 has demonstrated top-tier performance on a wide range of benchmarks evaluating language understanding, reasoning, mathematics, coding, human preference alignment, etc. Specifically, the open-weight flagship Qwen2.5-72B-Instruct outperforms a number of open and proprietary models and demonstrates competitive performance to the state-of-the-art open-weight model, Llama-3-405B-Instruct, which is around 5 times larger. Qwen2.5-Turbo and Qwen2.5-Plus offer superior cost-effectiveness while performing competitively against GPT-4o-mini and GPT-4o respectively. Additionally, as the foundation, Qwen2.5 models have been instrumental in training specialized models such as Qwen2.5-Math, Qwen2.5-Coder, QwQ, and multimodal models.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.15115v1 Announce Type: new \nAbstract: In this report, we introduce Qwen2.5, a comprehensive series of large language models (LLMs) designed to meet diverse needs. Compared to previous iterations, Qwen 2.5 has been significantly improved during both the pre-training and post-training stages. In terms of pre-training, we have scaled the high-quality pre-training datasets from the previous 7 trillion tokens to 18 trillion tokens. This provides a strong foundation for common sense, expert knowledge, and reasoning capabilities. In terms of post-training, we implement intricate supervised finetuning with over 1 million samples, as well as multistage reinforcement learning. Post-training techniques enhance human preference, and notably improve long text generation, structural data analysis, and instruction following. To handle diverse and varied use cases effectively, we present Qwen2.5 LLM series in rich sizes. Open-weight offerings include base and instruction-tuned models, with quantized versions available. In addition, for hosted solutions, the proprietary models currently include two mixture-of-experts (MoE) variants: Qwen2.5-Turbo and Qwen2.5-Plus, both available from Alibaba Cloud Model Studio. Qwen2.5 has demonstrated top-tier performance on a wide range of benchmarks evaluating language understanding, reasoning, mathematics, coding, human preference alignment, etc. Specifically, the open-weight flagship Qwen2.5-72B-Instruct outperforms a number of open and proprietary models and demonstrates competitive performance to the state-of-the-art open-weight model, Llama-3-405B-Instruct, which is around 5 times larger. Qwen2.5-Turbo and Qwen2.5-Plus offer superior cost-effectiveness while performing competitively against GPT-4o-mini and GPT-4o respectively. Additionally, as the foundation, Qwen2.5 models have been instrumental in training specialized models such as Qwen2.5-Math, Qwen2.5-Coder, QwQ, and multimodal models.'}",oai:arXiv.org:2412.15115v1,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Qwen (additional authors not shown),  : (additional authors not shown), An Yang (additional authors not shown), Baosong Yang (additional authors not shown), Beichen Zhang (additional authors not shown), Binyuan Hui (additional authors not shown), Bo Zheng (additional authors not shown), Bowen Yu (additional authors not shown), Chengyuan Li (additional authors not shown), Dayiheng Liu (additional authors not shown), Fei Huang (additional authors not shown), Haoran Wei (additional authors not shown), Huan Lin (additional authors not shown), Jian Yang (additional authors not shown), Jianhong Tu (additional authors not shown), Jianwei Zhang (additional authors not shown), Jianxin Yang (additional authors not shown), Jiaxi Yang (additional authors not shown), Jingren Zhou (additional authors not shown), Junyang Lin (additional authors not shown), Kai Dang (additional authors not shown), Keming Lu (additional authors not shown), Keqin Bao (additional authors not shown), Kexin Yang (additional authors not shown), Le Yu (additional authors not shown), Mei Li (additional authors not shown), Mingfeng Xue (additional authors not shown), Pei Zhang (additional authors not shown), Qin Zhu (additional authors not shown), Rui Men (additional authors not shown), Runji Lin (additional authors not shown), Tianhao Li (additional authors not shown), Tingyu Xia (additional authors not shown), Xingzhang Ren (additional authors not shown), Xuancheng Ren (additional authors not shown), Yang Fan (additional authors not shown), Yang Su (additional authors not shown), Yichang Zhang (additional authors not shown), Yu Wan (additional authors not shown), Yuqiong Liu (additional authors not shown), Zeyu Cui (additional authors not shown), Zhenru Zhang (additional authors not shown), Zihan Qiu (additional authors not shown)'}]","Qwen (additional authors not shown),  : (additional authors not shown), An Yang (additional authors not shown), Baosong Yang (additional authors not shown), Beichen Zhang (additional authors not shown), Binyuan Hui (additional authors not shown), Bo Zheng (additional authors not shown), Bowen Yu (additional authors not shown), Chengyuan Li (additional authors not shown), Dayiheng Liu (additional authors not shown), Fei Huang (additional authors not shown), Haoran Wei (additional authors not shown), Huan Lin (additional authors not shown), Jian Yang (additional authors not shown), Jianhong Tu (additional authors not shown), Jianwei Zhang (additional authors not shown), Jianxin Yang (additional authors not shown), Jiaxi Yang (additional authors not shown), Jingren Zhou (additional authors not shown), Junyang Lin (additional authors not shown), Kai Dang (additional authors not shown), Keming Lu (additional authors not shown), Keqin Bao (additional authors not shown), Kexin Yang (additional authors not shown), Le Yu (additional authors not shown), Mei Li (additional authors not shown), Mingfeng Xue (additional authors not shown), Pei Zhang (additional authors not shown), Qin Zhu (additional authors not shown), Rui Men (additional authors not shown), Runji Lin (additional authors not shown), Tianhao Li (additional authors not shown), Tingyu Xia (additional authors not shown), Xingzhang Ren (additional authors not shown), Xuancheng Ren (additional authors not shown), Yang Fan (additional authors not shown), Yang Su (additional authors not shown), Yichang Zhang (additional authors not shown), Yu Wan (additional authors not shown), Yuqiong Liu (additional authors not shown), Zeyu Cui (additional authors not shown), Zhenru Zhang (additional authors not shown), Zihan Qiu (additional authors not shown)","{'name': 'Qwen (additional authors not shown),  : (additional authors not shown), An Yang (additional authors not shown), Baosong Yang (additional authors not shown), Beichen Zhang (additional authors not shown), Binyuan Hui (additional authors not shown), Bo Zheng (additional authors not shown), Bowen Yu (additional authors not shown), Chengyuan Li (additional authors not shown), Dayiheng Liu (additional authors not shown), Fei Huang (additional authors not shown), Haoran Wei (additional authors not shown), Huan Lin (additional authors not shown), Jian Yang (additional authors not shown), Jianhong Tu (additional authors not shown), Jianwei Zhang (additional authors not shown), Jianxin Yang (additional authors not shown), Jiaxi Yang (additional authors not shown), Jingren Zhou (additional authors not shown), Junyang Lin (additional authors not shown), Kai Dang (additional authors not shown), Keming Lu (additional authors not shown), Keqin Bao (additional authors not shown), Kexin Yang (additional authors not shown), Le Yu (additional authors not shown), Mei Li (additional authors not shown), Mingfeng Xue (additional authors not shown), Pei Zhang (additional authors not shown), Qin Zhu (additional authors not shown), Rui Men (additional authors not shown), Runji Lin (additional authors not shown), Tianhao Li (additional authors not shown), Tingyu Xia (additional authors not shown), Xingzhang Ren (additional authors not shown), Xuancheng Ren (additional authors not shown), Yang Fan (additional authors not shown), Yang Su (additional authors not shown), Yichang Zhang (additional authors not shown), Yu Wan (additional authors not shown), Yuqiong Liu (additional authors not shown), Zeyu Cui (additional authors not shown), Zhenru Zhang (additional authors not shown), Zihan Qiu (additional authors not shown)'}",,
393,Outcome-Refining Process Supervision for Code Generation,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Outcome-Refining Process Supervision for Code Generation'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.15118'}]",https://arxiv.org/abs/2412.15118,"arXiv:2412.15118v1 Announce Type: new 
Abstract: Large Language Models have demonstrated remarkable capabilities in code generation, yet they often struggle with complex programming tasks that require deep algorithmic reasoning. While process supervision through learned reward models shows promise in guiding reasoning steps, it requires expensive training data and suffers from unreliable evaluation. We propose Outcome-Refining Process Supervision, a novel paradigm that treats outcome refinement itself as the process to be supervised. Our framework leverages concrete execution signals to ground the supervision of reasoning steps, while using tree-structured exploration to maintain multiple solution trajectories simultaneously. Experiments demonstrate that our approach enables even smaller models to achieve high success accuracy and performance metrics on competitive programming tasks, creates more reliable verification than traditional reward models without requiring training PRMs. Our approach achieves significant improvements across 5 models and 3 datasets: an average of 26.9% increase in correctness and 42.2% in efficiency. The results suggest that providing structured reasoning space with concrete verification signals is crucial for solving complex programming tasks. We open-source all our code and data at: https://github.com/zhuohaoyu/ORPS","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.15118v1 Announce Type: new \nAbstract: Large Language Models have demonstrated remarkable capabilities in code generation, yet they often struggle with complex programming tasks that require deep algorithmic reasoning. While process supervision through learned reward models shows promise in guiding reasoning steps, it requires expensive training data and suffers from unreliable evaluation. We propose Outcome-Refining Process Supervision, a novel paradigm that treats outcome refinement itself as the process to be supervised. Our framework leverages concrete execution signals to ground the supervision of reasoning steps, while using tree-structured exploration to maintain multiple solution trajectories simultaneously. Experiments demonstrate that our approach enables even smaller models to achieve high success accuracy and performance metrics on competitive programming tasks, creates more reliable verification than traditional reward models without requiring training PRMs. Our approach achieves significant improvements across 5 models and 3 datasets: an average of 26.9% increase in correctness and 42.2% in efficiency. The results suggest that providing structured reasoning space with concrete verification signals is crucial for solving complex programming tasks. We open-source all our code and data at: https://github.com/zhuohaoyu/ORPS'}",oai:arXiv.org:2412.15118v1,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'cs.SE', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Zhuohao Yu, Weizheng Gu, Yidong Wang, Zhengran Zeng, Jindong Wang, Wei Ye, Shikun Zhang'}]","Zhuohao Yu, Weizheng Gu, Yidong Wang, Zhengran Zeng, Jindong Wang, Wei Ye, Shikun Zhang","{'name': 'Zhuohao Yu, Weizheng Gu, Yidong Wang, Zhengran Zeng, Jindong Wang, Wei Ye, Shikun Zhang'}",,
394,Parallelized Autoregressive Visual Generation,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Parallelized Autoregressive Visual Generation'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.15119'}]",https://arxiv.org/abs/2412.15119,"arXiv:2412.15119v1 Announce Type: new 
Abstract: Autoregressive models have emerged as a powerful approach for visual generation but suffer from slow inference speed due to their sequential token-by-token prediction process. In this paper, we propose a simple yet effective approach for parallelized autoregressive visual generation that improves generation efficiency while preserving the advantages of autoregressive modeling. Our key insight is that parallel generation depends on visual token dependencies-tokens with weak dependencies can be generated in parallel, while strongly dependent adjacent tokens are difficult to generate together, as their independent sampling may lead to inconsistencies. Based on this observation, we develop a parallel generation strategy that generates distant tokens with weak dependencies in parallel while maintaining sequential generation for strongly dependent local tokens. Our approach can be seamlessly integrated into standard autoregressive models without modifying the architecture or tokenizer. Experiments on ImageNet and UCF-101 demonstrate that our method achieves a 3.6x speedup with comparable quality and up to 9.5x speedup with minimal quality degradation across both image and video generation tasks. We hope this work will inspire future research in efficient visual generation and unified autoregressive modeling. Project page: https://epiphqny.github.io/PAR-project.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.15119v1 Announce Type: new \nAbstract: Autoregressive models have emerged as a powerful approach for visual generation but suffer from slow inference speed due to their sequential token-by-token prediction process. In this paper, we propose a simple yet effective approach for parallelized autoregressive visual generation that improves generation efficiency while preserving the advantages of autoregressive modeling. Our key insight is that parallel generation depends on visual token dependencies-tokens with weak dependencies can be generated in parallel, while strongly dependent adjacent tokens are difficult to generate together, as their independent sampling may lead to inconsistencies. Based on this observation, we develop a parallel generation strategy that generates distant tokens with weak dependencies in parallel while maintaining sequential generation for strongly dependent local tokens. Our approach can be seamlessly integrated into standard autoregressive models without modifying the architecture or tokenizer. Experiments on ImageNet and UCF-101 demonstrate that our method achieves a 3.6x speedup with comparable quality and up to 9.5x speedup with minimal quality degradation across both image and video generation tasks. We hope this work will inspire future research in efficient visual generation and unified autoregressive modeling. Project page: https://epiphqny.github.io/PAR-project.'}",oai:arXiv.org:2412.15119v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Yuqing Wang, Shuhuai Ren, Zhijie Lin, Yujin Han, Haoyuan Guo, Zhenheng Yang, Difan Zou, Jiashi Feng, Xihui Liu'}]","Yuqing Wang, Shuhuai Ren, Zhijie Lin, Yujin Han, Haoyuan Guo, Zhenheng Yang, Difan Zou, Jiashi Feng, Xihui Liu","{'name': 'Yuqing Wang, Shuhuai Ren, Zhijie Lin, Yujin Han, Haoyuan Guo, Zhenheng Yang, Difan Zou, Jiashi Feng, Xihui Liu'}",,
395,Folding One Polyhedral Metric Graph into Another,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Folding One Polyhedral Metric Graph into Another'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.15121'}]",https://arxiv.org/abs/2412.15121,"arXiv:2412.15121v1 Announce Type: new 
Abstract: We analyze the problem of folding one polyhedron, viewed as a metric graph of its edges, into the shape of another, similar to 1D origami. We find such foldings between all pairs of Platonic solids and prove corresponding lower bounds, establishing the optimal scale factor when restricted to integers. Further, we establish that our folding problem is also NP-hard, even if the source graph is a tree. It turns out that the problem is hard to approximate, as we obtain NP-hardness even for determining the existence of a scale factor 1.5-{\epsilon}. Finally, we prove that, in general, the optimal scale factor has to be rational. This insight then immediately results in NP membership. In turn, verifying whether a given scale factor is indeed the smallest possible, requires two independent calls to an NP oracle, rendering the problem DP-complete.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.15121v1 Announce Type: new \nAbstract: We analyze the problem of folding one polyhedron, viewed as a metric graph of its edges, into the shape of another, similar to 1D origami. We find such foldings between all pairs of Platonic solids and prove corresponding lower bounds, establishing the optimal scale factor when restricted to integers. Further, we establish that our folding problem is also NP-hard, even if the source graph is a tree. It turns out that the problem is hard to approximate, as we obtain NP-hardness even for determining the existence of a scale factor 1.5-{\\epsilon}. Finally, we prove that, in general, the optimal scale factor has to be rational. This insight then immediately results in NP membership. In turn, verifying whether a given scale factor is indeed the smallest possible, requires two independent calls to an NP oracle, rendering the problem DP-complete.'}",oai:arXiv.org:2412.15121v1,False,"[{'term': 'cs.CG', 'scheme': None, 'label': None}, {'term': 'cs.CC', 'scheme': None, 'label': None}, {'term': 'cs.DM', 'scheme': None, 'label': None}, {'term': 'cs.SC', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Lily Chung, Erik D. Demaine, Martin L. Demaine, Markus Hecher, Rebecca Lin, Jayson Lynch, Chie Nara'}]","Lily Chung, Erik D. Demaine, Martin L. Demaine, Markus Hecher, Rebecca Lin, Jayson Lynch, Chie Nara","{'name': 'Lily Chung, Erik D. Demaine, Martin L. Demaine, Markus Hecher, Rebecca Lin, Jayson Lynch, Chie Nara'}",,
396,Solving the all pairs shortest path problem after minor update of a large dense graph,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Solving the all pairs shortest path problem after minor update of a large dense graph'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.15122'}]",https://arxiv.org/abs/2412.15122,"arXiv:2412.15122v1 Announce Type: new 
Abstract: The all pairs shortest path problem is a fundamental optimization problem in graph theory. We deal with re-calculating the all-pairs shortest path (APSP) matrix after a minor modification of a weighted dense graph, e.g., adding a node, removing a node, or updating an edge. We assume the APSP matrix for the original graph is already known. The graph can be directed or undirected. A cold-start calculation of the new APSP matrix by traditional algorithms, like the Floyd-Warshall algorithm or Dijkstra's algorithm, needs $ O(n^3) $ time. We propose two algorithms for warm-start calculation of the new APSP matrix. The best case complexity for a warm-start calculation is $ O(n^2) $, the worst case complexity is $ O(n^3) $. We implemented the algorithms and tested their performance with experiments. The result shows a warm-start calculation can save a great portion of calculation time, compared with cold-start calculation.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.15122v1 Announce Type: new \nAbstract: The all pairs shortest path problem is a fundamental optimization problem in graph theory. We deal with re-calculating the all-pairs shortest path (APSP) matrix after a minor modification of a weighted dense graph, e.g., adding a node, removing a node, or updating an edge. We assume the APSP matrix for the original graph is already known. The graph can be directed or undirected. A cold-start calculation of the new APSP matrix by traditional algorithms, like the Floyd-Warshall algorithm or Dijkstra's algorithm, needs $ O(n^3) $ time. We propose two algorithms for warm-start calculation of the new APSP matrix. The best case complexity for a warm-start calculation is $ O(n^2) $, the worst case complexity is $ O(n^3) $. We implemented the algorithms and tested their performance with experiments. The result shows a warm-start calculation can save a great portion of calculation time, compared with cold-start calculation.""}",oai:arXiv.org:2412.15122v1,False,"[{'term': 'cs.DS', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}",[{'name': 'Gangli Liu'}],Gangli Liu,{'name': 'Gangli Liu'},,
397,"Efficient Ranking, Order Statistics, and Sorting under CKKS","{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Efficient Ranking, Order Statistics, and Sorting under CKKS'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.15126'}]",https://arxiv.org/abs/2412.15126,"arXiv:2412.15126v1 Announce Type: new 
Abstract: Fully Homomorphic Encryption (FHE) enables operations on encrypted data, making it extremely useful for privacy-preserving applications, especially in cloud computing environments. In such contexts, operations like ranking, order statistics, and sorting are fundamental functionalities often required for database queries or as building blocks of larger protocols. However, the high computational overhead and limited native operations of FHE pose significant challenges for an efficient implementation of these tasks. These challenges are exacerbated by the fact that all these functionalities are based on comparing elements, which is a severely expensive operation under encryption.
  Previous solutions have typically based their designs on swap-based techniques, where two elements are conditionally swapped based on the results of their comparison. These methods aim to reduce the primary computational bottleneck: the comparison depth, which is the number of non-parallelizable homomorphic comparisons. The current state of the art solution for sorting by Lu et al. (IEEE S&amp;P'21), for instance, achieves a comparison depth of O(log^2(N)).
  In this paper, we address the challenge of reducing the comparison depth by shifting away from the swap-based paradigm. We present solutions for ranking, order statistics, and sorting, that all achieve a comparison depth of O(1), making our approach highly parallelizable. Leveraging the SIMD capabilities of the CKKS FHE scheme, our approach re-encodes the input vector under encryption to allow for simultaneous comparisons of all elements with each other. The homomorphic re-encoding incurs a minimal computational overhead of O(log(N)) rotations. Experimental results show that our approach ranks a 128-element vector in approximately 2.64s, computes its argmin/argmax in 14.18s, and sorts it in 21.10s.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.15126v1 Announce Type: new \nAbstract: Fully Homomorphic Encryption (FHE) enables operations on encrypted data, making it extremely useful for privacy-preserving applications, especially in cloud computing environments. In such contexts, operations like ranking, order statistics, and sorting are fundamental functionalities often required for database queries or as building blocks of larger protocols. However, the high computational overhead and limited native operations of FHE pose significant challenges for an efficient implementation of these tasks. These challenges are exacerbated by the fact that all these functionalities are based on comparing elements, which is a severely expensive operation under encryption.\n  Previous solutions have typically based their designs on swap-based techniques, where two elements are conditionally swapped based on the results of their comparison. These methods aim to reduce the primary computational bottleneck: the comparison depth, which is the number of non-parallelizable homomorphic comparisons. The current state of the art solution for sorting by Lu et al. (IEEE S&amp;P'21), for instance, achieves a comparison depth of O(log^2(N)).\n  In this paper, we address the challenge of reducing the comparison depth by shifting away from the swap-based paradigm. We present solutions for ranking, order statistics, and sorting, that all achieve a comparison depth of O(1), making our approach highly parallelizable. Leveraging the SIMD capabilities of the CKKS FHE scheme, our approach re-encodes the input vector under encryption to allow for simultaneous comparisons of all elements with each other. The homomorphic re-encoding incurs a minimal computational overhead of O(log(N)) rotations. Experimental results show that our approach ranks a 128-element vector in approximately 2.64s, computes its argmin/argmax in 14.18s, and sorts it in 21.10s.""}",oai:arXiv.org:2412.15126v1,False,"[{'term': 'cs.CR', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Federico Mazzone, Maarten Everts, Florian Hahn, Andreas Peter'}]","Federico Mazzone, Maarten Everts, Florian Hahn, Andreas Peter","{'name': 'Federico Mazzone, Maarten Everts, Florian Hahn, Andreas Peter'}",,
398,Adaptive Pruning for Large Language Models with Structural Importance Awareness,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Adaptive Pruning for Large Language Models with Structural Importance Awareness'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.15127'}]",https://arxiv.org/abs/2412.15127,"arXiv:2412.15127v1 Announce Type: new 
Abstract: The recent advancements in large language models (LLMs) have significantly improved language understanding and generation capabilities. However, it is difficult to deploy LLMs on resource-constrained edge devices due to their high computational and storage resource demands. To address this issue, we propose a novel LLM model pruning method, namely structurally-aware adaptive pruning (SAAP), to significantly reduce the computational and memory costs while maintaining model performance. We first define an adaptive importance fusion metric to evaluate the importance of all coupled structures in LLMs by considering their homoscedastic uncertainty. Then, we rank the importance of all modules to determine the specific layers that should be pruned to meet particular performance requirements. Furthermore, we develop a new group fine-tuning strategy to improve the inference efficiency of LLMs. Finally, we evaluate the proposed SAAP method on multiple LLMs across two common tasks, i.e., zero-shot classification and text generation. Experimental results show that our SAAP method outperforms several state-of-the-art baseline methods, achieving 2.17%, 2.37%, and 2.39% accuracy gains on LLaMA-7B, Vicuna-7B, and LLaMA-13B. Additionally, SAAP improves the token generation speed by 5%, showcasing its practical advantages in resource-constrained scenarios.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.15127v1 Announce Type: new \nAbstract: The recent advancements in large language models (LLMs) have significantly improved language understanding and generation capabilities. However, it is difficult to deploy LLMs on resource-constrained edge devices due to their high computational and storage resource demands. To address this issue, we propose a novel LLM model pruning method, namely structurally-aware adaptive pruning (SAAP), to significantly reduce the computational and memory costs while maintaining model performance. We first define an adaptive importance fusion metric to evaluate the importance of all coupled structures in LLMs by considering their homoscedastic uncertainty. Then, we rank the importance of all modules to determine the specific layers that should be pruned to meet particular performance requirements. Furthermore, we develop a new group fine-tuning strategy to improve the inference efficiency of LLMs. Finally, we evaluate the proposed SAAP method on multiple LLMs across two common tasks, i.e., zero-shot classification and text generation. Experimental results show that our SAAP method outperforms several state-of-the-art baseline methods, achieving 2.17%, 2.37%, and 2.39% accuracy gains on LLaMA-7B, Vicuna-7B, and LLaMA-13B. Additionally, SAAP improves the token generation speed by 5%, showcasing its practical advantages in resource-constrained scenarios.'}",oai:arXiv.org:2412.15127v1,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Haotian Zheng, Jinke Ren, Yushan Sun, Ruichen Zhang, Wenbo Zhang, Zhen Li, Dusit Niyato, Shuguang Cui, Yatong Han'}]","Haotian Zheng, Jinke Ren, Yushan Sun, Ruichen Zhang, Wenbo Zhang, Zhen Li, Dusit Niyato, Shuguang Cui, Yatong Han","{'name': 'Haotian Zheng, Jinke Ren, Yushan Sun, Ruichen Zhang, Wenbo Zhang, Zhen Li, Dusit Niyato, Shuguang Cui, Yatong Han'}",,
399,Jet: A Modern Transformer-Based Normalizing Flow,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Jet: A Modern Transformer-Based Normalizing Flow'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.15129'}]",https://arxiv.org/abs/2412.15129,"arXiv:2412.15129v1 Announce Type: new 
Abstract: In the past, normalizing generative flows have emerged as a promising class of generative models for natural images. This type of model has many modeling advantages: the ability to efficiently compute log-likelihood of the input data, fast generation and simple overall structure. Normalizing flows remained a topic of active research but later fell out of favor, as visual quality of the samples was not competitive with other model classes, such as GANs, VQ-VAE-based approaches or diffusion models. In this paper we revisit the design of the coupling-based normalizing flow models by carefully ablating prior design choices and using computational blocks based on the Vision Transformer architecture, not convolutional neural networks. As a result, we achieve state-of-the-art quantitative and qualitative performance with a much simpler architecture. While the overall visual quality is still behind the current state-of-the-art models, we argue that strong normalizing flow models can help advancing research frontier by serving as building components of more powerful generative models.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.15129v1 Announce Type: new \nAbstract: In the past, normalizing generative flows have emerged as a promising class of generative models for natural images. This type of model has many modeling advantages: the ability to efficiently compute log-likelihood of the input data, fast generation and simple overall structure. Normalizing flows remained a topic of active research but later fell out of favor, as visual quality of the samples was not competitive with other model classes, such as GANs, VQ-VAE-based approaches or diffusion models. In this paper we revisit the design of the coupling-based normalizing flow models by carefully ablating prior design choices and using computational blocks based on the Vision Transformer architecture, not convolutional neural networks. As a result, we achieve state-of-the-art quantitative and qualitative performance with a much simpler architecture. While the overall visual quality is still behind the current state-of-the-art models, we argue that strong normalizing flow models can help advancing research frontier by serving as building components of more powerful generative models.'}",oai:arXiv.org:2412.15129v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': ""Alexander Kolesnikov, Andr\\'e Susano Pinto, Michael Tschannen""}]","Alexander Kolesnikov, Andr\'e Susano Pinto, Michael Tschannen","{'name': ""Alexander Kolesnikov, Andr\\'e Susano Pinto, Michael Tschannen""}",,
400,Continuous Flattening and Reversing of Convex Polyhedral Linkages,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Continuous Flattening and Reversing of Convex Polyhedral Linkages'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.15130'}]",https://arxiv.org/abs/2412.15130,"arXiv:2412.15130v1 Announce Type: new 
Abstract: We prove two results about transforming any convex polyhedron, modeled as a linkage L of its edges. First, if we subdivide each edge of L in half, then L can be continuously flattened into a plane. Second, if L is equilateral and we again subdivide each edge in half, then L can be reversed, i.e., turned inside-out. A linear number of subdivisions is optimal up to constant factors, as we show (nonequilateral) examples that require a linear number of subdivisions. For nonequilateral linkages, we show that more subdivisions can be required: even a tetrahedron can require an arbitrary number of subdivisions to reverse. For nonequilateral tetrahedra, we provide an algorithm that matches this lower bound up to constant factors: logarithmic in the aspect ratio.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.15130v1 Announce Type: new \nAbstract: We prove two results about transforming any convex polyhedron, modeled as a linkage L of its edges. First, if we subdivide each edge of L in half, then L can be continuously flattened into a plane. Second, if L is equilateral and we again subdivide each edge in half, then L can be reversed, i.e., turned inside-out. A linear number of subdivisions is optimal up to constant factors, as we show (nonequilateral) examples that require a linear number of subdivisions. For nonequilateral linkages, we show that more subdivisions can be required: even a tetrahedron can require an arbitrary number of subdivisions to reverse. For nonequilateral tetrahedra, we provide an algorithm that matches this lower bound up to constant factors: logarithmic in the aspect ratio.'}",oai:arXiv.org:2412.15130v1,False,"[{'term': 'cs.CG', 'scheme': None, 'label': None}, {'term': 'cs.CC', 'scheme': None, 'label': None}, {'term': 'cs.DM', 'scheme': None, 'label': None}, {'term': 'cs.DS', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Erik D. Demaine, Martin L. Demaine, Markus Hecher, Rebecca Lin, Victor H. Luo, Chie Nara'}]","Erik D. Demaine, Martin L. Demaine, Markus Hecher, Rebecca Lin, Victor H. Luo, Chie Nara","{'name': 'Erik D. Demaine, Martin L. Demaine, Markus Hecher, Rebecca Lin, Victor H. Luo, Chie Nara'}",,
401,Probabilistic Strategy Logic with Degrees of Observability,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Probabilistic Strategy Logic with Degrees of Observability'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.15135'}]",https://arxiv.org/abs/2412.15135,"arXiv:2412.15135v1 Announce Type: new 
Abstract: There has been considerable work on reasoning about the strategic ability of agents under imperfect information. However, existing logics such as Probabilistic Strategy Logic are unable to express properties relating to information transparency. Information transparency concerns the extent to which agents' actions and behaviours are observable by other agents. Reasoning about information transparency is useful in many domains including security, privacy, and decision-making. In this paper, we present a formal framework for reasoning about information transparency properties in stochastic multi-agent systems. We extend Probabilistic Strategy Logic with new observability operators that capture the degree of observability of temporal properties by agents. We show that the model checking problem for the resulting logic is decidable.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.15135v1 Announce Type: new \nAbstract: There has been considerable work on reasoning about the strategic ability of agents under imperfect information. However, existing logics such as Probabilistic Strategy Logic are unable to express properties relating to information transparency. Information transparency concerns the extent to which agents' actions and behaviours are observable by other agents. Reasoning about information transparency is useful in many domains including security, privacy, and decision-making. In this paper, we present a formal framework for reasoning about information transparency properties in stochastic multi-agent systems. We extend Probabilistic Strategy Logic with new observability operators that capture the degree of observability of temporal properties by agents. We show that the model checking problem for the resulting logic is decidable.""}",oai:arXiv.org:2412.15135v1,False,"[{'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.LO', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Chunyan Mu, Nima Motamed, Natasha Alechina, Brian Logan'}]","Chunyan Mu, Nima Motamed, Natasha Alechina, Brian Logan","{'name': 'Chunyan Mu, Nima Motamed, Natasha Alechina, Brian Logan'}",,
402,Hydrogen in Aviation: Evaluating the Feasibility and Benefits of a Green Fuel Alternative,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Hydrogen in Aviation: Evaluating the Feasibility and Benefits of a Green Fuel Alternative'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.15137'}]",https://arxiv.org/abs/2412.15137,"arXiv:2412.15137v1 Announce Type: new 
Abstract: Growing concerns regarding environmental health have highlighted the aviation industry's impact and potential mitigation strategies. Previous research has indicated hydrogen's significant potential for reducing the industry's environmental impact, yet implementation challenges remain. Through analysis of light aircraft and military applications, we demonstrate that hydrogen-based systems can achieve performance metrics approaching those of traditional fuels while reducing emissions by up to 74.7%. Our findings show that hydrogen's superior energy-to-mass ratio (120 MJ/kg versus 43 MJ/kg for jet fuel) makes it particularly advantageous for aviation applications compared to battery-electric alternatives. Primary implementation challenges involve cryogenic storage systems (-253{\deg}C), tank placement optimization, and fueling infrastructure development. The observed efficiency penalties of only 2.23% in military applications suggest hydrogen's viability as a sustainable aviation fuel alternative.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.15137v1 Announce Type: new \nAbstract: Growing concerns regarding environmental health have highlighted the aviation industry's impact and potential mitigation strategies. Previous research has indicated hydrogen's significant potential for reducing the industry's environmental impact, yet implementation challenges remain. Through analysis of light aircraft and military applications, we demonstrate that hydrogen-based systems can achieve performance metrics approaching those of traditional fuels while reducing emissions by up to 74.7%. Our findings show that hydrogen's superior energy-to-mass ratio (120 MJ/kg versus 43 MJ/kg for jet fuel) makes it particularly advantageous for aviation applications compared to battery-electric alternatives. Primary implementation challenges involve cryogenic storage systems (-253{\\deg}C), tank placement optimization, and fueling infrastructure development. The observed efficiency penalties of only 2.23% in military applications suggest hydrogen's viability as a sustainable aviation fuel alternative.""}",oai:arXiv.org:2412.15137v1,False,"[{'term': 'eess.SY', 'scheme': None, 'label': None}, {'term': 'cs.SY', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Armaan Sharma, Mansur M. Arief'}]","Armaan Sharma, Mansur M. Arief","{'name': 'Armaan Sharma, Mansur M. Arief'}",,
403,Relaxed exception semantics for Arm-A (extended version),"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Relaxed exception semantics for Arm-A (extended version)'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.15140'}]",https://arxiv.org/abs/2412.15140,"arXiv:2412.15140v1 Announce Type: new 
Abstract: To manage exceptions, software relies on a key architectural guarantee, precision: that exceptions appear to execute between instructions. However, this definition, dating back over 60 years, fundamentally assumes a sequential programmers model. Modern architectures such as Arm-A with programmer-observable relaxed behaviour make such a naive definition inadequate, and it is unclear exactly what guarantees programmers have on exception entry and exit.
  In this paper, we clarify the concepts needed to discuss exceptions in the relaxed-memory setting -- a key aspect of precisely specifying the architectural interface between hardware and software. We explore the basic relaxed behaviour across exception boundaries, and the semantics of external aborts, using Arm-A as a representative modern architecture. We identify an important problem, present yet unexplored for decades: pinning down what it means for exceptions to be precise in a relaxed setting. We describe key phenomena that any definition should account for. We develop an axiomatic model for Arm-A precise exceptions, tooling for axiomatic model execution, and a library of tests. Finally we explore the relaxed semantics of software-generated interrupts, as used in sophisticated programming patterns, and sketch how they too could be modelled.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.15140v1 Announce Type: new \nAbstract: To manage exceptions, software relies on a key architectural guarantee, precision: that exceptions appear to execute between instructions. However, this definition, dating back over 60 years, fundamentally assumes a sequential programmers model. Modern architectures such as Arm-A with programmer-observable relaxed behaviour make such a naive definition inadequate, and it is unclear exactly what guarantees programmers have on exception entry and exit.\n  In this paper, we clarify the concepts needed to discuss exceptions in the relaxed-memory setting -- a key aspect of precisely specifying the architectural interface between hardware and software. We explore the basic relaxed behaviour across exception boundaries, and the semantics of external aborts, using Arm-A as a representative modern architecture. We identify an important problem, present yet unexplored for decades: pinning down what it means for exceptions to be precise in a relaxed setting. We describe key phenomena that any definition should account for. We develop an axiomatic model for Arm-A precise exceptions, tooling for axiomatic model execution, and a library of tests. Finally we explore the relaxed semantics of software-generated interrupts, as used in sophisticated programming patterns, and sketch how they too could be modelled.'}",oai:arXiv.org:2412.15140v1,False,"[{'term': 'cs.AR', 'scheme': None, 'label': None}, {'term': 'cs.PL', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Ben Simner, Alasdair Armstrong, Thomas Bauereiss, Brian Campbell, Ohad Kammar, Jean Pichon-Pharabod, and Peter Sewell'}]","Ben Simner, Alasdair Armstrong, Thomas Bauereiss, Brian Campbell, Ohad Kammar, Jean Pichon-Pharabod, and Peter Sewell","{'name': 'Ben Simner, Alasdair Armstrong, Thomas Bauereiss, Brian Campbell, Ohad Kammar, Jean Pichon-Pharabod, and Peter Sewell'}",,
404,"A review of high order strong stability preserving two-derivative explicit, implicit, and IMEX methods","{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'A review of high order strong stability preserving two-derivative explicit, implicit, and IMEX methods'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.15142'}]",https://arxiv.org/abs/2412.15142,"arXiv:2412.15142v1 Announce Type: new 
Abstract: High order strong stability preserving (SSP) time discretizations ensure the nonlinear non-inner-product strong stability properties of spatial discretizations suited for the stable simulation of hyperbolic PDEs. Over the past decade multiderivative time-stepping have been used for the time-evolution hyperbolic PDEs, so that the strong stability properties of these methods have become increasingly relevant. In this work we review sufficient conditions for a two-derivative multistage method to preserve the strong stability properties of spatial discretizations in a forward Euler and different conditions on the second derivative. In particular we present the SSP theory for explicit and implicit two-derivative Runge--Kutta schemes, and discuss a special condition on the second derivative under which these implicit methods may be unconditionally SSP. This condition is then used in the context of implicit-explicit (IMEX) multi-derivative Runge--Kutta schemes, where the time-step restriction is independent of the stiff term. Finally, we present the SSP theory for implicit-explicit (IMEX) multi-derivative general linear methods, and some novel second and third order methods where the time-step restriction is independent of the stiff term.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.15142v1 Announce Type: new \nAbstract: High order strong stability preserving (SSP) time discretizations ensure the nonlinear non-inner-product strong stability properties of spatial discretizations suited for the stable simulation of hyperbolic PDEs. Over the past decade multiderivative time-stepping have been used for the time-evolution hyperbolic PDEs, so that the strong stability properties of these methods have become increasingly relevant. In this work we review sufficient conditions for a two-derivative multistage method to preserve the strong stability properties of spatial discretizations in a forward Euler and different conditions on the second derivative. In particular we present the SSP theory for explicit and implicit two-derivative Runge--Kutta schemes, and discuss a special condition on the second derivative under which these implicit methods may be unconditionally SSP. This condition is then used in the context of implicit-explicit (IMEX) multi-derivative Runge--Kutta schemes, where the time-step restriction is independent of the stiff term. Finally, we present the SSP theory for implicit-explicit (IMEX) multi-derivative general linear methods, and some novel second and third order methods where the time-step restriction is independent of the stiff term.'}",oai:arXiv.org:2412.15142v1,False,"[{'term': 'math.NA', 'scheme': None, 'label': None}, {'term': 'cs.NA', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Sigal Gottlieb, Zachary J. Grant'}]","Sigal Gottlieb, Zachary J. Grant","{'name': 'Sigal Gottlieb, Zachary J. Grant'}",,
405,Cruise Control: Dynamic Model Selection for ML-Based Network Traffic Analysis,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Cruise Control: Dynamic Model Selection for ML-Based Network Traffic Analysis'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.15146'}]",https://arxiv.org/abs/2412.15146,"arXiv:2412.15146v1 Announce Type: new 
Abstract: Modern networks increasingly rely on machine learning models for real-time insights, including traffic classification, application quality of experience inference, and intrusion detection. However, existing approaches prioritize prediction accuracy without considering deployment constraints or the dynamism of network traffic, leading to potentially suboptimal performance. Because of this, deploying ML models in real-world networks with tight performance constraints remains an open challenge. In contrast with existing work that aims to select an optimal candidate model for each task based on offline information, we propose an online, system-driven approach to dynamically select the best ML model for network traffic analysis. To this end, we present Cruise Control, a system that pre-trains several models for a given task with different accuracy-cost tradeoffs and selects the most appropriate model based on lightweight signals representing the system's current traffic processing ability. Experimental results using two real-world traffic analysis tasks demonstrate Cruise Control's effectiveness in adapting to changing network conditions. Our evaluation shows that Cruise Control improves median accuracy by 2.78% while reducing packet loss by a factor of four compared to offline-selected models.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.15146v1 Announce Type: new \nAbstract: Modern networks increasingly rely on machine learning models for real-time insights, including traffic classification, application quality of experience inference, and intrusion detection. However, existing approaches prioritize prediction accuracy without considering deployment constraints or the dynamism of network traffic, leading to potentially suboptimal performance. Because of this, deploying ML models in real-world networks with tight performance constraints remains an open challenge. In contrast with existing work that aims to select an optimal candidate model for each task based on offline information, we propose an online, system-driven approach to dynamically select the best ML model for network traffic analysis. To this end, we present Cruise Control, a system that pre-trains several models for a given task with different accuracy-cost tradeoffs and selects the most appropriate model based on lightweight signals representing the system's current traffic processing ability. Experimental results using two real-world traffic analysis tasks demonstrate Cruise Control's effectiveness in adapting to changing network conditions. Our evaluation shows that Cruise Control improves median accuracy by 2.78% while reducing packet loss by a factor of four compared to offline-selected models.""}",oai:arXiv.org:2412.15146v1,False,"[{'term': 'cs.NI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by-nc-nd/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-nd/4.0/'}","[{'name': 'Johann Hugon, Paul Schmitt, Anthony Busson, Francesco Bronzino'}]","Johann Hugon, Paul Schmitt, Anthony Busson, Francesco Bronzino","{'name': 'Johann Hugon, Paul Schmitt, Anthony Busson, Francesco Bronzino'}",,
406,Leveraging Color Channel Independence for Improved Unsupervised Object Detection,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Leveraging Color Channel Independence for Improved Unsupervised Object Detection'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.15150'}]",https://arxiv.org/abs/2412.15150,"arXiv:2412.15150v1 Announce Type: new 
Abstract: Object-centric architectures can learn to extract distinct object representations from visual scenes, enabling downstream applications on the object level. Similarly to autoencoder-based image models, object-centric approaches have been trained on the unsupervised reconstruction loss of images encoded by RGB color spaces. In our work, we challenge the common assumption that RGB images are the optimal color space for unsupervised learning in computer vision. We discuss conceptually and empirically that other color spaces, such as HSV, bear essential characteristics for object-centric representation learning, like robustness to lighting conditions. We further show that models improve when requiring them to predict additional color channels. Specifically, we propose to transform the predicted targets to the RGB-S space, which extends RGB with HSV's saturation component and leads to markedly better reconstruction and disentanglement for five common evaluation datasets. The use of composite color spaces can be implemented with basically no computational overhead, is agnostic of the models' architecture, and is universally applicable across a wide range of visual computing tasks and training types. The findings of our approach encourage additional investigations in computer vision tasks beyond object-centric learning.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.15150v1 Announce Type: new \nAbstract: Object-centric architectures can learn to extract distinct object representations from visual scenes, enabling downstream applications on the object level. Similarly to autoencoder-based image models, object-centric approaches have been trained on the unsupervised reconstruction loss of images encoded by RGB color spaces. In our work, we challenge the common assumption that RGB images are the optimal color space for unsupervised learning in computer vision. We discuss conceptually and empirically that other color spaces, such as HSV, bear essential characteristics for object-centric representation learning, like robustness to lighting conditions. We further show that models improve when requiring them to predict additional color channels. Specifically, we propose to transform the predicted targets to the RGB-S space, which extends RGB with HSV's saturation component and leads to markedly better reconstruction and disentanglement for five common evaluation datasets. The use of composite color spaces can be implemented with basically no computational overhead, is agnostic of the models' architecture, and is universally applicable across a wide range of visual computing tasks and training types. The findings of our approach encourage additional investigations in computer vision tasks beyond object-centric learning.""}",oai:arXiv.org:2412.15150v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Bastian J\\""ackl, Yannick Metz, Udo Schlegel, Daniel A. Keim, Maximilian T. Fischer'}]","Bastian J\""ackl, Yannick Metz, Udo Schlegel, Daniel A. Keim, Maximilian T. Fischer","{'name': 'Bastian J\\""ackl, Yannick Metz, Udo Schlegel, Daniel A. Keim, Maximilian T. Fischer'}",,
407,Language Models as Continuous Self-Evolving Data Engineers,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Language Models as Continuous Self-Evolving Data Engineers'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.15151'}]",https://arxiv.org/abs/2412.15151,"arXiv:2412.15151v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities on various tasks, while the further evolvement is limited to the lack of high-quality training data. In addition, traditional training approaches rely too much on expert-labeled data, setting an upper limit on the performance of LLMs. To address this issue, we propose a novel paradigm that enables LLMs to train itself by autonomously generating, cleaning, reviewing, and annotating data with preference information, named LANCE. Our approach demonstrates that LLMs can serve as continuous self-evolving data engineers, significantly reducing the time and cost of the post-training data construction process. Through iterative fine-tuning on different variants of the Qwen2, we validate the effectiveness of LANCE across various tasks, showing that it can continuously improve model performance and maintain high-quality data generation. Across eight benchmark dimensions, LANCE resulted in an average score enhancement of 3.36 for Qwen2-7B and 2.70 for Qwen2-7B-Instruct. This training paradigm with autonomous data construction not only reduces the reliance on human experts or external models but also ensures that the data aligns with human values and preferences, paving the way for the development of future superintelligent systems that can exceed human capabilities.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.15151v1 Announce Type: new \nAbstract: Large Language Models (LLMs) have demonstrated remarkable capabilities on various tasks, while the further evolvement is limited to the lack of high-quality training data. In addition, traditional training approaches rely too much on expert-labeled data, setting an upper limit on the performance of LLMs. To address this issue, we propose a novel paradigm that enables LLMs to train itself by autonomously generating, cleaning, reviewing, and annotating data with preference information, named LANCE. Our approach demonstrates that LLMs can serve as continuous self-evolving data engineers, significantly reducing the time and cost of the post-training data construction process. Through iterative fine-tuning on different variants of the Qwen2, we validate the effectiveness of LANCE across various tasks, showing that it can continuously improve model performance and maintain high-quality data generation. Across eight benchmark dimensions, LANCE resulted in an average score enhancement of 3.36 for Qwen2-7B and 2.70 for Qwen2-7B-Instruct. This training paradigm with autonomous data construction not only reduces the reliance on human experts or external models but also ensures that the data aligns with human values and preferences, paving the way for the development of future superintelligent systems that can exceed human capabilities.'}",oai:arXiv.org:2412.15151v1,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Peidong Wang, Ming Wang, Zhiming Ma, Xiaocui Yang, Shi Feng, Daling Wang, Yifei Zhang'}]","Peidong Wang, Ming Wang, Zhiming Ma, Xiaocui Yang, Shi Feng, Daling Wang, Yifei Zhang","{'name': 'Peidong Wang, Ming Wang, Zhiming Ma, Xiaocui Yang, Shi Feng, Daling Wang, Yifei Zhang'}",,
408,Measuring DNA Microswimmer Locomotion in Complex Flow Environments,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Measuring DNA Microswimmer Locomotion in Complex Flow Environments'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.15152'}]",https://arxiv.org/abs/2412.15152,"arXiv:2412.15152v1 Announce Type: new 
Abstract: Microswimmers are sub-millimeter swimming microrobots that show potential as a platform for controllable locomotion in applications including targeted cargo delivery and minimally invasive surgery. To be viable for these target applications, microswimmers will eventually need to be able to navigate in environments with dynamic fluid flows and forces. Experimental studies with microswimmers towards this goal are currently rare because of the difficulty isolating intentional microswimmer motion from environment-induced motion. In this work, we present a method for measuring microswimmer locomotion within a complex flow environment using fiducial microspheres. By tracking the particle motion of ferromagnetic and non-magnetic polystyrene fiducial microspheres, we capture the effect of fluid flow and field gradients on microswimmer trajectories. We then determine the field-driven translation of these microswimmers relative to fluid flow and demonstrate the effectiveness of this method by illustrating the motion of multiple microswimmers through different flows.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.15152v1 Announce Type: new \nAbstract: Microswimmers are sub-millimeter swimming microrobots that show potential as a platform for controllable locomotion in applications including targeted cargo delivery and minimally invasive surgery. To be viable for these target applications, microswimmers will eventually need to be able to navigate in environments with dynamic fluid flows and forces. Experimental studies with microswimmers towards this goal are currently rare because of the difficulty isolating intentional microswimmer motion from environment-induced motion. In this work, we present a method for measuring microswimmer locomotion within a complex flow environment using fiducial microspheres. By tracking the particle motion of ferromagnetic and non-magnetic polystyrene fiducial microspheres, we capture the effect of fluid flow and field gradients on microswimmer trajectories. We then determine the field-driven translation of these microswimmers relative to fluid flow and demonstrate the effectiveness of this method by illustrating the motion of multiple microswimmers through different flows.'}",oai:arXiv.org:2412.15152v1,False,"[{'term': 'cs.RO', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by-nc-nd/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-nd/4.0/'}","[{'name': 'Taryn Imamura, Teresa A. Kent, Rebecca E. Taylor, Sarah Bergbreiter'}]","Taryn Imamura, Teresa A. Kent, Rebecca E. Taylor, Sarah Bergbreiter","{'name': 'Taryn Imamura, Teresa A. Kent, Rebecca E. Taylor, Sarah Bergbreiter'}",,
409,Prompt-A-Video: Prompt Your Video Diffusion Model via Preference-Aligned LLM,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Prompt-A-Video: Prompt Your Video Diffusion Model via Preference-Aligned LLM'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.15156'}]",https://arxiv.org/abs/2412.15156,"arXiv:2412.15156v1 Announce Type: new 
Abstract: Text-to-video models have made remarkable advancements through optimization on high-quality text-video pairs, where the textual prompts play a pivotal role in determining quality of output videos. However, achieving the desired output often entails multiple revisions and iterative inference to refine user-provided prompts. Current automatic methods for refining prompts encounter challenges such as Modality-Inconsistency, Cost-Discrepancy, and Model-Unaware when applied to text-to-video diffusion models. To address these problem, we introduce an LLM-based prompt adaptation framework, termed as Prompt-A-Video, which excels in crafting Video-Centric, Labor-Free and Preference-Aligned prompts tailored to specific video diffusion model. Our approach involves a meticulously crafted two-stage optimization and alignment system. Initially, we conduct a reward-guided prompt evolution pipeline to automatically create optimal prompts pool and leverage them for supervised fine-tuning (SFT) of the LLM. Then multi-dimensional rewards are employed to generate pairwise data for the SFT model, followed by the direct preference optimization (DPO) algorithm to further facilitate preference alignment. Through extensive experimentation and comparative analyses, we validate the effectiveness of Prompt-A-Video across diverse generation models, highlighting its potential to push the boundaries of video generation.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.15156v1 Announce Type: new \nAbstract: Text-to-video models have made remarkable advancements through optimization on high-quality text-video pairs, where the textual prompts play a pivotal role in determining quality of output videos. However, achieving the desired output often entails multiple revisions and iterative inference to refine user-provided prompts. Current automatic methods for refining prompts encounter challenges such as Modality-Inconsistency, Cost-Discrepancy, and Model-Unaware when applied to text-to-video diffusion models. To address these problem, we introduce an LLM-based prompt adaptation framework, termed as Prompt-A-Video, which excels in crafting Video-Centric, Labor-Free and Preference-Aligned prompts tailored to specific video diffusion model. Our approach involves a meticulously crafted two-stage optimization and alignment system. Initially, we conduct a reward-guided prompt evolution pipeline to automatically create optimal prompts pool and leverage them for supervised fine-tuning (SFT) of the LLM. Then multi-dimensional rewards are employed to generate pairwise data for the SFT model, followed by the direct preference optimization (DPO) algorithm to further facilitate preference alignment. Through extensive experimentation and comparative analyses, we validate the effectiveness of Prompt-A-Video across diverse generation models, highlighting its potential to push the boundaries of video generation.'}",oai:arXiv.org:2412.15156v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.CL', 'scheme': None, 'label': None}, {'term': 'cs.MM', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Yatai Ji, Jiacheng Zhang, Jie Wu, Shilong Zhang, Shoufa Chen, Chongjian GE, Peize Sun, Weifeng Chen, Wenqi Shao, Xuefeng Xiao, Weilin Huang, Ping Luo'}]","Yatai Ji, Jiacheng Zhang, Jie Wu, Shilong Zhang, Shoufa Chen, Chongjian GE, Peize Sun, Weifeng Chen, Wenqi Shao, Xuefeng Xiao, Weilin Huang, Ping Luo","{'name': 'Yatai Ji, Jiacheng Zhang, Jie Wu, Shilong Zhang, Shoufa Chen, Chongjian GE, Peize Sun, Weifeng Chen, Wenqi Shao, Xuefeng Xiao, Weilin Huang, Ping Luo'}",,
410,OnlineVPO: Align Video Diffusion Model with Online Video-Centric Preference Optimization,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'OnlineVPO: Align Video Diffusion Model with Online Video-Centric Preference Optimization'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.15159'}]",https://arxiv.org/abs/2412.15159,"arXiv:2412.15159v1 Announce Type: new 
Abstract: In recent years, the field of text-to-video (T2V) generation has made significant strides. Despite this progress, there is still a gap between theoretical advancements and practical application, amplified by issues like degraded image quality and flickering artifacts. Recent advancements in enhancing the video diffusion model (VDM) through feedback learning have shown promising results. However, these methods still exhibit notable limitations, such as misaligned feedback and inferior scalability. To tackle these issues, we introduce OnlineVPO, a more efficient preference learning approach tailored specifically for video diffusion models. Our method features two novel designs, firstly, instead of directly using image-based reward feedback, we leverage the video quality assessment (VQA) model trained on synthetic data as the reward model to provide distribution and modality-aligned feedback on the video diffusion model. Additionally, we introduce an online DPO algorithm to address the off-policy optimization and scalability issue in existing video preference learning frameworks. By employing the video reward model to offer concise video feedback on the fly, OnlineVPO offers effective and efficient preference guidance. Extensive experiments on the open-source video-diffusion model demonstrate OnlineVPO as a simple yet effective and more importantly scalable preference learning algorithm for video diffusion models, offering valuable insights for future advancements in this domain.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.15159v1 Announce Type: new \nAbstract: In recent years, the field of text-to-video (T2V) generation has made significant strides. Despite this progress, there is still a gap between theoretical advancements and practical application, amplified by issues like degraded image quality and flickering artifacts. Recent advancements in enhancing the video diffusion model (VDM) through feedback learning have shown promising results. However, these methods still exhibit notable limitations, such as misaligned feedback and inferior scalability. To tackle these issues, we introduce OnlineVPO, a more efficient preference learning approach tailored specifically for video diffusion models. Our method features two novel designs, firstly, instead of directly using image-based reward feedback, we leverage the video quality assessment (VQA) model trained on synthetic data as the reward model to provide distribution and modality-aligned feedback on the video diffusion model. Additionally, we introduce an online DPO algorithm to address the off-policy optimization and scalability issue in existing video preference learning frameworks. By employing the video reward model to offer concise video feedback on the fly, OnlineVPO offers effective and efficient preference guidance. Extensive experiments on the open-source video-diffusion model demonstrate OnlineVPO as a simple yet effective and more importantly scalable preference learning algorithm for video diffusion models, offering valuable insights for future advancements in this domain.'}",oai:arXiv.org:2412.15159v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Jiacheng Zhang, Jie Wu, Weifeng Chen, Yatai Ji, Xuefeng Xiao, Weilin Huang, Kai Han'}]","Jiacheng Zhang, Jie Wu, Weifeng Chen, Yatai Ji, Xuefeng Xiao, Weilin Huang, Kai Han","{'name': 'Jiacheng Zhang, Jie Wu, Weifeng Chen, Yatai Ji, Xuefeng Xiao, Weilin Huang, Kai Han'}",,
411,On the structure of the Schur squares of Twisted Generalized Reed-Solomon codes and application to cryptanalysis,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'On the structure of the Schur squares of Twisted Generalized Reed-Solomon codes and application to cryptanalysis'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.15160'}]",https://arxiv.org/abs/2412.15160,"arXiv:2412.15160v1 Announce Type: new 
Abstract: Twisted generalized Reed-Solomon (TGRS) codes constitute an interesting family of evaluation codes, containing a large class of maximum distance separable codes non-equivalent to generalized Reed-Solomon (GRS) ones. Moreover, the Schur squares of TGRS codes may be much larger than those of GRS codes with same dimension. Exploiting these structural differences, in 2018, Beelen, Bossert, Puchinger and Rosenkilde proposed a subfamily of Maximum Distance Separable (MDS) Twisted Reed-Solomon (TRS) codes over $\mathbb{F}_q$ with $\ell$ twists $q \approx n^{2^{\ell}}$ for McEliece encryption, claiming their resistance to both Sidelnikov Shestakov attack and Schur products--based attacks. In short, they claimed these codes to resist to classical key recovery attacks on McEliece encryption scheme instantiated with Reed-Solomon (RS) or GRS codes. In 2020, Lavauzelle and Renner presented an original attack on this system based on the computation of the subfield subcode of the public TRS code.
  In this paper, we show that the original claim on the resistance of TRS and TGRS codes to Schur products based--attacks is wrong. We identify a broad class of codes including TRS and TGRS ones that is distinguishable from random by computing the Schur square of some shortening of the code. Then, we focus on the case of single twist (i.e., $\ell = 1$), which is the most efficient one in terms of decryption complexity, to derive an attack. The technique is similar to the distinguisher-based attacks of RS code-based systems given by Couvreur, Gaborit, Gauthier-Uma\~na, Otmani, Tillich in 2014.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.15160v1 Announce Type: new \nAbstract: Twisted generalized Reed-Solomon (TGRS) codes constitute an interesting family of evaluation codes, containing a large class of maximum distance separable codes non-equivalent to generalized Reed-Solomon (GRS) ones. Moreover, the Schur squares of TGRS codes may be much larger than those of GRS codes with same dimension. Exploiting these structural differences, in 2018, Beelen, Bossert, Puchinger and Rosenkilde proposed a subfamily of Maximum Distance Separable (MDS) Twisted Reed-Solomon (TRS) codes over $\\mathbb{F}_q$ with $\\ell$ twists $q \\approx n^{2^{\\ell}}$ for McEliece encryption, claiming their resistance to both Sidelnikov Shestakov attack and Schur products--based attacks. In short, they claimed these codes to resist to classical key recovery attacks on McEliece encryption scheme instantiated with Reed-Solomon (RS) or GRS codes. In 2020, Lavauzelle and Renner presented an original attack on this system based on the computation of the subfield subcode of the public TRS code.\n  In this paper, we show that the original claim on the resistance of TRS and TGRS codes to Schur products based--attacks is wrong. We identify a broad class of codes including TRS and TGRS ones that is distinguishable from random by computing the Schur square of some shortening of the code. Then, we focus on the case of single twist (i.e., $\\ell = 1$), which is the most efficient one in terms of decryption complexity, to derive an attack. The technique is similar to the distinguisher-based attacks of RS code-based systems given by Couvreur, Gaborit, Gauthier-Uma\\~na, Otmani, Tillich in 2014.'}",oai:arXiv.org:2412.15160v1,False,"[{'term': 'cs.IT', 'scheme': None, 'label': None}, {'term': 'math.IT', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Alain Couvreur, Rakhi Pratihar, Nihan Tan{\\i}sal{\\i}, Ilaria Zappatore'}]","Alain Couvreur, Rakhi Pratihar, Nihan Tan{\i}sal{\i}, Ilaria Zappatore","{'name': 'Alain Couvreur, Rakhi Pratihar, Nihan Tan{\\i}sal{\\i}, Ilaria Zappatore'}",,
412,Equal Merit Does Not Imply Equality: Discrimination at Equilibrium in a Hiring Market with Symmetric Agents,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Equal Merit Does Not Imply Equality: Discrimination at Equilibrium in a Hiring Market with Symmetric Agents'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.15162'}]",https://arxiv.org/abs/2412.15162,"arXiv:2412.15162v1 Announce Type: new 
Abstract: Machine learning has grown in popularity to help assign resources and make decisions about users, which can result in discrimination. This includes hiring markets, where employers have increasingly been interested in using automated tools to help hire candidates. In response, there has been significant effort to understand and mitigate the sources of discrimination in these tools. However, previous work has largely assumed that discrimination, in any area of ML, is the result of some initial \textit{unequal distribution of resources} across groups: One group is on average less qualified, there is less training data for one group, or the classifier is less accurate on one group, etc. However, recent work have suggested that there are other sources of discrimination, such as relational inequality, that are notably non-distributional. First, we show consensus in strategy choice is a non-distributional source of inequality at equilibrium in games: We provide subgame perfect equilibria in a simple sequential model of a hiring market with Rubinstein-style bargaining between firms and candidates that exhibits asymmetric wages resulting from differences in agents' threat strategies during bargaining. Second, we give an initial analysis of how agents could learn such strategies via convergence of an online learning algorithm to asymmetric equilibria. Ultimately, this work motivates the further study of endogenous, possibly non-distributional, mechanisms of inequality in ML.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.15162v1 Announce Type: new \nAbstract: Machine learning has grown in popularity to help assign resources and make decisions about users, which can result in discrimination. This includes hiring markets, where employers have increasingly been interested in using automated tools to help hire candidates. In response, there has been significant effort to understand and mitigate the sources of discrimination in these tools. However, previous work has largely assumed that discrimination, in any area of ML, is the result of some initial \\textit{unequal distribution of resources} across groups: One group is on average less qualified, there is less training data for one group, or the classifier is less accurate on one group, etc. However, recent work have suggested that there are other sources of discrimination, such as relational inequality, that are notably non-distributional. First, we show consensus in strategy choice is a non-distributional source of inequality at equilibrium in games: We provide subgame perfect equilibria in a simple sequential model of a hiring market with Rubinstein-style bargaining between firms and candidates that exhibits asymmetric wages resulting from differences in agents' threat strategies during bargaining. Second, we give an initial analysis of how agents could learn such strategies via convergence of an online learning algorithm to asymmetric equilibria. Ultimately, this work motivates the further study of endogenous, possibly non-distributional, mechanisms of inequality in ML.""}",oai:arXiv.org:2412.15162v1,False,"[{'term': 'cs.GT', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Serafina Kamp, Benjamin Fish'}]","Serafina Kamp, Benjamin Fish","{'name': 'Serafina Kamp, Benjamin Fish'}",,
413,Operationalising Rawlsian Ethics for Fairness in Norm-Learning Agents,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Operationalising Rawlsian Ethics for Fairness in Norm-Learning Agents'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.15163'}]",https://arxiv.org/abs/2412.15163,"arXiv:2412.15163v1 Announce Type: new 
Abstract: Social norms are standards of behaviour common in a society. However, when agents make decisions without considering how others are impacted, norms can emerge that lead to the subjugation of certain agents. We present RAWL-E, a method to create ethical norm-learning agents. RAWL-E agents operationalise maximin, a fairness principle from Rawlsian ethics, in their decision-making processes to promote ethical norms by balancing societal well-being with individual goals. We evaluate RAWL-E agents in simulated harvesting scenarios. We find that norms emerging in RAWL-E agent societies enhance social welfare, fairness, and robustness, and yield higher minimum experience compared to those that emerge in agent societies that do not implement Rawlsian ethics.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.15163v1 Announce Type: new \nAbstract: Social norms are standards of behaviour common in a society. However, when agents make decisions without considering how others are impacted, norms can emerge that lead to the subjugation of certain agents. We present RAWL-E, a method to create ethical norm-learning agents. RAWL-E agents operationalise maximin, a fairness principle from Rawlsian ethics, in their decision-making processes to promote ethical norms by balancing societal well-being with individual goals. We evaluate RAWL-E agents in simulated harvesting scenarios. We find that norms emerging in RAWL-E agent societies enhance social welfare, fairness, and robustness, and yield higher minimum experience compared to those that emerge in agent societies that do not implement Rawlsian ethics.'}",oai:arXiv.org:2412.15163v1,False,"[{'term': 'cs.MA', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Jessica Woodgate, Paul Marshall, Nirav Ajmeri'}]","Jessica Woodgate, Paul Marshall, Nirav Ajmeri","{'name': 'Jessica Woodgate, Paul Marshall, Nirav Ajmeri'}",,
414,Human-Humanoid Robots Cross-Embodiment Behavior-Skill Transfer Using Decomposed Adversarial Learning from Demonstration,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Human-Humanoid Robots Cross-Embodiment Behavior-Skill Transfer Using Decomposed Adversarial Learning from Demonstration'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.15166'}]",https://arxiv.org/abs/2412.15166,"arXiv:2412.15166v1 Announce Type: new 
Abstract: Humanoid robots are envisioned as embodied intelligent agents capable of performing a wide range of human-level loco-manipulation tasks, particularly in scenarios requiring strenuous and repetitive labor. However, learning these skills is challenging due to the high degrees of freedom of humanoid robots, and collecting sufficient training data for humanoid is a laborious process. Given the rapid introduction of new humanoid platforms, a cross-embodiment framework that allows generalizable skill transfer is becoming increasingly critical. To address this, we propose a transferable framework that reduces the data bottleneck by using a unified digital human model as a common prototype and bypassing the need for re-training on every new robot platform. The model learns behavior primitives from human demonstrations through adversarial imitation, and the complex robot structures are decomposed into functional components, each trained independently and dynamically coordinated. Task generalization is achieved through a human-object interaction graph, and skills are transferred to different robots via embodiment-specific kinematic motion retargeting and dynamic fine-tuning. Our framework is validated on five humanoid robots with diverse configurations, demonstrating stable loco-manipulation and highlighting its effectiveness in reducing data requirements and increasing the efficiency of skill transfer across platforms.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.15166v1 Announce Type: new \nAbstract: Humanoid robots are envisioned as embodied intelligent agents capable of performing a wide range of human-level loco-manipulation tasks, particularly in scenarios requiring strenuous and repetitive labor. However, learning these skills is challenging due to the high degrees of freedom of humanoid robots, and collecting sufficient training data for humanoid is a laborious process. Given the rapid introduction of new humanoid platforms, a cross-embodiment framework that allows generalizable skill transfer is becoming increasingly critical. To address this, we propose a transferable framework that reduces the data bottleneck by using a unified digital human model as a common prototype and bypassing the need for re-training on every new robot platform. The model learns behavior primitives from human demonstrations through adversarial imitation, and the complex robot structures are decomposed into functional components, each trained independently and dynamically coordinated. Task generalization is achieved through a human-object interaction graph, and skills are transferred to different robots via embodiment-specific kinematic motion retargeting and dynamic fine-tuning. Our framework is validated on five humanoid robots with diverse configurations, demonstrating stable loco-manipulation and highlighting its effectiveness in reducing data requirements and increasing the efficiency of skill transfer across platforms.'}",oai:arXiv.org:2412.15166v1,False,"[{'term': 'cs.RO', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Junjia Liu, Zhuo Li, Minghao Yu, Zhipeng Dong, Sylvain Calinon, Darwin Caldwell, Fei Chen'}]","Junjia Liu, Zhuo Li, Minghao Yu, Zhipeng Dong, Sylvain Calinon, Darwin Caldwell, Fei Chen","{'name': 'Junjia Liu, Zhuo Li, Minghao Yu, Zhipeng Dong, Sylvain Calinon, Darwin Caldwell, Fei Chen'}",,
415,SqueezeMe: Efficient Gaussian Avatars for VR,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'SqueezeMe: Efficient Gaussian Avatars for VR'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.15171'}]",https://arxiv.org/abs/2412.15171,"arXiv:2412.15171v1 Announce Type: new 
Abstract: Gaussian Splatting has enabled real-time 3D human avatars with unprecedented levels of visual quality. While previous methods require a desktop GPU for real-time inference of a single avatar, we aim to squeeze multiple Gaussian avatars onto a portable virtual reality headset with real-time drivable inference. We begin by training a previous work, Animatable Gaussians, on a high quality dataset captured with 512 cameras. The Gaussians are animated by controlling base set of Gaussians with linear blend skinning (LBS) motion and then further adjusting the Gaussians with a neural network decoder to correct their appearance. When deploying the model on a Meta Quest 3 VR headset, we find two major computational bottlenecks: the decoder and the rendering. To accelerate the decoder, we train the Gaussians in UV-space instead of pixel-space, and we distill the decoder to a single neural network layer. Further, we discover that neighborhoods of Gaussians can share a single corrective from the decoder, which provides an additional speedup. To accelerate the rendering, we develop a custom pipeline in Vulkan that runs on the mobile GPU. Putting it all together, we run 3 Gaussian avatars concurrently at 72 FPS on a VR headset. Demo videos are at https://forresti.github.io/squeezeme.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.15171v1 Announce Type: new \nAbstract: Gaussian Splatting has enabled real-time 3D human avatars with unprecedented levels of visual quality. While previous methods require a desktop GPU for real-time inference of a single avatar, we aim to squeeze multiple Gaussian avatars onto a portable virtual reality headset with real-time drivable inference. We begin by training a previous work, Animatable Gaussians, on a high quality dataset captured with 512 cameras. The Gaussians are animated by controlling base set of Gaussians with linear blend skinning (LBS) motion and then further adjusting the Gaussians with a neural network decoder to correct their appearance. When deploying the model on a Meta Quest 3 VR headset, we find two major computational bottlenecks: the decoder and the rendering. To accelerate the decoder, we train the Gaussians in UV-space instead of pixel-space, and we distill the decoder to a single neural network layer. Further, we discover that neighborhoods of Gaussians can share a single corrective from the decoder, which provides an additional speedup. To accelerate the rendering, we develop a custom pipeline in Vulkan that runs on the mobile GPU. Putting it all together, we run 3 Gaussian avatars concurrently at 72 FPS on a VR headset. Demo videos are at https://forresti.github.io/squeezeme.'}",oai:arXiv.org:2412.15171v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Shunsuke Saito, Stanislav Pidhorskyi, Igor Santesteban, Forrest Iandola, Divam Gupta, Anuj Pahuja, Nemanja Bartolovic, Frank Yu, Emanuel Garbin, Tomas Simon'}]","Shunsuke Saito, Stanislav Pidhorskyi, Igor Santesteban, Forrest Iandola, Divam Gupta, Anuj Pahuja, Nemanja Bartolovic, Frank Yu, Emanuel Garbin, Tomas Simon","{'name': 'Shunsuke Saito, Stanislav Pidhorskyi, Igor Santesteban, Forrest Iandola, Divam Gupta, Anuj Pahuja, Nemanja Bartolovic, Frank Yu, Emanuel Garbin, Tomas Simon'}",,
416,Rethinking Uncertainty Estimation in Natural Language Generation,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Rethinking Uncertainty Estimation in Natural Language Generation'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.15176'}]",https://arxiv.org/abs/2412.15176,"arXiv:2412.15176v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly employed in real-world applications, driving the need to evaluate the trustworthiness of their generated text. To this end, reliable uncertainty estimation is essential. Since current LLMs generate text autoregressively through a stochastic process, the same prompt can lead to varying outputs. Consequently, leading uncertainty estimation methods generate and analyze multiple output sequences to determine the LLM's uncertainty. However, generating output sequences is computationally expensive, making these methods impractical at scale. In this work, we inspect the theoretical foundations of the leading methods and explore new directions to enhance their computational efficiency. Building on the framework of proper scoring rules, we find that the negative log-likelihood of the most likely output sequence constitutes a theoretically grounded uncertainty measure. To approximate this alternative measure, we propose G-NLL, which has the advantage of being obtained using only a single output sequence generated by greedy decoding. This makes uncertainty estimation more efficient and straightforward, while preserving theoretical rigor. Empirical results demonstrate that G-NLL achieves state-of-the-art performance across various LLMs and tasks. Our work lays the foundation for efficient and reliable uncertainty estimation in natural language generation, challenging the necessity of more computationally involved methods currently leading the field.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.15176v1 Announce Type: new \nAbstract: Large Language Models (LLMs) are increasingly employed in real-world applications, driving the need to evaluate the trustworthiness of their generated text. To this end, reliable uncertainty estimation is essential. Since current LLMs generate text autoregressively through a stochastic process, the same prompt can lead to varying outputs. Consequently, leading uncertainty estimation methods generate and analyze multiple output sequences to determine the LLM's uncertainty. However, generating output sequences is computationally expensive, making these methods impractical at scale. In this work, we inspect the theoretical foundations of the leading methods and explore new directions to enhance their computational efficiency. Building on the framework of proper scoring rules, we find that the negative log-likelihood of the most likely output sequence constitutes a theoretically grounded uncertainty measure. To approximate this alternative measure, we propose G-NLL, which has the advantage of being obtained using only a single output sequence generated by greedy decoding. This makes uncertainty estimation more efficient and straightforward, while preserving theoretical rigor. Empirical results demonstrate that G-NLL achieves state-of-the-art performance across various LLMs and tasks. Our work lays the foundation for efficient and reliable uncertainty estimation in natural language generation, challenging the necessity of more computationally involved methods currently leading the field.""}",oai:arXiv.org:2412.15176v1,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Lukas Aichberger, Kajetan Schweighofer, Sepp Hochreiter'}]","Lukas Aichberger, Kajetan Schweighofer, Sepp Hochreiter","{'name': 'Lukas Aichberger, Kajetan Schweighofer, Sepp Hochreiter'}",,
417,Critical-Questions-of-Thought: Steering LLM reasoning with Argumentative Querying,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Critical-Questions-of-Thought: Steering LLM reasoning with Argumentative Querying'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.15177'}]",https://arxiv.org/abs/2412.15177,"arXiv:2412.15177v1 Announce Type: new 
Abstract: Studies have underscored how, regardless of the recent breakthrough and swift advances in AI research, even state-of-the-art Large Language models (LLMs) continue to struggle when performing logical and mathematical reasoning. The results seem to suggest that LLMs still work as (highly advanced) data pattern identifiers, scoring poorly when attempting to generalise and solve reasoning problems the models have never previously seen or that are not close to samples presented in their training data. To address this compelling concern, this paper makes use of the notion of critical questions from the literature on argumentation theory, focusing in particular on Toulmin's model of argumentation. We show that employing these critical questions can improve the reasoning capabilities of LLMs. By probing the rationale behind the models' reasoning process, the LLM can assess whether some logical mistake is occurring and correct it before providing the final reply to the user prompt. The underlying idea is drawn from the gold standard of any valid argumentative procedure: the conclusion is valid if it is entailed by accepted premises. Or, to paraphrase such Aristotelian principle in a real-world approximation, characterised by incomplete information and presumptive logic, the conclusion is valid if not proved otherwise. This approach successfully steers the models' output through a reasoning pipeline, resulting in better performance against the baseline and its Chain-of-Thought (CoT) implementation. To this end, an extensive evaluation of the proposed approach on the MT-Bench Reasoning and Math tasks across a range of LLMs is provided.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.15177v1 Announce Type: new \nAbstract: Studies have underscored how, regardless of the recent breakthrough and swift advances in AI research, even state-of-the-art Large Language models (LLMs) continue to struggle when performing logical and mathematical reasoning. The results seem to suggest that LLMs still work as (highly advanced) data pattern identifiers, scoring poorly when attempting to generalise and solve reasoning problems the models have never previously seen or that are not close to samples presented in their training data. To address this compelling concern, this paper makes use of the notion of critical questions from the literature on argumentation theory, focusing in particular on Toulmin's model of argumentation. We show that employing these critical questions can improve the reasoning capabilities of LLMs. By probing the rationale behind the models' reasoning process, the LLM can assess whether some logical mistake is occurring and correct it before providing the final reply to the user prompt. The underlying idea is drawn from the gold standard of any valid argumentative procedure: the conclusion is valid if it is entailed by accepted premises. Or, to paraphrase such Aristotelian principle in a real-world approximation, characterised by incomplete information and presumptive logic, the conclusion is valid if not proved otherwise. This approach successfully steers the models' output through a reasoning pipeline, resulting in better performance against the baseline and its Chain-of-Thought (CoT) implementation. To this end, an extensive evaluation of the proposed approach on the MT-Bench Reasoning and Math tasks across a range of LLMs is provided.""}",oai:arXiv.org:2412.15177v1,False,"[{'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.CL', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by-nc-nd/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-nd/4.0/'}","[{'name': 'Federico Castagna, Isabel Sassoon, Simon Parsons'}]","Federico Castagna, Isabel Sassoon, Simon Parsons","{'name': 'Federico Castagna, Isabel Sassoon, Simon Parsons'}",,
418,HPC-Coder-V2: Studying Code LLMs Across Low-Resource Parallel Languages,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'HPC-Coder-V2: Studying Code LLMs Across Low-Resource Parallel Languages'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.15178'}]",https://arxiv.org/abs/2412.15178,"arXiv:2412.15178v1 Announce Type: new 
Abstract: Large Language Model (LLM) based coding tools have been tremendously successful as software development assistants, yet they are often designed for general purpose programming tasks and perform poorly for more specialized domains such as high performance computing. Creating specialized models and tools for these domains is crucial towards gaining the benefits of LLMs in areas such as HPC. While previous work has explored HPC-specific models, LLMs still struggle to generate parallel code and it is not at all clear what hurdles are still holding back these LLMs and what must be done to overcome them. In this work, we conduct an in-depth study along the many axes of fine-tuning a specialized HPC LLM in order to better understand the challenges. Based on our findings we fine-tune and evaluate a specialized HPC LLM that is shown to be the best performing open-source code LLM for parallel code generation to date.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.15178v1 Announce Type: new \nAbstract: Large Language Model (LLM) based coding tools have been tremendously successful as software development assistants, yet they are often designed for general purpose programming tasks and perform poorly for more specialized domains such as high performance computing. Creating specialized models and tools for these domains is crucial towards gaining the benefits of LLMs in areas such as HPC. While previous work has explored HPC-specific models, LLMs still struggle to generate parallel code and it is not at all clear what hurdles are still holding back these LLMs and what must be done to overcome them. In this work, we conduct an in-depth study along the many axes of fine-tuning a specialized HPC LLM in order to better understand the challenges. Based on our findings we fine-tune and evaluate a specialized HPC LLM that is shown to be the best performing open-source code LLM for parallel code generation to date.'}",oai:arXiv.org:2412.15178v1,False,"[{'term': 'cs.DC', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'cs.SE', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Aman Chaturvedi, Daniel Nichols, Siddharth Singh, Abhinav Bhatele'}]","Aman Chaturvedi, Daniel Nichols, Siddharth Singh, Abhinav Bhatele","{'name': 'Aman Chaturvedi, Daniel Nichols, Siddharth Singh, Abhinav Bhatele'}",,
419,STRAP: Robot Sub-Trajectory Retrieval for Augmented Policy Learning,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'STRAP: Robot Sub-Trajectory Retrieval for Augmented Policy Learning'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.15182'}]",https://arxiv.org/abs/2412.15182,"arXiv:2412.15182v1 Announce Type: new 
Abstract: Robot learning is witnessing a significant increase in the size, diversity, and complexity of pre-collected datasets, mirroring trends in domains such as natural language processing and computer vision. Many robot learning methods treat such datasets as multi-task expert data and learn a multi-task, generalist policy by training broadly across them. Notably, while these generalist policies can improve the average performance across many tasks, the performance of generalist policies on any one task is often suboptimal due to negative transfer between partitions of the data, compared to task-specific specialist policies. In this work, we argue for the paradigm of training policies during deployment given the scenarios they encounter: rather than deploying pre-trained policies to unseen problems in a zero-shot manner, we non-parametrically retrieve and train models directly on relevant data at test time. Furthermore, we show that many robotics tasks share considerable amounts of low-level behaviors and that retrieval at the ""sub""-trajectory granularity enables significantly improved data utilization, generalization, and robustness in adapting policies to novel problems. In contrast, existing full-trajectory retrieval methods tend to underutilize the data and miss out on shared cross-task content. This work proposes STRAP, a technique for leveraging pre-trained vision foundation models and dynamic time warping to retrieve sub-sequences of trajectories from large training corpora in a robust fashion. STRAP outperforms both prior retrieval algorithms and multi-task learning methods in simulated and real experiments, showing the ability to scale to much larger offline datasets in the real world as well as the ability to learn robust control policies with just a handful of real-world demonstrations.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.15182v1 Announce Type: new \nAbstract: Robot learning is witnessing a significant increase in the size, diversity, and complexity of pre-collected datasets, mirroring trends in domains such as natural language processing and computer vision. Many robot learning methods treat such datasets as multi-task expert data and learn a multi-task, generalist policy by training broadly across them. Notably, while these generalist policies can improve the average performance across many tasks, the performance of generalist policies on any one task is often suboptimal due to negative transfer between partitions of the data, compared to task-specific specialist policies. In this work, we argue for the paradigm of training policies during deployment given the scenarios they encounter: rather than deploying pre-trained policies to unseen problems in a zero-shot manner, we non-parametrically retrieve and train models directly on relevant data at test time. Furthermore, we show that many robotics tasks share considerable amounts of low-level behaviors and that retrieval at the ""sub""-trajectory granularity enables significantly improved data utilization, generalization, and robustness in adapting policies to novel problems. In contrast, existing full-trajectory retrieval methods tend to underutilize the data and miss out on shared cross-task content. This work proposes STRAP, a technique for leveraging pre-trained vision foundation models and dynamic time warping to retrieve sub-sequences of trajectories from large training corpora in a robust fashion. STRAP outperforms both prior retrieval algorithms and multi-task learning methods in simulated and real experiments, showing the ability to scale to much larger offline datasets in the real world as well as the ability to learn robust control policies with just a handful of real-world demonstrations.'}",oai:arXiv.org:2412.15182v1,False,"[{'term': 'cs.RO', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'cs.SY', 'scheme': None, 'label': None}, {'term': 'eess.SY', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Marius Memmel, Jacob Berg, Bingqing Chen, Abhishek Gupta, Jonathan Francis'}]","Marius Memmel, Jacob Berg, Bingqing Chen, Abhishek Gupta, Jonathan Francis","{'name': 'Marius Memmel, Jacob Berg, Bingqing Chen, Abhishek Gupta, Jonathan Francis'}",,
420,Data for Mathematical Copilots: Better Ways of Presenting Proofs for Machine Learning,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Data for Mathematical Copilots: Better Ways of Presenting Proofs for Machine Learning'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.15184'}]",https://arxiv.org/abs/2412.15184,"arXiv:2412.15184v1 Announce Type: new 
Abstract: The suite of datasets commonly used to train and evaluate the mathematical capabilities of AI-based mathematical copilots (primarily large language models) exhibit several shortcomings. These limitations include a restricted scope of mathematical complexity, typically not exceeding lower undergraduate-level mathematics, binary rating protocols and other issues, which makes comprehensive proof-based evaluation suites difficult. We systematically explore these limitations and contend that enhancing the capabilities of large language models, or any forthcoming advancements in AI-based mathematical assistants (copilots or ""thought partners""), necessitates a paradigm shift in the design of mathematical datasets and the evaluation criteria of mathematical ability: It is necessary to move away from result-based datasets (theorem statement to theorem proof) and convert the rich facets of mathematical research practice to data LLMs can train on. Examples of these are mathematical workflows (sequences of atomic, potentially subfield-dependent tasks that are often performed when creating new mathematics), which are an important part of the proof-discovery process. Additionally, we advocate for mathematical dataset developers to consider the concept of ""motivated proof"", introduced by G. P\'olya in 1949, which can serve as a blueprint for datasets that offer a better proof learning signal, alleviating some of the mentioned limitations. Lastly, we introduce math datasheets for datasets, extending the general, dataset-agnostic variants of datasheets: We provide a questionnaire designed specifically for math datasets that we urge dataset creators to include with their datasets. This will make creators aware of potential limitations of their datasets while at the same time making it easy for readers to assess it from the point of view of training and evaluating mathematical copilots.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.15184v1 Announce Type: new \nAbstract: The suite of datasets commonly used to train and evaluate the mathematical capabilities of AI-based mathematical copilots (primarily large language models) exhibit several shortcomings. These limitations include a restricted scope of mathematical complexity, typically not exceeding lower undergraduate-level mathematics, binary rating protocols and other issues, which makes comprehensive proof-based evaluation suites difficult. We systematically explore these limitations and contend that enhancing the capabilities of large language models, or any forthcoming advancements in AI-based mathematical assistants (copilots or ""thought partners""), necessitates a paradigm shift in the design of mathematical datasets and the evaluation criteria of mathematical ability: It is necessary to move away from result-based datasets (theorem statement to theorem proof) and convert the rich facets of mathematical research practice to data LLMs can train on. Examples of these are mathematical workflows (sequences of atomic, potentially subfield-dependent tasks that are often performed when creating new mathematics), which are an important part of the proof-discovery process. Additionally, we advocate for mathematical dataset developers to consider the concept of ""motivated proof"", introduced by G. P\\\'olya in 1949, which can serve as a blueprint for datasets that offer a better proof learning signal, alleviating some of the mentioned limitations. Lastly, we introduce math datasheets for datasets, extending the general, dataset-agnostic variants of datasheets: We provide a questionnaire designed specifically for math datasets that we urge dataset creators to include with their datasets. This will make creators aware of potential limitations of their datasets while at the same time making it easy for readers to assess it from the point of view of training and evaluating mathematical copilots.'}",oai:arXiv.org:2412.15184v1,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': ""Simon Frieder, Jonas Bayer, Katherine M. Collins, Julius Berner, Jacob Loader, Andr\\'as Juh\\'asz, Fabian Ruehle, Sean Welleck, Gabriel Poesia, Ryan-Rhys Griffiths, Adrian Weller, Anirudh Goyal, Thomas Lukasiewicz, Timothy Gowers""}]","Simon Frieder, Jonas Bayer, Katherine M. Collins, Julius Berner, Jacob Loader, Andr\'as Juh\'asz, Fabian Ruehle, Sean Welleck, Gabriel Poesia, Ryan-Rhys Griffiths, Adrian Weller, Anirudh Goyal, Thomas Lukasiewicz, Timothy Gowers","{'name': ""Simon Frieder, Jonas Bayer, Katherine M. Collins, Julius Berner, Jacob Loader, Andr\\'as Juh\\'asz, Fabian Ruehle, Sean Welleck, Gabriel Poesia, Ryan-Rhys Griffiths, Adrian Weller, Anirudh Goyal, Thomas Lukasiewicz, Timothy Gowers""}",,
421,Tiled Diffusion,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Tiled Diffusion'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.15185'}]",https://arxiv.org/abs/2412.15185,"arXiv:2412.15185v1 Announce Type: new 
Abstract: Image tiling -- the seamless connection of disparate images to create a coherent visual field -- is crucial for applications such as texture creation, video game asset development, and digital art. Traditionally, tiles have been constructed manually, a method that poses significant limitations in scalability and flexibility. Recent research has attempted to automate this process using generative models. However, current approaches primarily focus on tiling textures and manipulating models for single-image generation, without inherently supporting the creation of multiple interconnected tiles across diverse domains. This paper presents Tiled Diffusion, a novel approach that extends the capabilities of diffusion models to accommodate the generation of cohesive tiling patterns across various domains of image synthesis that require tiling. Our method supports a wide range of tiling scenarios, from self-tiling to complex many-to-many connections, enabling seamless integration of multiple images. Tiled Diffusion automates the tiling process, eliminating the need for manual intervention and enhancing creative possibilities in various applications, such as seamlessly tiling of existing images, tiled texture creation, and 360{\deg} synthesis.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.15185v1 Announce Type: new \nAbstract: Image tiling -- the seamless connection of disparate images to create a coherent visual field -- is crucial for applications such as texture creation, video game asset development, and digital art. Traditionally, tiles have been constructed manually, a method that poses significant limitations in scalability and flexibility. Recent research has attempted to automate this process using generative models. However, current approaches primarily focus on tiling textures and manipulating models for single-image generation, without inherently supporting the creation of multiple interconnected tiles across diverse domains. This paper presents Tiled Diffusion, a novel approach that extends the capabilities of diffusion models to accommodate the generation of cohesive tiling patterns across various domains of image synthesis that require tiling. Our method supports a wide range of tiling scenarios, from self-tiling to complex many-to-many connections, enabling seamless integration of multiple images. Tiled Diffusion automates the tiling process, eliminating the need for manual intervention and enhancing creative possibilities in various applications, such as seamlessly tiling of existing images, tiled texture creation, and 360{\\deg} synthesis.'}",oai:arXiv.org:2412.15185v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by-nc-nd/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-nd/4.0/'}","[{'name': 'Or Madar, Ohad Fried'}]","Or Madar, Ohad Fried","{'name': 'Or Madar, Ohad Fried'}",,
422,LlamaFusion: Adapting Pretrained Language Models for Multimodal Generation,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'LlamaFusion: Adapting Pretrained Language Models for Multimodal Generation'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.15188'}]",https://arxiv.org/abs/2412.15188,"arXiv:2412.15188v1 Announce Type: new 
Abstract: We present LlamaFusion, a framework for empowering pretrained text-only large language models (LLMs) with multimodal generative capabilities, enabling them to understand and generate both text and images in arbitrary sequences. LlamaFusion leverages existing Llama-3's weights for processing texts autoregressively while introducing additional and parallel transformer modules for processing images with diffusion. During training, the data from each modality is routed to its dedicated modules: modality-specific feedforward layers, query-key-value projections, and normalization layers process each modality independently, while the shared self-attention layers allow interactions across text and image features. By freezing the text-specific modules and only training the image-specific modules, LlamaFusion preserves the language capabilities of text-only LLMs while developing strong visual understanding and generation abilities. Compared to methods that pretrain multimodal generative models from scratch, our experiments demonstrate that, LlamaFusion improves image understanding by 20% and image generation by 3.6% using only 50% of the FLOPs while maintaining Llama-3's language capabilities. We also demonstrate that this framework can adapt existing vision-language models with multimodal generation ability. Overall, this framework not only leverages existing computational investments in text-only LLMs but also enables the parallel development of language and vision capabilities, presenting a promising direction for efficient multimodal model development.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.15188v1 Announce Type: new \nAbstract: We present LlamaFusion, a framework for empowering pretrained text-only large language models (LLMs) with multimodal generative capabilities, enabling them to understand and generate both text and images in arbitrary sequences. LlamaFusion leverages existing Llama-3's weights for processing texts autoregressively while introducing additional and parallel transformer modules for processing images with diffusion. During training, the data from each modality is routed to its dedicated modules: modality-specific feedforward layers, query-key-value projections, and normalization layers process each modality independently, while the shared self-attention layers allow interactions across text and image features. By freezing the text-specific modules and only training the image-specific modules, LlamaFusion preserves the language capabilities of text-only LLMs while developing strong visual understanding and generation abilities. Compared to methods that pretrain multimodal generative models from scratch, our experiments demonstrate that, LlamaFusion improves image understanding by 20% and image generation by 3.6% using only 50% of the FLOPs while maintaining Llama-3's language capabilities. We also demonstrate that this framework can adapt existing vision-language models with multimodal generation ability. Overall, this framework not only leverages existing computational investments in text-only LLMs but also enables the parallel development of language and vision capabilities, presenting a promising direction for efficient multimodal model development.""}",oai:arXiv.org:2412.15188v1,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by-nc-sa/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-sa/4.0/'}","[{'name': 'Weijia Shi, Xiaochuang Han, Chunting Zhou, Weixin Liang, Xi Victoria Lin, Luke Zettlemoyer, Lili Yu'}]","Weijia Shi, Xiaochuang Han, Chunting Zhou, Weixin Liang, Xi Victoria Lin, Luke Zettlemoyer, Lili Yu","{'name': 'Weijia Shi, Xiaochuang Han, Chunting Zhou, Weixin Liang, Xi Victoria Lin, Luke Zettlemoyer, Lili Yu'}",,
423,Face the Facts! Evaluating RAG-based Fact-checking Pipelines in Realistic Settings,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Face the Facts! Evaluating RAG-based Fact-checking Pipelines in Realistic Settings'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.15189'}]",https://arxiv.org/abs/2412.15189,"arXiv:2412.15189v1 Announce Type: new 
Abstract: Natural Language Processing and Generation systems have recently shown the potential to complement and streamline the costly and time-consuming job of professional fact-checkers. In this work, we lift several constraints of current state-of-the-art pipelines for automated fact-checking based on the Retrieval-Augmented Generation (RAG) paradigm. Our goal is to benchmark, under more realistic scenarios, RAG-based methods for the generation of verdicts - i.e., short texts discussing the veracity of a claim - evaluating them on stylistically complex claims and heterogeneous, yet reliable, knowledge bases. Our findings show a complex landscape, where, for example, LLM-based retrievers outperform other retrieval techniques, though they still struggle with heterogeneous knowledge bases; larger models excel in verdict faithfulness, while smaller models provide better context adherence, with human evaluations favouring zero-shot and one-shot approaches for informativeness, and fine-tuned models for emotional alignment.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.15189v1 Announce Type: new \nAbstract: Natural Language Processing and Generation systems have recently shown the potential to complement and streamline the costly and time-consuming job of professional fact-checkers. In this work, we lift several constraints of current state-of-the-art pipelines for automated fact-checking based on the Retrieval-Augmented Generation (RAG) paradigm. Our goal is to benchmark, under more realistic scenarios, RAG-based methods for the generation of verdicts - i.e., short texts discussing the veracity of a claim - evaluating them on stylistically complex claims and heterogeneous, yet reliable, knowledge bases. Our findings show a complex landscape, where, for example, LLM-based retrievers outperform other retrieval techniques, though they still struggle with heterogeneous knowledge bases; larger models excel in verdict faithfulness, while smaller models provide better context adherence, with human evaluations favouring zero-shot and one-shot approaches for informativeness, and fine-tuned models for emotional alignment.'}",oai:arXiv.org:2412.15189v1,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}, {'term': 'cs.CY', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Daniel Russo, Stefano Menini, Jacopo Staiano, Marco Guerini'}]","Daniel Russo, Stefano Menini, Jacopo Staiano, Marco Guerini","{'name': 'Daniel Russo, Stefano Menini, Jacopo Staiano, Marco Guerini'}",,
424,EarthDial: Turning Multi-sensory Earth Observations to Interactive Dialogues,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'EarthDial: Turning Multi-sensory Earth Observations to Interactive Dialogues'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.15190'}]",https://arxiv.org/abs/2412.15190,"arXiv:2412.15190v1 Announce Type: new 
Abstract: Automated analysis of vast Earth observation data via interactive Vision-Language Models (VLMs) can unlock new opportunities for environmental monitoring, disaster response, and resource management. Existing generic VLMs do not perform well on Remote Sensing data, while the recent Geo-spatial VLMs remain restricted to a fixed resolution and few sensor modalities. In this paper, we introduce EarthDial, a conversational assistant specifically designed for Earth Observation (EO) data, transforming complex, multi-sensory Earth observations into interactive, natural language dialogues. EarthDial supports multi-spectral, multi-temporal, and multi-resolution imagery, enabling a wide range of remote sensing tasks, including classification, detection, captioning, question answering, visual reasoning, and visual grounding. To achieve this, we introduce an extensive instruction tuning dataset comprising over 11.11M instruction pairs covering RGB, Synthetic Aperture Radar (SAR), and multispectral modalities such as Near-Infrared (NIR) and infrared. Furthermore, EarthDial handles bi-temporal and multi-temporal sequence analysis for applications like change detection. Our extensive experimental results on 37 downstream applications demonstrate that EarthDial outperforms existing generic and domain-specific models, achieving better generalization across various EO tasks.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.15190v1 Announce Type: new \nAbstract: Automated analysis of vast Earth observation data via interactive Vision-Language Models (VLMs) can unlock new opportunities for environmental monitoring, disaster response, and resource management. Existing generic VLMs do not perform well on Remote Sensing data, while the recent Geo-spatial VLMs remain restricted to a fixed resolution and few sensor modalities. In this paper, we introduce EarthDial, a conversational assistant specifically designed for Earth Observation (EO) data, transforming complex, multi-sensory Earth observations into interactive, natural language dialogues. EarthDial supports multi-spectral, multi-temporal, and multi-resolution imagery, enabling a wide range of remote sensing tasks, including classification, detection, captioning, question answering, visual reasoning, and visual grounding. To achieve this, we introduce an extensive instruction tuning dataset comprising over 11.11M instruction pairs covering RGB, Synthetic Aperture Radar (SAR), and multispectral modalities such as Near-Infrared (NIR) and infrared. Furthermore, EarthDial handles bi-temporal and multi-temporal sequence analysis for applications like change detection. Our extensive experimental results on 37 downstream applications demonstrate that EarthDial outperforms existing generic and domain-specific models, achieving better generalization across various EO tasks.'}",oai:arXiv.org:2412.15190v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Sagar Soni, Akshay Dudhane, Hiyam Debary, Mustansar Fiaz, Muhammad Akhtar Munir, Muhammad Sohail Danish, Paolo Fraccaro, Campbell D Watson, Levente J Klein, Fahad Shahbaz Khan, Salman Khan'}]","Sagar Soni, Akshay Dudhane, Hiyam Debary, Mustansar Fiaz, Muhammad Akhtar Munir, Muhammad Sohail Danish, Paolo Fraccaro, Campbell D Watson, Levente J Klein, Fahad Shahbaz Khan, Salman Khan","{'name': 'Sagar Soni, Akshay Dudhane, Hiyam Debary, Mustansar Fiaz, Muhammad Akhtar Munir, Muhammad Sohail Danish, Paolo Fraccaro, Campbell D Watson, Levente J Klein, Fahad Shahbaz Khan, Salman Khan'}",,
425,AV-Link: Temporally-Aligned Diffusion Features for Cross-Modal Audio-Video Generation,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'AV-Link: Temporally-Aligned Diffusion Features for Cross-Modal Audio-Video Generation'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.15191'}]",https://arxiv.org/abs/2412.15191,"arXiv:2412.15191v1 Announce Type: new 
Abstract: We propose AV-Link, a unified framework for Video-to-Audio and Audio-to-Video generation that leverages the activations of frozen video and audio diffusion models for temporally-aligned cross-modal conditioning. The key to our framework is a Fusion Block that enables bidirectional information exchange between our backbone video and audio diffusion models through a temporally-aligned self attention operation. Unlike prior work that uses feature extractors pretrained for other tasks for the conditioning signal, AV-Link can directly leverage features obtained by the complementary modality in a single framework i.e. video features to generate audio, or audio features to generate video. We extensively evaluate our design choices and demonstrate the ability of our method to achieve synchronized and high-quality audiovisual content, showcasing its potential for applications in immersive media generation. Project Page: snap-research.github.io/AVLink/","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.15191v1 Announce Type: new \nAbstract: We propose AV-Link, a unified framework for Video-to-Audio and Audio-to-Video generation that leverages the activations of frozen video and audio diffusion models for temporally-aligned cross-modal conditioning. The key to our framework is a Fusion Block that enables bidirectional information exchange between our backbone video and audio diffusion models through a temporally-aligned self attention operation. Unlike prior work that uses feature extractors pretrained for other tasks for the conditioning signal, AV-Link can directly leverage features obtained by the complementary modality in a single framework i.e. video features to generate audio, or audio features to generate video. We extensively evaluate our design choices and demonstrate the ability of our method to achieve synchronized and high-quality audiovisual content, showcasing its potential for applications in immersive media generation. Project Page: snap-research.github.io/AVLink/'}",oai:arXiv.org:2412.15191v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'cs.SD', 'scheme': None, 'label': None}, {'term': 'eess.AS', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Moayed Haji-Ali, Willi Menapace, Aliaksandr Siarohin, Ivan Skorokhodov, Alper Canberk, Kwot Sin Lee, Vicente Ordonez, Sergey Tulyakov'}]","Moayed Haji-Ali, Willi Menapace, Aliaksandr Siarohin, Ivan Skorokhodov, Alper Canberk, Kwot Sin Lee, Vicente Ordonez, Sergey Tulyakov","{'name': 'Moayed Haji-Ali, Willi Menapace, Aliaksandr Siarohin, Ivan Skorokhodov, Alper Canberk, Kwot Sin Lee, Vicente Ordonez, Sergey Tulyakov'}",,
426,MMLU-CF: A Contamination-free Multi-task Language Understanding Benchmark,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'MMLU-CF: A Contamination-free Multi-task Language Understanding Benchmark'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.15194'}]",https://arxiv.org/abs/2412.15194,"arXiv:2412.15194v1 Announce Type: new 
Abstract: Multiple-choice question (MCQ) datasets like Massive Multitask Language Understanding (MMLU) are widely used to evaluate the commonsense, understanding, and problem-solving abilities of large language models (LLMs). However, the open-source nature of these benchmarks and the broad sources of training data for LLMs have inevitably led to benchmark contamination, resulting in unreliable evaluation results. To alleviate this issue, we propose a contamination-free and more challenging MCQ benchmark called MMLU-CF. This benchmark reassesses LLMs' understanding of world knowledge by averting both unintentional and malicious data leakage. To avoid unintentional data leakage, we source data from a broader domain and design three decontamination rules. To prevent malicious data leakage, we divide the benchmark into validation and test sets with similar difficulty and subject distributions. The test set remains closed-source to ensure reliable results, while the validation set is publicly available to promote transparency and facilitate independent verification. Our evaluation of mainstream LLMs reveals that the powerful GPT-4o achieves merely a 5-shot score of 73.4% and a 0-shot score of 71.9% on the test set, which indicates the effectiveness of our approach in creating a more rigorous and contamination-free evaluation standard. The GitHub repository is available at https://github.com/microsoft/MMLU-CF and the dataset refers to https://huggingface.co/datasets/microsoft/MMLU-CF.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.15194v1 Announce Type: new \nAbstract: Multiple-choice question (MCQ) datasets like Massive Multitask Language Understanding (MMLU) are widely used to evaluate the commonsense, understanding, and problem-solving abilities of large language models (LLMs). However, the open-source nature of these benchmarks and the broad sources of training data for LLMs have inevitably led to benchmark contamination, resulting in unreliable evaluation results. To alleviate this issue, we propose a contamination-free and more challenging MCQ benchmark called MMLU-CF. This benchmark reassesses LLMs' understanding of world knowledge by averting both unintentional and malicious data leakage. To avoid unintentional data leakage, we source data from a broader domain and design three decontamination rules. To prevent malicious data leakage, we divide the benchmark into validation and test sets with similar difficulty and subject distributions. The test set remains closed-source to ensure reliable results, while the validation set is publicly available to promote transparency and facilitate independent verification. Our evaluation of mainstream LLMs reveals that the powerful GPT-4o achieves merely a 5-shot score of 73.4% and a 0-shot score of 71.9% on the test set, which indicates the effectiveness of our approach in creating a more rigorous and contamination-free evaluation standard. The GitHub repository is available at https://github.com/microsoft/MMLU-CF and the dataset refers to https://huggingface.co/datasets/microsoft/MMLU-CF.""}",oai:arXiv.org:2412.15194v1,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Qihao Zhao, Yangyu Huang, Tengchao Lv, Lei Cui, Qinzheng Sun, Shaoguang Mao, Xin Zhang, Ying Xin, Qiufeng Yin, Scarlett Li, Furu Wei'}]","Qihao Zhao, Yangyu Huang, Tengchao Lv, Lei Cui, Qinzheng Sun, Shaoguang Mao, Xin Zhang, Ying Xin, Qiufeng Yin, Scarlett Li, Furu Wei","{'name': 'Qihao Zhao, Yangyu Huang, Tengchao Lv, Lei Cui, Qinzheng Sun, Shaoguang Mao, Xin Zhang, Ying Xin, Qiufeng Yin, Scarlett Li, Furu Wei'}",,
427,Preventing Local Pitfalls in Vector Quantization via Optimal Transport,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Preventing Local Pitfalls in Vector Quantization via Optimal Transport'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.15195'}]",https://arxiv.org/abs/2412.15195,"arXiv:2412.15195v1 Announce Type: new 
Abstract: Vector-quantized networks (VQNs) have exhibited remarkable performance across various tasks, yet they are prone to training instability, which complicates the training process due to the necessity for techniques such as subtle initialization and model distillation. In this study, we identify the local minima issue as the primary cause of this instability. To address this, we integrate an optimal transport method in place of the nearest neighbor search to achieve a more globally informed assignment. We introduce OptVQ, a novel vector quantization method that employs the Sinkhorn algorithm to optimize the optimal transport problem, thereby enhancing the stability and efficiency of the training process. To mitigate the influence of diverse data distributions on the Sinkhorn algorithm, we implement a straightforward yet effective normalization strategy. Our comprehensive experiments on image reconstruction tasks demonstrate that OptVQ achieves 100% codebook utilization and surpasses current state-of-the-art VQNs in reconstruction quality.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.15195v1 Announce Type: new \nAbstract: Vector-quantized networks (VQNs) have exhibited remarkable performance across various tasks, yet they are prone to training instability, which complicates the training process due to the necessity for techniques such as subtle initialization and model distillation. In this study, we identify the local minima issue as the primary cause of this instability. To address this, we integrate an optimal transport method in place of the nearest neighbor search to achieve a more globally informed assignment. We introduce OptVQ, a novel vector quantization method that employs the Sinkhorn algorithm to optimize the optimal transport problem, thereby enhancing the stability and efficiency of the training process. To mitigate the influence of diverse data distributions on the Sinkhorn algorithm, we implement a straightforward yet effective normalization strategy. Our comprehensive experiments on image reconstruction tasks demonstrate that OptVQ achieves 100% codebook utilization and surpasses current state-of-the-art VQNs in reconstruction quality.'}",oai:arXiv.org:2412.15195v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Borui Zhang, Wenzhao Zheng, Jie Zhou, Jiwen Lu'}]","Borui Zhang, Wenzhao Zheng, Jie Zhou, Jiwen Lu","{'name': 'Borui Zhang, Wenzhao Zheng, Jie Zhou, Jiwen Lu'}",,
428,LiDAR-RT: Gaussian-based Ray Tracing for Dynamic LiDAR Re-simulation,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'LiDAR-RT: Gaussian-based Ray Tracing for Dynamic LiDAR Re-simulation'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.15199'}]",https://arxiv.org/abs/2412.15199,"arXiv:2412.15199v1 Announce Type: new 
Abstract: This paper targets the challenge of real-time LiDAR re-simulation in dynamic driving scenarios. Recent approaches utilize neural radiance fields combined with the physical modeling of LiDAR sensors to achieve high-fidelity re-simulation results. Unfortunately, these methods face limitations due to high computational demands in large-scale scenes and cannot perform real-time LiDAR rendering. To overcome these constraints, we propose LiDAR-RT, a novel framework that supports real-time, physically accurate LiDAR re-simulation for driving scenes. Our primary contribution is the development of an efficient and effective rendering pipeline, which integrates Gaussian primitives and hardware-accelerated ray tracing technology. Specifically, we model the physical properties of LiDAR sensors using Gaussian primitives with learnable parameters and incorporate scene graphs to handle scene dynamics. Building upon this scene representation, our framework first constructs a bounding volume hierarchy (BVH), then casts rays for each pixel and generates novel LiDAR views through a differentiable rendering algorithm. Importantly, our framework supports realistic rendering with flexible scene editing operations and various sensor configurations. Extensive experiments across multiple public benchmarks demonstrate that our method outperforms state-of-the-art methods in terms of rendering quality and efficiency. Our project page is at https://zju3dv.github.io/lidar-rt.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.15199v1 Announce Type: new \nAbstract: This paper targets the challenge of real-time LiDAR re-simulation in dynamic driving scenarios. Recent approaches utilize neural radiance fields combined with the physical modeling of LiDAR sensors to achieve high-fidelity re-simulation results. Unfortunately, these methods face limitations due to high computational demands in large-scale scenes and cannot perform real-time LiDAR rendering. To overcome these constraints, we propose LiDAR-RT, a novel framework that supports real-time, physically accurate LiDAR re-simulation for driving scenes. Our primary contribution is the development of an efficient and effective rendering pipeline, which integrates Gaussian primitives and hardware-accelerated ray tracing technology. Specifically, we model the physical properties of LiDAR sensors using Gaussian primitives with learnable parameters and incorporate scene graphs to handle scene dynamics. Building upon this scene representation, our framework first constructs a bounding volume hierarchy (BVH), then casts rays for each pixel and generates novel LiDAR views through a differentiable rendering algorithm. Importantly, our framework supports realistic rendering with flexible scene editing operations and various sensor configurations. Extensive experiments across multiple public benchmarks demonstrate that our method outperforms state-of-the-art methods in terms of rendering quality and efficiency. Our project page is at https://zju3dv.github.io/lidar-rt.'}",oai:arXiv.org:2412.15199v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'cs.RO', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Chenxu Zhou, Lvchang Fu, Sida Peng, Yunzhi Yan, Zhanhua Zhang, Yong Chen, Jiazhi Xia, Xiaowei Zhou'}]","Chenxu Zhou, Lvchang Fu, Sida Peng, Yunzhi Yan, Zhanhua Zhang, Yong Chen, Jiazhi Xia, Xiaowei Zhou","{'name': 'Chenxu Zhou, Lvchang Fu, Sida Peng, Yunzhi Yan, Zhanhua Zhang, Yong Chen, Jiazhi Xia, Xiaowei Zhou'}",,
429,DI-PCG: Diffusion-based Efficient Inverse Procedural Content Generation for High-quality 3D Asset Creation,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'DI-PCG: Diffusion-based Efficient Inverse Procedural Content Generation for High-quality 3D Asset Creation'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.15200'}]",https://arxiv.org/abs/2412.15200,"arXiv:2412.15200v1 Announce Type: new 
Abstract: Procedural Content Generation (PCG) is powerful in creating high-quality 3D contents, yet controlling it to produce desired shapes is difficult and often requires extensive parameter tuning. Inverse Procedural Content Generation aims to automatically find the best parameters under the input condition. However, existing sampling-based and neural network-based methods still suffer from numerous sample iterations or limited controllability. In this work, we present DI-PCG, a novel and efficient method for Inverse PCG from general image conditions. At its core is a lightweight diffusion transformer model, where PCG parameters are directly treated as the denoising target and the observed images as conditions to control parameter generation. DI-PCG is efficient and effective. With only 7.6M network parameters and 30 GPU hours to train, it demonstrates superior performance in recovering parameters accurately, and generalizing well to in-the-wild images. Quantitative and qualitative experiment results validate the effectiveness of DI-PCG in inverse PCG and image-to-3D generation tasks. DI-PCG offers a promising approach for efficient inverse PCG and represents a valuable exploration step towards a 3D generation path that models how to construct a 3D asset using parametric models.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.15200v1 Announce Type: new \nAbstract: Procedural Content Generation (PCG) is powerful in creating high-quality 3D contents, yet controlling it to produce desired shapes is difficult and often requires extensive parameter tuning. Inverse Procedural Content Generation aims to automatically find the best parameters under the input condition. However, existing sampling-based and neural network-based methods still suffer from numerous sample iterations or limited controllability. In this work, we present DI-PCG, a novel and efficient method for Inverse PCG from general image conditions. At its core is a lightweight diffusion transformer model, where PCG parameters are directly treated as the denoising target and the observed images as conditions to control parameter generation. DI-PCG is efficient and effective. With only 7.6M network parameters and 30 GPU hours to train, it demonstrates superior performance in recovering parameters accurately, and generalizing well to in-the-wild images. Quantitative and qualitative experiment results validate the effectiveness of DI-PCG in inverse PCG and image-to-3D generation tasks. DI-PCG offers a promising approach for efficient inverse PCG and represents a valuable exploration step towards a 3D generation path that models how to construct a 3D asset using parametric models.'}",oai:arXiv.org:2412.15200v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.GR', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Wang Zhao, Yan-Pei Cao, Jiale Xu, Yuejiang Dong, Ying Shan'}]","Wang Zhao, Yan-Pei Cao, Jiale Xu, Yuejiang Dong, Ying Shan","{'name': 'Wang Zhao, Yan-Pei Cao, Jiale Xu, Yuejiang Dong, Ying Shan'}",,
430,LongBench v2: Towards Deeper Understanding and Reasoning on Realistic Long-context Multitasks,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'LongBench v2: Towards Deeper Understanding and Reasoning on Realistic Long-context Multitasks'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.15204'}]",https://arxiv.org/abs/2412.15204,"arXiv:2412.15204v1 Announce Type: new 
Abstract: This paper introduces LongBench v2, a benchmark designed to assess the ability of LLMs to handle long-context problems requiring deep understanding and reasoning across real-world multitasks. LongBench v2 consists of 503 challenging multiple-choice questions, with contexts ranging from 8k to 2M words, across six major task categories: single-document QA, multi-document QA, long in-context learning, long-dialogue history understanding, code repository understanding, and long structured data understanding. To ensure the breadth and the practicality, we collect data from nearly 100 highly educated individuals with diverse professional backgrounds. We employ both automated and manual review processes to maintain high quality and difficulty, resulting in human experts achieving only 53.7% accuracy under a 15-minute time constraint. Our evaluation reveals that the best-performing model, when directly answers the questions, achieves only 50.1% accuracy. In contrast, the o1-preview model, which includes longer reasoning, achieves 57.7%, surpassing the human baseline by 4%. These results highlight the importance of enhanced reasoning ability and scaling inference-time compute to tackle the long-context challenges in LongBench v2. The project is available at https://longbench2.github.io.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.15204v1 Announce Type: new \nAbstract: This paper introduces LongBench v2, a benchmark designed to assess the ability of LLMs to handle long-context problems requiring deep understanding and reasoning across real-world multitasks. LongBench v2 consists of 503 challenging multiple-choice questions, with contexts ranging from 8k to 2M words, across six major task categories: single-document QA, multi-document QA, long in-context learning, long-dialogue history understanding, code repository understanding, and long structured data understanding. To ensure the breadth and the practicality, we collect data from nearly 100 highly educated individuals with diverse professional backgrounds. We employ both automated and manual review processes to maintain high quality and difficulty, resulting in human experts achieving only 53.7% accuracy under a 15-minute time constraint. Our evaluation reveals that the best-performing model, when directly answers the questions, achieves only 50.1% accuracy. In contrast, the o1-preview model, which includes longer reasoning, achieves 57.7%, surpassing the human baseline by 4%. These results highlight the importance of enhanced reasoning ability and scaling inference-time compute to tackle the long-context challenges in LongBench v2. The project is available at https://longbench2.github.io.'}",oai:arXiv.org:2412.15204v1,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Yushi Bai, Shangqing Tu, Jiajie Zhang, Hao Peng, Xiaozhi Wang, Xin Lv, Shulin Cao, Jiazheng Xu, Lei Hou, Yuxiao Dong, Jie Tang, Juanzi Li'}]","Yushi Bai, Shangqing Tu, Jiajie Zhang, Hao Peng, Xiaozhi Wang, Xin Lv, Shulin Cao, Jiazheng Xu, Lei Hou, Yuxiao Dong, Jie Tang, Juanzi Li","{'name': 'Yushi Bai, Shangqing Tu, Jiajie Zhang, Hao Peng, Xiaozhi Wang, Xin Lv, Shulin Cao, Jiazheng Xu, Lei Hou, Yuxiao Dong, Jie Tang, Juanzi Li'}",,
431,FlowAR: Scale-wise Autoregressive Image Generation Meets Flow Matching,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'FlowAR: Scale-wise Autoregressive Image Generation Meets Flow Matching'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.15205'}]",https://arxiv.org/abs/2412.15205,"arXiv:2412.15205v1 Announce Type: new 
Abstract: Autoregressive (AR) modeling has achieved remarkable success in natural language processing by enabling models to generate text with coherence and contextual understanding through next token prediction. Recently, in image generation, VAR proposes scale-wise autoregressive modeling, which extends the next token prediction to the next scale prediction, preserving the 2D structure of images. However, VAR encounters two primary challenges: (1) its complex and rigid scale design limits generalization in next scale prediction, and (2) the generator's dependence on a discrete tokenizer with the same complex scale structure restricts modularity and flexibility in updating the tokenizer. To address these limitations, we introduce FlowAR, a general next scale prediction method featuring a streamlined scale design, where each subsequent scale is simply double the previous one. This eliminates the need for VAR's intricate multi-scale residual tokenizer and enables the use of any off-the-shelf Variational AutoEncoder (VAE). Our simplified design enhances generalization in next scale prediction and facilitates the integration of Flow Matching for high-quality image synthesis. We validate the effectiveness of FlowAR on the challenging ImageNet-256 benchmark, demonstrating superior generation performance compared to previous methods. Codes will be available at \url{https://github.com/OliverRensu/FlowAR}.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.15205v1 Announce Type: new \nAbstract: Autoregressive (AR) modeling has achieved remarkable success in natural language processing by enabling models to generate text with coherence and contextual understanding through next token prediction. Recently, in image generation, VAR proposes scale-wise autoregressive modeling, which extends the next token prediction to the next scale prediction, preserving the 2D structure of images. However, VAR encounters two primary challenges: (1) its complex and rigid scale design limits generalization in next scale prediction, and (2) the generator's dependence on a discrete tokenizer with the same complex scale structure restricts modularity and flexibility in updating the tokenizer. To address these limitations, we introduce FlowAR, a general next scale prediction method featuring a streamlined scale design, where each subsequent scale is simply double the previous one. This eliminates the need for VAR's intricate multi-scale residual tokenizer and enables the use of any off-the-shelf Variational AutoEncoder (VAE). Our simplified design enhances generalization in next scale prediction and facilitates the integration of Flow Matching for high-quality image synthesis. We validate the effectiveness of FlowAR on the challenging ImageNet-256 benchmark, demonstrating superior generation performance compared to previous methods. Codes will be available at \\url{https://github.com/OliverRensu/FlowAR}.""}",oai:arXiv.org:2412.15205v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Sucheng Ren, Qihang Yu, Ju He, Xiaohui Shen, Alan Yuille, Liang-Chieh Chen'}]","Sucheng Ren, Qihang Yu, Ju He, Xiaohui Shen, Alan Yuille, Liang-Chieh Chen","{'name': 'Sucheng Ren, Qihang Yu, Ju He, Xiaohui Shen, Alan Yuille, Liang-Chieh Chen'}",,
432,AutoTrust: Benchmarking Trustworthiness in Large Vision Language Models for Autonomous Driving,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'AutoTrust: Benchmarking Trustworthiness in Large Vision Language Models for Autonomous Driving'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.15206'}]",https://arxiv.org/abs/2412.15206,"arXiv:2412.15206v1 Announce Type: new 
Abstract: Recent advancements in large vision language models (VLMs) tailored for autonomous driving (AD) have shown strong scene understanding and reasoning capabilities, making them undeniable candidates for end-to-end driving systems. However, limited work exists on studying the trustworthiness of DriveVLMs -- a critical factor that directly impacts public transportation safety. In this paper, we introduce AutoTrust, a comprehensive trustworthiness benchmark for large vision-language models in autonomous driving (DriveVLMs), considering diverse perspectives -- including trustfulness, safety, robustness, privacy, and fairness. We constructed the largest visual question-answering dataset for investigating trustworthiness issues in driving scenarios, comprising over 10k unique scenes and 18k queries. We evaluated six publicly available VLMs, spanning from generalist to specialist, from open-source to commercial models. Our exhaustive evaluations have unveiled previously undiscovered vulnerabilities of DriveVLMs to trustworthiness threats. Specifically, we found that the general VLMs like LLaVA-v1.6 and GPT-4o-mini surprisingly outperform specialized models fine-tuned for driving in terms of overall trustworthiness. DriveVLMs like DriveLM-Agent are particularly vulnerable to disclosing sensitive information. Additionally, both generalist and specialist VLMs remain susceptible to adversarial attacks and struggle to ensure unbiased decision-making across diverse environments and populations. Our findings call for immediate and decisive action to address the trustworthiness of DriveVLMs -- an issue of critical importance to public safety and the welfare of all citizens relying on autonomous transportation systems. Our benchmark is publicly available at \url{https://github.com/taco-group/AutoTrust}, and the leaderboard is released at \url{https://taco-group.github.io/AutoTrust/}.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.15206v1 Announce Type: new \nAbstract: Recent advancements in large vision language models (VLMs) tailored for autonomous driving (AD) have shown strong scene understanding and reasoning capabilities, making them undeniable candidates for end-to-end driving systems. However, limited work exists on studying the trustworthiness of DriveVLMs -- a critical factor that directly impacts public transportation safety. In this paper, we introduce AutoTrust, a comprehensive trustworthiness benchmark for large vision-language models in autonomous driving (DriveVLMs), considering diverse perspectives -- including trustfulness, safety, robustness, privacy, and fairness. We constructed the largest visual question-answering dataset for investigating trustworthiness issues in driving scenarios, comprising over 10k unique scenes and 18k queries. We evaluated six publicly available VLMs, spanning from generalist to specialist, from open-source to commercial models. Our exhaustive evaluations have unveiled previously undiscovered vulnerabilities of DriveVLMs to trustworthiness threats. Specifically, we found that the general VLMs like LLaVA-v1.6 and GPT-4o-mini surprisingly outperform specialized models fine-tuned for driving in terms of overall trustworthiness. DriveVLMs like DriveLM-Agent are particularly vulnerable to disclosing sensitive information. Additionally, both generalist and specialist VLMs remain susceptible to adversarial attacks and struggle to ensure unbiased decision-making across diverse environments and populations. Our findings call for immediate and decisive action to address the trustworthiness of DriveVLMs -- an issue of critical importance to public safety and the welfare of all citizens relying on autonomous transportation systems. Our benchmark is publicly available at \\url{https://github.com/taco-group/AutoTrust}, and the leaderboard is released at \\url{https://taco-group.github.io/AutoTrust/}.'}",oai:arXiv.org:2412.15206v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'cs.RO', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Shuo Xing, Hongyuan Hua, Xiangbo Gao, Shenzhe Zhu, Renjie Li, Kexin Tian, Xiaopeng Li, Heng Huang, Tianbao Yang, Zhangyang Wang, Yang Zhou, Huaxiu Yao, Zhengzhong Tu'}]","Shuo Xing, Hongyuan Hua, Xiangbo Gao, Shenzhe Zhu, Renjie Li, Kexin Tian, Xiaopeng Li, Heng Huang, Tianbao Yang, Zhangyang Wang, Yang Zhou, Huaxiu Yao, Zhengzhong Tu","{'name': 'Shuo Xing, Hongyuan Hua, Xiangbo Gao, Shenzhe Zhu, Renjie Li, Kexin Tian, Xiaopeng Li, Heng Huang, Tianbao Yang, Zhangyang Wang, Yang Zhou, Huaxiu Yao, Zhengzhong Tu'}",,
433,OpenEMMA: Open-Source Multimodal Model for End-to-End Autonomous Driving,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'OpenEMMA: Open-Source Multimodal Model for End-to-End Autonomous Driving'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.15208'}]",https://arxiv.org/abs/2412.15208,"arXiv:2412.15208v1 Announce Type: new 
Abstract: Since the advent of Multimodal Large Language Models (MLLMs), they have made a significant impact across a wide range of real-world applications, particularly in Autonomous Driving (AD). Their ability to process complex visual data and reason about intricate driving scenarios has paved the way for a new paradigm in end-to-end AD systems. However, the progress of developing end-to-end models for AD has been slow, as existing fine-tuning methods demand substantial resources, including extensive computational power, large-scale datasets, and significant funding. Drawing inspiration from recent advancements in inference computing, we propose OpenEMMA, an open-source end-to-end framework based on MLLMs. By incorporating the Chain-of-Thought reasoning process, OpenEMMA achieves significant improvements compared to the baseline when leveraging a diverse range of MLLMs. Furthermore, OpenEMMA demonstrates effectiveness, generalizability, and robustness across a variety of challenging driving scenarios, offering a more efficient and effective approach to autonomous driving. We release all the codes in https://github.com/taco-group/OpenEMMA.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.15208v1 Announce Type: new \nAbstract: Since the advent of Multimodal Large Language Models (MLLMs), they have made a significant impact across a wide range of real-world applications, particularly in Autonomous Driving (AD). Their ability to process complex visual data and reason about intricate driving scenarios has paved the way for a new paradigm in end-to-end AD systems. However, the progress of developing end-to-end models for AD has been slow, as existing fine-tuning methods demand substantial resources, including extensive computational power, large-scale datasets, and significant funding. Drawing inspiration from recent advancements in inference computing, we propose OpenEMMA, an open-source end-to-end framework based on MLLMs. By incorporating the Chain-of-Thought reasoning process, OpenEMMA achieves significant improvements compared to the baseline when leveraging a diverse range of MLLMs. Furthermore, OpenEMMA demonstrates effectiveness, generalizability, and robustness across a variety of challenging driving scenarios, offering a more efficient and effective approach to autonomous driving. We release all the codes in https://github.com/taco-group/OpenEMMA.'}",oai:arXiv.org:2412.15208v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'cs.RO', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Shuo Xing, Chengyuan Qian, Yuping Wang, Hongyuan Hua, Kexin Tian, Yang Zhou, Zhengzhong Tu'}]","Shuo Xing, Chengyuan Qian, Yuping Wang, Hongyuan Hua, Kexin Tian, Yang Zhou, Zhengzhong Tu","{'name': 'Shuo Xing, Chengyuan Qian, Yuping Wang, Hongyuan Hua, Kexin Tian, Yang Zhou, Zhengzhong Tu'}",,
434,PRIMA: Multi-Image Vision-Language Models for Reasoning Segmentation,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'PRIMA: Multi-Image Vision-Language Models for Reasoning Segmentation'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.15209'}]",https://arxiv.org/abs/2412.15209,"arXiv:2412.15209v1 Announce Type: new 
Abstract: Despite significant advancements in Large Vision-Language Models (LVLMs), existing pixel-grounding models operate on single-image settings, limiting their ability to perform detailed, fine-grained comparisons across multiple images. Conversely, current multi-image understanding models lack pixel-level grounding. Our work addresses this gap by introducing the task of multi-image pixel-grounded reasoning segmentation, and PRIMA, a novel LVLM that integrates pixel-level grounding with robust multi-image reasoning capabilities to produce contextually rich, pixel-grounded explanations. Central to PRIMA is an efficient vision module that queries fine-grained visual representations across multiple images, reducing TFLOPs by $25.3\%$. To support training and evaluation, we curate $M^4Seg$, a new reasoning segmentation benchmark consisting of $\sim$224K question-answer pairs that require fine-grained visual understanding across multiple images. Experimental results demonstrate PRIMA outperforms state-of-the-art baselines.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.15209v1 Announce Type: new \nAbstract: Despite significant advancements in Large Vision-Language Models (LVLMs), existing pixel-grounding models operate on single-image settings, limiting their ability to perform detailed, fine-grained comparisons across multiple images. Conversely, current multi-image understanding models lack pixel-level grounding. Our work addresses this gap by introducing the task of multi-image pixel-grounded reasoning segmentation, and PRIMA, a novel LVLM that integrates pixel-level grounding with robust multi-image reasoning capabilities to produce contextually rich, pixel-grounded explanations. Central to PRIMA is an efficient vision module that queries fine-grained visual representations across multiple images, reducing TFLOPs by $25.3\\%$. To support training and evaluation, we curate $M^4Seg$, a new reasoning segmentation benchmark consisting of $\\sim$224K question-answer pairs that require fine-grained visual understanding across multiple images. Experimental results demonstrate PRIMA outperforms state-of-the-art baselines.'}",oai:arXiv.org:2412.15209v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Muntasir Wahed, Kiet A. Nguyen, Adheesh Sunil Juvekar, Xinzhuo Li, Xiaona Zhou, Vedant Shah, Tianjiao Yu, Pinar Yanardag, Ismini Lourentzou'}]","Muntasir Wahed, Kiet A. Nguyen, Adheesh Sunil Juvekar, Xinzhuo Li, Xiaona Zhou, Vedant Shah, Tianjiao Yu, Pinar Yanardag, Ismini Lourentzou","{'name': 'Muntasir Wahed, Kiet A. Nguyen, Adheesh Sunil Juvekar, Xinzhuo Li, Xiaona Zhou, Vedant Shah, Tianjiao Yu, Pinar Yanardag, Ismini Lourentzou'}",,
435,Tokenisation is NP-Complete,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Tokenisation is NP-Complete'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.15210'}]",https://arxiv.org/abs/2412.15210,"arXiv:2412.15210v1 Announce Type: new 
Abstract: In this work, we prove the NP-completeness of two variants of tokenisation, defined as the problem of compressing a dataset to at most $\delta$ symbols by either finding a vocabulary directly (direct tokenisation), or selecting a sequence of merge operations (bottom-up tokenisation).","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.15210v1 Announce Type: new \nAbstract: In this work, we prove the NP-completeness of two variants of tokenisation, defined as the problem of compressing a dataset to at most $\\delta$ symbols by either finding a vocabulary directly (direct tokenisation), or selecting a sequence of merge operations (bottom-up tokenisation).'}",oai:arXiv.org:2412.15210v1,False,"[{'term': 'cs.DS', 'scheme': None, 'label': None}, {'term': 'cs.CL', 'scheme': None, 'label': None}, {'term': 'cs.FL', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Philip Whittington, Gregor Bachmann, Tiago Pimentel'}]","Philip Whittington, Gregor Bachmann, Tiago Pimentel","{'name': 'Philip Whittington, Gregor Bachmann, Tiago Pimentel'}",,
436,Generative Multiview Relighting for 3D Reconstruction under Extreme Illumination Variation,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Generative Multiview Relighting for 3D Reconstruction under Extreme Illumination Variation'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.15211'}]",https://arxiv.org/abs/2412.15211,"arXiv:2412.15211v1 Announce Type: new 
Abstract: Reconstructing the geometry and appearance of objects from photographs taken in different environments is difficult as the illumination and therefore the object appearance vary across captured images. This is particularly challenging for more specular objects whose appearance strongly depends on the viewing direction. Some prior approaches model appearance variation across images using a per-image embedding vector, while others use physically-based rendering to recover the materials and per-image illumination. Such approaches fail at faithfully recovering view-dependent appearance given significant variation in input illumination and tend to produce mostly diffuse results. We present an approach that reconstructs objects from images taken under different illuminations by first relighting the images under a single reference illumination with a multiview relighting diffusion model and then reconstructing the object's geometry and appearance with a radiance field architecture that is robust to the small remaining inconsistencies among the relit images. We validate our proposed approach on both synthetic and real datasets and demonstrate that it greatly outperforms existing techniques at reconstructing high-fidelity appearance from images taken under extreme illumination variation. Moreover, our approach is particularly effective at recovering view-dependent ""shiny"" appearance which cannot be reconstructed by prior methods.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.15211v1 Announce Type: new \nAbstract: Reconstructing the geometry and appearance of objects from photographs taken in different environments is difficult as the illumination and therefore the object appearance vary across captured images. This is particularly challenging for more specular objects whose appearance strongly depends on the viewing direction. Some prior approaches model appearance variation across images using a per-image embedding vector, while others use physically-based rendering to recover the materials and per-image illumination. Such approaches fail at faithfully recovering view-dependent appearance given significant variation in input illumination and tend to produce mostly diffuse results. We present an approach that reconstructs objects from images taken under different illuminations by first relighting the images under a single reference illumination with a multiview relighting diffusion model and then reconstructing the object\'s geometry and appearance with a radiance field architecture that is robust to the small remaining inconsistencies among the relit images. We validate our proposed approach on both synthetic and real datasets and demonstrate that it greatly outperforms existing techniques at reconstructing high-fidelity appearance from images taken under extreme illumination variation. Moreover, our approach is particularly effective at recovering view-dependent ""shiny"" appearance which cannot be reconstructed by prior methods.'}",oai:arXiv.org:2412.15211v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Hadi Alzayer, Philipp Henzler, Jonathan T. Barron, Jia-Bin Huang, Pratul P. Srinivasan, Dor Verbin'}]","Hadi Alzayer, Philipp Henzler, Jonathan T. Barron, Jia-Bin Huang, Pratul P. Srinivasan, Dor Verbin","{'name': 'Hadi Alzayer, Philipp Henzler, Jonathan T. Barron, Jia-Bin Huang, Pratul P. Srinivasan, Dor Verbin'}",,
437,Scaling 4D Representations,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Scaling 4D Representations'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.15212'}]",https://arxiv.org/abs/2412.15212,"arXiv:2412.15212v1 Announce Type: new 
Abstract: Scaling has not yet been convincingly demonstrated for pure self-supervised learning from video. However, prior work has focused evaluations on semantic-related tasks $\unicode{x2013}$ action classification, ImageNet classification, etc. In this paper we focus on evaluating self-supervised learning on non-semantic vision tasks that are more spatial (3D) and temporal (+1D = 4D), such as camera pose estimation, point and object tracking, and depth estimation. We show that by learning from very large video datasets, masked auto-encoding (MAE) with transformer video models actually scales, consistently improving performance on these 4D tasks, as model size increases from 20M all the way to the largest by far reported self-supervised video model $\unicode{x2013}$ 22B parameters. Rigorous apples-to-apples comparison with many recent image and video models demonstrates the benefits of scaling 4D representations.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.15212v1 Announce Type: new \nAbstract: Scaling has not yet been convincingly demonstrated for pure self-supervised learning from video. However, prior work has focused evaluations on semantic-related tasks $\\unicode{x2013}$ action classification, ImageNet classification, etc. In this paper we focus on evaluating self-supervised learning on non-semantic vision tasks that are more spatial (3D) and temporal (+1D = 4D), such as camera pose estimation, point and object tracking, and depth estimation. We show that by learning from very large video datasets, masked auto-encoding (MAE) with transformer video models actually scales, consistently improving performance on these 4D tasks, as model size increases from 20M all the way to the largest by far reported self-supervised video model $\\unicode{x2013}$ 22B parameters. Rigorous apples-to-apples comparison with many recent image and video models demonstrates the benefits of scaling 4D representations.'}",oai:arXiv.org:2412.15212v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': ""Jo\\~ao Carreira, Dilara Gokay, Michael King, Chuhan Zhang, Ignacio Rocco, Aravindh Mahendran, Thomas Albert Keck, Joseph Heyward, Skanda Koppula, Etienne Pot, Goker Erdogan, Yana Hasson, Yi Yang, Klaus Greff, Guillaume Le Moing, Sjoerd van Steenkiste, Daniel Zoran, Drew A. Hudson, Pedro V\\'elez, Luisa Polan\\'ia, Luke Friedman, Chris Duvarney, Ross Goroshin, Kelsey Allen, Jacob Walker, Rishabh Kabra, Eric Aboussouan, Jennifer Sun, Thomas Kipf, Carl Doersch, Viorica P\\u{a}tr\\u{a}ucean, Dima Damen, Pauline Luc, Mehdi S. M. Sajjadi, Andrew Zisserman""}]","Jo\~ao Carreira, Dilara Gokay, Michael King, Chuhan Zhang, Ignacio Rocco, Aravindh Mahendran, Thomas Albert Keck, Joseph Heyward, Skanda Koppula, Etienne Pot, Goker Erdogan, Yana Hasson, Yi Yang, Klaus Greff, Guillaume Le Moing, Sjoerd van Steenkiste, Daniel Zoran, Drew A. Hudson, Pedro V\'elez, Luisa Polan\'ia, Luke Friedman, Chris Duvarney, Ross Goroshin, Kelsey Allen, Jacob Walker, Rishabh Kabra, Eric Aboussouan, Jennifer Sun, Thomas Kipf, Carl Doersch, Viorica P\u{a}tr\u{a}ucean, Dima Damen, Pauline Luc, Mehdi S. M. Sajjadi, Andrew Zisserman","{'name': ""Jo\\~ao Carreira, Dilara Gokay, Michael King, Chuhan Zhang, Ignacio Rocco, Aravindh Mahendran, Thomas Albert Keck, Joseph Heyward, Skanda Koppula, Etienne Pot, Goker Erdogan, Yana Hasson, Yi Yang, Klaus Greff, Guillaume Le Moing, Sjoerd van Steenkiste, Daniel Zoran, Drew A. Hudson, Pedro V\\'elez, Luisa Polan\\'ia, Luke Friedman, Chris Duvarney, Ross Goroshin, Kelsey Allen, Jacob Walker, Rishabh Kabra, Eric Aboussouan, Jennifer Sun, Thomas Kipf, Carl Doersch, Viorica P\\u{a}tr\\u{a}ucean, Dima Damen, Pauline Luc, Mehdi S. M. Sajjadi, Andrew Zisserman""}",,
438,Flowing from Words to Pixels: A Framework for Cross-Modality Evolution,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Flowing from Words to Pixels: A Framework for Cross-Modality Evolution'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.15213'}]",https://arxiv.org/abs/2412.15213,"arXiv:2412.15213v1 Announce Type: new 
Abstract: Diffusion models, and their generalization, flow matching, have had a remarkable impact on the field of media generation. Here, the conventional approach is to learn the complex mapping from a simple source distribution of Gaussian noise to the target media distribution. For cross-modal tasks such as text-to-image generation, this same mapping from noise to image is learnt whilst including a conditioning mechanism in the model. One key and thus far relatively unexplored feature of flow matching is that, unlike Diffusion models, they are not constrained for the source distribution to be noise. Hence, in this paper, we propose a paradigm shift, and ask the question of whether we can instead train flow matching models to learn a direct mapping from the distribution of one modality to the distribution of another, thus obviating the need for both the noise distribution and conditioning mechanism. We present a general and simple framework, CrossFlow, for cross-modal flow matching. We show the importance of applying Variational Encoders to the input data, and introduce a method to enable Classifier-free guidance. Surprisingly, for text-to-image, CrossFlow with a vanilla transformer without cross attention slightly outperforms standard flow matching, and we show that it scales better with training steps and model size, while also allowing for interesting latent arithmetic which results in semantically meaningful edits in the output space. To demonstrate the generalizability of our approach, we also show that CrossFlow is on par with or outperforms the state-of-the-art for various cross-modal / intra-modal mapping tasks, viz. image captioning, depth estimation, and image super-resolution. We hope this paper contributes to accelerating progress in cross-modal media generation.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.15213v1 Announce Type: new \nAbstract: Diffusion models, and their generalization, flow matching, have had a remarkable impact on the field of media generation. Here, the conventional approach is to learn the complex mapping from a simple source distribution of Gaussian noise to the target media distribution. For cross-modal tasks such as text-to-image generation, this same mapping from noise to image is learnt whilst including a conditioning mechanism in the model. One key and thus far relatively unexplored feature of flow matching is that, unlike Diffusion models, they are not constrained for the source distribution to be noise. Hence, in this paper, we propose a paradigm shift, and ask the question of whether we can instead train flow matching models to learn a direct mapping from the distribution of one modality to the distribution of another, thus obviating the need for both the noise distribution and conditioning mechanism. We present a general and simple framework, CrossFlow, for cross-modal flow matching. We show the importance of applying Variational Encoders to the input data, and introduce a method to enable Classifier-free guidance. Surprisingly, for text-to-image, CrossFlow with a vanilla transformer without cross attention slightly outperforms standard flow matching, and we show that it scales better with training steps and model size, while also allowing for interesting latent arithmetic which results in semantically meaningful edits in the output space. To demonstrate the generalizability of our approach, we also show that CrossFlow is on par with or outperforms the state-of-the-art for various cross-modal / intra-modal mapping tasks, viz. image captioning, depth estimation, and image super-resolution. We hope this paper contributes to accelerating progress in cross-modal media generation.'}",oai:arXiv.org:2412.15213v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Qihao Liu, Xi Yin, Alan Yuille, Andrew Brown, Mannat Singh'}]","Qihao Liu, Xi Yin, Alan Yuille, Andrew Brown, Mannat Singh","{'name': 'Qihao Liu, Xi Yin, Alan Yuille, Andrew Brown, Mannat Singh'}",,
439,LeviTor: 3D Trajectory Oriented Image-to-Video Synthesis,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'LeviTor: 3D Trajectory Oriented Image-to-Video Synthesis'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.15214'}]",https://arxiv.org/abs/2412.15214,"arXiv:2412.15214v1 Announce Type: new 
Abstract: The intuitive nature of drag-based interaction has led to its growing adoption for controlling object trajectories in image-to-video synthesis. Still, existing methods that perform dragging in the 2D space usually face ambiguity when handling out-of-plane movements. In this work, we augment the interaction with a new dimension, i.e., the depth dimension, such that users are allowed to assign a relative depth for each point on the trajectory. That way, our new interaction paradigm not only inherits the convenience from 2D dragging, but facilitates trajectory control in the 3D space, broadening the scope of creativity. We propose a pioneering method for 3D trajectory control in image-to-video synthesis by abstracting object masks into a few cluster points. These points, accompanied by the depth information and the instance information, are finally fed into a video diffusion model as the control signal. Extensive experiments validate the effectiveness of our approach, dubbed LeviTor, in precisely manipulating the object movements when producing photo-realistic videos from static images. Project page: https://ppetrichor.github.io/levitor.github.io/","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.15214v1 Announce Type: new \nAbstract: The intuitive nature of drag-based interaction has led to its growing adoption for controlling object trajectories in image-to-video synthesis. Still, existing methods that perform dragging in the 2D space usually face ambiguity when handling out-of-plane movements. In this work, we augment the interaction with a new dimension, i.e., the depth dimension, such that users are allowed to assign a relative depth for each point on the trajectory. That way, our new interaction paradigm not only inherits the convenience from 2D dragging, but facilitates trajectory control in the 3D space, broadening the scope of creativity. We propose a pioneering method for 3D trajectory control in image-to-video synthesis by abstracting object masks into a few cluster points. These points, accompanied by the depth information and the instance information, are finally fed into a video diffusion model as the control signal. Extensive experiments validate the effectiveness of our approach, dubbed LeviTor, in precisely manipulating the object movements when producing photo-realistic videos from static images. Project page: https://ppetrichor.github.io/levitor.github.io/'}",oai:arXiv.org:2412.15214v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Hanlin Wang, Hao Ouyang, Qiuyu Wang, Wen Wang, Ka Leong Cheng, Qifeng Chen, Yujun Shen, Limin Wang'}]","Hanlin Wang, Hao Ouyang, Qiuyu Wang, Wen Wang, Ka Leong Cheng, Qifeng Chen, Yujun Shen, Limin Wang","{'name': 'Hanlin Wang, Hao Ouyang, Qiuyu Wang, Wen Wang, Ka Leong Cheng, Qifeng Chen, Yujun Shen, Limin Wang'}",,
440,EnvGS: Modeling View-Dependent Appearance with Environment Gaussian,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'EnvGS: Modeling View-Dependent Appearance with Environment Gaussian'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.15215'}]",https://arxiv.org/abs/2412.15215,"arXiv:2412.15215v1 Announce Type: new 
Abstract: Reconstructing complex reflections in real-world scenes from 2D images is essential for achieving photorealistic novel view synthesis. Existing methods that utilize environment maps to model reflections from distant lighting often struggle with high-frequency reflection details and fail to account for near-field reflections. In this work, we introduce EnvGS, a novel approach that employs a set of Gaussian primitives as an explicit 3D representation for capturing reflections of environments. These environment Gaussian primitives are incorporated with base Gaussian primitives to model the appearance of the whole scene. To efficiently render these environment Gaussian primitives, we developed a ray-tracing-based renderer that leverages the GPU's RT core for fast rendering. This allows us to jointly optimize our model for high-quality reconstruction while maintaining real-time rendering speeds. Results from multiple real-world and synthetic datasets demonstrate that our method produces significantly more detailed reflections, achieving the best rendering quality in real-time novel view synthesis.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.15215v1 Announce Type: new \nAbstract: Reconstructing complex reflections in real-world scenes from 2D images is essential for achieving photorealistic novel view synthesis. Existing methods that utilize environment maps to model reflections from distant lighting often struggle with high-frequency reflection details and fail to account for near-field reflections. In this work, we introduce EnvGS, a novel approach that employs a set of Gaussian primitives as an explicit 3D representation for capturing reflections of environments. These environment Gaussian primitives are incorporated with base Gaussian primitives to model the appearance of the whole scene. To efficiently render these environment Gaussian primitives, we developed a ray-tracing-based renderer that leverages the GPU's RT core for fast rendering. This allows us to jointly optimize our model for high-quality reconstruction while maintaining real-time rendering speeds. Results from multiple real-world and synthetic datasets demonstrate that our method produces significantly more detailed reflections, achieving the best rendering quality in real-time novel view synthesis.""}",oai:arXiv.org:2412.15215v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Tao Xie, Xi Chen, Zhen Xu, Yiman Xie, Yudong Jin, Yujun Shen, Sida Peng, Hujun Bao, Xiaowei Zhou'}]","Tao Xie, Xi Chen, Zhen Xu, Yiman Xie, Yudong Jin, Yujun Shen, Sida Peng, Hujun Bao, Xiaowei Zhou","{'name': 'Tao Xie, Xi Chen, Zhen Xu, Yiman Xie, Yudong Jin, Yujun Shen, Sida Peng, Hujun Bao, Xiaowei Zhou'}",,
441,UIP2P: Unsupervised Instruction-based Image Editing via Cycle Edit Consistency,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'UIP2P: Unsupervised Instruction-based Image Editing via Cycle Edit Consistency'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.15216'}]",https://arxiv.org/abs/2412.15216,"arXiv:2412.15216v1 Announce Type: new 
Abstract: We propose an unsupervised model for instruction-based image editing that eliminates the need for ground-truth edited images during training. Existing supervised methods depend on datasets containing triplets of input image, edited image, and edit instruction. These are generated by either existing editing methods or human-annotations, which introduce biases and limit their generalization ability. Our method addresses these challenges by introducing a novel editing mechanism called Cycle Edit Consistency (CEC), which applies forward and backward edits in one training step and enforces consistency in image and attention spaces. This allows us to bypass the need for ground-truth edited images and unlock training for the first time on datasets comprising either real image-caption pairs or image-caption-edit triplets. We empirically show that our unsupervised technique performs better across a broader range of edits with high fidelity and precision. By eliminating the need for pre-existing datasets of triplets, reducing biases associated with supervised methods, and proposing CEC, our work represents a significant advancement in unblocking scaling of instruction-based image editing.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.15216v1 Announce Type: new \nAbstract: We propose an unsupervised model for instruction-based image editing that eliminates the need for ground-truth edited images during training. Existing supervised methods depend on datasets containing triplets of input image, edited image, and edit instruction. These are generated by either existing editing methods or human-annotations, which introduce biases and limit their generalization ability. Our method addresses these challenges by introducing a novel editing mechanism called Cycle Edit Consistency (CEC), which applies forward and backward edits in one training step and enforces consistency in image and attention spaces. This allows us to bypass the need for ground-truth edited images and unlock training for the first time on datasets comprising either real image-caption pairs or image-caption-edit triplets. We empirically show that our unsupervised technique performs better across a broader range of edits with high fidelity and precision. By eliminating the need for pre-existing datasets of triplets, reducing biases associated with supervised methods, and proposing CEC, our work represents a significant advancement in unblocking scaling of instruction-based image editing.'}",oai:arXiv.org:2412.15216v1,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",new,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Enis Simsar, Alessio Tonioni, Yongqin Xian, Thomas Hofmann, Federico Tombari'}]","Enis Simsar, Alessio Tonioni, Yongqin Xian, Thomas Hofmann, Federico Tombari","{'name': 'Enis Simsar, Alessio Tonioni, Yongqin Xian, Thomas Hofmann, Federico Tombari'}",,
442,Goal Space Abstraction in Hierarchical Reinforcement Learning via Set-Based Reachability Analysis,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Goal Space Abstraction in Hierarchical Reinforcement Learning via Set-Based Reachability Analysis'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2309.07675'}]",https://arxiv.org/abs/2309.07675,"arXiv:2309.07675v2 Announce Type: cross 
Abstract: Open-ended learning benefits immensely from the use of symbolic methods for goal representation as they offer ways to structure knowledge for efficient and transferable learning. However, the existing Hierarchical Reinforcement Learning (HRL) approaches relying on symbolic reasoning are often limited as they require a manual goal representation. The challenge in autonomously discovering a symbolic goal representation is that it must preserve critical information, such as the environment dynamics. In this paper, we propose a developmental mechanism for goal discovery via an emergent representation that abstracts (i.e., groups together) sets of environment states that have similar roles in the task. We introduce a Feudal HRL algorithm that concurrently learns both the goal representation and a hierarchical policy. The algorithm uses symbolic reachability analysis for neural networks to approximate the transition relation among sets of states and to refine the goal representation. We evaluate our approach on complex navigation tasks, showing the learned representation is interpretable, transferrable and results in data efficient learning.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2309.07675v2 Announce Type: cross \nAbstract: Open-ended learning benefits immensely from the use of symbolic methods for goal representation as they offer ways to structure knowledge for efficient and transferable learning. However, the existing Hierarchical Reinforcement Learning (HRL) approaches relying on symbolic reasoning are often limited as they require a manual goal representation. The challenge in autonomously discovering a symbolic goal representation is that it must preserve critical information, such as the environment dynamics. In this paper, we propose a developmental mechanism for goal discovery via an emergent representation that abstracts (i.e., groups together) sets of environment states that have similar roles in the task. We introduce a Feudal HRL algorithm that concurrently learns both the goal representation and a hierarchical policy. The algorithm uses symbolic reachability analysis for neural networks to approximate the transition relation among sets of states and to refine the goal representation. We evaluate our approach on complex navigation tasks, showing the learned representation is interpretable, transferrable and results in data efficient learning.'}",oai:arXiv.org:2309.07675v2,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",cross,http://creativecommons.org/licenses/by-nc-nd/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-nd/4.0/'}","[{'name': 'Mehdi Zadem, Sergio Mover, Sao Mai Nguyen'}]","Mehdi Zadem, Sergio Mover, Sao Mai Nguyen","{'name': 'Mehdi Zadem, Sergio Mover, Sao Mai Nguyen'}",10.1109/ICDL55364.2023.10364473,
443,On Convex Optimal Value Functions For POSGs,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'On Convex Optimal Value Functions For POSGs'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2311.09459'}]",https://arxiv.org/abs/2311.09459,"arXiv:2311.09459v3 Announce Type: cross 
Abstract: Multi-agent planning and reinforcement learning can be challenging when agents cannot see the state of the world or communicate with each other due to communication costs, latency, or noise. Partially Observable Stochastic Games (POSGs) provide a mathematical framework for modelling such scenarios. This paper aims to improve the efficiency of planning and reinforcement learning algorithms for POSGs by identifying the underlying structure of optimal state-value functions. The approach involves reformulating the original game from the perspective of a trusted third party who plans on behalf of the agents simultaneously. From this viewpoint, the original POSGs can be viewed as Markov games where states are occupancy states, \ie posterior probability distributions over the hidden states of the world and the stream of actions and observations that agents have experienced so far. This study mainly proves that the optimal state-value function is a convex function of occupancy states expressed on an appropriate basis in all zero-sum, common-payoff, and Stackelberg POSGs.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2311.09459v3 Announce Type: cross \nAbstract: Multi-agent planning and reinforcement learning can be challenging when agents cannot see the state of the world or communicate with each other due to communication costs, latency, or noise. Partially Observable Stochastic Games (POSGs) provide a mathematical framework for modelling such scenarios. This paper aims to improve the efficiency of planning and reinforcement learning algorithms for POSGs by identifying the underlying structure of optimal state-value functions. The approach involves reformulating the original game from the perspective of a trusted third party who plans on behalf of the agents simultaneously. From this viewpoint, the original POSGs can be viewed as Markov games where states are occupancy states, \\ie posterior probability distributions over the hidden states of the world and the stream of actions and observations that agents have experienced so far. This study mainly proves that the optimal state-value function is a convex function of occupancy states expressed on an appropriate basis in all zero-sum, common-payoff, and Stackelberg POSGs.'}",oai:arXiv.org:2311.09459v3,False,"[{'term': 'cs.MA', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",cross,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Rafael F. Cunha, Jacopo Castellini, Johan Peralez, Jilles S. Dibangoye'}]","Rafael F. Cunha, Jacopo Castellini, Johan Peralez, Jilles S. Dibangoye","{'name': 'Rafael F. Cunha, Jacopo Castellini, Johan Peralez, Jilles S. Dibangoye'}",,
444,A Medical Low-Back Pain Physical Rehabilitation Dataset for Human Body Movement Analysis,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'A Medical Low-Back Pain Physical Rehabilitation Dataset for Human Body Movement Analysis'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2407.00521'}]",https://arxiv.org/abs/2407.00521,"arXiv:2407.00521v1 Announce Type: cross 
Abstract: While automatic monitoring and coaching of exercises are showing encouraging results in non-medical applications, they still have limitations such as errors and limited use contexts. To allow the development and assessment of physical rehabilitation by an intelligent tutoring system, we identify in this article four challenges to address and propose a medical dataset of clinical patients carrying out low back-pain rehabilitation exercises. The dataset includes 3D Kinect skeleton positions and orientations, RGB videos, 2D skeleton data, and medical annotations to assess the correctness, and error classification and localisation of body part and timespan. Along this dataset, we perform a complete research path, from data collection to processing, and finally a small benchmark. We evaluated on the dataset two baseline movement recognition algorithms, pertaining to two different approaches: the probabilistic approach with a Gaussian Mixture Model (GMM), and the deep learning approach with a Long-Short Term Memory (LSTM).
  This dataset is valuable because it includes rehabilitation relevant motions in a clinical setting with patients in their rehabilitation program, using a cost-effective, portable, and convenient sensor, and because it shows the potential for improvement on these challenges.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2407.00521v1 Announce Type: cross \nAbstract: While automatic monitoring and coaching of exercises are showing encouraging results in non-medical applications, they still have limitations such as errors and limited use contexts. To allow the development and assessment of physical rehabilitation by an intelligent tutoring system, we identify in this article four challenges to address and propose a medical dataset of clinical patients carrying out low back-pain rehabilitation exercises. The dataset includes 3D Kinect skeleton positions and orientations, RGB videos, 2D skeleton data, and medical annotations to assess the correctness, and error classification and localisation of body part and timespan. Along this dataset, we perform a complete research path, from data collection to processing, and finally a small benchmark. We evaluated on the dataset two baseline movement recognition algorithms, pertaining to two different approaches: the probabilistic approach with a Gaussian Mixture Model (GMM), and the deep learning approach with a Long-Short Term Memory (LSTM).\n  This dataset is valuable because it includes rehabilitation relevant motions in a clinical setting with patients in their rehabilitation program, using a cost-effective, portable, and convenient sensor, and because it shows the potential for improvement on these challenges.'}",oai:arXiv.org:2407.00521v1,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.HC', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",cross,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': ""Sao Mai Nguyen, Maxime Devanne, Olivier Remy-Neris, Mathieu Lempereur, Andr\\'e Thepaut""}]","Sao Mai Nguyen, Maxime Devanne, Olivier Remy-Neris, Mathieu Lempereur, Andr\'e Thepaut","{'name': ""Sao Mai Nguyen, Maxime Devanne, Olivier Remy-Neris, Mathieu Lempereur, Andr\\'e Thepaut""}",,IJCNN 2024
445,Optimally Solving Simultaneous-Move Dec-POMDPs: The Sequential Central Planning Approach,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Optimally Solving Simultaneous-Move Dec-POMDPs: The Sequential Central Planning Approach'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2408.13139'}]",https://arxiv.org/abs/2408.13139,"arXiv:2408.13139v2 Announce Type: cross 
Abstract: The centralized training for decentralized execution paradigm emerged as the state-of-the-art approach to $\epsilon$-optimally solving decentralized partially observable Markov decision processes. However, scalability remains a significant issue. This paper presents a novel and more scalable alternative, namely the sequential-move centralized training for decentralized execution. This paradigm further pushes the applicability of the Bellman's principle of optimality, raising three new properties. First, it allows a central planner to reason upon sufficient sequential-move statistics instead of prior simultaneous-move ones. Next, it proves that $\epsilon$-optimal value functions are piecewise linear and convex in such sufficient sequential-move statistics. Finally, it drops the complexity of the backup operators from double exponential to polynomial at the expense of longer planning horizons. Besides, it makes it easy to use single-agent methods, e.g., SARSA algorithm enhanced with these findings, while still preserving convergence guarantees. Experiments on two- as well as many-agent domains from the literature against $\epsilon$-optimal simultaneous-move solvers confirm the superiority of our novel approach. This paradigm opens the door for efficient planning and reinforcement learning methods for multi-agent systems.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2408.13139v2 Announce Type: cross \nAbstract: The centralized training for decentralized execution paradigm emerged as the state-of-the-art approach to $\\epsilon$-optimally solving decentralized partially observable Markov decision processes. However, scalability remains a significant issue. This paper presents a novel and more scalable alternative, namely the sequential-move centralized training for decentralized execution. This paradigm further pushes the applicability of the Bellman's principle of optimality, raising three new properties. First, it allows a central planner to reason upon sufficient sequential-move statistics instead of prior simultaneous-move ones. Next, it proves that $\\epsilon$-optimal value functions are piecewise linear and convex in such sufficient sequential-move statistics. Finally, it drops the complexity of the backup operators from double exponential to polynomial at the expense of longer planning horizons. Besides, it makes it easy to use single-agent methods, e.g., SARSA algorithm enhanced with these findings, while still preserving convergence guarantees. Experiments on two- as well as many-agent domains from the literature against $\\epsilon$-optimal simultaneous-move solvers confirm the superiority of our novel approach. This paradigm opens the door for efficient planning and reinforcement learning methods for multi-agent systems.""}",oai:arXiv.org:2408.13139v2,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'cs.MA', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",cross,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Johan Peralez, Aur\\`elien Delage, Jacopo Castellini, Rafael F. Cunha, Jilles S. Dibangoye'}]","Johan Peralez, Aur\`elien Delage, Jacopo Castellini, Rafael F. Cunha, Jilles S. Dibangoye","{'name': 'Johan Peralez, Aur\\`elien Delage, Jacopo Castellini, Rafael F. Cunha, Jilles S. Dibangoye'}",,
446,Whisper-GPT: A Hybrid Representation Audio Large Language Model,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Whisper-GPT: A Hybrid Representation Audio Large Language Model'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.11449'}]",https://arxiv.org/abs/2412.11449,"arXiv:2412.11449v1 Announce Type: cross 
Abstract: We propose WHISPER-GPT: A generative large language model (LLM) for speech and music that allows us to work with continuous audio representations and discrete tokens simultaneously as part of a single architecture. There has been a huge surge in generative audio, speech, and music models that utilize discrete audio tokens derived from neural compression algorithms, e.g. ENCODEC. However, one of the major drawbacks of this approach is handling the context length. It blows up for high-fidelity generative architecture if one has to account for all the audio contents at various frequencies for the next token prediction. By combining continuous audio representation like the spectrogram and discrete acoustic tokens, we retain the best of both worlds: Have all the information needed from the audio at a specific time instance in a single token, yet allow LLM to predict the future token to allow for sampling and other benefits discrete space provides. We show how our architecture improves the perplexity and negative log-likelihood scores for the next token prediction compared to a token-based LLM for speech and music.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.11449v1 Announce Type: cross \nAbstract: We propose WHISPER-GPT: A generative large language model (LLM) for speech and music that allows us to work with continuous audio representations and discrete tokens simultaneously as part of a single architecture. There has been a huge surge in generative audio, speech, and music models that utilize discrete audio tokens derived from neural compression algorithms, e.g. ENCODEC. However, one of the major drawbacks of this approach is handling the context length. It blows up for high-fidelity generative architecture if one has to account for all the audio contents at various frequencies for the next token prediction. By combining continuous audio representation like the spectrogram and discrete acoustic tokens, we retain the best of both worlds: Have all the information needed from the audio at a specific time instance in a single token, yet allow LLM to predict the future token to allow for sampling and other benefits discrete space provides. We show how our architecture improves the perplexity and negative log-likelihood scores for the next token prediction compared to a token-based LLM for speech and music.'}",oai:arXiv.org:2412.11449v1,False,"[{'term': 'cs.SD', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.CL', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'eess.AS', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",cross,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}",[{'name': 'Prateek Verma'}],Prateek Verma,{'name': 'Prateek Verma'},,
447,Temperature-Resilient Analog Neuromorphic Chip in Single-Polysilicon CMOS Technology,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Temperature-Resilient Analog Neuromorphic Chip in Single-Polysilicon CMOS Technology'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14029'}]",https://arxiv.org/abs/2412.14029,"arXiv:2412.14029v1 Announce Type: cross 
Abstract: In analog neuromorphic chips, designers can embed computing primitives in the intrinsic physical properties of devices and circuits, heavily reducing device count and energy consumption, and enabling high parallelism, because all devices are computing simultaneously. Neural network parameters can be stored in local analog non-volatile memories (NVMs), saving the energy required to move data between memory and logic. However, the main drawback of analog sub-threshold electronic circuits is their dramatic temperature sensitivity. In this paper, we demonstrate that a temperature compensation mechanism can be devised to solve this problem. We have designed and fabricated a chip implementing a two-layer analog neural network trained to classify low-resolution images of handwritten digits with a low-cost single-poly complementary metal-oxide-semiconductor (CMOS) process, using unconventional analog NVMs for weight storage. We demonstrate a temperature-resilient analog neuromorphic chip for image recognition operating between 10$^{\circ}$C and 60$^{\circ}$C without loss of classification accuracy, within 2\% of the corresponding software-based neural network in the whole temperature range.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14029v1 Announce Type: cross \nAbstract: In analog neuromorphic chips, designers can embed computing primitives in the intrinsic physical properties of devices and circuits, heavily reducing device count and energy consumption, and enabling high parallelism, because all devices are computing simultaneously. Neural network parameters can be stored in local analog non-volatile memories (NVMs), saving the energy required to move data between memory and logic. However, the main drawback of analog sub-threshold electronic circuits is their dramatic temperature sensitivity. In this paper, we demonstrate that a temperature compensation mechanism can be devised to solve this problem. We have designed and fabricated a chip implementing a two-layer analog neural network trained to classify low-resolution images of handwritten digits with a low-cost single-poly complementary metal-oxide-semiconductor (CMOS) process, using unconventional analog NVMs for weight storage. We demonstrate a temperature-resilient analog neuromorphic chip for image recognition operating between 10$^{\\circ}$C and 60$^{\\circ}$C without loss of classification accuracy, within 2\\% of the corresponding software-based neural network in the whole temperature range.'}",oai:arXiv.org:2412.14029v1,False,"[{'term': 'eess.IV', 'scheme': None, 'label': None}, {'term': 'cs.AR', 'scheme': None, 'label': None}, {'term': 'cs.ET', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",cross,http://creativecommons.org/publicdomain/zero/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/publicdomain/zero/1.0/'}","[{'name': 'Tommaso Rizzo, Sebastiano Strangio, Alessandro Catania, Giuseppe Iannaccone'}]","Tommaso Rizzo, Sebastiano Strangio, Alessandro Catania, Giuseppe Iannaccone","{'name': 'Tommaso Rizzo, Sebastiano Strangio, Alessandro Catania, Giuseppe Iannaccone'}",,
448,Advanced Reasoning and Transformation Engine for Multi-Step Insight Synthesis in Data Analytics with Large Language Models,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Advanced Reasoning and Transformation Engine for Multi-Step Insight Synthesis in Data Analytics with Large Language Models'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14146'}]",https://arxiv.org/abs/2412.14146,"arXiv:2412.14146v1 Announce Type: cross 
Abstract: This paper presents the Advanced Reasoning and Transformation Engine for Multi-Step Insight Synthesis in Data Analytics (ARTEMIS-DA), a novel framework designed to augment Large Language Models (LLMs) for solving complex, multi-step data analytics tasks. ARTEMIS-DA integrates three core components: the Planner, which dissects complex user queries into structured, sequential instructions encompassing data preprocessing, transformation, predictive modeling, and visualization; the Coder, which dynamically generates and executes Python code to implement these instructions; and the Grapher, which interprets generated visualizations to derive actionable insights. By orchestrating the collaboration between these components, ARTEMIS-DA effectively manages sophisticated analytical workflows involving advanced reasoning, multi-step transformations, and synthesis across diverse data modalities. The framework achieves state-of-the-art (SOTA) performance on benchmarks such as WikiTableQuestions and TabFact, demonstrating its ability to tackle intricate analytical tasks with precision and adaptability. By combining the reasoning capabilities of LLMs with automated code generation and execution and visual analysis, ARTEMIS-DA offers a robust, scalable solution for multi-step insight synthesis, addressing a wide range of challenges in data analytics.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14146v1 Announce Type: cross \nAbstract: This paper presents the Advanced Reasoning and Transformation Engine for Multi-Step Insight Synthesis in Data Analytics (ARTEMIS-DA), a novel framework designed to augment Large Language Models (LLMs) for solving complex, multi-step data analytics tasks. ARTEMIS-DA integrates three core components: the Planner, which dissects complex user queries into structured, sequential instructions encompassing data preprocessing, transformation, predictive modeling, and visualization; the Coder, which dynamically generates and executes Python code to implement these instructions; and the Grapher, which interprets generated visualizations to derive actionable insights. By orchestrating the collaboration between these components, ARTEMIS-DA effectively manages sophisticated analytical workflows involving advanced reasoning, multi-step transformations, and synthesis across diverse data modalities. The framework achieves state-of-the-art (SOTA) performance on benchmarks such as WikiTableQuestions and TabFact, demonstrating its ability to tackle intricate analytical tasks with precision and adaptability. By combining the reasoning capabilities of LLMs with automated code generation and execution and visual analysis, ARTEMIS-DA offers a robust, scalable solution for multi-step insight synthesis, addressing a wide range of challenges in data analytics.'}",oai:arXiv.org:2412.14146v1,False,"[{'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.DB', 'scheme': None, 'label': None}, {'term': 'cs.IR', 'scheme': None, 'label': None}, {'term': 'cs.MA', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",cross,http://creativecommons.org/licenses/by-nc-sa/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-sa/4.0/'}",[{'name': 'Atin Sakkeer Hussain'}],Atin Sakkeer Hussain,{'name': 'Atin Sakkeer Hussain'},,
449,Subset Selection Problems in Planar Point Sets,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Subset Selection Problems in Planar Point Sets'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14287'}]",https://arxiv.org/abs/2412.14287,"arXiv:2412.14287v1 Announce Type: cross 
Abstract: Given a finite set satisfying condition $\mathcal{A}$, the subset selection problem asks, how large of a subset satisfying condition $\mathcal{B}$ can we find? We make progress on three instances of subset selection problems in planar point sets. Let $n,s\in\mathbb{N}$ with $n\geq s$, and let $P\subseteq\mathbb{R}^2$ be a set of $n$ points, where at most $s$ points lie on the same line.
  Firstly, we select a general position subset of $P$, i.e., a subset containing no $3$ points on the same line. This problem was proposed by Erd\H{o}s under the regime when $s$ is a constant. For $s$ being non-constant, we give new lower and upper bounds on the maximum size of such a subset. In particular, we show that in the worst case such a set can have size at most $O(n/s)$ when $n^{1/3}\leq s\leq n$ and $O(n^{5/6+o(1)}/\sqrt{s})$ when $3\leq s\leq n^{1/3}$.
  Secondly, we select a monotone general position subset of $P$, that is, a subset in general position where the points are ordered from left to right and their $y$-coordinates are either non-decreasing or non-increasing. We present bounds on the maximum size of such a subset. In particular, when $s=\Theta(\sqrt{n})$, our upper and lower bounds differ only by a logarithmic factor.
  Lastly, we select a subset of $P$ with pairwise distinct slopes. This problem was initially studied by Erd\H{o}s, Graham, Ruzsa, and Taylor on the grid. We show that for $s=O(\sqrt{n})$ such a subset of size $\Omega((n/\log{s})^{1/3})$ can always be found in $P$. When $s=\Theta(\sqrt{n})$, this matches a lower bound given by Zhang on the grid. As for the upper bound, we show that in the worst case such a subset has size at most $O(\sqrt{n})$ for $2\leq s\leq n^{3/8}$ and $O((n/s)^{4/5})$ for $n^{3/8}\leq s=O(\sqrt{n})$.
  The proofs use a wide range of tools such as incidence geometry, probabilistic methods, the hypergraph container method, and additive combinatorics.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14287v1 Announce Type: cross \nAbstract: Given a finite set satisfying condition $\\mathcal{A}$, the subset selection problem asks, how large of a subset satisfying condition $\\mathcal{B}$ can we find? We make progress on three instances of subset selection problems in planar point sets. Let $n,s\\in\\mathbb{N}$ with $n\\geq s$, and let $P\\subseteq\\mathbb{R}^2$ be a set of $n$ points, where at most $s$ points lie on the same line.\n  Firstly, we select a general position subset of $P$, i.e., a subset containing no $3$ points on the same line. This problem was proposed by Erd\\H{o}s under the regime when $s$ is a constant. For $s$ being non-constant, we give new lower and upper bounds on the maximum size of such a subset. In particular, we show that in the worst case such a set can have size at most $O(n/s)$ when $n^{1/3}\\leq s\\leq n$ and $O(n^{5/6+o(1)}/\\sqrt{s})$ when $3\\leq s\\leq n^{1/3}$.\n  Secondly, we select a monotone general position subset of $P$, that is, a subset in general position where the points are ordered from left to right and their $y$-coordinates are either non-decreasing or non-increasing. We present bounds on the maximum size of such a subset. In particular, when $s=\\Theta(\\sqrt{n})$, our upper and lower bounds differ only by a logarithmic factor.\n  Lastly, we select a subset of $P$ with pairwise distinct slopes. This problem was initially studied by Erd\\H{o}s, Graham, Ruzsa, and Taylor on the grid. We show that for $s=O(\\sqrt{n})$ such a subset of size $\\Omega((n/\\log{s})^{1/3})$ can always be found in $P$. When $s=\\Theta(\\sqrt{n})$, this matches a lower bound given by Zhang on the grid. As for the upper bound, we show that in the worst case such a subset has size at most $O(\\sqrt{n})$ for $2\\leq s\\leq n^{3/8}$ and $O((n/s)^{4/5})$ for $n^{3/8}\\leq s=O(\\sqrt{n})$.\n  The proofs use a wide range of tools such as incidence geometry, probabilistic methods, the hypergraph container method, and additive combinatorics.'}",oai:arXiv.org:2412.14287v1,False,"[{'term': 'math.CO', 'scheme': None, 'label': None}, {'term': 'cs.CG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",cross,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': ""J\\'ozsef Balogh, Felix Christian Clemen, Adrian Dumitrescu, Dingyuan Liu""}]","J\'ozsef Balogh, Felix Christian Clemen, Adrian Dumitrescu, Dingyuan Liu","{'name': ""J\\'ozsef Balogh, Felix Christian Clemen, Adrian Dumitrescu, Dingyuan Liu""}",,
450,Projected gradient methods for nonconvex and stochastic optimization: new complexities and auto-conditioned stepsizes,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Projected gradient methods for nonconvex and stochastic optimization: new complexities and auto-conditioned stepsizes'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14291'}]",https://arxiv.org/abs/2412.14291,"arXiv:2412.14291v1 Announce Type: cross 
Abstract: We present a novel class of projected gradient (PG) methods for minimizing a smooth but not necessarily convex function over a convex compact set. We first provide a novel analysis of the ""vanilla"" PG method, achieving the best-known iteration complexity for finding an approximate stationary point of the problem. We then develop an ""auto-conditioned"" projected gradient (AC-PG) variant that achieves the same iteration complexity without requiring the input of the Lipschitz constant of the gradient or any line search procedure. The key idea is to estimate the Lipschitz constant using first-order information gathered from the previous iterations, and to show that the error caused by underestimating the Lipschitz constant can be properly controlled. We then generalize the PG methods to the stochastic setting, by proposing a stochastic projected gradient (SPG) method and a variance-reduced stochastic gradient (VR-SPG) method, achieving new complexity bounds in different oracle settings. We also present auto-conditioned stepsize policies for both stochastic PG methods and establish comparable convergence guarantees.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14291v1 Announce Type: cross \nAbstract: We present a novel class of projected gradient (PG) methods for minimizing a smooth but not necessarily convex function over a convex compact set. We first provide a novel analysis of the ""vanilla"" PG method, achieving the best-known iteration complexity for finding an approximate stationary point of the problem. We then develop an ""auto-conditioned"" projected gradient (AC-PG) variant that achieves the same iteration complexity without requiring the input of the Lipschitz constant of the gradient or any line search procedure. The key idea is to estimate the Lipschitz constant using first-order information gathered from the previous iterations, and to show that the error caused by underestimating the Lipschitz constant can be properly controlled. We then generalize the PG methods to the stochastic setting, by proposing a stochastic projected gradient (SPG) method and a variance-reduced stochastic gradient (VR-SPG) method, achieving new complexity bounds in different oracle settings. We also present auto-conditioned stepsize policies for both stochastic PG methods and establish comparable convergence guarantees.'}",oai:arXiv.org:2412.14291v1,False,"[{'term': 'math.OC', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'stat.ML', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",cross,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Guanghui Lan, Tianjiao Li, Yangyang Xu'}]","Guanghui Lan, Tianjiao Li, Yangyang Xu","{'name': 'Guanghui Lan, Tianjiao Li, Yangyang Xu'}",,
451,On the Robustness of Spectral Algorithms for Semirandom Stochastic Block Models,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'On the Robustness of Spectral Algorithms for Semirandom Stochastic Block Models'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14315'}]",https://arxiv.org/abs/2412.14315,"arXiv:2412.14315v1 Announce Type: cross 
Abstract: In a graph bisection problem, we are given a graph $G$ with two equally-sized unlabeled communities, and the goal is to recover the vertices in these communities. A popular heuristic, known as spectral clustering, is to output an estimated community assignment based on the eigenvector corresponding to the second smallest eigenvalue of the Laplacian of $G$. Spectral algorithms can be shown to provably recover the cluster structure for graphs generated from certain probabilistic models, such as the Stochastic Block Model (SBM). However, spectral clustering is known to be non-robust to model mis-specification. Techniques based on semidefinite programming have been shown to be more robust, but they incur significant computational overheads.
  In this work, we study the robustness of spectral algorithms against semirandom adversaries. Informally, a semirandom adversary is allowed to ``helpfully'' change the specification of the model in a way that is consistent with the ground-truth solution. Our semirandom adversaries in particular are allowed to add edges inside clusters or increase the probability that an edge appears inside a cluster. Semirandom adversaries are a useful tool to determine the extent to which an algorithm has overfit to statistical assumptions on the input.
  On the positive side, we identify classes of semirandom adversaries under which spectral bisection using the _unnormalized_ Laplacian is strongly consistent, i.e., it exactly recovers the planted partitioning. On the negative side, we show that in these classes spectral bisection with the _normalized_ Laplacian outputs a partitioning that makes a classification mistake on a constant fraction of the vertices. Finally, we demonstrate numerical experiments that complement our theoretical findings.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14315v1 Announce Type: cross \nAbstract: In a graph bisection problem, we are given a graph $G$ with two equally-sized unlabeled communities, and the goal is to recover the vertices in these communities. A popular heuristic, known as spectral clustering, is to output an estimated community assignment based on the eigenvector corresponding to the second smallest eigenvalue of the Laplacian of $G$. Spectral algorithms can be shown to provably recover the cluster structure for graphs generated from certain probabilistic models, such as the Stochastic Block Model (SBM). However, spectral clustering is known to be non-robust to model mis-specification. Techniques based on semidefinite programming have been shown to be more robust, but they incur significant computational overheads.\n  In this work, we study the robustness of spectral algorithms against semirandom adversaries. Informally, a semirandom adversary is allowed to ``helpfully'' change the specification of the model in a way that is consistent with the ground-truth solution. Our semirandom adversaries in particular are allowed to add edges inside clusters or increase the probability that an edge appears inside a cluster. Semirandom adversaries are a useful tool to determine the extent to which an algorithm has overfit to statistical assumptions on the input.\n  On the positive side, we identify classes of semirandom adversaries under which spectral bisection using the _unnormalized_ Laplacian is strongly consistent, i.e., it exactly recovers the planted partitioning. On the negative side, we show that in these classes spectral bisection with the _normalized_ Laplacian outputs a partitioning that makes a classification mistake on a constant fraction of the vertices. Finally, we demonstrate numerical experiments that complement our theoretical findings.""}",oai:arXiv.org:2412.14315v1,False,"[{'term': 'stat.ML', 'scheme': None, 'label': None}, {'term': 'cs.DS', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'cs.SI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",cross,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Aditya Bhaskara, Agastya Vibhuti Jha, Michael Kapralov, Naren Sarayu Manoj, Davide Mazzali, Weronika Wrzos-Kaminska'}]","Aditya Bhaskara, Agastya Vibhuti Jha, Michael Kapralov, Naren Sarayu Manoj, Davide Mazzali, Weronika Wrzos-Kaminska","{'name': 'Aditya Bhaskara, Agastya Vibhuti Jha, Michael Kapralov, Naren Sarayu Manoj, Davide Mazzali, Weronika Wrzos-Kaminska'}",,
452,Long-time accuracy of ensemble Kalman filters for chaotic and machine-learned dynamical systems,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Long-time accuracy of ensemble Kalman filters for chaotic and machine-learned dynamical systems'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14318'}]",https://arxiv.org/abs/2412.14318,"arXiv:2412.14318v1 Announce Type: cross 
Abstract: Filtering is concerned with online estimation of the state of a dynamical system from partial and noisy observations. In applications where the state is high dimensional, ensemble Kalman filters are often the method of choice. This paper establishes long-time accuracy of ensemble Kalman filters. We introduce conditions on the dynamics and the observations under which the estimation error remains small in the long-time horizon. Our theory covers a wide class of partially-observed chaotic dynamical systems, which includes the Navier-Stokes equations and Lorenz models. In addition, we prove long-time accuracy of ensemble Kalman filters with surrogate dynamics, thus validating the use of machine-learned forecast models in ensemble data assimilation.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14318v1 Announce Type: cross \nAbstract: Filtering is concerned with online estimation of the state of a dynamical system from partial and noisy observations. In applications where the state is high dimensional, ensemble Kalman filters are often the method of choice. This paper establishes long-time accuracy of ensemble Kalman filters. We introduce conditions on the dynamics and the observations under which the estimation error remains small in the long-time horizon. Our theory covers a wide class of partially-observed chaotic dynamical systems, which includes the Navier-Stokes equations and Lorenz models. In addition, we prove long-time accuracy of ensemble Kalman filters with surrogate dynamics, thus validating the use of machine-learned forecast models in ensemble data assimilation.'}",oai:arXiv.org:2412.14318v1,False,"[{'term': 'math.DS', 'scheme': None, 'label': None}, {'term': 'cs.NA', 'scheme': None, 'label': None}, {'term': 'math.NA', 'scheme': None, 'label': None}, {'term': 'stat.ML', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",cross,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Daniel Sanz-Alonso, Nathan Waniorek'}]","Daniel Sanz-Alonso, Nathan Waniorek","{'name': 'Daniel Sanz-Alonso, Nathan Waniorek'}",,
453,Using SimTeEx to simplify polynomial expressions with tensors,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Using SimTeEx to simplify polynomial expressions with tensors'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14390'}]",https://arxiv.org/abs/2412.14390,"arXiv:2412.14390v1 Announce Type: cross 
Abstract: Computations with tensors are ubiquitous in fundamental physics, and so is the usage of Einstein's dummy index convention for the contraction of indices. For instance, $T_{ia}U_{aj}$ is readily recognized as the same as $T_{ib}U_{bj}$, but a computer does not know that T[i,a]U[a,j] is equal to T[i,b]U[b,j]. Furthermore, tensors may have symmetries which can be used to simply expressions: if $U_{ij}$ is antisymmetric, then $\alpha T_{ia}U_{aj}+\beta T_{ib}U_{jb}=\left(\alpha-\beta\right)T_{ia}U_{aj}$. The fact that tensors can have elaborate symmetries, together with the problem of dummy indices, makes it complicated to simplify polynomial expressions with tensors. In this work I will present an algorithm for doing so, which was implemented in the Mathematica package SimTeEx (Simplify Tensor Expressions). It can handle any kind of tensor symmetry.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14390v1 Announce Type: cross \nAbstract: Computations with tensors are ubiquitous in fundamental physics, and so is the usage of Einstein's dummy index convention for the contraction of indices. For instance, $T_{ia}U_{aj}$ is readily recognized as the same as $T_{ib}U_{bj}$, but a computer does not know that T[i,a]U[a,j] is equal to T[i,b]U[b,j]. Furthermore, tensors may have symmetries which can be used to simply expressions: if $U_{ij}$ is antisymmetric, then $\\alpha T_{ia}U_{aj}+\\beta T_{ib}U_{jb}=\\left(\\alpha-\\beta\\right)T_{ia}U_{aj}$. The fact that tensors can have elaborate symmetries, together with the problem of dummy indices, makes it complicated to simplify polynomial expressions with tensors. In this work I will present an algorithm for doing so, which was implemented in the Mathematica package SimTeEx (Simplify Tensor Expressions). It can handle any kind of tensor symmetry.""}",oai:arXiv.org:2412.14390v1,False,"[{'term': 'hep-ph', 'scheme': None, 'label': None}, {'term': 'cs.SC', 'scheme': None, 'label': None}, {'term': 'gr-qc', 'scheme': None, 'label': None}, {'term': 'hep-th', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",cross,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}",[{'name': 'Renato M. Fonseca'}],Renato M. Fonseca,{'name': 'Renato M. Fonseca'},,
454,Short-term wind forecasting via surface pressure measurements: stochastic modeling and sensor placement,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Short-term wind forecasting via surface pressure measurements: stochastic modeling and sensor placement'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14403'}]",https://arxiv.org/abs/2412.14403,"arXiv:2412.14403v1 Announce Type: cross 
Abstract: We propose a short-term wind forecasting framework for predicting real-time variations in atmospheric turbulence based on nacelle-mounted anemometer and ground-level air-pressure measurements. Our approach combines linear stochastic estimation and Kalman filtering algorithms to assimilate and process real-time field measurements with the predictions of a stochastic reduced-order model that is confined to a two-dimensional plane at the hub height of turbines. We bridge the vertical gap between the computational plane of the model at hub height and the measurement plane on the ground using a projection technique that allows us to infer the pressure in one plane from the other. Depending on the quality of this inference, we show that customized variants of the extended and ensemble Kalman filters can be tuned to balance estimation quality and computational speed 1-1.5 diameters ahead and behind leading turbines. In particular, we show how synchronizing the sign of estimates with that of velocity fluctuations recorded at the nacelle can significantly improve the ability to follow temporal variations upwind of the leading turbine. We also propose a convex optimization-based framework for selecting a subset of pressure sensors that achieve a desired level of accuracy relative to the optimal Kalman filter that uses all sensing capabilities.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14403v1 Announce Type: cross \nAbstract: We propose a short-term wind forecasting framework for predicting real-time variations in atmospheric turbulence based on nacelle-mounted anemometer and ground-level air-pressure measurements. Our approach combines linear stochastic estimation and Kalman filtering algorithms to assimilate and process real-time field measurements with the predictions of a stochastic reduced-order model that is confined to a two-dimensional plane at the hub height of turbines. We bridge the vertical gap between the computational plane of the model at hub height and the measurement plane on the ground using a projection technique that allows us to infer the pressure in one plane from the other. Depending on the quality of this inference, we show that customized variants of the extended and ensemble Kalman filters can be tuned to balance estimation quality and computational speed 1-1.5 diameters ahead and behind leading turbines. In particular, we show how synchronizing the sign of estimates with that of velocity fluctuations recorded at the nacelle can significantly improve the ability to follow temporal variations upwind of the leading turbine. We also propose a convex optimization-based framework for selecting a subset of pressure sensors that achieve a desired level of accuracy relative to the optimal Kalman filter that uses all sensing capabilities.'}",oai:arXiv.org:2412.14403v1,False,"[{'term': 'physics.flu-dyn', 'scheme': None, 'label': None}, {'term': 'cs.SY', 'scheme': None, 'label': None}, {'term': 'eess.SY', 'scheme': None, 'label': None}, {'term': 'math.DS', 'scheme': None, 'label': None}, {'term': 'physics.ao-ph', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",cross,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Seyedalireza Abootorabi, Stefano Leonardi, Mario Rotea, Armin Zare'}]","Seyedalireza Abootorabi, Stefano Leonardi, Mario Rotea, Armin Zare","{'name': 'Seyedalireza Abootorabi, Stefano Leonardi, Mario Rotea, Armin Zare'}",,
455,Stochastic first-order methods with multi-extrapolated momentum for highly smooth unconstrained optimization,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Stochastic first-order methods with multi-extrapolated momentum for highly smooth unconstrained optimization'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14488'}]",https://arxiv.org/abs/2412.14488,"arXiv:2412.14488v1 Announce Type: cross 
Abstract: In this paper we consider an unconstrained stochastic optimization problem where the objective function exhibits a high order of smoothness. In particular, we propose a stochastic first-order method (SFOM) with multi-extrapolated momentum, in which multiple extrapolations are performed in each iteration, followed by a momentum step based on these extrapolations. We show that our proposed SFOM with multi-extrapolated momentum can accelerate optimization by exploiting the high-order smoothness of the objective function $f$. Specifically, assuming that the gradient and the $p$th-order derivative of $f$ are Lipschitz continuous for some $p\ge2$, and under some additional mild assumptions, we establish that our method achieves a sample complexity of $\widetilde{\mathcal{O}}(\epsilon^{-(3p+1)/p})$ for finding a point $x$ satisfying $\mathbb{E}[\|\nabla f(x)\|]\le\epsilon$. To the best of our knowledge, our method is the first SFOM to leverage arbitrary order smoothness of the objective function for acceleration, resulting in a sample complexity that strictly improves upon the best-known results without assuming the average smoothness condition. Finally, preliminary numerical experiments validate the practical performance of our method and corroborate our theoretical findings.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14488v1 Announce Type: cross \nAbstract: In this paper we consider an unconstrained stochastic optimization problem where the objective function exhibits a high order of smoothness. In particular, we propose a stochastic first-order method (SFOM) with multi-extrapolated momentum, in which multiple extrapolations are performed in each iteration, followed by a momentum step based on these extrapolations. We show that our proposed SFOM with multi-extrapolated momentum can accelerate optimization by exploiting the high-order smoothness of the objective function $f$. Specifically, assuming that the gradient and the $p$th-order derivative of $f$ are Lipschitz continuous for some $p\\ge2$, and under some additional mild assumptions, we establish that our method achieves a sample complexity of $\\widetilde{\\mathcal{O}}(\\epsilon^{-(3p+1)/p})$ for finding a point $x$ satisfying $\\mathbb{E}[\\|\\nabla f(x)\\|]\\le\\epsilon$. To the best of our knowledge, our method is the first SFOM to leverage arbitrary order smoothness of the objective function for acceleration, resulting in a sample complexity that strictly improves upon the best-known results without assuming the average smoothness condition. Finally, preliminary numerical experiments validate the practical performance of our method and corroborate our theoretical findings.'}",oai:arXiv.org:2412.14488v1,False,"[{'term': 'math.OC', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",cross,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}",[{'name': 'Chuan He'}],Chuan He,{'name': 'Chuan He'},,
456,Statistical Undersampling with Mutual Information and Support Points,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Statistical Undersampling with Mutual Information and Support Points'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14527'}]",https://arxiv.org/abs/2412.14527,"arXiv:2412.14527v1 Announce Type: cross 
Abstract: Class imbalance and distributional differences in large datasets present significant challenges for classification tasks machine learning, often leading to biased models and poor predictive performance for minority classes. This work introduces two novel undersampling approaches: mutual information-based stratified simple random sampling and support points optimization. These methods prioritize representative data selection, effectively minimizing information loss. Empirical results across multiple classification tasks demonstrate that our methods outperform traditional undersampling techniques, achieving higher balanced classification accuracy. These findings highlight the potential of combining statistical concepts with machine learning to address class imbalance in practical applications.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14527v1 Announce Type: cross \nAbstract: Class imbalance and distributional differences in large datasets present significant challenges for classification tasks machine learning, often leading to biased models and poor predictive performance for minority classes. This work introduces two novel undersampling approaches: mutual information-based stratified simple random sampling and support points optimization. These methods prioritize representative data selection, effectively minimizing information loss. Empirical results across multiple classification tasks demonstrate that our methods outperform traditional undersampling techniques, achieving higher balanced classification accuracy. These findings highlight the potential of combining statistical concepts with machine learning to address class imbalance in practical applications.'}",oai:arXiv.org:2412.14527v1,False,"[{'term': 'stat.ML', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",cross,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Alex Mak, Shubham Sahoo, Shivani Pandey, Yidan Yue, Linglong Kong'}]","Alex Mak, Shubham Sahoo, Shivani Pandey, Yidan Yue, Linglong Kong","{'name': 'Alex Mak, Shubham Sahoo, Shivani Pandey, Yidan Yue, Linglong Kong'}",,
457,Accelerated Patient-Specific Calibration via Differentiable Hemodynamics Simulations,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Accelerated Patient-Specific Calibration via Differentiable Hemodynamics Simulations'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14572'}]",https://arxiv.org/abs/2412.14572,"arXiv:2412.14572v1 Announce Type: cross 
Abstract: One of the goals of personalized medicine is to tailor diagnostics to individual patients. Diagnostics are performed in practice by measuring quantities, called biomarkers, that indicate the existence and progress of a disease. In common cardiovascular diseases, such as hypertension, biomarkers that are closely related to the clinical representation of a patient can be predicted using computational models. Personalizing computational models translates to considering patient-specific flow conditions, for example, the compliance of blood vessels that cannot be a priori known and quantities such as the patient geometry that can be measured using imaging. Therefore, a patient is identified by a set of measurable and nonmeasurable parameters needed to well-define a computational model; else, the computational model is not personalized, meaning it is prone to large prediction errors. Therefore, to personalize a computational model, sufficient information needs to be extracted from the data. The current methods by which this is done are either inefficient, due to relying on slow-converging optimization methods, or hard to interpret, due to using `black box` deep-learning algorithms. We propose a personalized diagnostic procedure based on a differentiable 0D-1D Navier-Stokes reduced order model solver and fast parameter inference methods that take advantage of gradients through the solver. By providing a faster method for performing parameter inference and sensitivity analysis through differentiability while maintaining the interpretability of well-understood mathematical models and numerical methods, the best of both worlds is combined. The performance of the proposed solver is validated against a well-established process on different geometries, and different parameter inference processes are successfully performed.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14572v1 Announce Type: cross \nAbstract: One of the goals of personalized medicine is to tailor diagnostics to individual patients. Diagnostics are performed in practice by measuring quantities, called biomarkers, that indicate the existence and progress of a disease. In common cardiovascular diseases, such as hypertension, biomarkers that are closely related to the clinical representation of a patient can be predicted using computational models. Personalizing computational models translates to considering patient-specific flow conditions, for example, the compliance of blood vessels that cannot be a priori known and quantities such as the patient geometry that can be measured using imaging. Therefore, a patient is identified by a set of measurable and nonmeasurable parameters needed to well-define a computational model; else, the computational model is not personalized, meaning it is prone to large prediction errors. Therefore, to personalize a computational model, sufficient information needs to be extracted from the data. The current methods by which this is done are either inefficient, due to relying on slow-converging optimization methods, or hard to interpret, due to using `black box` deep-learning algorithms. We propose a personalized diagnostic procedure based on a differentiable 0D-1D Navier-Stokes reduced order model solver and fast parameter inference methods that take advantage of gradients through the solver. By providing a faster method for performing parameter inference and sensitivity analysis through differentiability while maintaining the interpretability of well-understood mathematical models and numerical methods, the best of both worlds is combined. The performance of the proposed solver is validated against a well-established process on different geometries, and different parameter inference processes are successfully performed.'}",oai:arXiv.org:2412.14572v1,False,"[{'term': 'physics.med-ph', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'cs.MS', 'scheme': None, 'label': None}, {'term': 'physics.comp-ph', 'scheme': None, 'label': None}, {'term': 'q-bio.QM', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",cross,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Diego Renner, Georgios Kissas'}]","Diego Renner, Georgios Kissas","{'name': 'Diego Renner, Georgios Kissas'}",,
458,Fast inverse lithography based on a model-driven block stacking convolutional neural network,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Fast inverse lithography based on a model-driven block stacking convolutional neural network'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14599'}]",https://arxiv.org/abs/2412.14599,"arXiv:2412.14599v1 Announce Type: cross 
Abstract: In the realm of lithography, Optical Proximity Correction (OPC) is a crucial resolution enhancement technique that optimizes the transmission function of photomasks on a pixel-based to effectively counter Optical Proximity Effects (OPE). However, conventional pixel-based OPC methods often generate patterns that pose manufacturing challenges, thereby leading to the increased cost in practical scenarios. This paper presents a novel inverse lithographic approach to OPC, employing a model-driven, block stacking deep learning framework that expedites the generation of masks conducive to manufacturing. This method is founded on vector lithography modelling and streamlines the training process by eliminating the requirement for extensive labeled datasets. Furthermore, diversity of mask patterns is enhanced by employing a wave function collapse algorithm, which facilitates the random generation of a multitude of target patterns, therefore significantly expanding the range of mask paradigm. Numerical experiments have substantiated the efficacy of the proposed end-to-end approach, highlighting its superior capability to manage mask complexity within the context of advanced OPC lithography. This advancement is anticipated to enhance the feasibility and economic viability of OPC technology within actual manufacturing environments.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14599v1 Announce Type: cross \nAbstract: In the realm of lithography, Optical Proximity Correction (OPC) is a crucial resolution enhancement technique that optimizes the transmission function of photomasks on a pixel-based to effectively counter Optical Proximity Effects (OPE). However, conventional pixel-based OPC methods often generate patterns that pose manufacturing challenges, thereby leading to the increased cost in practical scenarios. This paper presents a novel inverse lithographic approach to OPC, employing a model-driven, block stacking deep learning framework that expedites the generation of masks conducive to manufacturing. This method is founded on vector lithography modelling and streamlines the training process by eliminating the requirement for extensive labeled datasets. Furthermore, diversity of mask patterns is enhanced by employing a wave function collapse algorithm, which facilitates the random generation of a multitude of target patterns, therefore significantly expanding the range of mask paradigm. Numerical experiments have substantiated the efficacy of the proposed end-to-end approach, highlighting its superior capability to manage mask complexity within the context of advanced OPC lithography. This advancement is anticipated to enhance the feasibility and economic viability of OPC technology within actual manufacturing environments.'}",oai:arXiv.org:2412.14599v1,False,"[{'term': 'physics.optics', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",cross,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Ruixiang Chen, Yang Zhao, Haoqin Li, Rui Chen'}]","Ruixiang Chen, Yang Zhao, Haoqin Li, Rui Chen","{'name': 'Ruixiang Chen, Yang Zhao, Haoqin Li, Rui Chen'}",,
459,Some permutation pentanomials over finite fields of even characteristic,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Some permutation pentanomials over finite fields of even characteristic'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14641'}]",https://arxiv.org/abs/2412.14641,"arXiv:2412.14641v1 Announce Type: cross 
Abstract: In a recent paper Zhang et al. constructed 17 families of permutation pentanomials of the form $x^t+x^{r_1(q-1)+t}+x^{r_2(q-1)+t}+x^{r_3(q-1)+t}+x^{r_4(q-1)+t}$ over $\mathbb{F}_{q^2}$ where $q=2^m$. In this paper for 14 of these 17 families we provide a simple explanation as to why they are permutations. We also extend these 14 families into three general classes of permutation pentanomials over $\mathbb{F}_{q^2}$.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14641v1 Announce Type: cross \nAbstract: In a recent paper Zhang et al. constructed 17 families of permutation pentanomials of the form $x^t+x^{r_1(q-1)+t}+x^{r_2(q-1)+t}+x^{r_3(q-1)+t}+x^{r_4(q-1)+t}$ over $\\mathbb{F}_{q^2}$ where $q=2^m$. In this paper for 14 of these 17 families we provide a simple explanation as to why they are permutations. We also extend these 14 families into three general classes of permutation pentanomials over $\\mathbb{F}_{q^2}$.'}",oai:arXiv.org:2412.14641v1,False,"[{'term': 'math.CO', 'scheme': None, 'label': None}, {'term': 'cs.DM', 'scheme': None, 'label': None}, {'term': 'math.NT', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",cross,http://creativecommons.org/licenses/by-nc-nd/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-nd/4.0/'}","[{'name': 'Farhana Kousar, Maosheng Xiong'}]","Farhana Kousar, Maosheng Xiong","{'name': 'Farhana Kousar, Maosheng Xiong'}",,
460,Permutation recovery of spikes in noisy high-dimensional tensor estimation,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Permutation recovery of spikes in noisy high-dimensional tensor estimation'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14650'}]",https://arxiv.org/abs/2412.14650,"arXiv:2412.14650v1 Announce Type: cross 
Abstract: We study the dynamics of gradient flow in high dimensions for the multi-spiked tensor problem, where the goal is to estimate $r$ unknown signal vectors (spikes) from noisy Gaussian tensor observations. Specifically, we analyze the maximum likelihood estimation procedure, which involves optimizing a highly nonconvex random function. We determine the sample complexity required for gradient flow to efficiently recover all spikes, without imposing any assumptions on the separation of the signal-to-noise ratios (SNRs). More precisely, our results provide the sample complexity required to guarantee recovery of the spikes up to a permutation. Our work builds on our companion paper [Ben Arous, Gerbelot, Piccolo 2024], which studies Langevin dynamics and determines the sample complexity and separation conditions for the SNRs necessary for ensuring exact recovery of the spikes (where the recovered permutation matches the identity). During the recovery process, the correlations between the estimators and the hidden vectors increase in a sequential manner. The order in which these correlations become significant depends on their initial values and the corresponding SNRs, which ultimately determines the permutation of the recovered spikes.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14650v1 Announce Type: cross \nAbstract: We study the dynamics of gradient flow in high dimensions for the multi-spiked tensor problem, where the goal is to estimate $r$ unknown signal vectors (spikes) from noisy Gaussian tensor observations. Specifically, we analyze the maximum likelihood estimation procedure, which involves optimizing a highly nonconvex random function. We determine the sample complexity required for gradient flow to efficiently recover all spikes, without imposing any assumptions on the separation of the signal-to-noise ratios (SNRs). More precisely, our results provide the sample complexity required to guarantee recovery of the spikes up to a permutation. Our work builds on our companion paper [Ben Arous, Gerbelot, Piccolo 2024], which studies Langevin dynamics and determines the sample complexity and separation conditions for the SNRs necessary for ensuring exact recovery of the spikes (where the recovered permutation matches the identity). During the recovery process, the correlations between the estimators and the hidden vectors increase in a sequential manner. The order in which these correlations become significant depends on their initial values and the corresponding SNRs, which ultimately determines the permutation of the recovered spikes.'}",oai:arXiv.org:2412.14650v1,False,"[{'term': 'math.PR', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'stat.ML', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",cross,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': ""G\\'erard Ben Arous, C\\'Cedric Gerbelot, Vanessa Piccolo""}]","G\'erard Ben Arous, C\'Cedric Gerbelot, Vanessa Piccolo","{'name': ""G\\'erard Ben Arous, C\\'Cedric Gerbelot, Vanessa Piccolo""}",,
461,A brief history of quantum vs classical computational advantage,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'A brief history of quantum vs classical computational advantage'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14703'}]",https://arxiv.org/abs/2412.14703,"arXiv:2412.14703v1 Announce Type: cross 
Abstract: In this review article we summarize all experiments claiming quantum computational advantage to date. Our review highlights challenges, loopholes, and refutations appearing in subsequent work to provide a complete picture of the current statuses of these experiments. In addition, we also discuss theoretical computational advantage in example problems such as approximate optimization and recommendation systems. Finally, we review recent experiments in quantum error correction -- the biggest frontier to reach experimental quantum advantage in Shor's algorithm.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14703v1 Announce Type: cross \nAbstract: In this review article we summarize all experiments claiming quantum computational advantage to date. Our review highlights challenges, loopholes, and refutations appearing in subsequent work to provide a complete picture of the current statuses of these experiments. In addition, we also discuss theoretical computational advantage in example problems such as approximate optimization and recommendation systems. Finally, we review recent experiments in quantum error correction -- the biggest frontier to reach experimental quantum advantage in Shor's algorithm.""}",oai:arXiv.org:2412.14703v1,False,"[{'term': 'quant-ph', 'scheme': None, 'label': None}, {'term': 'cs.ET', 'scheme': None, 'label': None}, {'term': 'physics.hist-ph', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",cross,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}",[{'name': 'Ryan LaRose'}],Ryan LaRose,{'name': 'Ryan LaRose'},,
462,Dimension reduction for path signatures,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Dimension reduction for path signatures'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14723'}]",https://arxiv.org/abs/2412.14723,"arXiv:2412.14723v1 Announce Type: cross 
Abstract: This paper focuses on the mathematical framework for reducing the complexity of models using path signatures. The structure of these signatures, which can be interpreted as collections of iterated integrals along paths, is discussed and their applications in areas such as stochastic differential equations (SDEs) and financial modeling are pointed out. In particular, exploiting the rough paths view, solutions of SDEs continuously depend on the lift of the driver. Such continuous mappings can be approximated using (truncated) signatures, which are solutions of high-dimensional linear systems. In order to lower the complexity of these models, this paper presents methods for reducing the order of high-dimensional truncated signature models while retaining essential characteristics. The derivation of reduced models and the universal approximation property of (truncated) signatures are treated in detail. Numerical examples, including applications to the (rough) Bergomi model in financial markets, illustrate the proposed reduction techniques and highlight their effectiveness.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14723v1 Announce Type: cross \nAbstract: This paper focuses on the mathematical framework for reducing the complexity of models using path signatures. The structure of these signatures, which can be interpreted as collections of iterated integrals along paths, is discussed and their applications in areas such as stochastic differential equations (SDEs) and financial modeling are pointed out. In particular, exploiting the rough paths view, solutions of SDEs continuously depend on the lift of the driver. Such continuous mappings can be approximated using (truncated) signatures, which are solutions of high-dimensional linear systems. In order to lower the complexity of these models, this paper presents methods for reducing the order of high-dimensional truncated signature models while retaining essential characteristics. The derivation of reduced models and the universal approximation property of (truncated) signatures are treated in detail. Numerical examples, including applications to the (rough) Bergomi model in financial markets, illustrate the proposed reduction techniques and highlight their effectiveness.'}",oai:arXiv.org:2412.14723v1,False,"[{'term': 'math.PR', 'scheme': None, 'label': None}, {'term': 'cs.NA', 'scheme': None, 'label': None}, {'term': 'math.NA', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",cross,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Christian Bayer, Martin Redmann'}]","Christian Bayer, Martin Redmann","{'name': 'Christian Bayer, Martin Redmann'}",,
463,Deep Learning Based Recalibration of SDSS and DESI BAO Alleviates Hubble and Clustering Tensions,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Deep Learning Based Recalibration of SDSS and DESI BAO Alleviates Hubble and Clustering Tensions'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14750'}]",https://arxiv.org/abs/2412.14750,"arXiv:2412.14750v1 Announce Type: cross 
Abstract: Conventional calibration of Baryon Acoustic Oscillations (BAO) data relies on estimation of the sound horizon at drag epoch $r_d$ from early universe observations by assuming a cosmological model. We present a recalibration of two independent BAO datasets, SDSS and DESI, by employing deep learning techniques for model-independent estimation of $r_d$, and explore the impacts on $\Lambda$CDM cosmological parameters. Significant reductions in both Hubble ($H_0$) and clustering ($S_8$) tensions are observed for both the recalibrated datasets. Moderate shifts in some other parameters hint towards further exploration of such data-driven approaches.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14750v1 Announce Type: cross \nAbstract: Conventional calibration of Baryon Acoustic Oscillations (BAO) data relies on estimation of the sound horizon at drag epoch $r_d$ from early universe observations by assuming a cosmological model. We present a recalibration of two independent BAO datasets, SDSS and DESI, by employing deep learning techniques for model-independent estimation of $r_d$, and explore the impacts on $\\Lambda$CDM cosmological parameters. Significant reductions in both Hubble ($H_0$) and clustering ($S_8$) tensions are observed for both the recalibrated datasets. Moderate shifts in some other parameters hint towards further exploration of such data-driven approaches.'}",oai:arXiv.org:2412.14750v1,False,"[{'term': 'astro-ph.CO', 'scheme': None, 'label': None}, {'term': 'astro-ph.IM', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",cross,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Rahul Shah, Purba Mukherjee, Soumadeep Saha, Utpal Garain, Supratik Pal'}]","Rahul Shah, Purba Mukherjee, Soumadeep Saha, Utpal Garain, Supratik Pal","{'name': 'Rahul Shah, Purba Mukherjee, Soumadeep Saha, Utpal Garain, Supratik Pal'}",,
464,Opportunities and limitations of explaining quantum machine learning,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Opportunities and limitations of explaining quantum machine learning'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14753'}]",https://arxiv.org/abs/2412.14753,"arXiv:2412.14753v1 Announce Type: cross 
Abstract: A common trait of many machine learning models is that it is often difficult to understand and explain what caused the model to produce the given output. While the explainability of neural networks has been an active field of research in the last years, comparably little is known for quantum machine learning models. Despite a few recent works analyzing some specific aspects of explainability, as of now there is no clear big picture perspective as to what can be expected from quantum learning models in terms of explainability. In this work, we address this issue by identifying promising research avenues in this direction and lining out the expected future results. We additionally propose two explanation methods designed specifically for quantum machine learning models, as first of their kind to the best of our knowledge. Next to our pre-view of the field, we compare both existing and novel methods to explain the predictions of quantum learning models. By studying explainability in quantum machine learning, we can contribute to the sustainable development of the field, preventing trust issues in the future.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14753v1 Announce Type: cross \nAbstract: A common trait of many machine learning models is that it is often difficult to understand and explain what caused the model to produce the given output. While the explainability of neural networks has been an active field of research in the last years, comparably little is known for quantum machine learning models. Despite a few recent works analyzing some specific aspects of explainability, as of now there is no clear big picture perspective as to what can be expected from quantum learning models in terms of explainability. In this work, we address this issue by identifying promising research avenues in this direction and lining out the expected future results. We additionally propose two explanation methods designed specifically for quantum machine learning models, as first of their kind to the best of our knowledge. Next to our pre-view of the field, we compare both existing and novel methods to explain the predictions of quantum learning models. By studying explainability in quantum machine learning, we can contribute to the sustainable development of the field, preventing trust issues in the future.'}",oai:arXiv.org:2412.14753v1,False,"[{'term': 'quant-ph', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'stat.ML', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",cross,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': ""Elies Gil-Fuster, Jonas R. Naujoks, Gr\\'egoire Montavon, Thomas Wiegand, Wojciech Samek, Jens Eisert""}]","Elies Gil-Fuster, Jonas R. Naujoks, Gr\'egoire Montavon, Thomas Wiegand, Wojciech Samek, Jens Eisert","{'name': ""Elies Gil-Fuster, Jonas R. Naujoks, Gr\\'egoire Montavon, Thomas Wiegand, Wojciech Samek, Jens Eisert""}",,
465,Space-time Peer-to-Peer Distribution of Multi-party Entanglement for Any Quantum Network,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Space-time Peer-to-Peer Distribution of Multi-party Entanglement for Any Quantum Network'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14757'}]",https://arxiv.org/abs/2412.14757,"arXiv:2412.14757v1 Announce Type: cross 
Abstract: Graph states are a class of important multiparty entangled states, of which bell pairs are the special case. Realizing a robust and fast distribution of arbitrary graph states in the downstream layer of the quantum network can be essential for further large-scale quantum networks. We propose a novel quantum network protocol called P2PGSD inspired by the classical Peer-to-Peer (P2P) network to efficiently implement the general graph state distribution in the network layer, which demonstrates advantages in resource efficiency and scalability over existing methods for sparse graph states. An explicit mathematical model for a general graph state distribution problem has also been constructed, above which the intractability for a wide class of resource minimization problems is proved and the optimality of the existing algorithms is discussed. In addition, we proposed the spacetime network inspired by the symmetry from relativity for memory management in network problems and used it to improve our proposed algorithm. The advantages of our protocols are confirmed by numerical simulations showing an improvement of up to 50\% for general sparse graph states, paving the way for a resource-efficient multiparty entanglement distribution across any network topology.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14757v1 Announce Type: cross \nAbstract: Graph states are a class of important multiparty entangled states, of which bell pairs are the special case. Realizing a robust and fast distribution of arbitrary graph states in the downstream layer of the quantum network can be essential for further large-scale quantum networks. We propose a novel quantum network protocol called P2PGSD inspired by the classical Peer-to-Peer (P2P) network to efficiently implement the general graph state distribution in the network layer, which demonstrates advantages in resource efficiency and scalability over existing methods for sparse graph states. An explicit mathematical model for a general graph state distribution problem has also been constructed, above which the intractability for a wide class of resource minimization problems is proved and the optimality of the existing algorithms is discussed. In addition, we proposed the spacetime network inspired by the symmetry from relativity for memory management in network problems and used it to improve our proposed algorithm. The advantages of our protocols are confirmed by numerical simulations showing an improvement of up to 50\\% for general sparse graph states, paving the way for a resource-efficient multiparty entanglement distribution across any network topology.'}",oai:arXiv.org:2412.14757v1,False,"[{'term': 'quant-ph', 'scheme': None, 'label': None}, {'term': 'cs.NI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",cross,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Yuexun Huang, Xiangyu Ren, Bikun Li, Yat Wong, Liang Jiang'}]","Yuexun Huang, Xiangyu Ren, Bikun Li, Yat Wong, Liang Jiang","{'name': 'Yuexun Huang, Xiangyu Ren, Bikun Li, Yat Wong, Liang Jiang'}",,
466,Energy and polarization based on-line interference mitigation in radio interferometry,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Energy and polarization based on-line interference mitigation in radio interferometry'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14775'}]",https://arxiv.org/abs/2412.14775,"arXiv:2412.14775v1 Announce Type: cross 
Abstract: Radio frequency interference (RFI) is a persistent contaminant in terrestrial radio astronomy. While new radio interferometers are becoming operational, novel sources of RFI are also emerging. In order to strengthen the mitigation of RFI in modern radio interferometers, we propose an on-line RFI mitigation scheme that can be run in the correlator of such interferometers. We combine statistics based on the energy as well as the polarization alignment of the correlated signal to develop an on-line RFI mitigation scheme that can be applied to a data stream produced by the correlator in real-time, especially targeted at low duty-cycle or transient RFI detection. In order to improve the computational efficiency, we explore the use of both single precision and half precision floating point operations in implementing the RFI mitigation algorithm. This ideally suits its deployment in accelerator computing devices such as graphics processing units (GPUs) as used by the LOFAR correlator. We provide results based on real data to demonstrate the efficacy of the proposed method.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14775v1 Announce Type: cross \nAbstract: Radio frequency interference (RFI) is a persistent contaminant in terrestrial radio astronomy. While new radio interferometers are becoming operational, novel sources of RFI are also emerging. In order to strengthen the mitigation of RFI in modern radio interferometers, we propose an on-line RFI mitigation scheme that can be run in the correlator of such interferometers. We combine statistics based on the energy as well as the polarization alignment of the correlated signal to develop an on-line RFI mitigation scheme that can be applied to a data stream produced by the correlator in real-time, especially targeted at low duty-cycle or transient RFI detection. In order to improve the computational efficiency, we explore the use of both single precision and half precision floating point operations in implementing the RFI mitigation algorithm. This ideally suits its deployment in accelerator computing devices such as graphics processing units (GPUs) as used by the LOFAR correlator. We provide results based on real data to demonstrate the efficacy of the proposed method.'}",oai:arXiv.org:2412.14775v1,False,"[{'term': 'astro-ph.IM', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",cross,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Sarod Yatawatta, Albert-Jan Boonstra, Chris P. Broekema'}]","Sarod Yatawatta, Albert-Jan Boonstra, Chris P. Broekema","{'name': 'Sarod Yatawatta, Albert-Jan Boonstra, Chris P. Broekema'}",,
467,Large Induced Subgraphs of Bounded Degree in Planar Graphs and Beyond,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Large Induced Subgraphs of Bounded Degree in Planar Graphs and Beyond'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14784'}]",https://arxiv.org/abs/2412.14784,"arXiv:2412.14784v1 Announce Type: cross 
Abstract: In this paper, we introduce and study the following question. Let $\mathcal G$ be a family of graphs and let $k\geq 3$ be an integer. What is the largest value $f_k(n)$ such that every $n$-vertex graph in $\mathcal G$ has an induced subgraph with degree at most $k$ and with $f_k(n)$ vertices? Similar questions, in which one seeks a large induced forest, or a large induced linear forest, or a large induced $d$-degenerate graph, rather than a large induced graph of bounded degree, have been studied for decades and have given rise to some of the most fascinating and elusive conjectures in Graph Theory. We tackle our problem when $\mathcal G$ is the class of the outerplanar graphs, or the class of the planar graphs, or the class of the graphs whose degree is bounded by a value $d>k$. In all cases, we provide upper and lower bounds on the value of $f_k(n)$. For example, we prove that every $n$-vertex planar graph has an induced subgraph with degree at most $3$ and with $\frac{5n}{13}>0.384n$ vertices, and that there exist $n$-vertex planar graphs whose largest induced subgraph with degree at most $3$ has $\frac{4n}{7}+O(1)<0.572n+O(1)$ vertices.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14784v1 Announce Type: cross \nAbstract: In this paper, we introduce and study the following question. Let $\\mathcal G$ be a family of graphs and let $k\\geq 3$ be an integer. What is the largest value $f_k(n)$ such that every $n$-vertex graph in $\\mathcal G$ has an induced subgraph with degree at most $k$ and with $f_k(n)$ vertices? Similar questions, in which one seeks a large induced forest, or a large induced linear forest, or a large induced $d$-degenerate graph, rather than a large induced graph of bounded degree, have been studied for decades and have given rise to some of the most fascinating and elusive conjectures in Graph Theory. We tackle our problem when $\\mathcal G$ is the class of the outerplanar graphs, or the class of the planar graphs, or the class of the graphs whose degree is bounded by a value $d>k$. In all cases, we provide upper and lower bounds on the value of $f_k(n)$. For example, we prove that every $n$-vertex planar graph has an induced subgraph with degree at most $3$ and with $\\frac{5n}{13}>0.384n$ vertices, and that there exist $n$-vertex planar graphs whose largest induced subgraph with degree at most $3$ has $\\frac{4n}{7}+O(1)<0.572n+O(1)$ vertices.'}",oai:arXiv.org:2412.14784v1,False,"[{'term': 'math.CO', 'scheme': None, 'label': None}, {'term': 'cs.DM', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",cross,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': ""Marco D'Elia, Fabrizio Frati""}]","Marco D'Elia, Fabrizio Frati","{'name': ""Marco D'Elia, Fabrizio Frati""}",,
468,Asymptotically Enumerating Independent Sets in Regular $k$-Partite $k$-Uniform Hypergraphs,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Asymptotically Enumerating Independent Sets in Regular $k$-Partite $k$-Uniform Hypergraphs'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14845'}]",https://arxiv.org/abs/2412.14845,"arXiv:2412.14845v1 Announce Type: cross 
Abstract: The number of independent sets in regular bipartite expander graphs can be efficiently approximated by expressing it as the partition function of a suitable polymer model and truncating its cluster expansion. While this approach has been extensively used for graphs, surprisingly little is known about analogous questions in the context of hypergraphs. In this work, we apply this method to asymptotically determine the number of independent sets in regular $k$-partite $k$-uniform hypergraphs which satisfy natural expansion properties. The resulting formula depends only on the local structure of the hypergraph, making it computationally efficient. In particular, we provide a simple closed-form expression for linear hypergraphs.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14845v1 Announce Type: cross \nAbstract: The number of independent sets in regular bipartite expander graphs can be efficiently approximated by expressing it as the partition function of a suitable polymer model and truncating its cluster expansion. While this approach has been extensively used for graphs, surprisingly little is known about analogous questions in the context of hypergraphs. In this work, we apply this method to asymptotically determine the number of independent sets in regular $k$-partite $k$-uniform hypergraphs which satisfy natural expansion properties. The resulting formula depends only on the local structure of the hypergraph, making it computationally efficient. In particular, we provide a simple closed-form expression for linear hypergraphs.'}",oai:arXiv.org:2412.14845v1,False,"[{'term': 'math.CO', 'scheme': None, 'label': None}, {'term': 'cs.DM', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",cross,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Patrick Arras, Frederik Garbe, Felix Joos'}]","Patrick Arras, Frederik Garbe, Felix Joos","{'name': 'Patrick Arras, Frederik Garbe, Felix Joos'}",,
469,"Head and Neck Tumor Segmentation of MRI from Pre- and Mid-radiotherapy with Pre-training, Data Augmentation and Dual Flow UNet","{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Head and Neck Tumor Segmentation of MRI from Pre- and Mid-radiotherapy with Pre-training, Data Augmentation and Dual Flow UNet'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14846'}]",https://arxiv.org/abs/2412.14846,"arXiv:2412.14846v1 Announce Type: cross 
Abstract: Head and neck tumors and metastatic lymph nodes are crucial for treatment planning and prognostic analysis. Accurate segmentation and quantitative analysis of these structures require pixel-level annotation, making automated segmentation techniques essential for the diagnosis and treatment of head and neck cancer. In this study, we investigated the effects of multiple strategies on the segmentation of pre-radiotherapy (pre-RT) and mid-radiotherapy (mid-RT) images. For the segmentation of pre-RT images, we utilized: 1) a fully supervised learning approach, and 2) the same approach enhanced with pre-trained weights and the MixUp data augmentation technique. For mid-RT images, we introduced a novel computational-friendly network architecture that features separate encoders for mid-RT images and registered pre-RT images with their labels. The mid-RT encoder branch integrates information from pre-RT images and labels progressively during the forward propagation. We selected the highest-performing model from each fold and used their predictions to create an ensemble average for inference. In the final test, our models achieved a segmentation performance of 82.38% for pre-RT and 72.53% for mid-RT on aggregated Dice Similarity Coefficient (DSC) as HiLab. Our code is available at https://github.com/WltyBY/HNTS-MRG2024_train_code.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14846v1 Announce Type: cross \nAbstract: Head and neck tumors and metastatic lymph nodes are crucial for treatment planning and prognostic analysis. Accurate segmentation and quantitative analysis of these structures require pixel-level annotation, making automated segmentation techniques essential for the diagnosis and treatment of head and neck cancer. In this study, we investigated the effects of multiple strategies on the segmentation of pre-radiotherapy (pre-RT) and mid-radiotherapy (mid-RT) images. For the segmentation of pre-RT images, we utilized: 1) a fully supervised learning approach, and 2) the same approach enhanced with pre-trained weights and the MixUp data augmentation technique. For mid-RT images, we introduced a novel computational-friendly network architecture that features separate encoders for mid-RT images and registered pre-RT images with their labels. The mid-RT encoder branch integrates information from pre-RT images and labels progressively during the forward propagation. We selected the highest-performing model from each fold and used their predictions to create an ensemble average for inference. In the final test, our models achieved a segmentation performance of 82.38% for pre-RT and 72.53% for mid-RT on aggregated Dice Similarity Coefficient (DSC) as HiLab. Our code is available at https://github.com/WltyBY/HNTS-MRG2024_train_code.'}",oai:arXiv.org:2412.14846v1,False,"[{'term': 'eess.IV', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",cross,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Litingyu Wang, Wenjun Liao, Shichuan Zhang, Guotai Wang'}]","Litingyu Wang, Wenjun Liao, Shichuan Zhang, Guotai Wang","{'name': 'Litingyu Wang, Wenjun Liao, Shichuan Zhang, Guotai Wang'}",,
470,Surrogate-assisted multi-objective design of complex multibody systems,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Surrogate-assisted multi-objective design of complex multibody systems'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14854'}]",https://arxiv.org/abs/2412.14854,"arXiv:2412.14854v1 Announce Type: cross 
Abstract: The optimization of large-scale multibody systems is a numerically challenging task, in particular when considering multiple conflicting criteria at the same time. In this situation, we need to approximate the Pareto set of optimal compromises, which is significantly more expensive than finding a single optimum in single-objective optimization. To prevent large costs, the usage of surrogate models, constructed from a small but informative number of expensive model evaluations, is a very popular and widely studied approach. The central challenge then is to ensure a high quality (that is, near-optimality) of the solutions that were obtained using the surrogate model, which can be hard to guarantee with a single pre-computed surrogate. We present a back-and-forth approach between surrogate modeling and multi-objective optimization to improve the quality of the obtained solutions. Using the example of an expensive-to-evaluate multibody system, we compare different strategies regarding multi-objective optimization, sampling and also surrogate modeling, to identify the most promising approach in terms of computational efficiency and solution quality.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14854v1 Announce Type: cross \nAbstract: The optimization of large-scale multibody systems is a numerically challenging task, in particular when considering multiple conflicting criteria at the same time. In this situation, we need to approximate the Pareto set of optimal compromises, which is significantly more expensive than finding a single optimum in single-objective optimization. To prevent large costs, the usage of surrogate models, constructed from a small but informative number of expensive model evaluations, is a very popular and widely studied approach. The central challenge then is to ensure a high quality (that is, near-optimality) of the solutions that were obtained using the surrogate model, which can be hard to guarantee with a single pre-computed surrogate. We present a back-and-forth approach between surrogate modeling and multi-objective optimization to improve the quality of the obtained solutions. Using the example of an expensive-to-evaluate multibody system, we compare different strategies regarding multi-objective optimization, sampling and also surrogate modeling, to identify the most promising approach in terms of computational efficiency and solution quality.'}",oai:arXiv.org:2412.14854v1,False,"[{'term': 'math.OC', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",cross,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Augustina C. Amakor, Manuel B. Berkemeier, Meike Wohlleben, Walter Sextro, Sebastian Peitz'}]","Augustina C. Amakor, Manuel B. Berkemeier, Meike Wohlleben, Walter Sextro, Sebastian Peitz","{'name': 'Augustina C. Amakor, Manuel B. Berkemeier, Meike Wohlleben, Walter Sextro, Sebastian Peitz'}",,
471,"Scale This, Not That: Investigating Key Dataset Attributes for Efficient Speech Enhancement Scaling","{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Scale This, Not That: Investigating Key Dataset Attributes for Efficient Speech Enhancement Scaling'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14890'}]",https://arxiv.org/abs/2412.14890,"arXiv:2412.14890v1 Announce Type: cross 
Abstract: Recent speech enhancement models have shown impressive performance gains by scaling up model complexity and training data. However, the impact of dataset variability (e.g. text, language, speaker, and noise) has been underexplored. Analyzing each attribute individually is often challenging, as multiple attributes are usually entangled in commonly used datasets, posing a significant obstacle in understanding the distinct contributions of each attribute to the model's performance. To address this challenge, we propose a generation-training-evaluation framework that leverages zero-shot text-to-speech systems to investigate the impact of controlled attribute variations on speech enhancement performance. It enables us to synthesize training datasets in a scalable manner while carefully altering each attribute. Based on the proposed framework, we analyze the scaling effects of various dataset attributes on the performance of both discriminative and generative SE models. Extensive experiments on multi-domain corpora imply that acoustic attributes (e.g., speaker and noise) are much more important to current speech enhancement models than semantic attributes (e.g., language and text), offering new insights for future research.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14890v1 Announce Type: cross \nAbstract: Recent speech enhancement models have shown impressive performance gains by scaling up model complexity and training data. However, the impact of dataset variability (e.g. text, language, speaker, and noise) has been underexplored. Analyzing each attribute individually is often challenging, as multiple attributes are usually entangled in commonly used datasets, posing a significant obstacle in understanding the distinct contributions of each attribute to the model's performance. To address this challenge, we propose a generation-training-evaluation framework that leverages zero-shot text-to-speech systems to investigate the impact of controlled attribute variations on speech enhancement performance. It enables us to synthesize training datasets in a scalable manner while carefully altering each attribute. Based on the proposed framework, we analyze the scaling effects of various dataset attributes on the performance of both discriminative and generative SE models. Extensive experiments on multi-domain corpora imply that acoustic attributes (e.g., speaker and noise) are much more important to current speech enhancement models than semantic attributes (e.g., language and text), offering new insights for future research.""}",oai:arXiv.org:2412.14890v1,False,"[{'term': 'eess.AS', 'scheme': None, 'label': None}, {'term': 'cs.SD', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",cross,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Leying Zhang, Wangyou Zhang, Chenda Li, Yanmin Qian'}]","Leying Zhang, Wangyou Zhang, Chenda Li, Yanmin Qian","{'name': 'Leying Zhang, Wangyou Zhang, Chenda Li, Yanmin Qian'}",,
472,From Point to probabilistic gradient boosting for claim frequency and severity prediction,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'From Point to probabilistic gradient boosting for claim frequency and severity prediction'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14916'}]",https://arxiv.org/abs/2412.14916,"arXiv:2412.14916v1 Announce Type: cross 
Abstract: Gradient boosting for decision tree algorithms are increasingly used in actuarial applications as they show superior predictive performance over traditional generalized linear models. Many improvements and sophistications to the first gradient boosting machine algorithm exist. We present in a unified notation, and contrast, all the existing point and probabilistic gradient boosting for decision tree algorithms: GBM, XGBoost, DART, LightGBM, CatBoost, EGBM, PGBM, XGBoostLSS, cyclic GBM, and NGBoost. In this comprehensive numerical study, we compare their performance on five publicly available datasets for claim frequency and severity, of various size and comprising different number of (high cardinality) categorical variables. We explain how varying exposure-to-risk can be handled with boosting in frequency models. We compare the algorithms on the basis of computational efficiency, predictive performance, and model adequacy. LightGBM and XGBoostLSS win in terms of computational efficiency. The fully interpretable EGBM achieves competitive predictive performance compared to the black box algorithms considered. We find that there is no trade-off between model adequacy and predictive accuracy: both are achievable simultaneously.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14916v1 Announce Type: cross \nAbstract: Gradient boosting for decision tree algorithms are increasingly used in actuarial applications as they show superior predictive performance over traditional generalized linear models. Many improvements and sophistications to the first gradient boosting machine algorithm exist. We present in a unified notation, and contrast, all the existing point and probabilistic gradient boosting for decision tree algorithms: GBM, XGBoost, DART, LightGBM, CatBoost, EGBM, PGBM, XGBoostLSS, cyclic GBM, and NGBoost. In this comprehensive numerical study, we compare their performance on five publicly available datasets for claim frequency and severity, of various size and comprising different number of (high cardinality) categorical variables. We explain how varying exposure-to-risk can be handled with boosting in frequency models. We compare the algorithms on the basis of computational efficiency, predictive performance, and model adequacy. LightGBM and XGBoostLSS win in terms of computational efficiency. The fully interpretable EGBM achieves competitive predictive performance compared to the black box algorithms considered. We find that there is no trade-off between model adequacy and predictive accuracy: both are achievable simultaneously.'}",oai:arXiv.org:2412.14916v1,False,"[{'term': 'stat.ML', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",cross,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': ""Dominik Chevalier, Marie-Pier C\\^ot\\'e""}]","Dominik Chevalier, Marie-Pier C\^ot\'e","{'name': ""Dominik Chevalier, Marie-Pier C\\^ot\\'e""}",,
473,An Overview on Over-the-airElectromagnetic Signal Processing,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'An Overview on Over-the-airElectromagnetic Signal Processing'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14968'}]",https://arxiv.org/abs/2412.14968,"arXiv:2412.14968v1 Announce Type: cross 
Abstract: This article provides a tutorial on over-the-air electromagnetic signal processing (ESP) for next-generation wireless networks, addressing the limitations of digital processing to enhance the efficiency and sustainability of future 6th Generation (6G) systems. It explores the integration of electromagnetism and signal processing (SP) highlighting how their convergence can drive innovations for 6G technologies. Key topics include electromagnetic (EM) wave-based processing, the application of metamaterials and advanced antennas to optimize EM field manipulation with a reduced number of radiofrequency chains, and their applications in holographic multiple-input multiple-output systems. By showcasing enabling technologies and use cases, the article demonstrates how wave-based processing can minimize energy consumption, complexity, and latency, offering an effective framework for more sustainable and efficient wireless systems. This article aims to assist researchers and professionals in integrating advanced EM technologies with conventional SP methods.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14968v1 Announce Type: cross \nAbstract: This article provides a tutorial on over-the-air electromagnetic signal processing (ESP) for next-generation wireless networks, addressing the limitations of digital processing to enhance the efficiency and sustainability of future 6th Generation (6G) systems. It explores the integration of electromagnetism and signal processing (SP) highlighting how their convergence can drive innovations for 6G technologies. Key topics include electromagnetic (EM) wave-based processing, the application of metamaterials and advanced antennas to optimize EM field manipulation with a reduced number of radiofrequency chains, and their applications in holographic multiple-input multiple-output systems. By showcasing enabling technologies and use cases, the article demonstrates how wave-based processing can minimize energy consumption, complexity, and latency, offering an effective framework for more sustainable and efficient wireless systems. This article aims to assist researchers and professionals in integrating advanced EM technologies with conventional SP methods.'}",oai:arXiv.org:2412.14968v1,False,"[{'term': 'eess.SP', 'scheme': None, 'label': None}, {'term': 'cs.IT', 'scheme': None, 'label': None}, {'term': 'math.IT', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",cross,http://creativecommons.org/licenses/by-nc-nd/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-nd/4.0/'}","[{'name': 'Davide Dardari, Giulia Torcolacci, Gianni Pasolini, Nicolo Decarli'}]","Davide Dardari, Giulia Torcolacci, Gianni Pasolini, Nicolo Decarli","{'name': 'Davide Dardari, Giulia Torcolacci, Gianni Pasolini, Nicolo Decarli'}",,
474,Noise Analysis and Modeling of the PMD Flexx2 Depth Camera for Robotic Applications,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Noise Analysis and Modeling of the PMD Flexx2 Depth Camera for Robotic Applications'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.15040'}]",https://arxiv.org/abs/2412.15040,"arXiv:2412.15040v1 Announce Type: cross 
Abstract: Time of Flight ToF cameras renowned for their ability to capture realtime 3D information have become indispensable for agile mobile robotics These cameras utilize light signals to accurately measure distances enabling robots to navigate complex environments with precision Innovative depth cameras characterized by their compact size and lightweight design such as the recently released PMD Flexx2 are particularly suited for mobile robots Capable of achieving high frame rates while capturing depth information this innovative sensor is suitable for tasks such as robot navigation and terrain mapping Operating on the ToF measurement principle the sensor offers multiple benefits over classic stereobased depth cameras However the depth images produced by the camera are subject to noise from multiple sources complicating their simulation This paper proposes an accurate quantification and modeling of the nonsystematic noise of the PMD Flexx2 We propose models for both axial and lateral noise across various camera modes assuming Gaussian distributions Axial noise modeled as a function of distance and incidence angle demonstrated a low average KullbackLeibler KL divergence of 0015 nats reflecting precise noise characterization Lateral noise deviating from a Gaussian distribution was modeled conservatively yielding a satisfactory KL divergence of 0868 nats These results validate our noise models crucial for accurately simulating sensor behavior in virtual environments and reducing the simtoreal gap in learningbased control approaches","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.15040v1 Announce Type: cross \nAbstract: Time of Flight ToF cameras renowned for their ability to capture realtime 3D information have become indispensable for agile mobile robotics These cameras utilize light signals to accurately measure distances enabling robots to navigate complex environments with precision Innovative depth cameras characterized by their compact size and lightweight design such as the recently released PMD Flexx2 are particularly suited for mobile robots Capable of achieving high frame rates while capturing depth information this innovative sensor is suitable for tasks such as robot navigation and terrain mapping Operating on the ToF measurement principle the sensor offers multiple benefits over classic stereobased depth cameras However the depth images produced by the camera are subject to noise from multiple sources complicating their simulation This paper proposes an accurate quantification and modeling of the nonsystematic noise of the PMD Flexx2 We propose models for both axial and lateral noise across various camera modes assuming Gaussian distributions Axial noise modeled as a function of distance and incidence angle demonstrated a low average KullbackLeibler KL divergence of 0015 nats reflecting precise noise characterization Lateral noise deviating from a Gaussian distribution was modeled conservatively yielding a satisfactory KL divergence of 0868 nats These results validate our noise models crucial for accurately simulating sensor behavior in virtual environments and reducing the simtoreal gap in learningbased control approaches'}",oai:arXiv.org:2412.15040v1,False,"[{'term': 'eess.IV', 'scheme': None, 'label': None}, {'term': 'cs.RO', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",cross,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Yuke Cai, Davide Plozza, Steven Marty, Paul Joseph, Michele Magno'}]","Yuke Cai, Davide Plozza, Steven Marty, Paul Joseph, Michele Magno","{'name': 'Yuke Cai, Davide Plozza, Steven Marty, Paul Joseph, Michele Magno'}",10.1109/COINS61597.2024.10622644,"IEEE International Conference on Omni-layer Intelligent Systems (COINS), 2024, pp. 422-427"
475,Tests for model misspecification in simulation-based inference: from local distortions to global model checks,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Tests for model misspecification in simulation-based inference: from local distortions to global model checks'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.15100'}]",https://arxiv.org/abs/2412.15100,"arXiv:2412.15100v1 Announce Type: cross 
Abstract: Model misspecification analysis strategies, such as anomaly detection, model validation, and model comparison are a key component of scientific model development. Over the last few years, there has been a rapid rise in the use of simulation-based inference (SBI) techniques for Bayesian parameter estimation, applied to increasingly complex forward models. To move towards fully simulation-based analysis pipelines, however, there is an urgent need for a comprehensive simulation-based framework for model misspecification analysis. In this work, we provide a solid and flexible foundation for a wide range of model discrepancy analysis tasks, using distortion-driven model misspecification tests. From a theoretical perspective, we introduce the statistical framework built around performing many hypothesis tests for distortions of the simulation model. We also make explicit analytic connections to classical techniques: anomaly detection, model validation, and goodness-of-fit residual analysis. Furthermore, we introduce an efficient self-calibrating training algorithm that is useful for practitioners. We demonstrate the performance of the framework in multiple scenarios, making the connection to classical results where they are valid. Finally, we show how to conduct such a distortion-driven model misspecification test for real gravitational wave data, specifically on the event GW150914.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.15100v1 Announce Type: cross \nAbstract: Model misspecification analysis strategies, such as anomaly detection, model validation, and model comparison are a key component of scientific model development. Over the last few years, there has been a rapid rise in the use of simulation-based inference (SBI) techniques for Bayesian parameter estimation, applied to increasingly complex forward models. To move towards fully simulation-based analysis pipelines, however, there is an urgent need for a comprehensive simulation-based framework for model misspecification analysis. In this work, we provide a solid and flexible foundation for a wide range of model discrepancy analysis tasks, using distortion-driven model misspecification tests. From a theoretical perspective, we introduce the statistical framework built around performing many hypothesis tests for distortions of the simulation model. We also make explicit analytic connections to classical techniques: anomaly detection, model validation, and goodness-of-fit residual analysis. Furthermore, we introduce an efficient self-calibrating training algorithm that is useful for practitioners. We demonstrate the performance of the framework in multiple scenarios, making the connection to classical results where they are valid. Finally, we show how to conduct such a distortion-driven model misspecification test for real gravitational wave data, specifically on the event GW150914.'}",oai:arXiv.org:2412.15100v1,False,"[{'term': 'astro-ph.IM', 'scheme': None, 'label': None}, {'term': 'astro-ph.CO', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'gr-qc', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",cross,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Noemi Anau Montel, James Alvey, Christoph Weniger'}]","Noemi Anau Montel, James Alvey, Christoph Weniger","{'name': 'Noemi Anau Montel, James Alvey, Christoph Weniger'}",,
476,Exploiting sparse structures and synergy designs to advance situational awareness of electrical power grid,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Exploiting sparse structures and synergy designs to advance situational awareness of electrical power grid'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.15105'}]",https://arxiv.org/abs/2412.15105,"arXiv:2412.15105v1 Announce Type: cross 
Abstract: The growing threats of uncertainties, anomalies, and cyberattacks on power grids are driving a critical need to advance situational awareness which allows system operators to form a complete and accurate picture of the present and future state. Simulation and estimation are foundational tools in this process. However, existing tools lack the robustness and efficiency required to achieve the level of situational awareness needed for the ever-evolving threat landscape. Industry-standard (steady-state) simulators are not robust to blackouts, often leading to non-converging or non-actionable results. Estimation tools lack robustness to anomalous data, returning erroneous system states. Efficiency is the other major concern as nonlinearities and scalability issues make large systems slow to converge.
  This thesis addresses robustness and efficiency gaps through a dual-fold contribution. We first address the inherent limitations in the existing physics-based and data-driven worlds; and then transcend the boundaries of conventional algorithmic design in the direction of a new paradigm -- Physics-ML Synergy -- which integrates the strengths of the two worlds. Our approaches are built on circuit formulation which provides a unified framework that applies to both transmission and distribution. Sparse optimization acts as the key enabler to make these tools intrinsically robust and immune to random threats, pinpointing dominant sources of (random) blackouts and data errors. Further, we explore sparsity-exploiting optimizations to develop lightweight ML models whose prediction and detection capabilities are a complement to physics-based tools; and whose lightweight designs advance generalization and scalability. Finally, Physics-ML Synergy brings robustness and efficiency further against targeted cyberthreats, by interconnecting our physics-based tools with lightweight ML.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.15105v1 Announce Type: cross \nAbstract: The growing threats of uncertainties, anomalies, and cyberattacks on power grids are driving a critical need to advance situational awareness which allows system operators to form a complete and accurate picture of the present and future state. Simulation and estimation are foundational tools in this process. However, existing tools lack the robustness and efficiency required to achieve the level of situational awareness needed for the ever-evolving threat landscape. Industry-standard (steady-state) simulators are not robust to blackouts, often leading to non-converging or non-actionable results. Estimation tools lack robustness to anomalous data, returning erroneous system states. Efficiency is the other major concern as nonlinearities and scalability issues make large systems slow to converge.\n  This thesis addresses robustness and efficiency gaps through a dual-fold contribution. We first address the inherent limitations in the existing physics-based and data-driven worlds; and then transcend the boundaries of conventional algorithmic design in the direction of a new paradigm -- Physics-ML Synergy -- which integrates the strengths of the two worlds. Our approaches are built on circuit formulation which provides a unified framework that applies to both transmission and distribution. Sparse optimization acts as the key enabler to make these tools intrinsically robust and immune to random threats, pinpointing dominant sources of (random) blackouts and data errors. Further, we explore sparsity-exploiting optimizations to develop lightweight ML models whose prediction and detection capabilities are a complement to physics-based tools; and whose lightweight designs advance generalization and scalability. Finally, Physics-ML Synergy brings robustness and efficiency further against targeted cyberthreats, by interconnecting our physics-based tools with lightweight ML.'}",oai:arXiv.org:2412.15105v1,False,"[{'term': 'eess.SP', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",cross,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}",[{'name': 'Shimiao Li'}],Shimiao Li,{'name': 'Shimiao Li'},,
477,MAPFAST: A Deep Algorithm Selector for Multi Agent Path Finding using Shortest Path Embeddings,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'MAPFAST: A Deep Algorithm Selector for Multi Agent Path Finding using Shortest Path Embeddings'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2102.12461'}]",https://arxiv.org/abs/2102.12461,"arXiv:2102.12461v2 Announce Type: replace 
Abstract: Solving the Multi-Agent Path Finding (MAPF) problem optimally is known to be NP-Hard for both make-span and total arrival time minimization. While many algorithms have been developed to solve MAPF problems, there is no dominating optimal MAPF algorithm that works well in all types of problems and no standard guidelines for when to use which algorithm. In this work, we develop the deep convolutional network MAPFAST (Multi-Agent Path Finding Algorithm SelecTor), which takes a MAPF problem instance and attempts to select the fastest algorithm to use from a portfolio of algorithms. We improve the performance of our model by including single-agent shortest paths in the instance embedding given to our model and by utilizing supplemental loss functions in addition to a classification loss. We evaluate our model on a large and diverse dataset of MAPF instances, showing that it outperforms all individual algorithms in its portfolio as well as the state-of-the-art optimal MAPF algorithm selector. We also provide an analysis of algorithm behavior in our dataset to gain a deeper understanding of optimal MAPF algorithms' strengths and weaknesses to help other researchers leverage different heuristics in algorithm designs.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2102.12461v2 Announce Type: replace \nAbstract: Solving the Multi-Agent Path Finding (MAPF) problem optimally is known to be NP-Hard for both make-span and total arrival time minimization. While many algorithms have been developed to solve MAPF problems, there is no dominating optimal MAPF algorithm that works well in all types of problems and no standard guidelines for when to use which algorithm. In this work, we develop the deep convolutional network MAPFAST (Multi-Agent Path Finding Algorithm SelecTor), which takes a MAPF problem instance and attempts to select the fastest algorithm to use from a portfolio of algorithms. We improve the performance of our model by including single-agent shortest paths in the instance embedding given to our model and by utilizing supplemental loss functions in addition to a classification loss. We evaluate our model on a large and diverse dataset of MAPF instances, showing that it outperforms all individual algorithms in its portfolio as well as the state-of-the-art optimal MAPF algorithm selector. We also provide an analysis of algorithm behavior in our dataset to gain a deeper understanding of optimal MAPF algorithms' strengths and weaknesses to help other researchers leverage different heuristics in algorithm designs.""}",oai:arXiv.org:2102.12461v2,False,"[{'term': 'cs.MA', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by-nc-nd/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-nd/4.0/'}","[{'name': 'Jingyao Ren, Vikraman Sathiyanarayanan, Eric Ewing, Baskin Senbaslar, Nora Ayanian'}]","Jingyao Ren, Vikraman Sathiyanarayanan, Eric Ewing, Baskin Senbaslar, Nora Ayanian","{'name': 'Jingyao Ren, Vikraman Sathiyanarayanan, Eric Ewing, Baskin Senbaslar, Nora Ayanian'}",,"AAMAS-2021, 1055-1062"
478,Alpha-NML Universal Predictors,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Alpha-NML Universal Predictors'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2202.12737'}]",https://arxiv.org/abs/2202.12737,"arXiv:2202.12737v4 Announce Type: replace 
Abstract: Inspired by the connection between classical regret measures employed in universal prediction and R\'{e}nyi divergence, we introduce a new class of universal predictors that depend on a real parameter $\alpha\geq 1$. This class interpolates two well-known predictors, the mixture estimators, that include the Laplace and the Krichevsky-Trofimov predictors, and the Normalized Maximum Likelihood (NML) estimator. We point out some advantages of this new class of predictors and study its benefits from two complementary viewpoints: (1) we prove its optimality when the maximal R\'{e}nyi divergence is considered as a regret measure, which can be interpreted operationally as a middle ground between the standard average and worst-case regret measures; (2) we discuss how it can be employed when NML is not a viable option, as an alternative to other predictors such as Luckiness NML. Finally, we apply the $\alpha$-NML predictor to the class of discrete memoryless sources (DMS), where we derive simple formulas to compute the predictor and analyze its asymptotic performance in terms of worst-case regret.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2202.12737v4 Announce Type: replace \nAbstract: Inspired by the connection between classical regret measures employed in universal prediction and R\\'{e}nyi divergence, we introduce a new class of universal predictors that depend on a real parameter $\\alpha\\geq 1$. This class interpolates two well-known predictors, the mixture estimators, that include the Laplace and the Krichevsky-Trofimov predictors, and the Normalized Maximum Likelihood (NML) estimator. We point out some advantages of this new class of predictors and study its benefits from two complementary viewpoints: (1) we prove its optimality when the maximal R\\'{e}nyi divergence is considered as a regret measure, which can be interpreted operationally as a middle ground between the standard average and worst-case regret measures; (2) we discuss how it can be employed when NML is not a viable option, as an alternative to other predictors such as Luckiness NML. Finally, we apply the $\\alpha$-NML predictor to the class of discrete memoryless sources (DMS), where we derive simple formulas to compute the predictor and analyze its asymptotic performance in terms of worst-case regret.""}",oai:arXiv.org:2202.12737v4,False,"[{'term': 'cs.IT', 'scheme': None, 'label': None}, {'term': 'math.IT', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Marco Bondaschi, Michael Gastpar'}]","Marco Bondaschi, Michael Gastpar","{'name': 'Marco Bondaschi, Michael Gastpar'}",,
479,Baxos: Backing off for Robust and Efficient Consensus,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Baxos: Backing off for Robust and Efficient Consensus'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2204.10934'}]",https://arxiv.org/abs/2204.10934,"arXiv:2204.10934v2 Announce Type: replace 
Abstract: Leader-based consensus algorithms are vulnerable to liveness and performance downgrade attacks. We explore the possibility of replacing leader election in Multi-Paxos with random exponential backoff (REB), a simpler approach that requires minimum modifications to the two phase Synod Paxos and achieves better resiliency under attacks. We propose Baxos, a new resilient consensus protocol that leverages a random exponential backoff scheme as a replacement for leader election in consensus algorithms. Our backoff scheme addresses the common challenges of random exponential backoff such as scalability and robustness to changing wide area latency. We extensively evaluate Baxos to illustrate its performance and robustness against two liveness and performance downgrade attacks using an implementation running on Amazon EC2 in a wide area network and a combination of a micro benchmark and YCSB-A workload on Redis. Our results show that Baxos offers more robustness to liveness and performance downgrade attacks than leader-based consensus protocols. Baxos outperforms Multi-Paxos and Raft up to 128% in throughput under liveness and performance downgrade attacks under worst case contention scenarios where each replica proposes requests concurrently.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2204.10934v2 Announce Type: replace \nAbstract: Leader-based consensus algorithms are vulnerable to liveness and performance downgrade attacks. We explore the possibility of replacing leader election in Multi-Paxos with random exponential backoff (REB), a simpler approach that requires minimum modifications to the two phase Synod Paxos and achieves better resiliency under attacks. We propose Baxos, a new resilient consensus protocol that leverages a random exponential backoff scheme as a replacement for leader election in consensus algorithms. Our backoff scheme addresses the common challenges of random exponential backoff such as scalability and robustness to changing wide area latency. We extensively evaluate Baxos to illustrate its performance and robustness against two liveness and performance downgrade attacks using an implementation running on Amazon EC2 in a wide area network and a combination of a micro benchmark and YCSB-A workload on Redis. Our results show that Baxos offers more robustness to liveness and performance downgrade attacks than leader-based consensus protocols. Baxos outperforms Multi-Paxos and Raft up to 128% in throughput under liveness and performance downgrade attacks under worst case contention scenarios where each replica proposes requests concurrently.'}",oai:arXiv.org:2204.10934v2,False,"[{'term': 'cs.DC', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Pasindu Tennage, Cristina Basescu, Eleftherios Kokoris Kogias, Ewa Syta, Philipp Jovanovic, Bryan Ford'}]","Pasindu Tennage, Cristina Basescu, Eleftherios Kokoris Kogias, Ewa Syta, Philipp Jovanovic, Bryan Ford","{'name': 'Pasindu Tennage, Cristina Basescu, Eleftherios Kokoris Kogias, Ewa Syta, Philipp Jovanovic, Bryan Ford'}",,
480,An analysis of the Rayleigh-Ritz and refined Rayleigh-Ritz methods for regular nonlinear eigenvalue problems,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'An analysis of the Rayleigh-Ritz and refined Rayleigh-Ritz methods for regular nonlinear eigenvalue problems'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2212.00302'}]",https://arxiv.org/abs/2212.00302,"arXiv:2212.00302v3 Announce Type: replace 
Abstract: We establish a general convergence theory of the Rayleigh--Ritz method and the refined Rayleigh--Ritz method for computing some simple eigenpair $(\lambda_{*},x_{*})$ of a given analytic regular nonlinear eigenvalue problem (NEP). In terms of the deviation $\varepsilon$ of $x_{*}$ from a given subspace $\mathcal{W}$, we establish a priori convergence results on the Ritz value, the Ritz vector and the refined Ritz vector. The results show that, as $\varepsilon\rightarrow 0$, there exists a Ritz value that unconditionally converges to $\lambda_*$ and the corresponding refined Ritz vector does so too but the Ritz vector converges conditionally and it may fail to converge and even may not be unique. We also present an error bound for the approximate eigenvector in terms of the computable residual norm of a given approximate eigenpair, and give lower and upper bounds for the error of the refined Ritz vector and the Ritz vector as well as for that of the corresponding residual norms. These results nontrivially extend some convergence results on these two methods for the linear eigenvalue problem to the NEP. Examples are constructed to illustrate the main results.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2212.00302v3 Announce Type: replace \nAbstract: We establish a general convergence theory of the Rayleigh--Ritz method and the refined Rayleigh--Ritz method for computing some simple eigenpair $(\\lambda_{*},x_{*})$ of a given analytic regular nonlinear eigenvalue problem (NEP). In terms of the deviation $\\varepsilon$ of $x_{*}$ from a given subspace $\\mathcal{W}$, we establish a priori convergence results on the Ritz value, the Ritz vector and the refined Ritz vector. The results show that, as $\\varepsilon\\rightarrow 0$, there exists a Ritz value that unconditionally converges to $\\lambda_*$ and the corresponding refined Ritz vector does so too but the Ritz vector converges conditionally and it may fail to converge and even may not be unique. We also present an error bound for the approximate eigenvector in terms of the computable residual norm of a given approximate eigenpair, and give lower and upper bounds for the error of the refined Ritz vector and the Ritz vector as well as for that of the corresponding residual norms. These results nontrivially extend some convergence results on these two methods for the linear eigenvalue problem to the NEP. Examples are constructed to illustrate the main results.'}",oai:arXiv.org:2212.00302v3,False,"[{'term': 'math.NA', 'scheme': None, 'label': None}, {'term': 'cs.NA', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Zhongxiao Jia, Qingqing Zheng'}]","Zhongxiao Jia, Qingqing Zheng","{'name': 'Zhongxiao Jia, Qingqing Zheng'}",,
481,Metric Compatible Training for Online Backfilling in Large-Scale Retrieval,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Metric Compatible Training for Online Backfilling in Large-Scale Retrieval'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2301.03767'}]",https://arxiv.org/abs/2301.03767,"arXiv:2301.03767v2 Announce Type: replace 
Abstract: Backfilling is the process of re-extracting all gallery embeddings from upgraded models in image retrieval systems. It inevitably requires a prohibitively large amount of computational cost and even entails the downtime of the service. Although backward-compatible learning sidesteps this challenge by tackling query-side representations, this leads to suboptimal solutions in principle because gallery embeddings cannot benefit from model upgrades. We address this dilemma by introducing an online backfilling algorithm, which enables us to achieve a progressive performance improvement during the backfilling process while not sacrificing the final performance of new model after the completion of backfilling. To this end, we first propose a simple distance rank merge technique for online backfilling. Then, we incorporate a reverse transformation module for more effective and efficient merging, which is further enhanced by adopting a metric-compatible contrastive learning approach. These two components help to make the distances of old and new models compatible, resulting in desirable merge results during backfilling with no extra computational overhead. Extensive experiments show the effectiveness of our framework on four standard benchmarks in various settings.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2301.03767v2 Announce Type: replace \nAbstract: Backfilling is the process of re-extracting all gallery embeddings from upgraded models in image retrieval systems. It inevitably requires a prohibitively large amount of computational cost and even entails the downtime of the service. Although backward-compatible learning sidesteps this challenge by tackling query-side representations, this leads to suboptimal solutions in principle because gallery embeddings cannot benefit from model upgrades. We address this dilemma by introducing an online backfilling algorithm, which enables us to achieve a progressive performance improvement during the backfilling process while not sacrificing the final performance of new model after the completion of backfilling. To this end, we first propose a simple distance rank merge technique for online backfilling. Then, we incorporate a reverse transformation module for more effective and efficient merging, which is further enhanced by adopting a metric-compatible contrastive learning approach. These two components help to make the distances of old and new models compatible, resulting in desirable merge results during backfilling with no extra computational overhead. Extensive experiments show the effectiveness of our framework on four standard benchmarks in various settings.'}",oai:arXiv.org:2301.03767v2,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.IR', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Seonguk Seo, Mustafa Gokhan Uzunbas, Bohyung Han, Sara Cao, Ser-Nam Lim'}]","Seonguk Seo, Mustafa Gokhan Uzunbas, Bohyung Han, Sara Cao, Ser-Nam Lim","{'name': 'Seonguk Seo, Mustafa Gokhan Uzunbas, Bohyung Han, Sara Cao, Ser-Nam Lim'}",,
482,Selective Uncertainty Propagation in Offline RL,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Selective Uncertainty Propagation in Offline RL'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2302.00284'}]",https://arxiv.org/abs/2302.00284,"arXiv:2302.00284v3 Announce Type: replace 
Abstract: We consider the finite-horizon offline reinforcement learning (RL) setting, and are motivated by the challenge of learning the policy at any step h in dynamic programming (DP) algorithms. To learn this, it is sufficient to evaluate the treatment effect of deviating from the behavioral policy at step h after having optimized the policy for all future steps. Since the policy at any step can affect next-state distributions, the related distributional shift challenges can make this problem far more statistically hard than estimating such treatment effects in the stochastic contextual bandit setting. However, the hardness of many real-world RL instances lies between the two regimes. We develop a flexible and general method called selective uncertainty propagation for confidence interval construction that adapts to the hardness of the associated distribution shift challenges. We show benefits of our approach on toy environments and demonstrate the benefits of these techniques for offline policy learning.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2302.00284v3 Announce Type: replace \nAbstract: We consider the finite-horizon offline reinforcement learning (RL) setting, and are motivated by the challenge of learning the policy at any step h in dynamic programming (DP) algorithms. To learn this, it is sufficient to evaluate the treatment effect of deviating from the behavioral policy at step h after having optimized the policy for all future steps. Since the policy at any step can affect next-state distributions, the related distributional shift challenges can make this problem far more statistically hard than estimating such treatment effects in the stochastic contextual bandit setting. However, the hardness of many real-world RL instances lies between the two regimes. We develop a flexible and general method called selective uncertainty propagation for confidence interval construction that adapts to the hardness of the associated distribution shift challenges. We show benefits of our approach on toy environments and demonstrate the benefits of these techniques for offline policy learning.'}",oai:arXiv.org:2302.00284v3,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by-nc-nd/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-nd/4.0/'}","[{'name': 'Sanath Kumar Krishnamurthy, Tanmay Gangwani, Sumeet Katariya, Branislav Kveton, Shrey Modi, Anshuka Rangi'}]","Sanath Kumar Krishnamurthy, Tanmay Gangwani, Sumeet Katariya, Branislav Kveton, Shrey Modi, Anshuka Rangi","{'name': 'Sanath Kumar Krishnamurthy, Tanmay Gangwani, Sumeet Katariya, Branislav Kveton, Shrey Modi, Anshuka Rangi'}",,
483,New Constructions of Optimal Binary LCD Codes,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'New Constructions of Optimal Binary LCD Codes'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2302.00906'}]",https://arxiv.org/abs/2302.00906,"arXiv:2302.00906v2 Announce Type: replace 
Abstract: Linear complementary dual (LCD) codes can provide an optimum linear coding solution for the two-user binary adder channel. LCD codes also can be used to against side-channel attacks and fault non-invasive attacks. Let $d_{LCD}(n, k)$ denote the maximum value of $d$ for which a binary $[n,k, d]$ LCD code exists. In \cite{BS21}, Bouyuklieva conjectured that $d_{LCD}(n+1, k)=d_{LCD}(n, k)$ or $d_{LCD}(n, k) + 1$ for any lenth $n$ and dimension $k \ge 2$. In this paper, we first prove Bouyuklieva's conjecture \cite{BS21} by constructing a binary $[n,k,d-1]$ LCD codes from a binary $[n+1,k,d]$ $LCD_{o,e}$ code, when $d \ge 3$ and $k \ge 2$. Then we provide a distance lower bound for binary LCD codes by expanded codes, and use this bound and some methods such as puncturing, shortening, expanding and extension, we construct some new binary LCD codes. Finally, we improve some previously known values of $d_{LCD}(n, k)$ of lengths $38 \le n \le 40$ and dimensions $9 \le k \le 15$. We also obtain some values of $d_{LCD}(n, k)$ with $41 \le n \le 50$ and $6 \le k \le n-6$.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2302.00906v2 Announce Type: replace \nAbstract: Linear complementary dual (LCD) codes can provide an optimum linear coding solution for the two-user binary adder channel. LCD codes also can be used to against side-channel attacks and fault non-invasive attacks. Let $d_{LCD}(n, k)$ denote the maximum value of $d$ for which a binary $[n,k, d]$ LCD code exists. In \\cite{BS21}, Bouyuklieva conjectured that $d_{LCD}(n+1, k)=d_{LCD}(n, k)$ or $d_{LCD}(n, k) + 1$ for any lenth $n$ and dimension $k \\ge 2$. In this paper, we first prove Bouyuklieva's conjecture \\cite{BS21} by constructing a binary $[n,k,d-1]$ LCD codes from a binary $[n+1,k,d]$ $LCD_{o,e}$ code, when $d \\ge 3$ and $k \\ge 2$. Then we provide a distance lower bound for binary LCD codes by expanded codes, and use this bound and some methods such as puncturing, shortening, expanding and extension, we construct some new binary LCD codes. Finally, we improve some previously known values of $d_{LCD}(n, k)$ of lengths $38 \\le n \\le 40$ and dimensions $9 \\le k \\le 15$. We also obtain some values of $d_{LCD}(n, k)$ with $41 \\le n \\le 50$ and $6 \\le k \\le n-6$.""}",oai:arXiv.org:2302.00906v2,False,"[{'term': 'cs.IT', 'scheme': None, 'label': None}, {'term': 'math.IT', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Guodong Wang, Shengwei Liu, Hongwei Liu'}]","Guodong Wang, Shengwei Liu, Hongwei Liu","{'name': 'Guodong Wang, Shengwei Liu, Hongwei Liu'}",,
484,Learning Discretized Neural Networks under Ricci Flow,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Learning Discretized Neural Networks under Ricci Flow'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2302.03390'}]",https://arxiv.org/abs/2302.03390,"arXiv:2302.03390v5 Announce Type: replace 
Abstract: In this paper, we study Discretized Neural Networks (DNNs) composed of low-precision weights and activations, which suffer from either infinite or zero gradients due to the non-differentiable discrete function during training. Most training-based DNNs in such scenarios employ the standard Straight-Through Estimator (STE) to approximate the gradient w.r.t. discrete values. However, the use of STE introduces the problem of gradient mismatch, arising from perturbations in the approximated gradient. To address this problem, this paper reveals that this mismatch can be interpreted as a metric perturbation in a Riemannian manifold, viewed through the lens of duality theory. Building on information geometry, we construct the Linearly Nearly Euclidean (LNE) manifold for DNNs, providing a background for addressing perturbations. By introducing a partial differential equation on metrics, i.e., the Ricci flow, we establish the dynamical stability and convergence of the LNE metric with the $L^2$-norm perturbation. In contrast to previous perturbation theories with convergence rates in fractional powers, the metric perturbation under the Ricci flow exhibits exponential decay in the LNE manifold. Experimental results across various datasets demonstrate that our method achieves superior and more stable performance for DNNs compared to other representative training-based methods.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2302.03390v5 Announce Type: replace \nAbstract: In this paper, we study Discretized Neural Networks (DNNs) composed of low-precision weights and activations, which suffer from either infinite or zero gradients due to the non-differentiable discrete function during training. Most training-based DNNs in such scenarios employ the standard Straight-Through Estimator (STE) to approximate the gradient w.r.t. discrete values. However, the use of STE introduces the problem of gradient mismatch, arising from perturbations in the approximated gradient. To address this problem, this paper reveals that this mismatch can be interpreted as a metric perturbation in a Riemannian manifold, viewed through the lens of duality theory. Building on information geometry, we construct the Linearly Nearly Euclidean (LNE) manifold for DNNs, providing a background for addressing perturbations. By introducing a partial differential equation on metrics, i.e., the Ricci flow, we establish the dynamical stability and convergence of the LNE metric with the $L^2$-norm perturbation. In contrast to previous perturbation theories with convergence rates in fractional powers, the metric perturbation under the Ricci flow exhibits exponential decay in the LNE manifold. Experimental results across various datasets demonstrate that our method achieves superior and more stable performance for DNNs compared to other representative training-based methods.'}",oai:arXiv.org:2302.03390v5,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'cs.IT', 'scheme': None, 'label': None}, {'term': 'cs.NE', 'scheme': None, 'label': None}, {'term': 'math.IT', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Jun Chen, Hanwen Chen, Mengmeng Wang, Guang Dai, Ivor W. Tsang, Yong Liu'}]","Jun Chen, Hanwen Chen, Mengmeng Wang, Guang Dai, Ivor W. Tsang, Yong Liu","{'name': 'Jun Chen, Hanwen Chen, Mengmeng Wang, Guang Dai, Ivor W. Tsang, Yong Liu'}",,"Journal of Machine Learning Research, 2024"
485,Towards Fair Machine Learning Software: Understanding and Addressing Model Bias Through Counterfactual Thinking,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Towards Fair Machine Learning Software: Understanding and Addressing Model Bias Through Counterfactual Thinking'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2302.08018'}]",https://arxiv.org/abs/2302.08018,"arXiv:2302.08018v2 Announce Type: replace 
Abstract: The increasing use of Machine Learning (ML) software can lead to unfair and unethical decisions, thus fairness bugs in software are becoming a growing concern. Addressing these fairness bugs often involves sacrificing ML performance, such as accuracy. To address this issue, we present a novel counterfactual approach that uses counterfactual thinking to tackle the root causes of bias in ML software. In addition, our approach combines models optimized for both performance and fairness, resulting in an optimal solution in both aspects. We conducted a thorough evaluation of our approach on 10 benchmark tasks using a combination of 5 performance metrics, 3 fairness metrics, and 15 measurement scenarios, all applied to 8 real-world datasets. The conducted extensive evaluations show that the proposed method significantly improves the fairness of ML software while maintaining competitive performance, outperforming state-of-the-art solutions in 84.6% of overall cases based on a recent benchmarking tool.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2302.08018v2 Announce Type: replace \nAbstract: The increasing use of Machine Learning (ML) software can lead to unfair and unethical decisions, thus fairness bugs in software are becoming a growing concern. Addressing these fairness bugs often involves sacrificing ML performance, such as accuracy. To address this issue, we present a novel counterfactual approach that uses counterfactual thinking to tackle the root causes of bias in ML software. In addition, our approach combines models optimized for both performance and fairness, resulting in an optimal solution in both aspects. We conducted a thorough evaluation of our approach on 10 benchmark tasks using a combination of 5 performance metrics, 3 fairness metrics, and 15 measurement scenarios, all applied to 8 real-world datasets. The conducted extensive evaluations show that the proposed method significantly improves the fairness of ML software while maintaining competitive performance, outperforming state-of-the-art solutions in 84.6% of overall cases based on a recent benchmarking tool.'}",oai:arXiv.org:2302.08018v2,False,"[{'term': 'cs.SE', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Zichong Wang, Yang Zhou, Israat Haque, David Lo, Wenbin Zhang'}]","Zichong Wang, Yang Zhou, Israat Haque, David Lo, Wenbin Zhang","{'name': 'Zichong Wang, Yang Zhou, Israat Haque, David Lo, Wenbin Zhang'}",,
486,On the Expressivity of Persistent Homology in Graph Learning,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'On the Expressivity of Persistent Homology in Graph Learning'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2302.09826'}]",https://arxiv.org/abs/2302.09826,"arXiv:2302.09826v4 Announce Type: replace 
Abstract: Persistent homology, a technique from computational topology, has recently shown strong empirical performance in the context of graph classification. Being able to capture long range graph properties via higher-order topological features, such as cycles of arbitrary length, in combination with multi-scale topological descriptors, has improved predictive performance for data sets with prominent topological structures, such as molecules. At the same time, the theoretical properties of persistent homology have not been formally assessed in this context. This paper intends to bridge the gap between computational topology and graph machine learning by providing a brief introduction to persistent homology in the context of graphs, as well as a theoretical discussion and empirical analysis of its expressivity for graph learning tasks.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2302.09826v4 Announce Type: replace \nAbstract: Persistent homology, a technique from computational topology, has recently shown strong empirical performance in the context of graph classification. Being able to capture long range graph properties via higher-order topological features, such as cycles of arbitrary length, in combination with multi-scale topological descriptors, has improved predictive performance for data sets with prominent topological structures, such as molecules. At the same time, the theoretical properties of persistent homology have not been formally assessed in this context. This paper intends to bridge the gap between computational topology and graph machine learning by providing a brief introduction to persistent homology in the context of graphs, as well as a theoretical discussion and empirical analysis of its expressivity for graph learning tasks.'}",oai:arXiv.org:2302.09826v4,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'math.AT', 'scheme': None, 'label': None}, {'term': 'stat.ML', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': ""Rub\\'en Ballester, Bastian Rieck""}]","Rub\'en Ballester, Bastian Rieck","{'name': ""Rub\\'en Ballester, Bastian Rieck""}",,
487,A Deep Learning-Based and Fully Automated Pipeline for Regurgitant Mitral Valve Anatomy Analysis from 3D Echocardiography,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'A Deep Learning-Based and Fully Automated Pipeline for Regurgitant Mitral Valve Anatomy Analysis from 3D Echocardiography'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2302.10634'}]",https://arxiv.org/abs/2302.10634,"arXiv:2302.10634v2 Announce Type: replace 
Abstract: 3D transesophageal echocardiography (3DTEE), is the recommended method for diagnosing mitral regurgitation (MR). 3DTEE provides a high-quality 3D image of the mitral valve (MV), allowing for precise segmentation and measurement of the regurgitant valve anatomy. However, manual TEE segmentations are time-consuming and prone to intra-operator variability, affecting the reliability of the measurements. To address this, we developed a fully automated pipeline using a 3D convolutional neural network (CNN) to segment MV substructures (annulus, anterior leaflet, and posterior leaflet) and quantify MV anatomy. The 3D CNN, based on a multi-decoder residual U-Net architecture, was trained and tested on a dataset comprising 100 3DTEE images with corresponding segmentations. Within the pipeline, a custom algorithm refines the CNN-based segmentations and extracts MV models, from which anatomical landmarks and features are quantified. The accuracy of the proposed method was assessed using Dice score and mean surface distance (MSD) against ground truth segmentations, and the extracted anatomical parameters were compared against a semiautomated commercial software TomTec Image Arena. The trained 3D CNN achieved an average Dice score of 0.79 and MSD of 0.47 mm for the combined segmentation of the annulus, anterior and posterior leaflet. The proposed CNN architecture outperformed a baseline residual U-Net architecture in MV substructure segmentation, and the refinement of the predicted annulus segmentation improved MSD by 8.36%. The annular and leaflet linear measurements differed by less than 7.94 mm and 3.67 mm, respectively, compared to the 3D measurements obtained with TomTec Image Arena. The proposed pipeline was faster than the commercial software, with a modeling time of 12.54 s and a quantification time of 54.42 s.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2302.10634v2 Announce Type: replace \nAbstract: 3D transesophageal echocardiography (3DTEE), is the recommended method for diagnosing mitral regurgitation (MR). 3DTEE provides a high-quality 3D image of the mitral valve (MV), allowing for precise segmentation and measurement of the regurgitant valve anatomy. However, manual TEE segmentations are time-consuming and prone to intra-operator variability, affecting the reliability of the measurements. To address this, we developed a fully automated pipeline using a 3D convolutional neural network (CNN) to segment MV substructures (annulus, anterior leaflet, and posterior leaflet) and quantify MV anatomy. The 3D CNN, based on a multi-decoder residual U-Net architecture, was trained and tested on a dataset comprising 100 3DTEE images with corresponding segmentations. Within the pipeline, a custom algorithm refines the CNN-based segmentations and extracts MV models, from which anatomical landmarks and features are quantified. The accuracy of the proposed method was assessed using Dice score and mean surface distance (MSD) against ground truth segmentations, and the extracted anatomical parameters were compared against a semiautomated commercial software TomTec Image Arena. The trained 3D CNN achieved an average Dice score of 0.79 and MSD of 0.47 mm for the combined segmentation of the annulus, anterior and posterior leaflet. The proposed CNN architecture outperformed a baseline residual U-Net architecture in MV substructure segmentation, and the refinement of the predicted annulus segmentation improved MSD by 8.36%. The annular and leaflet linear measurements differed by less than 7.94 mm and 3.67 mm, respectively, compared to the 3D measurements obtained with TomTec Image Arena. The proposed pipeline was faster than the commercial software, with a modeling time of 12.54 s and a quantification time of 54.42 s.'}",oai:arXiv.org:2302.10634v2,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'q-bio.QM', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Riccardo Munaf\\`o, Simone Saitta, Giacomo Ingallina, Paolo Denti, Francesco Maisano, Eustachio Agricola, Alberto Redaelli, Emiliano Votta'}]","Riccardo Munaf\`o, Simone Saitta, Giacomo Ingallina, Paolo Denti, Francesco Maisano, Eustachio Agricola, Alberto Redaelli, Emiliano Votta","{'name': 'Riccardo Munaf\\`o, Simone Saitta, Giacomo Ingallina, Paolo Denti, Francesco Maisano, Eustachio Agricola, Alberto Redaelli, Emiliano Votta'}",10.1109/ACCESS.2024.3349698,IEEE Access (Volume:12) (2024) 5295 - 5308
488,Real-Time Damage Detection in Fiber Lifting Ropes Using Lightweight Convolutional Neural Networks,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Real-Time Damage Detection in Fiber Lifting Ropes Using Lightweight Convolutional Neural Networks'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2302.11947'}]",https://arxiv.org/abs/2302.11947,"arXiv:2302.11947v2 Announce Type: replace 
Abstract: The health and safety hazards posed by worn crane lifting ropes mandate periodic inspection for damage. This task is time-consuming, prone to human error, halts operation, and may result in the premature disposal of ropes. Therefore, we propose using efficient deep learning and computer vision methods to automate the process of detecting damaged ropes. Specifically, we present a vision-based system for detecting damage in synthetic fiber rope images using lightweight convolutional neural networks. We develop a camera-based apparatus to photograph the lifting rope's surface, while in operation, and capture the progressive wear-and-tear as well as the more significant degradation in the rope's health state. Experts from Konecranes annotate the collected images in accordance with the rope's condition; normal or damaged. Then, we pre-process the images, systematically design a deep learning model, evaluate its detection and prediction performance, analyze its computational complexity, and compare it with various other models. Experimental results show the proposed model outperforms other similar techniques with 96.5% accuracy, 94.8% precision, 98.3% recall, 96.5% F1-score, and 99.3% AUC. Besides, they demonstrate the model's real-time operation, low memory footprint, robustness to various environmental and operational conditions, and adequacy for deployment in industrial applications such as lifting, mooring, towing, climbing, and sailing.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2302.11947v2 Announce Type: replace \nAbstract: The health and safety hazards posed by worn crane lifting ropes mandate periodic inspection for damage. This task is time-consuming, prone to human error, halts operation, and may result in the premature disposal of ropes. Therefore, we propose using efficient deep learning and computer vision methods to automate the process of detecting damaged ropes. Specifically, we present a vision-based system for detecting damage in synthetic fiber rope images using lightweight convolutional neural networks. We develop a camera-based apparatus to photograph the lifting rope's surface, while in operation, and capture the progressive wear-and-tear as well as the more significant degradation in the rope's health state. Experts from Konecranes annotate the collected images in accordance with the rope's condition; normal or damaged. Then, we pre-process the images, systematically design a deep learning model, evaluate its detection and prediction performance, analyze its computational complexity, and compare it with various other models. Experimental results show the proposed model outperforms other similar techniques with 96.5% accuracy, 94.8% precision, 98.3% recall, 96.5% F1-score, and 99.3% AUC. Besides, they demonstrate the model's real-time operation, low memory footprint, robustness to various environmental and operational conditions, and adequacy for deployment in industrial applications such as lifting, mooring, towing, climbing, and sailing.""}",oai:arXiv.org:2302.11947v2,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': ""Tuomas Jalonen, Mohammad Al-Sa'd, Roope Mellanen, Serkan Kiranyaz, Moncef Gabbouj""}]","Tuomas Jalonen, Mohammad Al-Sa'd, Roope Mellanen, Serkan Kiranyaz, Moncef Gabbouj","{'name': ""Tuomas Jalonen, Mohammad Al-Sa'd, Roope Mellanen, Serkan Kiranyaz, Moncef Gabbouj""}",,
489,"Parallel, Distributed, and Quantum Exact Single-Source Shortest Paths with Negative Edge Weights","{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Parallel, Distributed, and Quantum Exact Single-Source Shortest Paths with Negative Edge Weights'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2303.00811'}]",https://arxiv.org/abs/2303.00811,"arXiv:2303.00811v2 Announce Type: replace 
Abstract: This paper presents parallel, distributed and quantum algorithms for single-source shortest paths when edges can have negative weights (negative-weight SSSP). We show a framework that reduces negative-weight SSSP in all these setting to $n^{o(1)}$ calls to any SSSP algorithm that works with a virtual source. More specifically, for a graph with $m$ edges, $n$ vertices, undirected hop-diameter $D$, and polynomially bounded integer edge weights, we show randomized algorithms for negative-weight SSSP with (i) $W_{SSSP}(m,n)n^{o(1)}$ work and $S_{SSSP}(m,n)n^{o(1)}$ span, given access to an SSSP algorithm with $W_{SSSP}(m,n)$ work and $S_{SSSP}(m,n)$ span in the parallel model, (ii) $T_{SSSP}(n,D)n^{o(1)}$, given access to an SSSP algorithm that takes $T_{SSSP}(n,D)$ rounds in $\mathsf{CONGEST}$, (iii) $Q_{SSSP}(m,n)n^{o(1)}$ quantum edge queries, given access to a non-negative-weight SSSP algorithm that takes $Q_{SSSP}(m,n)$ queries in the quantum edge query model. This work builds off the recent result of [Bernstein, Nanongkai, Wulff-Nilsen, FOCS'22], which gives a near-linear time algorithm for negative-weight SSSP in the sequential setting.
  Using current state-of-the-art SSSP algorithms yields randomized algorithms for negative-weight SSSP with (i) $m^{1+o(1)}$ work and $n^{1/2+o(1)}$ span in the parallel model, (ii) $(n^{2/5}D^{2/5} + \sqrt{n} + D)n^{o(1)}$ rounds in $\mathsf{CONGEST}$, (iii) $m^{1/2}n^{1/2+o(1)}$ quantum queries to the adjacency list or $n^{1.5+o(1)}$ quantum queries to the adjacency matrix.
  Our main technical contribution is an efficient reduction for computing a low-diameter decomposition (LDD) of directed graphs to computations of SSSP with a virtual source. Efficiently computing an LDD has heretofore only been known for undirected graphs in both the parallel and distributed models.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2303.00811v2 Announce Type: replace \nAbstract: This paper presents parallel, distributed and quantum algorithms for single-source shortest paths when edges can have negative weights (negative-weight SSSP). We show a framework that reduces negative-weight SSSP in all these setting to $n^{o(1)}$ calls to any SSSP algorithm that works with a virtual source. More specifically, for a graph with $m$ edges, $n$ vertices, undirected hop-diameter $D$, and polynomially bounded integer edge weights, we show randomized algorithms for negative-weight SSSP with (i) $W_{SSSP}(m,n)n^{o(1)}$ work and $S_{SSSP}(m,n)n^{o(1)}$ span, given access to an SSSP algorithm with $W_{SSSP}(m,n)$ work and $S_{SSSP}(m,n)$ span in the parallel model, (ii) $T_{SSSP}(n,D)n^{o(1)}$, given access to an SSSP algorithm that takes $T_{SSSP}(n,D)$ rounds in $\\mathsf{CONGEST}$, (iii) $Q_{SSSP}(m,n)n^{o(1)}$ quantum edge queries, given access to a non-negative-weight SSSP algorithm that takes $Q_{SSSP}(m,n)$ queries in the quantum edge query model. This work builds off the recent result of [Bernstein, Nanongkai, Wulff-Nilsen, FOCS'22], which gives a near-linear time algorithm for negative-weight SSSP in the sequential setting.\n  Using current state-of-the-art SSSP algorithms yields randomized algorithms for negative-weight SSSP with (i) $m^{1+o(1)}$ work and $n^{1/2+o(1)}$ span in the parallel model, (ii) $(n^{2/5}D^{2/5} + \\sqrt{n} + D)n^{o(1)}$ rounds in $\\mathsf{CONGEST}$, (iii) $m^{1/2}n^{1/2+o(1)}$ quantum queries to the adjacency list or $n^{1.5+o(1)}$ quantum queries to the adjacency matrix.\n  Our main technical contribution is an efficient reduction for computing a low-diameter decomposition (LDD) of directed graphs to computations of SSSP with a virtual source. Efficiently computing an LDD has heretofore only been known for undirected graphs in both the parallel and distributed models.""}",oai:arXiv.org:2303.00811v2,False,"[{'term': 'cs.DC', 'scheme': None, 'label': None}, {'term': 'cs.DS', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Vikrant Ashvinkumar, Aaron Bernstein, Nairen Cao, Christoph Grunau, Bernhard Haeupler, Yonggang Jiang, Danupon Nanongkai, Hsin Hao Su'}]","Vikrant Ashvinkumar, Aaron Bernstein, Nairen Cao, Christoph Grunau, Bernhard Haeupler, Yonggang Jiang, Danupon Nanongkai, Hsin Hao Su","{'name': 'Vikrant Ashvinkumar, Aaron Bernstein, Nairen Cao, Christoph Grunau, Bernhard Haeupler, Yonggang Jiang, Danupon Nanongkai, Hsin Hao Su'}",,
490,SCB-dataset: A Dataset for Detecting Student Classroom Behavior,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'SCB-dataset: A Dataset for Detecting Student Classroom Behavior'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2304.02488'}]",https://arxiv.org/abs/2304.02488,"arXiv:2304.02488v4 Announce Type: replace 
Abstract: The use of deep learning methods for automatic detection of students' classroom behavior is a promising approach to analyze their class performance and enhance teaching effectiveness. However, the lack of publicly available datasets on student behavior poses a challenge for researchers in this field. To address this issue, we propose a Student Classroom Behavior dataset (SCB-dataset) that reflects real-life scenarios. Our dataset includes 11,248 labels and 4,003 images, with a focus on hand-raising behavior. We evaluated the dataset using the YOLOv7 algorithm, achieving a mean average precision (map) of up to 85.3%. We believe that our dataset can serve as a robust foundation for future research in the field of student behavior detection and promote further advancements in this area.Our SCB-dataset can be downloaded from: https://github.com/Whiffe/SCB-dataset","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2304.02488v4 Announce Type: replace \nAbstract: The use of deep learning methods for automatic detection of students' classroom behavior is a promising approach to analyze their class performance and enhance teaching effectiveness. However, the lack of publicly available datasets on student behavior poses a challenge for researchers in this field. To address this issue, we propose a Student Classroom Behavior dataset (SCB-dataset) that reflects real-life scenarios. Our dataset includes 11,248 labels and 4,003 images, with a focus on hand-raising behavior. We evaluated the dataset using the YOLOv7 algorithm, achieving a mean average precision (map) of up to 85.3%. We believe that our dataset can serve as a robust foundation for future research in the field of student behavior detection and promote further advancements in this area.Our SCB-dataset can be downloaded from: https://github.com/Whiffe/SCB-dataset""}",oai:arXiv.org:2304.02488v4,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}",[{'name': 'Fan Yang'}],Fan Yang,{'name': 'Fan Yang'},,
491,UOR: Universal Backdoor Attacks on Pre-trained Language Models,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'UOR: Universal Backdoor Attacks on Pre-trained Language Models'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2305.09574'}]",https://arxiv.org/abs/2305.09574,"arXiv:2305.09574v2 Announce Type: replace 
Abstract: Backdoors implanted in pre-trained language models (PLMs) can be transferred to various downstream tasks, which exposes a severe security threat. However, most existing backdoor attacks against PLMs are un-targeted and task-specific. Few targeted and task-agnostic methods use manually pre-defined triggers and output representations, which prevent the attacks from being more effective and general. In this paper, we first summarize the requirements that a more threatening backdoor attack against PLMs should satisfy, and then propose a new backdoor attack method called UOR, which breaks the bottleneck of the previous approach by turning manual selection into automatic optimization. Specifically, we define poisoned supervised contrastive learning which can automatically learn the more uniform and universal output representations of triggers for various PLMs. Moreover, we use gradient search to select appropriate trigger words which can be adaptive to different PLMs and vocabularies. Experiments show that our method can achieve better attack performance on various text classification tasks compared to manual methods. Further, we tested our method on PLMs with different architectures, different usage paradigms, and more difficult tasks, which demonstrated the universality of our method.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2305.09574v2 Announce Type: replace \nAbstract: Backdoors implanted in pre-trained language models (PLMs) can be transferred to various downstream tasks, which exposes a severe security threat. However, most existing backdoor attacks against PLMs are un-targeted and task-specific. Few targeted and task-agnostic methods use manually pre-defined triggers and output representations, which prevent the attacks from being more effective and general. In this paper, we first summarize the requirements that a more threatening backdoor attack against PLMs should satisfy, and then propose a new backdoor attack method called UOR, which breaks the bottleneck of the previous approach by turning manual selection into automatic optimization. Specifically, we define poisoned supervised contrastive learning which can automatically learn the more uniform and universal output representations of triggers for various PLMs. Moreover, we use gradient search to select appropriate trigger words which can be adaptive to different PLMs and vocabularies. Experiments show that our method can achieve better attack performance on various text classification tasks compared to manual methods. Further, we tested our method on PLMs with different architectures, different usage paradigms, and more difficult tasks, which demonstrated the universality of our method.'}",oai:arXiv.org:2305.09574v2,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.CR', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Wei Du, Peixuan Li, Boqun Li, Haodong Zhao, Gongshen Liu'}]","Wei Du, Peixuan Li, Boqun Li, Haodong Zhao, Gongshen Liu","{'name': 'Wei Du, Peixuan Li, Boqun Li, Haodong Zhao, Gongshen Liu'}",,
492,Parameterised distance to local irregularity,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Parameterised distance to local irregularity'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2307.04583'}]",https://arxiv.org/abs/2307.04583,"arXiv:2307.04583v4 Announce Type: replace 
Abstract: A graph $G$ is \emph{locally irregular} if no two of its adjacent vertices have the same degree. In [Fioravantes et al. Complexity of finding maximum locally irregular induced subgraph. {\it SWAT}, 2022], the authors introduced and studied the problem of finding a locally irregular induced subgraph of a given a graph $G$ of maximum order, or, equivalently, computing a subset $S$ of $V(G)$ of minimum order, whose deletion from $G$ results in a locally irregular graph; $S$ is denoted as an \emph{optimal vertex-irregulator of $G$}. In this work we provide an in-depth analysis of the parameterised complexity of computing an optimal vertex-irregulator of a given graph $G$. Moreover, we introduce and study a variation of this problem, where $S$ is a substet of the edges of $G$; in this case, $S$ is denoted as an \emph{optimal edge-irregulator of $G$}. In particular, we prove that computing an optimal vertex-irregulator of a graph $G$ is in FPT when parameterised by the vertex integrity, neighborhood diversity or cluster deletion number of $G$, while it is $W[1]$-hard when parameterised by the feedback vertex set number or the treedepth of $G$. In the case of computing an optimal edge-irregulator of a graph $G$, we prove that this problem is in FPT when parameterised by the vertex integrity of $G$, while it is NP-hard even if $G$ is a planar bipartite graph of maximum degree $4$, and $W[1]$-hard when parameterised by the size of the solution, the feedback vertex set or the treedepth of $G$. Our results paint a comprehensive picture of the tractability of both problems studied here, considering most of the standard graph-structural parameters.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2307.04583v4 Announce Type: replace \nAbstract: A graph $G$ is \\emph{locally irregular} if no two of its adjacent vertices have the same degree. In [Fioravantes et al. Complexity of finding maximum locally irregular induced subgraph. {\\it SWAT}, 2022], the authors introduced and studied the problem of finding a locally irregular induced subgraph of a given a graph $G$ of maximum order, or, equivalently, computing a subset $S$ of $V(G)$ of minimum order, whose deletion from $G$ results in a locally irregular graph; $S$ is denoted as an \\emph{optimal vertex-irregulator of $G$}. In this work we provide an in-depth analysis of the parameterised complexity of computing an optimal vertex-irregulator of a given graph $G$. Moreover, we introduce and study a variation of this problem, where $S$ is a substet of the edges of $G$; in this case, $S$ is denoted as an \\emph{optimal edge-irregulator of $G$}. In particular, we prove that computing an optimal vertex-irregulator of a graph $G$ is in FPT when parameterised by the vertex integrity, neighborhood diversity or cluster deletion number of $G$, while it is $W[1]$-hard when parameterised by the feedback vertex set number or the treedepth of $G$. In the case of computing an optimal edge-irregulator of a graph $G$, we prove that this problem is in FPT when parameterised by the vertex integrity of $G$, while it is NP-hard even if $G$ is a planar bipartite graph of maximum degree $4$, and $W[1]$-hard when parameterised by the size of the solution, the feedback vertex set or the treedepth of $G$. Our results paint a comprehensive picture of the tractability of both problems studied here, considering most of the standard graph-structural parameters.'}",oai:arXiv.org:2307.04583v4,False,"[{'term': 'cs.CC', 'scheme': None, 'label': None}, {'term': 'cs.DM', 'scheme': None, 'label': None}, {'term': 'cs.DS', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Foivos Fioravantes, Nikolaos Melissinos, Theofilos Triommatis'}]","Foivos Fioravantes, Nikolaos Melissinos, Theofilos Triommatis","{'name': 'Foivos Fioravantes, Nikolaos Melissinos, Theofilos Triommatis'}",,
493,MinkSORT: A 3D deep feature extractor using sparse convolutions to improve 3D multi-object tracking in greenhouse tomato plants,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'MinkSORT: A 3D deep feature extractor using sparse convolutions to improve 3D multi-object tracking in greenhouse tomato plants'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2307.05219'}]",https://arxiv.org/abs/2307.05219,"arXiv:2307.05219v2 Announce Type: replace 
Abstract: The agro-food industry is turning to robots to address the challenge of labour shortage. However, agro-food environments pose difficulties for robots due to high variation and occlusions. In the presence of these challenges, accurate world models, with information about object location, shape, and properties, are crucial for robots to perform tasks accurately. Building such models is challenging due to the complex and unique nature of agro-food environments, and errors in the model can lead to task execution issues. In this paper, MinkSORT, a novel method for generating tracking features using a 3D sparse convolutional network in a deepSORT-like approach, is proposed to improve the accuracy of world models in agro-food environments. MinkSORT was evaluated using real-world data collected in a tomato greenhouse, where it significantly improved the performance of a baseline model that tracks tomato positions in 3D using a Kalman filter and Mahalanobis distance. MinkSORT improved the HOTA from 42.8% to 44.77%, the association accuracy from 32.55% to 35.55%, and the MOTA from 57.63% to 58.81%. Different contrastive loss functions for training MinkSORT were also evaluated, and it was demonstrated that it leads to improved performance in terms of three separate precision and recall detection outcomes. The proposed method improves world model accuracy, enabling robots to perform tasks such as harvesting and plant maintenance with greater efficiency and accuracy, which is essential for meeting the growing demand for food in a sustainable manner.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2307.05219v2 Announce Type: replace \nAbstract: The agro-food industry is turning to robots to address the challenge of labour shortage. However, agro-food environments pose difficulties for robots due to high variation and occlusions. In the presence of these challenges, accurate world models, with information about object location, shape, and properties, are crucial for robots to perform tasks accurately. Building such models is challenging due to the complex and unique nature of agro-food environments, and errors in the model can lead to task execution issues. In this paper, MinkSORT, a novel method for generating tracking features using a 3D sparse convolutional network in a deepSORT-like approach, is proposed to improve the accuracy of world models in agro-food environments. MinkSORT was evaluated using real-world data collected in a tomato greenhouse, where it significantly improved the performance of a baseline model that tracks tomato positions in 3D using a Kalman filter and Mahalanobis distance. MinkSORT improved the HOTA from 42.8% to 44.77%, the association accuracy from 32.55% to 35.55%, and the MOTA from 57.63% to 58.81%. Different contrastive loss functions for training MinkSORT were also evaluated, and it was demonstrated that it leads to improved performance in terms of three separate precision and recall detection outcomes. The proposed method improves world model accuracy, enabling robots to perform tasks such as harvesting and plant maintenance with greater efficiency and accuracy, which is essential for meeting the growing demand for food in a sustainable manner.'}",oai:arXiv.org:2307.05219v2,False,"[{'term': 'cs.RO', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'David Rapado-Rincon, Eldert J. van Henten, Gert Kootstra'}]","David Rapado-Rincon, Eldert J. van Henten, Gert Kootstra","{'name': 'David Rapado-Rincon, Eldert J. van Henten, Gert Kootstra'}",10.1016/j.biosystemseng.2023.11.003,
494,Accuracy Controlled Schemes for the Eigenvalue Problem of the Radiative Transfer Equation,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Accuracy Controlled Schemes for the Eigenvalue Problem of the Radiative Transfer Equation'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2307.07780'}]",https://arxiv.org/abs/2307.07780,"arXiv:2307.07780v4 Announce Type: replace 
Abstract: The criticality problem in nuclear engineering asks for the principal eigenpair of a Boltzmann operator describing neutron transport in a reactor core. Being able to reliably design, and control such reactors requires assessing these quantities within quantifiable accuracy tolerances. In this paper we propose a paradigm that deviates from the common practice of approximately solving the corresponding spectral problem with a fixed, presumably sufficiently fine discretization. Instead, the present approach is based on first contriving iterative schemes, formulated in function space, that are shown to converge at a quantitative rate without assuming any a priori excess regularity properties, and that exploit only properties of the optical parameters in the underlying radiative transfer model. We develop the analytical and numerical tools for approximately realizing each iteration step within judiciously chosen accuracy tolerances, verified by a posteriori estimates, so as to still warrant quantifiable convergence to the exact eigenpair. This is carried out in full first for a Newton scheme. Since this is only locally convergent we analyze in addition the convergence of a power iteration in function space to produce sufficiently accurate initial guesses. Here we have to deal with intrinsic difficulties posed by compact but unsymmetric operators preventing standard arguments used in the finite dimensional case. Our main point is that we can avoid any condition on an initial guess to be already in a small neighborhood of the exact solution. We close with a discussion of remaining intrinsic obstructions to a certifiable numerical implementation, mainly related to not knowing the gap between the principal eigenvalue and the next smaller one in modulus.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2307.07780v4 Announce Type: replace \nAbstract: The criticality problem in nuclear engineering asks for the principal eigenpair of a Boltzmann operator describing neutron transport in a reactor core. Being able to reliably design, and control such reactors requires assessing these quantities within quantifiable accuracy tolerances. In this paper we propose a paradigm that deviates from the common practice of approximately solving the corresponding spectral problem with a fixed, presumably sufficiently fine discretization. Instead, the present approach is based on first contriving iterative schemes, formulated in function space, that are shown to converge at a quantitative rate without assuming any a priori excess regularity properties, and that exploit only properties of the optical parameters in the underlying radiative transfer model. We develop the analytical and numerical tools for approximately realizing each iteration step within judiciously chosen accuracy tolerances, verified by a posteriori estimates, so as to still warrant quantifiable convergence to the exact eigenpair. This is carried out in full first for a Newton scheme. Since this is only locally convergent we analyze in addition the convergence of a power iteration in function space to produce sufficiently accurate initial guesses. Here we have to deal with intrinsic difficulties posed by compact but unsymmetric operators preventing standard arguments used in the finite dimensional case. Our main point is that we can avoid any condition on an initial guess to be already in a small neighborhood of the exact solution. We close with a discussion of remaining intrinsic obstructions to a certifiable numerical implementation, mainly related to not knowing the gap between the principal eigenvalue and the next smaller one in modulus.'}",oai:arXiv.org:2307.07780v4,False,"[{'term': 'math.NA', 'scheme': None, 'label': None}, {'term': 'cs.NA', 'scheme': None, 'label': None}, {'term': 'math.AP', 'scheme': None, 'label': None}, {'term': 'math.SP', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Wolfgang Dahmen, Olga Mula'}]","Wolfgang Dahmen, Olga Mula","{'name': 'Wolfgang Dahmen, Olga Mula'}",,
495,Scaling Laws for Imitation Learning in Single-Agent Games,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Scaling Laws for Imitation Learning in Single-Agent Games'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2307.09423'}]",https://arxiv.org/abs/2307.09423,"arXiv:2307.09423v3 Announce Type: replace 
Abstract: Imitation Learning (IL) is one of the most widely used methods in machine learning. Yet, many works find it is often unable to fully recover the underlying expert behavior, even in constrained environments like single-agent games. However, none of these works deeply investigate the role of scaling up the model and data size. Inspired by recent work in Natural Language Processing (NLP) where ""scaling up"" has resulted in increasingly more capable LLMs, we investigate whether carefully scaling up model and data size can bring similar improvements in the imitation learning setting for single-agent games. We first demonstrate our findings on a variety of Atari games, and thereafter focus on the extremely challenging game of NetHack. In all games, we find that IL loss and mean return scale smoothly with the compute budget (FLOPs) and are strongly correlated, resulting in power laws for training compute-optimal IL agents. Finally, we forecast and train several NetHack agents with IL and find they outperform prior state-of-the-art by 1.5x in all settings. Our work both demonstrates the scaling behavior of imitation learning in a variety of single-agent games, as well as the viability of scaling up current approaches for increasingly capable agents in NetHack, a game that remains elusively hard for current AI systems.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2307.09423v3 Announce Type: replace \nAbstract: Imitation Learning (IL) is one of the most widely used methods in machine learning. Yet, many works find it is often unable to fully recover the underlying expert behavior, even in constrained environments like single-agent games. However, none of these works deeply investigate the role of scaling up the model and data size. Inspired by recent work in Natural Language Processing (NLP) where ""scaling up"" has resulted in increasingly more capable LLMs, we investigate whether carefully scaling up model and data size can bring similar improvements in the imitation learning setting for single-agent games. We first demonstrate our findings on a variety of Atari games, and thereafter focus on the extremely challenging game of NetHack. In all games, we find that IL loss and mean return scale smoothly with the compute budget (FLOPs) and are strongly correlated, resulting in power laws for training compute-optimal IL agents. Finally, we forecast and train several NetHack agents with IL and find they outperform prior state-of-the-art by 1.5x in all settings. Our work both demonstrates the scaling behavior of imitation learning in a variety of single-agent games, as well as the viability of scaling up current approaches for increasingly capable agents in NetHack, a game that remains elusively hard for current AI systems.'}",oai:arXiv.org:2307.09423v3,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'stat.ML', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Jens Tuyls, Dhruv Madeka, Kari Torkkola, Dean Foster, Karthik Narasimhan, Sham Kakade'}]","Jens Tuyls, Dhruv Madeka, Kari Torkkola, Dean Foster, Karthik Narasimhan, Sham Kakade","{'name': 'Jens Tuyls, Dhruv Madeka, Kari Torkkola, Dean Foster, Karthik Narasimhan, Sham Kakade'}",,
496,Introducing Interactions in Multi-Objective Optimization of Software Architectures,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Introducing Interactions in Multi-Objective Optimization of Software Architectures'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2308.15084'}]",https://arxiv.org/abs/2308.15084,"arXiv:2308.15084v2 Announce Type: replace 
Abstract: Software architecture optimization aims to enhance non-functional attributes like performance and reliability while meeting functional requirements. Multi-objective optimization employs metaheuristic search techniques, such as genetic algorithms, to explore feasible architectural changes and propose alternatives to designers. However, this resource-intensive process may not always align with practical constraints. This study investigates the impact of designer interactions on multi-objective software architecture optimization. Designers can intervene at intermediate points in the fully automated optimization process, making choices that guide exploration towards more desirable solutions. Through several controlled experiments as well as an initial user study (14 subjects), we compare this interactive approach with a fully automated optimization process, which serves as a baseline. The findings demonstrate that designer interactions lead to a more focused solution space, resulting in improved architectural quality. By directing the search towards regions of interest, the interaction uncovers architectures that remain unexplored in the fully automated process. In the user study, participants found that our interactive approach provides a better trade-off between sufficient exploration of the solution space and the required computation time.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2308.15084v2 Announce Type: replace \nAbstract: Software architecture optimization aims to enhance non-functional attributes like performance and reliability while meeting functional requirements. Multi-objective optimization employs metaheuristic search techniques, such as genetic algorithms, to explore feasible architectural changes and propose alternatives to designers. However, this resource-intensive process may not always align with practical constraints. This study investigates the impact of designer interactions on multi-objective software architecture optimization. Designers can intervene at intermediate points in the fully automated optimization process, making choices that guide exploration towards more desirable solutions. Through several controlled experiments as well as an initial user study (14 subjects), we compare this interactive approach with a fully automated optimization process, which serves as a baseline. The findings demonstrate that designer interactions lead to a more focused solution space, resulting in improved architectural quality. By directing the search towards regions of interest, the interaction uncovers architectures that remain unexplored in the fully automated process. In the user study, participants found that our interactive approach provides a better trade-off between sufficient exploration of the solution space and the required computation time.'}",oai:arXiv.org:2308.15084v2,False,"[{'term': 'cs.SE', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': ""Vittorio Cortellessa, J. Andres Diaz-Pace, Daniele Di Pompeo, Sebastian Frank, Pooyan Jamshidi, Michele Tucci, Andr\\'e van Hoorn""}]","Vittorio Cortellessa, J. Andres Diaz-Pace, Daniele Di Pompeo, Sebastian Frank, Pooyan Jamshidi, Michele Tucci, Andr\'e van Hoorn","{'name': ""Vittorio Cortellessa, J. Andres Diaz-Pace, Daniele Di Pompeo, Sebastian Frank, Pooyan Jamshidi, Michele Tucci, Andr\\'e van Hoorn""}",,
497,DTW+S: Shape-based Comparison of Time-series with Ordered Local Trend,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'DTW+S: Shape-based Comparison of Time-series with Ordered Local Trend'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2309.03579'}]",https://arxiv.org/abs/2309.03579,"arXiv:2309.03579v3 Announce Type: replace 
Abstract: Measuring distance or similarity between time-series data is a fundamental aspect of many applications including classification, clustering, and ensembling/alignment. Existing measures may fail to capture similarities among local trends (shapes) and may even produce misleading results. Our goal is to develop a measure that looks for similar trends occurring around similar times and is easily interpretable for researchers in applied domains. This is particularly useful for applications where time-series have a sequence of meaningful local trends that are ordered, such as in epidemics (a surge to an increase to a peak to a decrease). We propose a novel measure, DTW+S, which creates an interpretable ""closeness-preserving"" matrix representation of the time-series, where each column represents local trends, and then it applies Dynamic Time Warping to compute distances between these matrices. We present a theoretical analysis that supports the choice of this representation. We demonstrate the utility of DTW+S in several tasks. For the clustering of epidemic curves, we show that DTW+S is the only measure able to produce good clustering compared to the baselines. For ensemble building, we propose a combination of DTW+S and barycenter averaging that results in the best preservation of characteristics of the underlying trajectories. We also demonstrate that our approach results in better classification compared to Dynamic Time Warping for a class of datasets, particularly when local trends rather than scale play a decisive role.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2309.03579v3 Announce Type: replace \nAbstract: Measuring distance or similarity between time-series data is a fundamental aspect of many applications including classification, clustering, and ensembling/alignment. Existing measures may fail to capture similarities among local trends (shapes) and may even produce misleading results. Our goal is to develop a measure that looks for similar trends occurring around similar times and is easily interpretable for researchers in applied domains. This is particularly useful for applications where time-series have a sequence of meaningful local trends that are ordered, such as in epidemics (a surge to an increase to a peak to a decrease). We propose a novel measure, DTW+S, which creates an interpretable ""closeness-preserving"" matrix representation of the time-series, where each column represents local trends, and then it applies Dynamic Time Warping to compute distances between these matrices. We present a theoretical analysis that supports the choice of this representation. We demonstrate the utility of DTW+S in several tasks. For the clustering of epidemic curves, we show that DTW+S is the only measure able to produce good clustering compared to the baselines. For ensemble building, we propose a combination of DTW+S and barycenter averaging that results in the best preservation of characteristics of the underlying trajectories. We also demonstrate that our approach results in better classification compared to Dynamic Time Warping for a class of datasets, particularly when local trends rather than scale play a decisive role.'}",oai:arXiv.org:2309.03579v3,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}",[{'name': 'Ajitesh Srivastava'}],Ajitesh Srivastava,{'name': 'Ajitesh Srivastava'},,
498,From Programming Bugs to Multimillion-Dollar Scams: An Analysis of Trapdoor Tokens on Uniswap,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'From Programming Bugs to Multimillion-Dollar Scams: An Analysis of Trapdoor Tokens on Uniswap'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2309.04700'}]",https://arxiv.org/abs/2309.04700,"arXiv:2309.04700v4 Announce Type: replace 
Abstract: We investigate in this work a recently emerged type of scam ERC-20 token called Trapdoor, which has cost investors billions of US dollars on Uniswap, the largest decentralised exchange on Ethereum, from 2020 to 2023. In essence, Trapdoor tokens allow users to buy but preventing them from selling by embedding logical bugs and/or owner-only features in their smart contracts. By manually inspecting a number of Trapdoor samples, we established the first systematic classification of Trapdoor tokens and a comprehensive list of techniques that scammers used to embed and conceal malicious codes, accompanied by a detailed analysis of representative scam contracts. In particular, we developed TrapdoorAnalyser, a fine-grained detection tool that generates and crosschecks the error-log of a buy-and-sell test and the list of embedded Trapdoor indicators from a contract-semantic check to reliably identify a Trapdoor token. TrapdoorAnalyser not only outperforms the state-of-the-art commercial tool GoPlus in accuracy, but also provides traces of malicious code with a full explanation, which most of the existing tools lack. Using TrapdoorAnalyser, we constructed the very first dataset of about 30,000 Trapdoor and non-Trapdoor tokens on UniswapV2, which allows us to train several machine learning algorithms that can detect with very high accuracy even Trapdoor tokens with no available Solidity source codes.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2309.04700v4 Announce Type: replace \nAbstract: We investigate in this work a recently emerged type of scam ERC-20 token called Trapdoor, which has cost investors billions of US dollars on Uniswap, the largest decentralised exchange on Ethereum, from 2020 to 2023. In essence, Trapdoor tokens allow users to buy but preventing them from selling by embedding logical bugs and/or owner-only features in their smart contracts. By manually inspecting a number of Trapdoor samples, we established the first systematic classification of Trapdoor tokens and a comprehensive list of techniques that scammers used to embed and conceal malicious codes, accompanied by a detailed analysis of representative scam contracts. In particular, we developed TrapdoorAnalyser, a fine-grained detection tool that generates and crosschecks the error-log of a buy-and-sell test and the list of embedded Trapdoor indicators from a contract-semantic check to reliably identify a Trapdoor token. TrapdoorAnalyser not only outperforms the state-of-the-art commercial tool GoPlus in accuracy, but also provides traces of malicious code with a full explanation, which most of the existing tools lack. Using TrapdoorAnalyser, we constructed the very first dataset of about 30,000 Trapdoor and non-Trapdoor tokens on UniswapV2, which allows us to train several machine learning algorithms that can detect with very high accuracy even Trapdoor tokens with no available Solidity source codes.'}",oai:arXiv.org:2309.04700v4,False,"[{'term': 'cs.CR', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Phuong Duy Huynh, Thisal De Silva, Son Hoang Dau, Xiaodong Li, Iqbal Gondal, Emanuele Viterbo'}]","Phuong Duy Huynh, Thisal De Silva, Son Hoang Dau, Xiaodong Li, Iqbal Gondal, Emanuele Viterbo","{'name': 'Phuong Duy Huynh, Thisal De Silva, Son Hoang Dau, Xiaodong Li, Iqbal Gondal, Emanuele Viterbo'}",,
499,LiON: Learning Point-wise Abstaining Penalty for LiDAR Outlier DetectioN Using Diverse Synthetic Data,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'LiON: Learning Point-wise Abstaining Penalty for LiDAR Outlier DetectioN Using Diverse Synthetic Data'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2309.10230'}]",https://arxiv.org/abs/2309.10230,"arXiv:2309.10230v3 Announce Type: replace 
Abstract: LiDAR-based semantic scene understanding is an important module in the modern autonomous driving perception stack. However, identifying outlier points in a LiDAR point cloud is challenging as LiDAR point clouds lack semantically-rich information. While former SOTA methods adopt heuristic architectures, we revisit this problem from the perspective of Selective Classification, which introduces a selective function into the standard closed-set classification setup. Our solution is built upon the basic idea of abstaining from choosing any inlier categories but learns a point-wise abstaining penalty with a margin-based loss. Apart from learning paradigms, synthesizing outliers to approximate unlimited real outliers is also critical, so we propose a strong synthesis pipeline that generates outliers originated from various factors: object categories, sampling patterns and sizes. We demonstrate that learning different abstaining penalties, apart from point-wise penalty, for different types of (synthesized) outliers can further improve the performance. We benchmark our method on SemanticKITTI and nuScenes and achieve SOTA results. Codes are available at https://github.com/Daniellli/LiON/.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2309.10230v3 Announce Type: replace \nAbstract: LiDAR-based semantic scene understanding is an important module in the modern autonomous driving perception stack. However, identifying outlier points in a LiDAR point cloud is challenging as LiDAR point clouds lack semantically-rich information. While former SOTA methods adopt heuristic architectures, we revisit this problem from the perspective of Selective Classification, which introduces a selective function into the standard closed-set classification setup. Our solution is built upon the basic idea of abstaining from choosing any inlier categories but learns a point-wise abstaining penalty with a margin-based loss. Apart from learning paradigms, synthesizing outliers to approximate unlimited real outliers is also critical, so we propose a strong synthesis pipeline that generates outliers originated from various factors: object categories, sampling patterns and sizes. We demonstrate that learning different abstaining penalties, apart from point-wise penalty, for different types of (synthesized) outliers can further improve the performance. We benchmark our method on SemanticKITTI and nuScenes and achieve SOTA results. Codes are available at https://github.com/Daniellli/LiON/.'}",oai:arXiv.org:2309.10230v3,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Shaocong Xu, Pengfei Li, Qianpu Sun, Xinyu Liu, Yang Li, Shihui Guo, Zhen Wang, Bo Jiang, Rui Wang, Kehua Sheng, Bo Zhang, Li Jiang, Hao Zhao, Yilun Chen'}]","Shaocong Xu, Pengfei Li, Qianpu Sun, Xinyu Liu, Yang Li, Shihui Guo, Zhen Wang, Bo Jiang, Rui Wang, Kehua Sheng, Bo Zhang, Li Jiang, Hao Zhao, Yilun Chen","{'name': 'Shaocong Xu, Pengfei Li, Qianpu Sun, Xinyu Liu, Yang Li, Shihui Guo, Zhen Wang, Bo Jiang, Rui Wang, Kehua Sheng, Bo Zhang, Li Jiang, Hao Zhao, Yilun Chen'}",,
500,Scalable Acceleration for Classification-Based Derivative-Free Optimization,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Scalable Acceleration for Classification-Based Derivative-Free Optimization'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2309.11036'}]",https://arxiv.org/abs/2309.11036,"arXiv:2309.11036v2 Announce Type: replace 
Abstract: Derivative-free optimization algorithms play an important role in scientific and engineering design optimization problems, especially when derivative information is not accessible. In this paper, we study the framework of sequential classification-based derivative-free optimization algorithms. By introducing learning theoretic concept hypothesis-target shattering rate, we revisit the computational complexity upper bound of SRACOS (Hu, Qian, and Yu 2017). Inspired by the revisited upper bound, we propose an algorithm named RACE-CARS, which adds a random region-shrinking step compared with SRACOS. We further establish theorems showing the acceleration by region shrinking. Experiments on the synthetic functions as well as black-box tuning for language-model-as-a-service demonstrate empirically the efficiency of RACE-CARS. An ablation experiment on the introduced hyperparameters is also conducted, revealing the mechanism of RACE-CARS and putting forward an empirical hyper-parameter tuning guidance.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2309.11036v2 Announce Type: replace \nAbstract: Derivative-free optimization algorithms play an important role in scientific and engineering design optimization problems, especially when derivative information is not accessible. In this paper, we study the framework of sequential classification-based derivative-free optimization algorithms. By introducing learning theoretic concept hypothesis-target shattering rate, we revisit the computational complexity upper bound of SRACOS (Hu, Qian, and Yu 2017). Inspired by the revisited upper bound, we propose an algorithm named RACE-CARS, which adds a random region-shrinking step compared with SRACOS. We further establish theorems showing the acceleration by region shrinking. Experiments on the synthetic functions as well as black-box tuning for language-model-as-a-service demonstrate empirically the efficiency of RACE-CARS. An ablation experiment on the introduced hyperparameters is also conducted, revealing the mechanism of RACE-CARS and putting forward an empirical hyper-parameter tuning guidance.'}",oai:arXiv.org:2309.11036v2,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'cs.NA', 'scheme': None, 'label': None}, {'term': 'math.NA', 'scheme': None, 'label': None}, {'term': 'math.OC', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by-nc-nd/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-nd/4.0/'}","[{'name': 'Tianyi Han, Jingya Li, Zhipeng Guo, Yuan Jin'}]","Tianyi Han, Jingya Li, Zhipeng Guo, Yuan Jin","{'name': 'Tianyi Han, Jingya Li, Zhipeng Guo, Yuan Jin'}",,
501,Fixing Large Language Models' Specification Misunderstanding for Better Code Generation,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""Fixing Large Language Models' Specification Misunderstanding for Better Code Generation""}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2309.16120'}]",https://arxiv.org/abs/2309.16120,"arXiv:2309.16120v3 Announce Type: replace 
Abstract: Code generation is to automatically generate source code conforming to a given programming specification, which has received extensive attention especially with the development of large language models (LLMs). Due to the inherent difficulty of code generation, the code generated by LLMs may not be aligned with the specification. Although thought-eliciting prompting techniques have been proposed to enhance the code generation performance of LLMs, producing correct understanding for complicated programming problems remains challenging, resulting in unsatisfactory performance. Also, some feedback-based prompting techniques have been proposed to fix incorrect code using error messages produced by test execution. However, when the generated code deviates significantly from the ground truth, they encounter difficulties in improving performance based on such coarse-grained information. In this work, we propose a novel prompting technique, called {\mu}FiX, to improve the code generation performance of LLMs by devising both sophisticated thought-eliciting prompting and feedback-based prompting and making the first exploration on their synergy. It first exploits test case analysis to obtain specification understanding and enables a self-improvement process to identify and refine the misunderstanding in the thought-eliciting prompting phase. {\mu}FiX further fixes the specification understanding towards the direction reducing the gap between the provided understanding (from the first phase) and the actual understanding implicitly utilized by LLMs for code generation in the feedback-based prompting phase. By improving the understanding with {\mu}FiX, the code generation performance of LLMs can be largely improved. Our evaluation on two advanced LLMs (ChatGPT and DeepSeek-Coder) with six widely-used benchmarks by comparing with 15 baselines, demonstrates the effectiveness of {\mu}FiX ...","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2309.16120v3 Announce Type: replace \nAbstract: Code generation is to automatically generate source code conforming to a given programming specification, which has received extensive attention especially with the development of large language models (LLMs). Due to the inherent difficulty of code generation, the code generated by LLMs may not be aligned with the specification. Although thought-eliciting prompting techniques have been proposed to enhance the code generation performance of LLMs, producing correct understanding for complicated programming problems remains challenging, resulting in unsatisfactory performance. Also, some feedback-based prompting techniques have been proposed to fix incorrect code using error messages produced by test execution. However, when the generated code deviates significantly from the ground truth, they encounter difficulties in improving performance based on such coarse-grained information. In this work, we propose a novel prompting technique, called {\\mu}FiX, to improve the code generation performance of LLMs by devising both sophisticated thought-eliciting prompting and feedback-based prompting and making the first exploration on their synergy. It first exploits test case analysis to obtain specification understanding and enables a self-improvement process to identify and refine the misunderstanding in the thought-eliciting prompting phase. {\\mu}FiX further fixes the specification understanding towards the direction reducing the gap between the provided understanding (from the first phase) and the actual understanding implicitly utilized by LLMs for code generation in the feedback-based prompting phase. By improving the understanding with {\\mu}FiX, the code generation performance of LLMs can be largely improved. Our evaluation on two advanced LLMs (ChatGPT and DeepSeek-Coder) with six widely-used benchmarks by comparing with 15 baselines, demonstrates the effectiveness of {\\mu}FiX ...'}",oai:arXiv.org:2309.16120v3,False,"[{'term': 'cs.SE', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Zhao Tian, Junjie Chen, Xiangyu Zhang'}]","Zhao Tian, Junjie Chen, Xiangyu Zhang","{'name': 'Zhao Tian, Junjie Chen, Xiangyu Zhang'}",,
502,Electromagnetic Information Theory-Based Statistical Channel Model for Improved Channel Estimation,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Electromagnetic Information Theory-Based Statistical Channel Model for Improved Channel Estimation'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2310.12446'}]",https://arxiv.org/abs/2310.12446,"arXiv:2310.12446v3 Announce Type: replace 
Abstract: Electromagnetic information theory (EIT) is an emerging interdisciplinary subject that integrates classical Maxwell electromagnetics and Shannon information theory. The goal of EIT is to uncover the information transmission mechanisms from an electromagnetic (EM) perspective in wireless systems. Existing works on EIT are mainly focused on the analysis of EM channel characteristics, degrees-of-freedom, and system capacity. However, these works do not clarify how to integrate EIT knowledge into the design and optimization of wireless systems. To fill in this gap, in this paper, we propose an EIT-based statistical channel model with simplified parameterization. Thanks to the simplified closed-form expression of the EMCF, it can be readily applied to various channel modeling and inference tasks. Specifically, by averaging the solutions of Maxwell's equations over a tunable von Mises distribution, we obtain a spatio-temporal correlation function (STCF) model of the EM channel, which we name as the EMCF. Furthermore, by tuning the parameters of the EMCF, we propose an EIT-based covariance estimator (EIT-Cov) to accurately capture the channel covariance. Since classical MMSE estimators can exploit prior information contained in the channel covariance matrix, we further propose the EIT-MMSE channel estimator by substituting EMCF for the covariance matrix. Simulation results show that both the proposed EIT-Cov covariance estimator and the EIT-MMSE channel estimator outperform their baseline algorithms, thus proving that EIT is beneficial to wireless communication systems.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2310.12446v3 Announce Type: replace \nAbstract: Electromagnetic information theory (EIT) is an emerging interdisciplinary subject that integrates classical Maxwell electromagnetics and Shannon information theory. The goal of EIT is to uncover the information transmission mechanisms from an electromagnetic (EM) perspective in wireless systems. Existing works on EIT are mainly focused on the analysis of EM channel characteristics, degrees-of-freedom, and system capacity. However, these works do not clarify how to integrate EIT knowledge into the design and optimization of wireless systems. To fill in this gap, in this paper, we propose an EIT-based statistical channel model with simplified parameterization. Thanks to the simplified closed-form expression of the EMCF, it can be readily applied to various channel modeling and inference tasks. Specifically, by averaging the solutions of Maxwell's equations over a tunable von Mises distribution, we obtain a spatio-temporal correlation function (STCF) model of the EM channel, which we name as the EMCF. Furthermore, by tuning the parameters of the EMCF, we propose an EIT-based covariance estimator (EIT-Cov) to accurately capture the channel covariance. Since classical MMSE estimators can exploit prior information contained in the channel covariance matrix, we further propose the EIT-MMSE channel estimator by substituting EMCF for the covariance matrix. Simulation results show that both the proposed EIT-Cov covariance estimator and the EIT-MMSE channel estimator outperform their baseline algorithms, thus proving that EIT is beneficial to wireless communication systems.""}",oai:arXiv.org:2310.12446v3,False,"[{'term': 'cs.IT', 'scheme': None, 'label': None}, {'term': 'eess.SP', 'scheme': None, 'label': None}, {'term': 'math.IT', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/publicdomain/zero/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/publicdomain/zero/1.0/'}","[{'name': 'Jieao Zhu, Zhongzhichao Wan, Linglong Dai, Tie Jun Cui'}]","Jieao Zhu, Zhongzhichao Wan, Linglong Dai, Tie Jun Cui","{'name': 'Jieao Zhu, Zhongzhichao Wan, Linglong Dai, Tie Jun Cui'}",,
503,DavIR: Data Selection via Implicit Reward for Large Language Models,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'DavIR: Data Selection via Implicit Reward for Large Language Models'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2310.13008'}]",https://arxiv.org/abs/2310.13008,"arXiv:2310.13008v2 Announce Type: replace 
Abstract: We introduce DavIR, a model-based data selection method for post-training Large Language Models. DavIR generalizes Reducible Holdout Loss to core-set selection problem of causal language modeling, and quantifies the learnability of a given datum with respect to a pre-trained LLM based on relative reduction in loss during fine-tuning, a metric we show to be closely related to the implicit reward model described in Direct Preference Optimization (DPO). We show that 6% of Alpaca dataset selected with DavIR can steer both the LLaMA and Gemma model family to produce superior performance compared to the same models trained on the full 52K dataset. We also show that Alpaca dataset compressed with DavIR can be combined with GSM8K dataset to effectively balance open-domain freeform QA and mathematical reasoning capabilities. Finally, we apply the DavIR objective to DPO and develop a normalized DavIR-DPO objective which improves alignment performance of Zephyr-7B-SFT model by 8% (relative) on AlpacaEval, compared against training on vanilla DPO objective.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2310.13008v2 Announce Type: replace \nAbstract: We introduce DavIR, a model-based data selection method for post-training Large Language Models. DavIR generalizes Reducible Holdout Loss to core-set selection problem of causal language modeling, and quantifies the learnability of a given datum with respect to a pre-trained LLM based on relative reduction in loss during fine-tuning, a metric we show to be closely related to the implicit reward model described in Direct Preference Optimization (DPO). We show that 6% of Alpaca dataset selected with DavIR can steer both the LLaMA and Gemma model family to produce superior performance compared to the same models trained on the full 52K dataset. We also show that Alpaca dataset compressed with DavIR can be combined with GSM8K dataset to effectively balance open-domain freeform QA and mathematical reasoning capabilities. Finally, we apply the DavIR objective to DPO and develop a normalized DavIR-DPO objective which improves alignment performance of Zephyr-7B-SFT model by 8% (relative) on AlpacaEval, compared against training on vanilla DPO objective.'}",oai:arXiv.org:2310.13008v2,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.CL', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Haotian Zhou, Tingkai Liu, Qianli Ma, Yufeng Zhang, Jianbo Yuan, Pengfei Liu, Yang You, Hongxia Yang'}]","Haotian Zhou, Tingkai Liu, Qianli Ma, Yufeng Zhang, Jianbo Yuan, Pengfei Liu, Yang You, Hongxia Yang","{'name': 'Haotian Zhou, Tingkai Liu, Qianli Ma, Yufeng Zhang, Jianbo Yuan, Pengfei Liu, Yang You, Hongxia Yang'}",,
504,Faster Algorithms for Text-to-Pattern Hamming Distances,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Faster Algorithms for Text-to-Pattern Hamming Distances'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2310.13174'}]",https://arxiv.org/abs/2310.13174,"arXiv:2310.13174v3 Announce Type: replace 
Abstract: We study the classic Text-to-Pattern Hamming Distances problem: given a pattern $P$ of length $m$ and a text $T$ of length $n$, both over a polynomial-size alphabet, compute the Hamming distance between $P$ and $T[i\, .\, . \, i+m-1]$ for every shift $i$, under the standard Word-RAM model with $\Theta(\log n)$-bit words.
  - We provide an $O(n\sqrt{m})$ time Las Vegas randomized algorithm for this problem, beating the decades-old $O(n \sqrt{m \log m})$ running time [Abrahamson, SICOMP 1987]. We also obtain a deterministic algorithm, with a slightly higher $O(n\sqrt{m}(\log m\log\log m)^{1/4})$ running time. Our randomized algorithm extends to the $k$-bounded setting, with running time $O\big(n+\frac{nk}{\sqrt{m}}\big)$, removing all the extra logarithmic factors from earlier algorithms [Gawrychowski and Uzna\'{n}ski, ICALP 2018; Chan, Golan, Kociumaka, Kopelowitz and Porat, STOC 2020].
  - For the $(1+\epsilon)$-approximate version of Text-to-Pattern Hamming Distances, we give an $\tilde{O}(\epsilon^{-0.93}n)$ time Monte Carlo randomized algorithm, beating the previous $\tilde{O}(\epsilon^{-1}n)$ running time [Kopelowitz and Porat, FOCS 2015; Kopelowitz and Porat, SOSA 2018].
  Our approximation algorithm exploits a connection with $3$SUM, and uses a combination of Fredman's trick, equality matrix product, and random sampling; in particular, we obtain new results on approximate counting versions of $3$SUM and Exact Triangle, which may be of independent interest. Our exact algorithms use a novel combination of hashing, bit-packed FFT, and recursion; in particular, we obtain a faster algorithm for computing the sumset of two integer sets, in the regime when the universe size is close to quadratic in the number of elements.
  We also prove a fine-grained equivalence between the exact Text-to-Pattern Hamming Distances problem and a range-restricted, counting version of $3$SUM.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2310.13174v3 Announce Type: replace \nAbstract: We study the classic Text-to-Pattern Hamming Distances problem: given a pattern $P$ of length $m$ and a text $T$ of length $n$, both over a polynomial-size alphabet, compute the Hamming distance between $P$ and $T[i\\, .\\, . \\, i+m-1]$ for every shift $i$, under the standard Word-RAM model with $\\Theta(\\log n)$-bit words.\n  - We provide an $O(n\\sqrt{m})$ time Las Vegas randomized algorithm for this problem, beating the decades-old $O(n \\sqrt{m \\log m})$ running time [Abrahamson, SICOMP 1987]. We also obtain a deterministic algorithm, with a slightly higher $O(n\\sqrt{m}(\\log m\\log\\log m)^{1/4})$ running time. Our randomized algorithm extends to the $k$-bounded setting, with running time $O\\big(n+\\frac{nk}{\\sqrt{m}}\\big)$, removing all the extra logarithmic factors from earlier algorithms [Gawrychowski and Uzna\\'{n}ski, ICALP 2018; Chan, Golan, Kociumaka, Kopelowitz and Porat, STOC 2020].\n  - For the $(1+\\epsilon)$-approximate version of Text-to-Pattern Hamming Distances, we give an $\\tilde{O}(\\epsilon^{-0.93}n)$ time Monte Carlo randomized algorithm, beating the previous $\\tilde{O}(\\epsilon^{-1}n)$ running time [Kopelowitz and Porat, FOCS 2015; Kopelowitz and Porat, SOSA 2018].\n  Our approximation algorithm exploits a connection with $3$SUM, and uses a combination of Fredman's trick, equality matrix product, and random sampling; in particular, we obtain new results on approximate counting versions of $3$SUM and Exact Triangle, which may be of independent interest. Our exact algorithms use a novel combination of hashing, bit-packed FFT, and recursion; in particular, we obtain a faster algorithm for computing the sumset of two integer sets, in the regime when the universe size is close to quadratic in the number of elements.\n  We also prove a fine-grained equivalence between the exact Text-to-Pattern Hamming Distances problem and a range-restricted, counting version of $3$SUM.""}",oai:arXiv.org:2310.13174v3,False,"[{'term': 'cs.DS', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Timothy M. Chan, Ce Jin, Virginia Vassilevska Williams, Yinzhan Xu'}]","Timothy M. Chan, Ce Jin, Virginia Vassilevska Williams, Yinzhan Xu","{'name': 'Timothy M. Chan, Ce Jin, Virginia Vassilevska Williams, Yinzhan Xu'}",,
505,"Audio-Visual Speaker Tracking: Progress, Challenges, and Future Directions","{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Audio-Visual Speaker Tracking: Progress, Challenges, and Future Directions'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2310.14778'}]",https://arxiv.org/abs/2310.14778,"arXiv:2310.14778v3 Announce Type: replace 
Abstract: Audio-visual speaker tracking has drawn increasing attention over the past few years due to its academic values and wide application. Audio and visual modalities can provide complementary information for localization and tracking. With audio and visual information, the Bayesian-based filter can solve the problem of data association, audio-visual fusion and track management. In this paper, we conduct a comprehensive overview of audio-visual speaker tracking. To our knowledge, this is the first extensive survey over the past five years. We introduce the family of Bayesian filters and summarize the methods for obtaining audio-visual measurements. In addition, the existing trackers and their performance on AV16.3 dataset are summarized. In the past few years, deep learning techniques have thrived, which also boosts the development of audio visual speaker tracking. The influence of deep learning techniques in terms of measurement extraction and state estimation is also discussed. At last, we discuss the connections between audio-visual speaker tracking and other areas such as speech separation and distributed speaker tracking.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2310.14778v3 Announce Type: replace \nAbstract: Audio-visual speaker tracking has drawn increasing attention over the past few years due to its academic values and wide application. Audio and visual modalities can provide complementary information for localization and tracking. With audio and visual information, the Bayesian-based filter can solve the problem of data association, audio-visual fusion and track management. In this paper, we conduct a comprehensive overview of audio-visual speaker tracking. To our knowledge, this is the first extensive survey over the past five years. We introduce the family of Bayesian filters and summarize the methods for obtaining audio-visual measurements. In addition, the existing trackers and their performance on AV16.3 dataset are summarized. In the past few years, deep learning techniques have thrived, which also boosts the development of audio visual speaker tracking. The influence of deep learning techniques in terms of measurement extraction and state estimation is also discussed. At last, we discuss the connections between audio-visual speaker tracking and other areas such as speech separation and distributed speaker tracking.'}",oai:arXiv.org:2310.14778v3,False,"[{'term': 'cs.MM', 'scheme': None, 'label': None}, {'term': 'cs.SD', 'scheme': None, 'label': None}, {'term': 'eess.AS', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Jinzheng Zhao, Yong Xu, Xinyuan Qian, Davide Berghi, Peipei Wu, Meng Cui, Jianyuan Sun, Philip J. B. Jackson, Wenwu Wang'}]","Jinzheng Zhao, Yong Xu, Xinyuan Qian, Davide Berghi, Peipei Wu, Meng Cui, Jianyuan Sun, Philip J. B. Jackson, Wenwu Wang","{'name': 'Jinzheng Zhao, Yong Xu, Xinyuan Qian, Davide Berghi, Peipei Wu, Meng Cui, Jianyuan Sun, Philip J. B. Jackson, Wenwu Wang'}",,
506,Robust and Communication-Efficient Federated Domain Adaptation via Random Features,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Robust and Communication-Efficient Federated Domain Adaptation via Random Features'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2311.04686'}]",https://arxiv.org/abs/2311.04686,"arXiv:2311.04686v2 Announce Type: replace 
Abstract: Modern machine learning (ML) models have grown to a scale where training them on a single machine becomes impractical. As a result, there is a growing trend to leverage federated learning (FL) techniques to train large ML models in a distributed and collaborative manner. These models, however, when deployed on new devices, might struggle to generalize well due to domain shifts. In this context, federated domain adaptation (FDA) emerges as a powerful approach to address this challenge.
  Most existing FDA approaches typically focus on aligning the distributions between source and target domains by minimizing their (e.g., MMD) distance. Such strategies, however, inevitably introduce high communication overheads and can be highly sensitive to network reliability.
  In this paper, we introduce RF-TCA, an enhancement to the standard Transfer Component Analysis approach that significantly accelerates computation without compromising theoretical and empirical performance. Leveraging the computational advantage of RF-TCA, we further extend it to FDA setting with FedRF-TCA. The proposed FedRF-TCA protocol boasts communication complexity that is independent of the sample size, while maintaining performance that is either comparable to or even surpasses state-of-the-art FDA methods. We present extensive experiments to showcase the superior performance and robustness (to network condition) of FedRF-TCA.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2311.04686v2 Announce Type: replace \nAbstract: Modern machine learning (ML) models have grown to a scale where training them on a single machine becomes impractical. As a result, there is a growing trend to leverage federated learning (FL) techniques to train large ML models in a distributed and collaborative manner. These models, however, when deployed on new devices, might struggle to generalize well due to domain shifts. In this context, federated domain adaptation (FDA) emerges as a powerful approach to address this challenge.\n  Most existing FDA approaches typically focus on aligning the distributions between source and target domains by minimizing their (e.g., MMD) distance. Such strategies, however, inevitably introduce high communication overheads and can be highly sensitive to network reliability.\n  In this paper, we introduce RF-TCA, an enhancement to the standard Transfer Component Analysis approach that significantly accelerates computation without compromising theoretical and empirical performance. Leveraging the computational advantage of RF-TCA, we further extend it to FDA setting with FedRF-TCA. The proposed FedRF-TCA protocol boasts communication complexity that is independent of the sample size, while maintaining performance that is either comparable to or even surpasses state-of-the-art FDA methods. We present extensive experiments to showcase the superior performance and robustness (to network condition) of FedRF-TCA.'}",oai:arXiv.org:2311.04686v2,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'cs.DC', 'scheme': None, 'label': None}, {'term': 'stat.ML', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Zhanbo Feng, Yuanjie Wang, Jie Li, Fan Yang, Jiong Lou, Tiebin Mi, Robert. C. Qiu, Zhenyu Liao'}]","Zhanbo Feng, Yuanjie Wang, Jie Li, Fan Yang, Jiong Lou, Tiebin Mi, Robert. C. Qiu, Zhenyu Liao","{'name': 'Zhanbo Feng, Yuanjie Wang, Jie Li, Fan Yang, Jiong Lou, Tiebin Mi, Robert. C. Qiu, Zhenyu Liao'}",10.1109/TKDE.2024.3510296,
507,A filtered multilevel Monte Carlo method for estimating the expectation of cell-centered discretized random fields,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'A filtered multilevel Monte Carlo method for estimating the expectation of cell-centered discretized random fields'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2311.06069'}]",https://arxiv.org/abs/2311.06069,"arXiv:2311.06069v2 Announce Type: replace 
Abstract: We investigate the use of multilevel Monte Carlo (MLMC) methods for estimating the expectation of discretized random fields. Specifically, we consider a setting in which the input and output vectors of numerical simulators have inconsistent dimensions across the multilevel hierarchy. This requires the introduction of grid transfer operators borrowed from multigrid methods. By adapting mathematical tools from multigrid methods, we perform a theoretical spectral analysis of the MLMC estimator of the expectation of discretized random fields, in the specific case of linear, symmetric and circulant simulators. We then propose filtered MLMC (F-MLMC) estimators based on a filtering mechanism similar to the smoothing process of multigrid methods, and we show that the filtering operators improve the estimation of both the small- and large-scale components of the variance, resulting in a reduction of the total variance of the estimator. Next, the conclusions of the spectral analysis are experimentally verified with a one-dimensional illustration. Finally, the proposed F-MLMC estimator is applied to the problem of estimating the discretized variance field of a diffusion-based covariance operator, which amounts to estimating the expectation of a discretized random field. The numerical experiments support the conclusions of the theoretical analysis even with non-linear simulators, and demonstrate the improvements brought by the F-MLMC estimator compared to both a crude MC and an unfiltered MLMC estimator.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2311.06069v2 Announce Type: replace \nAbstract: We investigate the use of multilevel Monte Carlo (MLMC) methods for estimating the expectation of discretized random fields. Specifically, we consider a setting in which the input and output vectors of numerical simulators have inconsistent dimensions across the multilevel hierarchy. This requires the introduction of grid transfer operators borrowed from multigrid methods. By adapting mathematical tools from multigrid methods, we perform a theoretical spectral analysis of the MLMC estimator of the expectation of discretized random fields, in the specific case of linear, symmetric and circulant simulators. We then propose filtered MLMC (F-MLMC) estimators based on a filtering mechanism similar to the smoothing process of multigrid methods, and we show that the filtering operators improve the estimation of both the small- and large-scale components of the variance, resulting in a reduction of the total variance of the estimator. Next, the conclusions of the spectral analysis are experimentally verified with a one-dimensional illustration. Finally, the proposed F-MLMC estimator is applied to the problem of estimating the discretized variance field of a diffusion-based covariance operator, which amounts to estimating the expectation of a discretized random field. The numerical experiments support the conclusions of the theoretical analysis even with non-linear simulators, and demonstrate the improvements brought by the F-MLMC estimator compared to both a crude MC and an unfiltered MLMC estimator.'}",oai:arXiv.org:2311.06069v2,False,"[{'term': 'math.NA', 'scheme': None, 'label': None}, {'term': 'cs.NA', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'J\\\'er\\\'emy Briant (Irit CNRS-INP, CECI CNRS-Cerfacs), Paul Mycek (CECI CNRS-Cerfacs), Mayeul Destouches (Met Office), Olivier Goux (CECI CNRS-Cerfacs), Serge Gratton (Irit CNRS-INP, ANITI), Selime G\\""urol (CECI CNRS-Cerfacs), Ehouarn Simon (Irit CNRS-INP), Anthony T. Weaver (CECI CNRS-Cerfacs)'}]","J\'er\'emy Briant (Irit CNRS-INP, CECI CNRS-Cerfacs), Paul Mycek (CECI CNRS-Cerfacs), Mayeul Destouches (Met Office), Olivier Goux (CECI CNRS-Cerfacs), Serge Gratton (Irit CNRS-INP, ANITI), Selime G\""urol (CECI CNRS-Cerfacs), Ehouarn Simon (Irit CNRS-INP), Anthony T. Weaver (CECI CNRS-Cerfacs)","{'name': 'J\\\'er\\\'emy Briant (Irit CNRS-INP, CECI CNRS-Cerfacs), Paul Mycek (CECI CNRS-Cerfacs), Mayeul Destouches (Met Office), Olivier Goux (CECI CNRS-Cerfacs), Serge Gratton (Irit CNRS-INP, ANITI), Selime G\\""urol (CECI CNRS-Cerfacs), Ehouarn Simon (Irit CNRS-INP), Anthony T. Weaver (CECI CNRS-Cerfacs)'}",,
508,MetaSymNet: A Tree-like Symbol Network with Adaptive Architecture and Activation Functions,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'MetaSymNet: A Tree-like Symbol Network with Adaptive Architecture and Activation Functions'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2311.07326'}]",https://arxiv.org/abs/2311.07326,"arXiv:2311.07326v2 Announce Type: replace 
Abstract: Mathematical formulas serve as the means of communication between humans and nature, encapsulating the operational laws governing natural phenomena. The concise formulation of these laws is a crucial objective in scientific research and an important challenge for artificial intelligence (AI). While traditional artificial neural networks (MLP) excel at data fitting, they often yield uninterpretable black box results that hinder our understanding of the relationship between variables x and predicted values y. Moreover, the fixed network architecture in MLP often gives rise to redundancy in both network structure and parameters. To address these issues, we propose MetaSymNet, a novel neural network that dynamically adjusts its structure in real-time, allowing for both expansion and contraction. This adaptive network employs the PANGU meta function as its activation function, which is a unique type capable of evolving into various basic functions during training to compose mathematical formulas tailored to specific needs. We then evolve the neural network into a concise, interpretable mathematical expression. To evaluate MetaSymNet's performance, we compare it with four state-of-the-art symbolic regression algorithms across more than 10 public datasets comprising 222 formulas. Our experimental results demonstrate that our algorithm outperforms others consistently regardless of noise presence or absence. Furthermore, we assess MetaSymNet against MLP and SVM regarding their fitting ability and extrapolation capability, these are two essential aspects of machine learning algorithms. The findings reveal that our algorithm excels in both areas. Finally, we compared MetaSymNet with MLP using iterative pruning in network structure complexity. The results show that MetaSymNet's network structure complexity is obviously less than MLP under the same goodness of fit.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2311.07326v2 Announce Type: replace \nAbstract: Mathematical formulas serve as the means of communication between humans and nature, encapsulating the operational laws governing natural phenomena. The concise formulation of these laws is a crucial objective in scientific research and an important challenge for artificial intelligence (AI). While traditional artificial neural networks (MLP) excel at data fitting, they often yield uninterpretable black box results that hinder our understanding of the relationship between variables x and predicted values y. Moreover, the fixed network architecture in MLP often gives rise to redundancy in both network structure and parameters. To address these issues, we propose MetaSymNet, a novel neural network that dynamically adjusts its structure in real-time, allowing for both expansion and contraction. This adaptive network employs the PANGU meta function as its activation function, which is a unique type capable of evolving into various basic functions during training to compose mathematical formulas tailored to specific needs. We then evolve the neural network into a concise, interpretable mathematical expression. To evaluate MetaSymNet's performance, we compare it with four state-of-the-art symbolic regression algorithms across more than 10 public datasets comprising 222 formulas. Our experimental results demonstrate that our algorithm outperforms others consistently regardless of noise presence or absence. Furthermore, we assess MetaSymNet against MLP and SVM regarding their fitting ability and extrapolation capability, these are two essential aspects of machine learning algorithms. The findings reveal that our algorithm excels in both areas. Finally, we compared MetaSymNet with MLP using iterative pruning in network structure complexity. The results show that MetaSymNet's network structure complexity is obviously less than MLP under the same goodness of fit.""}",oai:arXiv.org:2311.07326v2,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Yanjie Li, Weijun Li, Lina Yu, Min Wu, Jinyi Liu, Wenqiang Li, Meilan Hao, Shu Wei, Yusong Deng'}]","Yanjie Li, Weijun Li, Lina Yu, Min Wu, Jinyi Liu, Wenqiang Li, Meilan Hao, Shu Wei, Yusong Deng","{'name': 'Yanjie Li, Weijun Li, Lina Yu, Min Wu, Jinyi Liu, Wenqiang Li, Meilan Hao, Shu Wei, Yusong Deng'}",,
509,RankFeat&RankWeight: Rank-1 Feature/Weight Removal for Out-of-distribution Detection,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'RankFeat&RankWeight: Rank-1 Feature/Weight Removal for Out-of-distribution Detection'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2311.13959'}]",https://arxiv.org/abs/2311.13959,"arXiv:2311.13959v3 Announce Type: replace 
Abstract: The task of out-of-distribution (OOD) detection is crucial for deploying machine learning models in real-world settings. In this paper, we observe that the singular value distributions of the in-distribution (ID) and OOD features are quite different: the OOD feature matrix tends to have a larger dominant singular value than the ID feature, and the class predictions of OOD samples are largely determined by it. This observation motivates us to propose \texttt{RankFeat}, a simple yet effective \emph{post hoc} approach for OOD detection by removing the rank-1 matrix composed of the largest singular value and the associated singular vectors from the high-level feature. \texttt{RankFeat} achieves \emph{state-of-the-art} performance and reduces the average false positive rate (FPR95) by 17.90\% compared with the previous best method. The success of \texttt{RankFeat} motivates us to investigate whether a similar phenomenon would exist in the parameter matrices of neural networks. We thus propose \texttt{RankWeight} which removes the rank-1 weight from the parameter matrices of a single deep layer. Our \texttt{RankWeight}is also \emph{post hoc} and only requires computing the rank-1 matrix once. As a standalone approach, \texttt{RankWeight} has very competitive performance against other methods across various backbones. Moreover, \texttt{RankWeight} enjoys flexible compatibility with a wide range of OOD detection methods. The combination of \texttt{RankWeight} and \texttt{RankFeat} refreshes the new \emph{state-of-the-art} performance, achieving the FPR95 as low as 16.13\% on the ImageNet-1k benchmark. Extensive ablation studies and comprehensive theoretical analyses are presented to support the empirical results. Code is publicly available via \url{https://github.com/KingJamesSong/RankFeat}.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2311.13959v3 Announce Type: replace \nAbstract: The task of out-of-distribution (OOD) detection is crucial for deploying machine learning models in real-world settings. In this paper, we observe that the singular value distributions of the in-distribution (ID) and OOD features are quite different: the OOD feature matrix tends to have a larger dominant singular value than the ID feature, and the class predictions of OOD samples are largely determined by it. This observation motivates us to propose \\texttt{RankFeat}, a simple yet effective \\emph{post hoc} approach for OOD detection by removing the rank-1 matrix composed of the largest singular value and the associated singular vectors from the high-level feature. \\texttt{RankFeat} achieves \\emph{state-of-the-art} performance and reduces the average false positive rate (FPR95) by 17.90\\% compared with the previous best method. The success of \\texttt{RankFeat} motivates us to investigate whether a similar phenomenon would exist in the parameter matrices of neural networks. We thus propose \\texttt{RankWeight} which removes the rank-1 weight from the parameter matrices of a single deep layer. Our \\texttt{RankWeight}is also \\emph{post hoc} and only requires computing the rank-1 matrix once. As a standalone approach, \\texttt{RankWeight} has very competitive performance against other methods across various backbones. Moreover, \\texttt{RankWeight} enjoys flexible compatibility with a wide range of OOD detection methods. The combination of \\texttt{RankWeight} and \\texttt{RankFeat} refreshes the new \\emph{state-of-the-art} performance, achieving the FPR95 as low as 16.13\\% on the ImageNet-1k benchmark. Extensive ablation studies and comprehensive theoretical analyses are presented to support the empirical results. Code is publicly available via \\url{https://github.com/KingJamesSong/RankFeat}.'}",oai:arXiv.org:2311.13959v3,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by-nc-nd/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-nd/4.0/'}","[{'name': 'Yue Song, Wei Wang, Nicu Sebe'}]","Yue Song, Wei Wang, Nicu Sebe","{'name': 'Yue Song, Wei Wang, Nicu Sebe'}",,
510,Union-over-Intersections: Object Detection beyond Winner-Takes-All,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Union-over-Intersections: Object Detection beyond Winner-Takes-All'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2311.18512'}]",https://arxiv.org/abs/2311.18512,"arXiv:2311.18512v2 Announce Type: replace 
Abstract: This paper revisits the problem of predicting box locations in object detection architectures. Typically, each box proposal or box query aims to directly maximize the intersection-over-union score with the ground truth, followed by a winner-takes-all non-maximum suppression where only the highest scoring box in each region is retained. We observe that both steps are sub-optimal: the first involves regressing proposals to the entire ground truth, which is a difficult task even with large receptive fields, and the second neglects valuable information from boxes other than the top candidate. Instead of regressing proposals to the whole ground truth, we propose a simpler approach: regress only to the area of intersection between the proposal and the ground truth. This avoids the need for proposals to extrapolate beyond their visual scope, improving localization accuracy. Rather than adopting a winner-takes-all strategy, we take the union over the regressed intersections of all boxes in a region to generate the final box outputs. Our plug-and-play method integrates seamlessly into proposal-based, grid-based, and query-based detection architectures with minimal modifications, consistently improving object localization and instance segmentation. We demonstrate its broad applicability and versatility across various detection and segmentation tasks.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2311.18512v2 Announce Type: replace \nAbstract: This paper revisits the problem of predicting box locations in object detection architectures. Typically, each box proposal or box query aims to directly maximize the intersection-over-union score with the ground truth, followed by a winner-takes-all non-maximum suppression where only the highest scoring box in each region is retained. We observe that both steps are sub-optimal: the first involves regressing proposals to the entire ground truth, which is a difficult task even with large receptive fields, and the second neglects valuable information from boxes other than the top candidate. Instead of regressing proposals to the whole ground truth, we propose a simpler approach: regress only to the area of intersection between the proposal and the ground truth. This avoids the need for proposals to extrapolate beyond their visual scope, improving localization accuracy. Rather than adopting a winner-takes-all strategy, we take the union over the regressed intersections of all boxes in a region to generate the final box outputs. Our plug-and-play method integrates seamlessly into proposal-based, grid-based, and query-based detection architectures with minimal modifications, consistently improving object localization and instance segmentation. We demonstrate its broad applicability and versatility across various detection and segmentation tasks.'}",oai:arXiv.org:2311.18512v2,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Aritra Bhowmik, Pascal Mettes, Martin R. Oswald, Cees G. M. Snoek'}]","Aritra Bhowmik, Pascal Mettes, Martin R. Oswald, Cees G. M. Snoek","{'name': 'Aritra Bhowmik, Pascal Mettes, Martin R. Oswald, Cees G. M. Snoek'}",,
511,Agent-OM: Leveraging LLM Agents for Ontology Matching,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Agent-OM: Leveraging LLM Agents for Ontology Matching'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2312.00326'}]",https://arxiv.org/abs/2312.00326,"arXiv:2312.00326v5 Announce Type: replace 
Abstract: Ontology matching (OM) enables semantic interoperability between different ontologies and resolves their conceptual heterogeneity by aligning related entities. OM systems currently have two prevailing design paradigms: conventional knowledge-based expert systems and newer machine learning-based predictive systems. While large language models (LLMs) and LLM agents have revolutionised data engineering and have been applied creatively in many domains, their potential for OM remains underexplored. This study introduces a novel agent-powered LLM-based design paradigm for OM systems. With consideration of several specific challenges in leveraging LLM agents for OM, we propose a generic framework, namely Agent-OM (Agent for Ontology Matching), consisting of two Siamese agents for retrieval and matching, with a set of simple OM tools. Our framework is implemented in a proof-of-concept system. Evaluations of three Ontology Alignment Evaluation Initiative (OAEI) tracks over state-of-the-art OM systems show that our system can achieve results very close to the long-standing best performance on simple OM tasks and can significantly improve the performance on complex and few-shot OM tasks.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2312.00326v5 Announce Type: replace \nAbstract: Ontology matching (OM) enables semantic interoperability between different ontologies and resolves their conceptual heterogeneity by aligning related entities. OM systems currently have two prevailing design paradigms: conventional knowledge-based expert systems and newer machine learning-based predictive systems. While large language models (LLMs) and LLM agents have revolutionised data engineering and have been applied creatively in many domains, their potential for OM remains underexplored. This study introduces a novel agent-powered LLM-based design paradigm for OM systems. With consideration of several specific challenges in leveraging LLM agents for OM, we propose a generic framework, namely Agent-OM (Agent for Ontology Matching), consisting of two Siamese agents for retrieval and matching, with a set of simple OM tools. Our framework is implemented in a proof-of-concept system. Evaluations of three Ontology Alignment Evaluation Initiative (OAEI) tracks over state-of-the-art OM systems show that our system can achieve results very close to the long-standing best performance on simple OM tasks and can significantly improve the performance on complex and few-shot OM tasks.'}",oai:arXiv.org:2312.00326v5,False,"[{'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.CL', 'scheme': None, 'label': None}, {'term': 'cs.IR', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Zhangcheng Qiang, Weiqing Wang, Kerry Taylor'}]","Zhangcheng Qiang, Weiqing Wang, Kerry Taylor","{'name': 'Zhangcheng Qiang, Weiqing Wang, Kerry Taylor'}",,
512,Point Cloud Semantic Segmentation with Sparse and Inhomogeneous Annotations,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Point Cloud Semantic Segmentation with Sparse and Inhomogeneous Annotations'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2312.06259'}]",https://arxiv.org/abs/2312.06259,"arXiv:2312.06259v2 Announce Type: replace 
Abstract: Utilizing uniformly distributed sparse annotations, weakly supervised learning alleviates the heavy reliance on fine-grained annotations in point cloud semantic segmentation tasks. However, few works discuss the inhomogeneity of sparse annotations, albeit it is common in real-world scenarios. Therefore, this work introduces the probability density function into the gradient sampling approximation method to qualitatively analyze the impact of annotation sparsity and inhomogeneity under weakly supervised learning. Based on our analysis, we propose an Adaptive Annotation Distribution Network (AADNet) capable of robust learning on arbitrarily distributed sparse annotations. Specifically, we propose a label-aware point cloud downsampling strategy to increase the proportion of annotations involved in the training stage. Furthermore, we design the multiplicative dynamic entropy as the gradient calibration function to mitigate the gradient bias caused by non-uniformly distributed sparse annotations and explicitly reduce the epistemic uncertainty. Without any prior restrictions and additional information, our proposed method achieves comprehensive performance improvements at multiple label rates and different annotation distributions.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2312.06259v2 Announce Type: replace \nAbstract: Utilizing uniformly distributed sparse annotations, weakly supervised learning alleviates the heavy reliance on fine-grained annotations in point cloud semantic segmentation tasks. However, few works discuss the inhomogeneity of sparse annotations, albeit it is common in real-world scenarios. Therefore, this work introduces the probability density function into the gradient sampling approximation method to qualitatively analyze the impact of annotation sparsity and inhomogeneity under weakly supervised learning. Based on our analysis, we propose an Adaptive Annotation Distribution Network (AADNet) capable of robust learning on arbitrarily distributed sparse annotations. Specifically, we propose a label-aware point cloud downsampling strategy to increase the proportion of annotations involved in the training stage. Furthermore, we design the multiplicative dynamic entropy as the gradient calibration function to mitigate the gradient bias caused by non-uniformly distributed sparse annotations and explicitly reduce the epistemic uncertainty. Without any prior restrictions and additional information, our proposed method achieves comprehensive performance improvements at multiple label rates and different annotation distributions.'}",oai:arXiv.org:2312.06259v2,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Zhiyi Pan, Nan Zhang, Wei Gao, Shan Liu, Ge Li'}]","Zhiyi Pan, Nan Zhang, Wei Gao, Shan Liu, Ge Li","{'name': 'Zhiyi Pan, Nan Zhang, Wei Gao, Shan Liu, Ge Li'}",,
513,A flexible specification approach for verifying total correctness of fine-grained concurrent modules,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'A flexible specification approach for verifying total correctness of fine-grained concurrent modules'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2312.15379'}]",https://arxiv.org/abs/2312.15379,"arXiv:2312.15379v2 Announce Type: replace 
Abstract: A well-established approach to proving progress properties such as deadlock-freedom and termination is to associate obligations with threads. For example, in most existing work the proof rule for lock acquisition prescribes a standard usage protocol by burdening the acquiring thread with an obligation to release the lock. The fact that the obligation creation is hardcoded into the acquire operation, however, rules out non-standard clients e.g. where the release happens in a different thread.
  We overcome this limitation by instead having the blocking operations take the obligation creation operations required for the specific client scenario as arguments. We dub this simple instance of higher-order programming with auxiliary code Sassy. To illustrate Sassy, we extend HeapLang, a simple, higher-order, concurrent programming language with erasable code and state. The resulting language gets stuck if no progress is made. Consequently, we can apply standard safety separation logic to compositionally reason about termination in a fine-grained concurrent setting.
  We validated Sassy by developing (non-foundational) machine-checked proofs of representative locks -- an unfair Spinlock (competitive succession), a fair Ticketlock (direct handoff succession) and the hierarchically constructed Cohortlock that is starvation-free if the underlying locks are starvation-free -- against our specifications using an encoding of the approach in the VeriFast program verifier for C and Java.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2312.15379v2 Announce Type: replace \nAbstract: A well-established approach to proving progress properties such as deadlock-freedom and termination is to associate obligations with threads. For example, in most existing work the proof rule for lock acquisition prescribes a standard usage protocol by burdening the acquiring thread with an obligation to release the lock. The fact that the obligation creation is hardcoded into the acquire operation, however, rules out non-standard clients e.g. where the release happens in a different thread.\n  We overcome this limitation by instead having the blocking operations take the obligation creation operations required for the specific client scenario as arguments. We dub this simple instance of higher-order programming with auxiliary code Sassy. To illustrate Sassy, we extend HeapLang, a simple, higher-order, concurrent programming language with erasable code and state. The resulting language gets stuck if no progress is made. Consequently, we can apply standard safety separation logic to compositionally reason about termination in a fine-grained concurrent setting.\n  We validated Sassy by developing (non-foundational) machine-checked proofs of representative locks -- an unfair Spinlock (competitive succession), a fair Ticketlock (direct handoff succession) and the hierarchically constructed Cohortlock that is starvation-free if the underlying locks are starvation-free -- against our specifications using an encoding of the approach in the VeriFast program verifier for C and Java.'}",oai:arXiv.org:2312.15379v2,False,"[{'term': 'cs.PL', 'scheme': None, 'label': None}, {'term': 'cs.LO', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Justus Fasse, Bart Jacobs'}]","Justus Fasse, Bart Jacobs","{'name': 'Justus Fasse, Bart Jacobs'}",,
514,DualDynamics: Synergizing Implicit and Explicit Methods for Robust Irregular Time Series Analysis,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'DualDynamics: Synergizing Implicit and Explicit Methods for Robust Irregular Time Series Analysis'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2401.04979'}]",https://arxiv.org/abs/2401.04979,"arXiv:2401.04979v3 Announce Type: replace 
Abstract: Real-world time series analysis faces significant challenges when dealing with irregular and incomplete data. While Neural Differential Equation (NDE) based methods have shown promise, they struggle with limited expressiveness, scalability issues, and stability concerns. Conversely, Neural Flows offer stability but falter with irregular data. We introduce 'DualDynamics', a novel framework that synergistically combines NDE-based method and Neural Flow-based method. This approach enhances expressive power while balancing computational demands, addressing critical limitations of existing techniques. We demonstrate DualDynamics' effectiveness across diverse tasks: classification of robustness to dataset shift, irregularly-sampled series analysis, interpolation of missing data, and forecasting with partial observations. Our results show consistent outperformance over state-of-the-art methods, indicating DualDynamics' potential to advance irregular time series analysis significantly.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2401.04979v3 Announce Type: replace \nAbstract: Real-world time series analysis faces significant challenges when dealing with irregular and incomplete data. While Neural Differential Equation (NDE) based methods have shown promise, they struggle with limited expressiveness, scalability issues, and stability concerns. Conversely, Neural Flows offer stability but falter with irregular data. We introduce 'DualDynamics', a novel framework that synergistically combines NDE-based method and Neural Flow-based method. This approach enhances expressive power while balancing computational demands, addressing critical limitations of existing techniques. We demonstrate DualDynamics' effectiveness across diverse tasks: classification of robustness to dataset shift, irregularly-sampled series analysis, interpolation of missing data, and forecasting with partial observations. Our results show consistent outperformance over state-of-the-art methods, indicating DualDynamics' potential to advance irregular time series analysis significantly.""}",oai:arXiv.org:2401.04979v3,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'YongKyung Oh, Dongyoung Lim, Sungil Kim'}]","YongKyung Oh, Dongyoung Lim, Sungil Kim","{'name': 'YongKyung Oh, Dongyoung Lim, Sungil Kim'}",,
515,"Deep learning in motion deblurring: current status, benchmarks and future prospects","{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Deep learning in motion deblurring: current status, benchmarks and future prospects'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2401.05055'}]",https://arxiv.org/abs/2401.05055,"arXiv:2401.05055v2 Announce Type: replace 
Abstract: Motion deblurring is one of the fundamental problems of computer vision and has received continuous attention. The variability in blur, both within and across images, imposes limitations on non-blind deblurring techniques that rely on estimating the blur kernel. As a response, blind motion deblurring has emerged, aiming to restore clear and detailed images without prior knowledge of the blur type, fueled by the advancements in deep learning methodologies. Despite strides in this field, a comprehensive synthesis of recent progress in deep learning-based blind motion deblurring is notably absent. This paper fills that gap by providing an exhaustive overview of the role of deep learning in blind motion deblurring, encompassing datasets, evaluation metrics, and methods developed over the last six years. Specifically, we first introduce the types of motion blur and the fundamental principles of deblurring. Next, we outline the shortcomings of traditional non-blind deblurring algorithms, emphasizing the advantages of employing deep learning techniques for deblurring tasks. Following this, we categorize and summarize existing blind motion deblurring methods based on different backbone networks, including convolutional neural networks, generative adversarial networks, recurrent neural networks, and Transformer networks. Subsequently, we elaborate not only on the fundamental principles of these different categories but also provide a comprehensive summary and comparison of their advantages and limitations. Qualitative and quantitative experimental results conducted on four widely used datasets further compare the performance of SOTA methods. Finally, an analysis of present challenges and future pathways. All collected models, benchmark datasets, source code links, and codes for evaluation have been made publicly available at https://github.com/VisionVerse/Blind-Motion-Deblurring-Survey","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2401.05055v2 Announce Type: replace \nAbstract: Motion deblurring is one of the fundamental problems of computer vision and has received continuous attention. The variability in blur, both within and across images, imposes limitations on non-blind deblurring techniques that rely on estimating the blur kernel. As a response, blind motion deblurring has emerged, aiming to restore clear and detailed images without prior knowledge of the blur type, fueled by the advancements in deep learning methodologies. Despite strides in this field, a comprehensive synthesis of recent progress in deep learning-based blind motion deblurring is notably absent. This paper fills that gap by providing an exhaustive overview of the role of deep learning in blind motion deblurring, encompassing datasets, evaluation metrics, and methods developed over the last six years. Specifically, we first introduce the types of motion blur and the fundamental principles of deblurring. Next, we outline the shortcomings of traditional non-blind deblurring algorithms, emphasizing the advantages of employing deep learning techniques for deblurring tasks. Following this, we categorize and summarize existing blind motion deblurring methods based on different backbone networks, including convolutional neural networks, generative adversarial networks, recurrent neural networks, and Transformer networks. Subsequently, we elaborate not only on the fundamental principles of these different categories but also provide a comprehensive summary and comparison of their advantages and limitations. Qualitative and quantitative experimental results conducted on four widely used datasets further compare the performance of SOTA methods. Finally, an analysis of present challenges and future pathways. All collected models, benchmark datasets, source code links, and codes for evaluation have been made publicly available at https://github.com/VisionVerse/Blind-Motion-Deblurring-Survey'}",oai:arXiv.org:2401.05055v2,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Yawen Xiang, Heng Zhou, Chengyang Li, Fangwei Sun, Zhongbo Li, Yongqiang Xie'}]","Yawen Xiang, Heng Zhou, Chengyang Li, Fangwei Sun, Zhongbo Li, Yongqiang Xie","{'name': 'Yawen Xiang, Heng Zhou, Chengyang Li, Fangwei Sun, Zhongbo Li, Yongqiang Xie'}",10.1007/s00371-024-03632-8,
516,Wideband Beamforming for RIS Assisted Near-Field Communications,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Wideband Beamforming for RIS Assisted Near-Field Communications'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2401.11141'}]",https://arxiv.org/abs/2401.11141,"arXiv:2401.11141v3 Announce Type: replace 
Abstract: A near-field wideband beamforming scheme is investigated for reconfigurable intelligent surface (RIS) assisted multiple-input multiple-output (MIMO) systems, in which a deep learning-based end-to-end (E2E) optimization framework is proposed to maximize the system spectral efficiency. To deal with the near-field double beam split effect, the base station is equipped with frequency-dependent hybrid precoding architecture by introducing sub-connected true time delay (TTD) units, while two specific RIS architectures, namely true time delay-based RIS (TTD-RIS) and virtual subarray-based RIS (SA-RIS), are exploited to realize the frequency-dependent passive beamforming at the RIS. Furthermore, the efficient E2E beamforming models without explicit channel state information are proposed, which jointly exploits the uplink channel training module and the downlink wideband beamforming module. In the proposed network architecture of the E2E models, the classical communication signal processing methods, i.e., polarized filtering and sparsity transform, are leveraged to develop a signal-guided beamforming network. Numerical results show that the proposed E2E models have superior beamforming performance and robustness to conventional beamforming benchmarks. Furthermore, the tradeoff between the beamforming gain and the hardware complexity is investigated for different frequency-dependent RIS architectures, in which the TTD-RIS can achieve better spectral efficiency than the SA-RIS while requiring additional energy consumption and hardware cost.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2401.11141v3 Announce Type: replace \nAbstract: A near-field wideband beamforming scheme is investigated for reconfigurable intelligent surface (RIS) assisted multiple-input multiple-output (MIMO) systems, in which a deep learning-based end-to-end (E2E) optimization framework is proposed to maximize the system spectral efficiency. To deal with the near-field double beam split effect, the base station is equipped with frequency-dependent hybrid precoding architecture by introducing sub-connected true time delay (TTD) units, while two specific RIS architectures, namely true time delay-based RIS (TTD-RIS) and virtual subarray-based RIS (SA-RIS), are exploited to realize the frequency-dependent passive beamforming at the RIS. Furthermore, the efficient E2E beamforming models without explicit channel state information are proposed, which jointly exploits the uplink channel training module and the downlink wideband beamforming module. In the proposed network architecture of the E2E models, the classical communication signal processing methods, i.e., polarized filtering and sparsity transform, are leveraged to develop a signal-guided beamforming network. Numerical results show that the proposed E2E models have superior beamforming performance and robustness to conventional beamforming benchmarks. Furthermore, the tradeoff between the beamforming gain and the hardware complexity is investigated for different frequency-dependent RIS architectures, in which the TTD-RIS can achieve better spectral efficiency than the SA-RIS while requiring additional energy consumption and hardware cost.'}",oai:arXiv.org:2401.11141v3,False,"[{'term': 'cs.IT', 'scheme': None, 'label': None}, {'term': 'eess.SP', 'scheme': None, 'label': None}, {'term': 'math.IT', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Ji Wang, Jian Xiao, Yixuan Zou, Wenwu Xie, Yuanwei Liu'}]","Ji Wang, Jian Xiao, Yixuan Zou, Wenwu Xie, Yuanwei Liu","{'name': 'Ji Wang, Jian Xiao, Yixuan Zou, Wenwu Xie, Yuanwei Liu'}",10.1109/TWC.2024.3447570,"in IEEE Transactions on Wireless Communications, vol. 23, no. 11, pp. 16836-16851, Nov. 2024"
517,TurboSVM-FL: Boosting Federated Learning through SVM Aggregation for Lazy Clients,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'TurboSVM-FL: Boosting Federated Learning through SVM Aggregation for Lazy Clients'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2401.12012'}]",https://arxiv.org/abs/2401.12012,"arXiv:2401.12012v5 Announce Type: replace 
Abstract: Federated learning is a distributed collaborative machine learning paradigm that has gained strong momentum in recent years. In federated learning, a central server periodically coordinates models with clients and aggregates the models trained locally by clients without necessitating access to local data. Despite its potential, the implementation of federated learning continues to encounter several challenges, predominantly the slow convergence that is largely due to data heterogeneity. The slow convergence becomes particularly problematic in cross-device federated learning scenarios where clients may be strongly limited by computing power and storage space, and hence counteracting methods that induce additional computation or memory cost on the client side such as auxiliary objective terms and larger training iterations can be impractical. In this paper, we propose a novel federated aggregation strategy, TurboSVM-FL, that poses no additional computation burden on the client side and can significantly accelerate convergence for federated classification task, especially when clients are ""lazy"" and train their models solely for few epochs for next global aggregation. TurboSVM-FL extensively utilizes support vector machine to conduct selective aggregation and max-margin spread-out regularization on class embeddings. We evaluate TurboSVM-FL on multiple datasets including FEMNIST, CelebA, and Shakespeare using user-independent validation with non-iid data distribution. Our results show that TurboSVM-FL can significantly outperform existing popular algorithms on convergence rate and reduce communication rounds while delivering better test metrics including accuracy, F1 score, and MCC.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2401.12012v5 Announce Type: replace \nAbstract: Federated learning is a distributed collaborative machine learning paradigm that has gained strong momentum in recent years. In federated learning, a central server periodically coordinates models with clients and aggregates the models trained locally by clients without necessitating access to local data. Despite its potential, the implementation of federated learning continues to encounter several challenges, predominantly the slow convergence that is largely due to data heterogeneity. The slow convergence becomes particularly problematic in cross-device federated learning scenarios where clients may be strongly limited by computing power and storage space, and hence counteracting methods that induce additional computation or memory cost on the client side such as auxiliary objective terms and larger training iterations can be impractical. In this paper, we propose a novel federated aggregation strategy, TurboSVM-FL, that poses no additional computation burden on the client side and can significantly accelerate convergence for federated classification task, especially when clients are ""lazy"" and train their models solely for few epochs for next global aggregation. TurboSVM-FL extensively utilizes support vector machine to conduct selective aggregation and max-margin spread-out regularization on class embeddings. We evaluate TurboSVM-FL on multiple datasets including FEMNIST, CelebA, and Shakespeare using user-independent validation with non-iid data distribution. Our results show that TurboSVM-FL can significantly outperform existing popular algorithms on convergence rate and reduce communication rounds while delivering better test metrics including accuracy, F1 score, and MCC.'}",oai:arXiv.org:2401.12012v5,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'cs.DC', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Mengdi Wang, Anna Bodonhelyi, Efe Bozkir, Enkelejda Kasneci'}]","Mengdi Wang, Anna Bodonhelyi, Efe Bozkir, Enkelejda Kasneci","{'name': 'Mengdi Wang, Anna Bodonhelyi, Efe Bozkir, Enkelejda Kasneci'}",10.1609/aaai.v38i14.29481,Proceedings of the AAAI Conference on Artificial Intelligence 2024 (AAAI'24)
518,A Vision-Language Foundation Model to Enhance Efficiency of Chest X-ray Interpretation,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'A Vision-Language Foundation Model to Enhance Efficiency of Chest X-ray Interpretation'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2401.12208'}]",https://arxiv.org/abs/2401.12208,"arXiv:2401.12208v2 Announce Type: replace 
Abstract: Over 1.4 billion chest X-rays (CXRs) are performed annually due to their cost-effectiveness as an initial diagnostic test. This scale of radiological studies provides a significant opportunity to streamline CXR interpretation and documentation. While foundation models are a promising solution, the lack of publicly available large-scale datasets and benchmarks inhibits their iterative development and real-world evaluation. To overcome these challenges, we constructed a large-scale dataset (CheXinstruct), which we utilized to train a vision-language foundation model (CheXagent). We systematically demonstrated competitive performance across eight distinct task types on our novel evaluation benchmark (CheXbench). Beyond technical validation, we assessed the real-world utility of CheXagent in directly drafting radiology reports. Our clinical assessment with eight radiologists revealed a 36% time saving for residents using CheXagent-drafted reports, while attending radiologists showed no significant time difference editing resident-drafted or CheXagent-drafted reports. The CheXagent-drafted reports improved the writing efficiency of both radiology residents and attending radiologists in 81% and 61% of cases, respectively, without loss of quality. Overall, we demonstrate that CheXagent can effectively perform a variety of CXR interpretation tasks and holds potential to assist radiologists in routine clinical workflows.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2401.12208v2 Announce Type: replace \nAbstract: Over 1.4 billion chest X-rays (CXRs) are performed annually due to their cost-effectiveness as an initial diagnostic test. This scale of radiological studies provides a significant opportunity to streamline CXR interpretation and documentation. While foundation models are a promising solution, the lack of publicly available large-scale datasets and benchmarks inhibits their iterative development and real-world evaluation. To overcome these challenges, we constructed a large-scale dataset (CheXinstruct), which we utilized to train a vision-language foundation model (CheXagent). We systematically demonstrated competitive performance across eight distinct task types on our novel evaluation benchmark (CheXbench). Beyond technical validation, we assessed the real-world utility of CheXagent in directly drafting radiology reports. Our clinical assessment with eight radiologists revealed a 36% time saving for residents using CheXagent-drafted reports, while attending radiologists showed no significant time difference editing resident-drafted or CheXagent-drafted reports. The CheXagent-drafted reports improved the writing efficiency of both radiology residents and attending radiologists in 81% and 61% of cases, respectively, without loss of quality. Overall, we demonstrate that CheXagent can effectively perform a variety of CXR interpretation tasks and holds potential to assist radiologists in routine clinical workflows.'}",oai:arXiv.org:2401.12208v2,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.CL', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Zhihong Chen, Maya Varma, Justin Xu, Magdalini Paschali, Dave Van Veen, Andrew Johnston, Alaa Youssef, Louis Blankemeier, Christian Bluethgen, Stephan Altmayer, Jeya Maria Jose Valanarasu, Mohamed Siddig Eltayeb Muneer, Eduardo Pontes Reis, Joseph Paul Cohen, Cameron Olsen, Tanishq Mathew Abraham, Emily B. Tsai, Christopher F. Beaulieu, Jenia Jitsev, Sergios Gatidis, Jean-Benoit Delbrouck, Akshay S. Chaudhari, Curtis P. Langlotz'}]","Zhihong Chen, Maya Varma, Justin Xu, Magdalini Paschali, Dave Van Veen, Andrew Johnston, Alaa Youssef, Louis Blankemeier, Christian Bluethgen, Stephan Altmayer, Jeya Maria Jose Valanarasu, Mohamed Siddig Eltayeb Muneer, Eduardo Pontes Reis, Joseph Paul Cohen, Cameron Olsen, Tanishq Mathew Abraham, Emily B. Tsai, Christopher F. Beaulieu, Jenia Jitsev, Sergios Gatidis, Jean-Benoit Delbrouck, Akshay S. Chaudhari, Curtis P. Langlotz","{'name': 'Zhihong Chen, Maya Varma, Justin Xu, Magdalini Paschali, Dave Van Veen, Andrew Johnston, Alaa Youssef, Louis Blankemeier, Christian Bluethgen, Stephan Altmayer, Jeya Maria Jose Valanarasu, Mohamed Siddig Eltayeb Muneer, Eduardo Pontes Reis, Joseph Paul Cohen, Cameron Olsen, Tanishq Mathew Abraham, Emily B. Tsai, Christopher F. Beaulieu, Jenia Jitsev, Sergios Gatidis, Jean-Benoit Delbrouck, Akshay S. Chaudhari, Curtis P. Langlotz'}",,
519,From Training-Free to Adaptive: Empirical Insights into MLLMs' Understanding of Detection Information,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""From Training-Free to Adaptive: Empirical Insights into MLLMs' Understanding of Detection Information""}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2401.17981'}]",https://arxiv.org/abs/2401.17981,"arXiv:2401.17981v3 Announce Type: replace 
Abstract: Despite the impressive capabilities of Multimodal Large Language Models (MLLMs) in integrating text and image modalities, challenges remain in accurately interpreting detailed visual elements. Vision detection models excel at recognizing fine-grained image details, prompting researchers to use them to enhance MLLMs. One effective strategy is to infuse detection information in text format, which has proven simple and effective. However, most studies utilize this method without training, leaving the potential of adaptive training largely unexplored. Adaptive training could significantly enhance MLLMs' comprehension of unique inputs while filtering out irrelevant information. This paper addresses the crucial question: How does training impact MLLMs' understanding of infused textual detection information? We systematically experiment with various representative models to evaluate the effects of training-free, retraining, and fine-tuning strategies. We also examine the influence of training on MLLMs' original abilities and the interchangeability of detection models. Our findings indicate that fine-tuning a pre-trained MLLM to incorporate textual detection information delivers superior results compared to training-free and retraining methods, improving performance by 6.71% across 10 widely recognized benchmarks. Furthermore, fine-tuning enables MLLMs to retain performance enhancements even when detection models are swapped, indicating improved understanding of formatted textual data. We release our codes to support further exploration of fusion strategies for vision detection models and the enhancement of MLLMs' fine-grained multimodal capabilities.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2401.17981v3 Announce Type: replace \nAbstract: Despite the impressive capabilities of Multimodal Large Language Models (MLLMs) in integrating text and image modalities, challenges remain in accurately interpreting detailed visual elements. Vision detection models excel at recognizing fine-grained image details, prompting researchers to use them to enhance MLLMs. One effective strategy is to infuse detection information in text format, which has proven simple and effective. However, most studies utilize this method without training, leaving the potential of adaptive training largely unexplored. Adaptive training could significantly enhance MLLMs' comprehension of unique inputs while filtering out irrelevant information. This paper addresses the crucial question: How does training impact MLLMs' understanding of infused textual detection information? We systematically experiment with various representative models to evaluate the effects of training-free, retraining, and fine-tuning strategies. We also examine the influence of training on MLLMs' original abilities and the interchangeability of detection models. Our findings indicate that fine-tuning a pre-trained MLLM to incorporate textual detection information delivers superior results compared to training-free and retraining methods, improving performance by 6.71% across 10 widely recognized benchmarks. Furthermore, fine-tuning enables MLLMs to retain performance enhancements even when detection models are swapped, indicating improved understanding of formatted textual data. We release our codes to support further exploration of fusion strategies for vision detection models and the enhancement of MLLMs' fine-grained multimodal capabilities.""}",oai:arXiv.org:2401.17981v3,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Qirui Jiao, Daoyuan Chen, Yilun Huang, Yaliang Li, Ying Shen'}]","Qirui Jiao, Daoyuan Chen, Yilun Huang, Yaliang Li, Ying Shen","{'name': 'Qirui Jiao, Daoyuan Chen, Yilun Huang, Yaliang Li, Ying Shen'}",,
520,DNS-Rec: Data-aware Neural Architecture Search for Recommender Systems,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'DNS-Rec: Data-aware Neural Architecture Search for Recommender Systems'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2402.00390'}]",https://arxiv.org/abs/2402.00390,"arXiv:2402.00390v2 Announce Type: replace 
Abstract: In the era of data proliferation, efficiently sifting through vast information to extract meaningful insights has become increasingly crucial. This paper addresses the computational overhead and resource inefficiency prevalent in existing Sequential Recommender Systems (SRSs). We introduce an innovative approach combining pruning methods with advanced model designs. Furthermore, we delve into resource-constrained Neural Architecture Search (NAS), an emerging technique in recommender systems, to optimize models in terms of FLOPs, latency, and energy consumption while maintaining or enhancing accuracy. Our principal contribution is the development of a Data-aware Neural Architecture Search for Recommender System (DNS-Rec). DNS-Rec is specifically designed to tailor compact network architectures for attention-based SRS models, thereby ensuring accuracy retention. It incorporates data-aware gates to enhance the performance of the recommendation network by learning information from historical user-item interactions. Moreover, DNS-Rec employs a dynamic resource constraint strategy, stabilizing the search process and yielding more suitable architectural solutions. We demonstrate the effectiveness of our approach through rigorous experiments conducted on three benchmark datasets, which highlight the superiority of DNS-Rec in SRSs. Our findings set a new standard for future research in efficient and accurate recommendation systems, marking a significant step forward in this rapidly evolving field.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2402.00390v2 Announce Type: replace \nAbstract: In the era of data proliferation, efficiently sifting through vast information to extract meaningful insights has become increasingly crucial. This paper addresses the computational overhead and resource inefficiency prevalent in existing Sequential Recommender Systems (SRSs). We introduce an innovative approach combining pruning methods with advanced model designs. Furthermore, we delve into resource-constrained Neural Architecture Search (NAS), an emerging technique in recommender systems, to optimize models in terms of FLOPs, latency, and energy consumption while maintaining or enhancing accuracy. Our principal contribution is the development of a Data-aware Neural Architecture Search for Recommender System (DNS-Rec). DNS-Rec is specifically designed to tailor compact network architectures for attention-based SRS models, thereby ensuring accuracy retention. It incorporates data-aware gates to enhance the performance of the recommendation network by learning information from historical user-item interactions. Moreover, DNS-Rec employs a dynamic resource constraint strategy, stabilizing the search process and yielding more suitable architectural solutions. We demonstrate the effectiveness of our approach through rigorous experiments conducted on three benchmark datasets, which highlight the superiority of DNS-Rec in SRSs. Our findings set a new standard for future research in efficient and accurate recommendation systems, marking a significant step forward in this rapidly evolving field.'}",oai:arXiv.org:2402.00390v2,False,"[{'term': 'cs.IR', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Sheng Zhang, Maolin Wang, Yao Zhao, Chenyi Zhuang, Jinjie Gu, Ruocheng Guo, Xiangyu Zhao, Zijian Zhang, Hongzhi Yin'}]","Sheng Zhang, Maolin Wang, Yao Zhao, Chenyi Zhuang, Jinjie Gu, Ruocheng Guo, Xiangyu Zhao, Zijian Zhang, Hongzhi Yin","{'name': 'Sheng Zhang, Maolin Wang, Yao Zhao, Chenyi Zhuang, Jinjie Gu, Ruocheng Guo, Xiangyu Zhao, Zijian Zhang, Hongzhi Yin'}",,
521,Developing and Evaluating a Design Method for Positive Artificial Intelligence,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Developing and Evaluating a Design Method for Positive Artificial Intelligence'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2402.01499'}]",https://arxiv.org/abs/2402.01499,"arXiv:2402.01499v3 Announce Type: replace 
Abstract: As artificial intelligence (AI) continues advancing, ensuring positive societal impacts becomes critical, especially as AI systems become increasingly ubiquitous in various aspects of life. However, developing ""AI for good"" poses substantial challenges around aligning systems with complex human values. Presently, we lack mature methods for addressing these challenges. This article presents and evaluates the Positive AI design method aimed at addressing this gap. The method provides a human-centered process to translate wellbeing aspirations into concrete practices. First, we explain the method's four key steps: contextualizing, operationalizing, optimizing, and implementing wellbeing supported by continuous measurement for feedback cycles. We then present a multiple case study where novice designers applied the method, revealing strengths and weaknesses related to efficacy and usability. Next, an expert evaluation study assessed the quality of the resulting concepts, rating them moderately high for feasibility, desirability, and plausibility of achieving intended wellbeing benefits. Together, these studies provide preliminary validation of the method's ability to improve AI design, while surfacing areas needing refinement like developing support for complex steps. Proposed adaptations such as examples and evaluation heuristics could address weaknesses. Further research should examine sustained application over multiple projects. This human-centered approach shows promise for realizing the vision of 'AI for Wellbeing' that does not just avoid harm, but actively benefits humanity.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2402.01499v3 Announce Type: replace \nAbstract: As artificial intelligence (AI) continues advancing, ensuring positive societal impacts becomes critical, especially as AI systems become increasingly ubiquitous in various aspects of life. However, developing ""AI for good"" poses substantial challenges around aligning systems with complex human values. Presently, we lack mature methods for addressing these challenges. This article presents and evaluates the Positive AI design method aimed at addressing this gap. The method provides a human-centered process to translate wellbeing aspirations into concrete practices. First, we explain the method\'s four key steps: contextualizing, operationalizing, optimizing, and implementing wellbeing supported by continuous measurement for feedback cycles. We then present a multiple case study where novice designers applied the method, revealing strengths and weaknesses related to efficacy and usability. Next, an expert evaluation study assessed the quality of the resulting concepts, rating them moderately high for feasibility, desirability, and plausibility of achieving intended wellbeing benefits. Together, these studies provide preliminary validation of the method\'s ability to improve AI design, while surfacing areas needing refinement like developing support for complex steps. Proposed adaptations such as examples and evaluation heuristics could address weaknesses. Further research should examine sustained application over multiple projects. This human-centered approach shows promise for realizing the vision of \'AI for Wellbeing\' that does not just avoid harm, but actively benefits humanity.'}",oai:arXiv.org:2402.01499v3,False,"[{'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Willem van der Maden, Derek Lomas, Paul Hekkert'}]","Willem van der Maden, Derek Lomas, Paul Hekkert","{'name': 'Willem van der Maden, Derek Lomas, Paul Hekkert'}",10.1017/S0890060424000155,AIEDAM 38 (2024) e14
522,HotRAP: Hot Record Retention and Promotion for LSM-trees with Tiered Storage,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'HotRAP: Hot Record Retention and Promotion for LSM-trees with Tiered Storage'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2402.02070'}]",https://arxiv.org/abs/2402.02070,"arXiv:2402.02070v3 Announce Type: replace 
Abstract: The multi-level design of Log-Structured Merge-trees (LSM-trees) naturally fits the tiered storage architecture: the upper levels (recently inserted/updated records) are kept in fast storage to guarantee performance while the lower levels (the majority of records) are placed in slower but cheaper storage to reduce cost. However, frequently accessed records may have been compacted and reside in slow storage. Existing algorithms are inefficient in promoting these ``hot'' records to fast storage, leading to compromised read performance. We present HotRAP, a key-value store based on RocksDB that can timely promote hot records individually from slow to fast storage and keep them in fast storage while they are hot. HotRAP uses an on-disk data structure (a specially-made LSM-tree) to track the hotness of keys and includes three pathways to ensure that hot records reach fast storage with short delays. Our experiments show that HotRAP outperforms state-of-the-art LSM-trees on tiered storage by up to 5.4$\times$ compared to the second best under read-only and read-write-balanced YCSB workloads with common access skew patterns, and by up to 1.9$\times$ compared to the second best under Twitter production workloads.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2402.02070v3 Announce Type: replace \nAbstract: The multi-level design of Log-Structured Merge-trees (LSM-trees) naturally fits the tiered storage architecture: the upper levels (recently inserted/updated records) are kept in fast storage to guarantee performance while the lower levels (the majority of records) are placed in slower but cheaper storage to reduce cost. However, frequently accessed records may have been compacted and reside in slow storage. Existing algorithms are inefficient in promoting these ``hot'' records to fast storage, leading to compromised read performance. We present HotRAP, a key-value store based on RocksDB that can timely promote hot records individually from slow to fast storage and keep them in fast storage while they are hot. HotRAP uses an on-disk data structure (a specially-made LSM-tree) to track the hotness of keys and includes three pathways to ensure that hot records reach fast storage with short delays. Our experiments show that HotRAP outperforms state-of-the-art LSM-trees on tiered storage by up to 5.4$\\times$ compared to the second best under read-only and read-write-balanced YCSB workloads with common access skew patterns, and by up to 1.9$\\times$ compared to the second best under Twitter production workloads.""}",oai:arXiv.org:2402.02070v3,False,"[{'term': 'cs.DB', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Jiansheng Qiu, Fangzhou Yuan, Mingyu Gao, Huanchen Zhang'}]","Jiansheng Qiu, Fangzhou Yuan, Mingyu Gao, Huanchen Zhang","{'name': 'Jiansheng Qiu, Fangzhou Yuan, Mingyu Gao, Huanchen Zhang'}",,
523,XTSFormer: Cross-Temporal-Scale Transformer for Irregular-Time Event Prediction in Clinical Applications,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'XTSFormer: Cross-Temporal-Scale Transformer for Irregular-Time Event Prediction in Clinical Applications'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2402.02258'}]",https://arxiv.org/abs/2402.02258,"arXiv:2402.02258v2 Announce Type: replace 
Abstract: Adverse clinical events related to unsafe care are among the top ten causes of death in the U.S. Accurate modeling and prediction of clinical events from electronic health records (EHRs) play a crucial role in patient safety enhancement. An example is modeling de facto care pathways that characterize common step-by-step plans for treatment or care. However, clinical event data pose several unique challenges, including the irregularity of time intervals between consecutive events, the existence of cycles, periodicity, multi-scale event interactions, and the high computational costs associated with long event sequences. Existing neural temporal point processes (TPPs) methods do not effectively capture the multi-scale nature of event interactions, which is common in many real-world clinical applications. To address these issues, we propose the cross-temporal-scale transformer (XTSFormer), specifically designed for irregularly timed event data. Our model consists of two vital components: a novel Feature-based Cycle-aware Time Positional Encoding (FCPE) that adeptly captures the cyclical nature of time, and a hierarchical multi-scale temporal attention mechanism, where different temporal scales are determined by a bottom-up clustering approach. Extensive experiments on several real-world EHR datasets show that our XTSFormer outperforms multiple baseline methods. The code is available at https://github.com/spatialdatasciencegroup/XTSFormer.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2402.02258v2 Announce Type: replace \nAbstract: Adverse clinical events related to unsafe care are among the top ten causes of death in the U.S. Accurate modeling and prediction of clinical events from electronic health records (EHRs) play a crucial role in patient safety enhancement. An example is modeling de facto care pathways that characterize common step-by-step plans for treatment or care. However, clinical event data pose several unique challenges, including the irregularity of time intervals between consecutive events, the existence of cycles, periodicity, multi-scale event interactions, and the high computational costs associated with long event sequences. Existing neural temporal point processes (TPPs) methods do not effectively capture the multi-scale nature of event interactions, which is common in many real-world clinical applications. To address these issues, we propose the cross-temporal-scale transformer (XTSFormer), specifically designed for irregularly timed event data. Our model consists of two vital components: a novel Feature-based Cycle-aware Time Positional Encoding (FCPE) that adeptly captures the cyclical nature of time, and a hierarchical multi-scale temporal attention mechanism, where different temporal scales are determined by a bottom-up clustering approach. Extensive experiments on several real-world EHR datasets show that our XTSFormer outperforms multiple baseline methods. The code is available at https://github.com/spatialdatasciencegroup/XTSFormer.'}",oai:arXiv.org:2402.02258v2,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Tingsong Xiao, Zelin Xu, Wenchong He, Zhengkun Xiao, Yupu Zhang, Zibo Liu, Shigang Chen, My T. Thai, Jiang Bian, Parisa Rashidi, Zhe Jiang'}]","Tingsong Xiao, Zelin Xu, Wenchong He, Zhengkun Xiao, Yupu Zhang, Zibo Liu, Shigang Chen, My T. Thai, Jiang Bian, Parisa Rashidi, Zhe Jiang","{'name': 'Tingsong Xiao, Zelin Xu, Wenchong He, Zhengkun Xiao, Yupu Zhang, Zibo Liu, Shigang Chen, My T. Thai, Jiang Bian, Parisa Rashidi, Zhe Jiang'}",,
524,From Prejudice to Parity: A New Approach to Debiasing Large Language Model Word Embeddings,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'From Prejudice to Parity: A New Approach to Debiasing Large Language Model Word Embeddings'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2402.11512'}]",https://arxiv.org/abs/2402.11512,"arXiv:2402.11512v5 Announce Type: replace 
Abstract: Embeddings play a pivotal role in the efficacy of Large Language Models. They are the bedrock on which these models grasp contextual relationships and foster a more nuanced understanding of language and consequently perform remarkably on a plethora of complex tasks that require a fundamental understanding of human language. Given that these embeddings themselves often reflect or exhibit bias, it stands to reason that these models may also inadvertently learn this bias. In this work, we build on the seminal previous work and propose DeepSoftDebias, an algorithm that uses a neural network to perform 'soft debiasing'. We exhaustively evaluate this algorithm across a variety of SOTA datasets, accuracy metrics, and challenging NLP tasks. We find that DeepSoftDebias outperforms the current state-of-the-art methods at reducing bias across gender, race, and religion.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2402.11512v5 Announce Type: replace \nAbstract: Embeddings play a pivotal role in the efficacy of Large Language Models. They are the bedrock on which these models grasp contextual relationships and foster a more nuanced understanding of language and consequently perform remarkably on a plethora of complex tasks that require a fundamental understanding of human language. Given that these embeddings themselves often reflect or exhibit bias, it stands to reason that these models may also inadvertently learn this bias. In this work, we build on the seminal previous work and propose DeepSoftDebias, an algorithm that uses a neural network to perform 'soft debiasing'. We exhaustively evaluate this algorithm across a variety of SOTA datasets, accuracy metrics, and challenging NLP tasks. We find that DeepSoftDebias outperforms the current state-of-the-art methods at reducing bias across gender, race, and religion.""}",oai:arXiv.org:2402.11512v5,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}, {'term': 'cs.CY', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Aishik Rakshit, Smriti Singh, Shuvam Keshari, Arijit Ghosh Chowdhury, Vinija Jain, Aman Chadha'}]","Aishik Rakshit, Smriti Singh, Shuvam Keshari, Arijit Ghosh Chowdhury, Vinija Jain, Aman Chadha","{'name': 'Aishik Rakshit, Smriti Singh, Shuvam Keshari, Arijit Ghosh Chowdhury, Vinija Jain, Aman Chadha'}",,
525,Contractivity of neural ODEs: an eigenvalue optimization problem,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Contractivity of neural ODEs: an eigenvalue optimization problem'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2402.13092'}]",https://arxiv.org/abs/2402.13092,"arXiv:2402.13092v3 Announce Type: replace 
Abstract: We propose a novel methodology to solve a key eigenvalue optimization problem which arises in the contractivity analysis of neural ODEs. When looking at contractivity properties of a one layer weight-tied neural ODE $\dot{u}(t)=\sigma(Au(t)+b)$ (with $u,b \in {\mathbb R}^n$, $A$ is a given $n \times n$ matrix, $\sigma : {\mathbb R} \to {\mathbb R}$ denotes an activation function and for a vector $z \in {\mathbb R}^n$, $\sigma(z) \in {\mathbb R}^n$ has to be interpreted entry-wise), we are led to study the logarithmic norm of a set of products of type $D A$, where $D$ is a diagonal matrix such that ${\mathrm{diag}}(D) \in \sigma'({\mathbb R}^n)$. Specifically, given a real number $c$ (usually $c=0$), the problem consists in finding the largest positive interval $\text{I}\subseteq \mathbb [0,\infty)$ such that the logarithmic norm $\mu(DA) \le c$ for all diagonal matrices $D$ with $D_{ii}\in \text{I}$. We propose a two-level nested methodology: an inner level where, for a given $\text{I}$, we compute an optimizer $D^\star(\text{I})$ by a gradient system approach, and an outer level where we tune $\text{I}$ so that the value $c$ is reached by $\mu(D^\star(\text{I})A)$. We extend the proposed two-level approach to the general multilayer, and possibly time-dependent, case $\dot{u}(t) = \sigma( A_k(t) \ldots \sigma ( A_{1}(t) u(t) + b_{1}(t) ) \ldots + b_{k}(t) )$ and we propose several numerical examples to illustrate its behaviour, including its stabilizing performance on a one-layer neural ODE applied to the classification of the MNIST handwritten digits dataset.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2402.13092v3 Announce Type: replace \nAbstract: We propose a novel methodology to solve a key eigenvalue optimization problem which arises in the contractivity analysis of neural ODEs. When looking at contractivity properties of a one layer weight-tied neural ODE $\\dot{u}(t)=\\sigma(Au(t)+b)$ (with $u,b \\in {\\mathbb R}^n$, $A$ is a given $n \\times n$ matrix, $\\sigma : {\\mathbb R} \\to {\\mathbb R}$ denotes an activation function and for a vector $z \\in {\\mathbb R}^n$, $\\sigma(z) \\in {\\mathbb R}^n$ has to be interpreted entry-wise), we are led to study the logarithmic norm of a set of products of type $D A$, where $D$ is a diagonal matrix such that ${\\mathrm{diag}}(D) \\in \\sigma'({\\mathbb R}^n)$. Specifically, given a real number $c$ (usually $c=0$), the problem consists in finding the largest positive interval $\\text{I}\\subseteq \\mathbb [0,\\infty)$ such that the logarithmic norm $\\mu(DA) \\le c$ for all diagonal matrices $D$ with $D_{ii}\\in \\text{I}$. We propose a two-level nested methodology: an inner level where, for a given $\\text{I}$, we compute an optimizer $D^\\star(\\text{I})$ by a gradient system approach, and an outer level where we tune $\\text{I}$ so that the value $c$ is reached by $\\mu(D^\\star(\\text{I})A)$. We extend the proposed two-level approach to the general multilayer, and possibly time-dependent, case $\\dot{u}(t) = \\sigma( A_k(t) \\ldots \\sigma ( A_{1}(t) u(t) + b_{1}(t) ) \\ldots + b_{k}(t) )$ and we propose several numerical examples to illustrate its behaviour, including its stabilizing performance on a one-layer neural ODE applied to the classification of the MNIST handwritten digits dataset.""}",oai:arXiv.org:2402.13092v3,False,"[{'term': 'math.NA', 'scheme': None, 'label': None}, {'term': 'cs.NA', 'scheme': None, 'label': None}, {'term': 'math.OC', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Nicola Guglielmi, Arturo De Marinis, Anton Savostianov, Francesco Tudisco'}]","Nicola Guglielmi, Arturo De Marinis, Anton Savostianov, Francesco Tudisco","{'name': 'Nicola Guglielmi, Arturo De Marinis, Anton Savostianov, Francesco Tudisco'}",,
526,Hands-Free VR,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Hands-Free VR'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2402.15083'}]",https://arxiv.org/abs/2402.15083,"arXiv:2402.15083v2 Announce Type: replace 
Abstract: The paper introduces Hands-Free VR, a voice-based natural-language interface for VR. The user gives a command using their voice, the speech audio data is converted to text using a speech-to-text deep learning model that is fine-tuned for robustness to word phonetic similarity and to spoken English accents, and the text is mapped to an executable VR command using a large language model that is robust to natural language diversity. Hands-Free VR was evaluated in a controlled within-subjects study (N = 22) that asked participants to find specific objects and to place them in various configurations. In the control condition participants used a conventional VR user interface to grab, carry, and position the objects using the handheld controllers. In the experimental condition participants used Hands-Free VR. The results confirm that: (1) Hands-Free VR is robust to spoken English accents, as for 20 of our participants English was not their first language, and to word phonetic similarity, correctly transcribing the voice command 96.71% of the time; (2) Hands-Free VR is robust to natural language diversity, correctly mapping the transcribed command to an executable command in 97.83% of the time; (3) Hands-Free VR had a significant efficiency advantage over the conventional VR interface in terms of task completion time, total viewpoint translation, total view direction rotation, and total left and right hand translations; (4) Hands-Free VR received high user preference ratings in terms of ease of use, intuitiveness, ergonomics, reliability, and desirability.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2402.15083v2 Announce Type: replace \nAbstract: The paper introduces Hands-Free VR, a voice-based natural-language interface for VR. The user gives a command using their voice, the speech audio data is converted to text using a speech-to-text deep learning model that is fine-tuned for robustness to word phonetic similarity and to spoken English accents, and the text is mapped to an executable VR command using a large language model that is robust to natural language diversity. Hands-Free VR was evaluated in a controlled within-subjects study (N = 22) that asked participants to find specific objects and to place them in various configurations. In the control condition participants used a conventional VR user interface to grab, carry, and position the objects using the handheld controllers. In the experimental condition participants used Hands-Free VR. The results confirm that: (1) Hands-Free VR is robust to spoken English accents, as for 20 of our participants English was not their first language, and to word phonetic similarity, correctly transcribing the voice command 96.71% of the time; (2) Hands-Free VR is robust to natural language diversity, correctly mapping the transcribed command to an executable command in 97.83% of the time; (3) Hands-Free VR had a significant efficiency advantage over the conventional VR interface in terms of task completion time, total viewpoint translation, total view direction rotation, and total left and right hand translations; (4) Hands-Free VR received high user preference ratings in terms of ease of use, intuitiveness, ergonomics, reliability, and desirability.'}",oai:arXiv.org:2402.15083v2,False,"[{'term': 'cs.HC', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.CL', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': ""Jorge Askur Vazquez Fernandez, Jae Joong Lee, Santiago Andr\\'es Serrano Vacca, Alejandra Magana, Radim Pesam, Bedrich Benes, Voicu Popescu""}]","Jorge Askur Vazquez Fernandez, Jae Joong Lee, Santiago Andr\'es Serrano Vacca, Alejandra Magana, Radim Pesam, Bedrich Benes, Voicu Popescu","{'name': ""Jorge Askur Vazquez Fernandez, Jae Joong Lee, Santiago Andr\\'es Serrano Vacca, Alejandra Magana, Radim Pesam, Bedrich Benes, Voicu Popescu""}",,
527,Deep Reinforcement Learning Enhanced Rate-Splitting Multiple Access for Interference Mitigation,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Deep Reinforcement Learning Enhanced Rate-Splitting Multiple Access for Interference Mitigation'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2403.05974'}]",https://arxiv.org/abs/2403.05974,"arXiv:2403.05974v2 Announce Type: replace 
Abstract: This study explores the application of the rate-splitting multiple access (RSMA) technique, vital for interference mitigation in modern communication systems. It investigates the use of precoding methods in RSMA, especially in complex multiple-antenna interference channels, employing deep reinforcement learning. The aim is to optimize precoders and power allocation for common and private data streams involving multiple decision-makers. A multi-agent deep deterministic policy gradient (MADDPG) framework is employed to address this complexity, where decentralized agents collectively learn to optimize actions in a continuous policy space. We also explore the challenges posed by imperfect channel side information at the transmitter. Additionally, decoding order estimation is addressed to determine the optimal decoding sequence for common and private data sequences. Simulation results demonstrate the effectiveness of the proposed RSMA method based on MADDPG, achieving the upper bound in single-antenna scenarios and closely approaching theoretical limits in multi-antenna scenarios. Comparative analysis shows superiority over other techniques such as MADDPG without rate-splitting, maximal ratio transmission (MRT), zero-forcing (ZF), and leakage-based precoding methods. These findings highlight the potential of deep reinforcement learning-driven RSMA in reducing interference and enhancing system performance in communication systems.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2403.05974v2 Announce Type: replace \nAbstract: This study explores the application of the rate-splitting multiple access (RSMA) technique, vital for interference mitigation in modern communication systems. It investigates the use of precoding methods in RSMA, especially in complex multiple-antenna interference channels, employing deep reinforcement learning. The aim is to optimize precoders and power allocation for common and private data streams involving multiple decision-makers. A multi-agent deep deterministic policy gradient (MADDPG) framework is employed to address this complexity, where decentralized agents collectively learn to optimize actions in a continuous policy space. We also explore the challenges posed by imperfect channel side information at the transmitter. Additionally, decoding order estimation is addressed to determine the optimal decoding sequence for common and private data sequences. Simulation results demonstrate the effectiveness of the proposed RSMA method based on MADDPG, achieving the upper bound in single-antenna scenarios and closely approaching theoretical limits in multi-antenna scenarios. Comparative analysis shows superiority over other techniques such as MADDPG without rate-splitting, maximal ratio transmission (MRT), zero-forcing (ZF), and leakage-based precoding methods. These findings highlight the potential of deep reinforcement learning-driven RSMA in reducing interference and enhancing system performance in communication systems.'}",oai:arXiv.org:2403.05974v2,False,"[{'term': 'cs.IT', 'scheme': None, 'label': None}, {'term': 'cs.MA', 'scheme': None, 'label': None}, {'term': 'eess.SP', 'scheme': None, 'label': None}, {'term': 'math.IT', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Osman Nuri Irkicatal, Elif Tugce Ceran, Melda Yuksel'}]","Osman Nuri Irkicatal, Elif Tugce Ceran, Melda Yuksel","{'name': 'Osman Nuri Irkicatal, Elif Tugce Ceran, Melda Yuksel'}",,
528,Hessian-free force-gradient integrators,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Hessian-free force-gradient integrators'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2403.10370'}]",https://arxiv.org/abs/2403.10370,"arXiv:2403.10370v2 Announce Type: replace 
Abstract: We propose a new framework of Hessian-free force-gradient integrators that do not require the analytical expression of the force-gradient term based on the Hessian of the potential. Due to that the new class of decomposition algorithms for separable Hamiltonian systems with quadratic kinetic energy may be particularly useful when applied to Hamiltonian systems where an evaluation of the Hessian is significantly more expensive than an evaluation of its gradient, e.g. in molecular dynamics simulations of classical systems. Numerical experiments of an N-body problem, as well as applications to the molecular dynamics step in the Hybrid Monte Carlo (HMC) algorithm for lattice simulations of the Schwinger model and Quantum Chromodynamics (QCD) verify these expectations.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2403.10370v2 Announce Type: replace \nAbstract: We propose a new framework of Hessian-free force-gradient integrators that do not require the analytical expression of the force-gradient term based on the Hessian of the potential. Due to that the new class of decomposition algorithms for separable Hamiltonian systems with quadratic kinetic energy may be particularly useful when applied to Hamiltonian systems where an evaluation of the Hessian is significantly more expensive than an evaluation of its gradient, e.g. in molecular dynamics simulations of classical systems. Numerical experiments of an N-body problem, as well as applications to the molecular dynamics step in the Hybrid Monte Carlo (HMC) algorithm for lattice simulations of the Schwinger model and Quantum Chromodynamics (QCD) verify these expectations.'}",oai:arXiv.org:2403.10370v2,False,"[{'term': 'math.NA', 'scheme': None, 'label': None}, {'term': 'cs.NA', 'scheme': None, 'label': None}, {'term': 'hep-lat', 'scheme': None, 'label': None}, {'term': 'math-ph', 'scheme': None, 'label': None}, {'term': 'math.MP', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by-nc-nd/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-nd/4.0/'}","[{'name': 'Kevin Sch\\""afers, Jacob Finkenrath, Michael G\\""unther, Francesco Knechtli'}]","Kevin Sch\""afers, Jacob Finkenrath, Michael G\""unther, Francesco Knechtli","{'name': 'Kevin Sch\\""afers, Jacob Finkenrath, Michael G\\""unther, Francesco Knechtli'}",,
529,PALM: Pushing Adaptive Learning Rate Mechanisms for Continual Test-Time Adaptation,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'PALM: Pushing Adaptive Learning Rate Mechanisms for Continual Test-Time Adaptation'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2403.10650'}]",https://arxiv.org/abs/2403.10650,"arXiv:2403.10650v3 Announce Type: replace 
Abstract: Real-world vision models in dynamic environments face rapid shifts in domain distributions, leading to decreased recognition performance. Using unlabeled test data, continuous test-time adaptation (CTTA) directly adjusts a pre-trained source discriminative model to these changing domains. A highly effective CTTA method involves applying layer-wise adaptive learning rates for selectively adapting pre-trained layers. However, it suffers from the poor estimation of domain shift and the inaccuracies arising from the pseudo-labels. This work aims to overcome these limitations by identifying layers for adaptation via quantifying model prediction uncertainty without relying on pseudo-labels. We utilize the magnitude of gradients as a metric, calculated by backpropagating the KL divergence between the softmax output and a uniform distribution, to select layers for further adaptation. Subsequently, for the parameters exclusively belonging to these selected layers, with the remaining ones frozen, we evaluate their sensitivity to approximate the domain shift and adjust their learning rates accordingly. We conduct extensive image classification experiments on CIFAR-10C, CIFAR-100C, and ImageNet-C, demonstrating the superior efficacy of our method compared to prior approaches.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2403.10650v3 Announce Type: replace \nAbstract: Real-world vision models in dynamic environments face rapid shifts in domain distributions, leading to decreased recognition performance. Using unlabeled test data, continuous test-time adaptation (CTTA) directly adjusts a pre-trained source discriminative model to these changing domains. A highly effective CTTA method involves applying layer-wise adaptive learning rates for selectively adapting pre-trained layers. However, it suffers from the poor estimation of domain shift and the inaccuracies arising from the pseudo-labels. This work aims to overcome these limitations by identifying layers for adaptation via quantifying model prediction uncertainty without relying on pseudo-labels. We utilize the magnitude of gradients as a metric, calculated by backpropagating the KL divergence between the softmax output and a uniform distribution, to select layers for further adaptation. Subsequently, for the parameters exclusively belonging to these selected layers, with the remaining ones frozen, we evaluate their sensitivity to approximate the domain shift and adjust their learning rates accordingly. We conduct extensive image classification experiments on CIFAR-10C, CIFAR-100C, and ImageNet-C, demonstrating the superior efficacy of our method compared to prior approaches.'}",oai:arXiv.org:2403.10650v3,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Sarthak Kumar Maharana, Baoming Zhang, Yunhui Guo'}]","Sarthak Kumar Maharana, Baoming Zhang, Yunhui Guo","{'name': 'Sarthak Kumar Maharana, Baoming Zhang, Yunhui Guo'}",,
530,AGFSync: Leveraging AI-Generated Feedback for Preference Optimization in Text-to-Image Generation,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'AGFSync: Leveraging AI-Generated Feedback for Preference Optimization in Text-to-Image Generation'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2403.13352'}]",https://arxiv.org/abs/2403.13352,"arXiv:2403.13352v5 Announce Type: replace 
Abstract: Text-to-Image (T2I) diffusion models have achieved remarkable success in image generation. Despite their progress, challenges remain in both prompt-following ability, image quality and lack of high-quality datasets, which are essential for refining these models. As acquiring labeled data is costly, we introduce AGFSync, a framework that enhances T2I diffusion models through Direct Preference Optimization (DPO) in a fully AI-driven approach. AGFSync utilizes Vision-Language Models (VLM) to assess image quality across style, coherence, and aesthetics, generating feedback data within an AI-driven loop. By applying AGFSync to leading T2I models such as SD v1.4, v1.5, and SDXL-base, our extensive experiments on the TIFA dataset demonstrate notable improvements in VQA scores, aesthetic evaluations, and performance on the HPSv2 benchmark, consistently outperforming the base models. AGFSync's method of refining T2I diffusion models paves the way for scalable alignment techniques. Our code and dataset are publicly available at https://anjingkun.github.io/AGFSync.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2403.13352v5 Announce Type: replace \nAbstract: Text-to-Image (T2I) diffusion models have achieved remarkable success in image generation. Despite their progress, challenges remain in both prompt-following ability, image quality and lack of high-quality datasets, which are essential for refining these models. As acquiring labeled data is costly, we introduce AGFSync, a framework that enhances T2I diffusion models through Direct Preference Optimization (DPO) in a fully AI-driven approach. AGFSync utilizes Vision-Language Models (VLM) to assess image quality across style, coherence, and aesthetics, generating feedback data within an AI-driven loop. By applying AGFSync to leading T2I models such as SD v1.4, v1.5, and SDXL-base, our extensive experiments on the TIFA dataset demonstrate notable improvements in VQA scores, aesthetic evaluations, and performance on the HPSv2 benchmark, consistently outperforming the base models. AGFSync's method of refining T2I diffusion models paves the way for scalable alignment techniques. Our code and dataset are publicly available at https://anjingkun.github.io/AGFSync.""}",oai:arXiv.org:2403.13352v5,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Jingkun An, Yinghao Zhu, Zongjian Li, Enshen Zhou, Haoran Feng, Xijie Huang, Bohua Chen, Yemin Shi, Chengwei Pan'}]","Jingkun An, Yinghao Zhu, Zongjian Li, Enshen Zhou, Haoran Feng, Xijie Huang, Bohua Chen, Yemin Shi, Chengwei Pan","{'name': 'Jingkun An, Yinghao Zhu, Zongjian Li, Enshen Zhou, Haoran Feng, Xijie Huang, Bohua Chen, Yemin Shi, Chengwei Pan'}",,
531,DepthFM: Fast Monocular Depth Estimation with Flow Matching,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'DepthFM: Fast Monocular Depth Estimation with Flow Matching'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2403.13788'}]",https://arxiv.org/abs/2403.13788,"arXiv:2403.13788v2 Announce Type: replace 
Abstract: Current discriminative depth estimation methods often produce blurry artifacts, while generative approaches suffer from slow sampling due to curvatures in the noise-to-depth transport. Our method addresses these challenges by framing depth estimation as a direct transport between image and depth distributions. We are the first to explore flow matching in this field, and we demonstrate that its interpolation trajectories enhance both training and sampling efficiency while preserving high performance. While generative models typically require extensive training data, we mitigate this dependency by integrating external knowledge from a pre-trained image diffusion model, enabling effective transfer even across differing objectives. To further boost our model performance, we employ synthetic data and utilize image-depth pairs generated by a discriminative model on an in-the-wild image dataset. As a generative model, our model can reliably estimate depth confidence, which provides an additional advantage. Our approach achieves competitive zero-shot performance on standard benchmarks of complex natural scenes while improving sampling efficiency and only requiring minimal synthetic data for training.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2403.13788v2 Announce Type: replace \nAbstract: Current discriminative depth estimation methods often produce blurry artifacts, while generative approaches suffer from slow sampling due to curvatures in the noise-to-depth transport. Our method addresses these challenges by framing depth estimation as a direct transport between image and depth distributions. We are the first to explore flow matching in this field, and we demonstrate that its interpolation trajectories enhance both training and sampling efficiency while preserving high performance. While generative models typically require extensive training data, we mitigate this dependency by integrating external knowledge from a pre-trained image diffusion model, enabling effective transfer even across differing objectives. To further boost our model performance, we employ synthetic data and utilize image-depth pairs generated by a discriminative model on an in-the-wild image dataset. As a generative model, our model can reliably estimate depth confidence, which provides an additional advantage. Our approach achieves competitive zero-shot performance on standard benchmarks of complex natural scenes while improving sampling efficiency and only requiring minimal synthetic data for training.'}",oai:arXiv.org:2403.13788v2,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Ming Gui, Johannes Schusterbauer, Ulrich Prestel, Pingchuan Ma, Dmytro Kotovenko, Olga Grebenkova, Stefan Andreas Baumann, Vincent Tao Hu, Bj\\""orn Ommer'}]","Ming Gui, Johannes Schusterbauer, Ulrich Prestel, Pingchuan Ma, Dmytro Kotovenko, Olga Grebenkova, Stefan Andreas Baumann, Vincent Tao Hu, Bj\""orn Ommer","{'name': 'Ming Gui, Johannes Schusterbauer, Ulrich Prestel, Pingchuan Ma, Dmytro Kotovenko, Olga Grebenkova, Stefan Andreas Baumann, Vincent Tao Hu, Bj\\""orn Ommer'}",,
532,Spectral Motion Alignment for Video Motion Transfer using Diffusion Models,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Spectral Motion Alignment for Video Motion Transfer using Diffusion Models'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2403.15249'}]",https://arxiv.org/abs/2403.15249,"arXiv:2403.15249v2 Announce Type: replace 
Abstract: The evolution of diffusion models has greatly impacted video generation and understanding. Particularly, text-to-video diffusion models (VDMs) have significantly facilitated the customization of input video with target appearance, motion, etc. Despite these advances, challenges persist in accurately distilling motion information from video frames. While existing works leverage the consecutive frame residual as the target motion vector, they inherently lack global motion context and are vulnerable to frame-wise distortions. To address this, we present Spectral Motion Alignment (SMA), a novel framework that refines and aligns motion vectors using Fourier and wavelet transforms. SMA learns motion patterns by incorporating frequency-domain regularization, facilitating the learning of whole-frame global motion dynamics, and mitigating spatial artifacts. Extensive experiments demonstrate SMA's efficacy in improving motion transfer while maintaining computational efficiency and compatibility across various video customization frameworks.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2403.15249v2 Announce Type: replace \nAbstract: The evolution of diffusion models has greatly impacted video generation and understanding. Particularly, text-to-video diffusion models (VDMs) have significantly facilitated the customization of input video with target appearance, motion, etc. Despite these advances, challenges persist in accurately distilling motion information from video frames. While existing works leverage the consecutive frame residual as the target motion vector, they inherently lack global motion context and are vulnerable to frame-wise distortions. To address this, we present Spectral Motion Alignment (SMA), a novel framework that refines and aligns motion vectors using Fourier and wavelet transforms. SMA learns motion patterns by incorporating frequency-domain regularization, facilitating the learning of whole-frame global motion dynamics, and mitigating spatial artifacts. Extensive experiments demonstrate SMA's efficacy in improving motion transfer while maintaining computational efficiency and compatibility across various video customization frameworks.""}",oai:arXiv.org:2403.15249v2,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Geon Yeong Park, Hyeonho Jeong, Sang Wan Lee, Jong Chul Ye'}]","Geon Yeong Park, Hyeonho Jeong, Sang Wan Lee, Jong Chul Ye","{'name': 'Geon Yeong Park, Hyeonho Jeong, Sang Wan Lee, Jong Chul Ye'}",,
533,Analyzing Consumer IoT Traffic from Security and Privacy Perspectives: a Comprehensive Survey,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Analyzing Consumer IoT Traffic from Security and Privacy Perspectives: a Comprehensive Survey'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2403.16149'}]",https://arxiv.org/abs/2403.16149,"arXiv:2403.16149v3 Announce Type: replace 
Abstract: The Consumer Internet of Things (CIoT), a notable segment within the IoT domain, involves the integration of IoT technology into consumer electronics and devices, such as smart homes and smart wearables. Compared to traditional IoT fields, CIoT differs notably in target users, product types, and design approaches. While offering convenience to users, it also raises new security and privacy concerns. Network traffic analysis, a widely used technique in the security community, has been extensively applied to investigate these concerns about CIoT. Compared to network traffic analysis in other fields such as mobile apps and websites, CIoT presents unique characteristics, introducing new challenges and research opportunities. Researchers have made significant contributions in this area. To aid researchers in understanding the application of traffic analysis tools for studying CIoT security and privacy risks, this survey reviews 303 publications on traffic analysis within the CIoT security and privacy domain from January 2018 to June 2024, focusing on three research questions. Our work: 1) outlines the CIoT traffic analysis process and highlights its differences from general network traffic analysis. 2) summarizes and classifies existing research into four categories according to its application objectives: device fingerprinting, user activity inference, malicious traffic detection, and measurement. 3) explores emerging challenges and potential future research directions based on each step of the CIoT traffic analysis process. This will provide new insights to the community and guide the industry towards safer product designs.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2403.16149v3 Announce Type: replace \nAbstract: The Consumer Internet of Things (CIoT), a notable segment within the IoT domain, involves the integration of IoT technology into consumer electronics and devices, such as smart homes and smart wearables. Compared to traditional IoT fields, CIoT differs notably in target users, product types, and design approaches. While offering convenience to users, it also raises new security and privacy concerns. Network traffic analysis, a widely used technique in the security community, has been extensively applied to investigate these concerns about CIoT. Compared to network traffic analysis in other fields such as mobile apps and websites, CIoT presents unique characteristics, introducing new challenges and research opportunities. Researchers have made significant contributions in this area. To aid researchers in understanding the application of traffic analysis tools for studying CIoT security and privacy risks, this survey reviews 303 publications on traffic analysis within the CIoT security and privacy domain from January 2018 to June 2024, focusing on three research questions. Our work: 1) outlines the CIoT traffic analysis process and highlights its differences from general network traffic analysis. 2) summarizes and classifies existing research into four categories according to its application objectives: device fingerprinting, user activity inference, malicious traffic detection, and measurement. 3) explores emerging challenges and potential future research directions based on each step of the CIoT traffic analysis process. This will provide new insights to the community and guide the industry towards safer product designs.'}",oai:arXiv.org:2403.16149v3,False,"[{'term': 'cs.CR', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Yan Jia, Yuxin Song, Zihou Liu, Qingyin Tan, Yang Song, Yu Zhang, Zheli Liu'}]","Yan Jia, Yuxin Song, Zihou Liu, Qingyin Tan, Yang Song, Yu Zhang, Zheli Liu","{'name': 'Yan Jia, Yuxin Song, Zihou Liu, Qingyin Tan, Yang Song, Yu Zhang, Zheli Liu'}",,
534,VHM: Versatile and Honest Vision Language Model for Remote Sensing Image Analysis,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'VHM: Versatile and Honest Vision Language Model for Remote Sensing Image Analysis'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2403.20213'}]",https://arxiv.org/abs/2403.20213,"arXiv:2403.20213v4 Announce Type: replace 
Abstract: This paper develops a Versatile and Honest vision language Model (VHM) for remote sensing image analysis. VHM is built on a large-scale remote sensing image-text dataset with rich-content captions (VersaD), and an honest instruction dataset comprising both factual and deceptive questions (HnstD). Unlike prevailing remote sensing image-text datasets, in which image captions focus on a few prominent objects and their relationships, VersaD captions provide detailed information about image properties, object attributes, and the overall scene. This comprehensive captioning enables VHM to thoroughly understand remote sensing images and perform diverse remote sensing tasks. Moreover, different from existing remote sensing instruction datasets that only include factual questions, HnstD contains additional deceptive questions stemming from the non-existence of objects. This feature prevents VHM from producing affirmative answers to nonsense queries, thereby ensuring its honesty. In our experiments, VHM significantly outperforms various vision language models on common tasks of scene classification, visual question answering, and visual grounding. Additionally, VHM achieves competent performance on several unexplored tasks, such as building vectorizing, multi-label classification and honest question answering. We will release the code, data and model weights at https://github.com/opendatalab/VHM .","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2403.20213v4 Announce Type: replace \nAbstract: This paper develops a Versatile and Honest vision language Model (VHM) for remote sensing image analysis. VHM is built on a large-scale remote sensing image-text dataset with rich-content captions (VersaD), and an honest instruction dataset comprising both factual and deceptive questions (HnstD). Unlike prevailing remote sensing image-text datasets, in which image captions focus on a few prominent objects and their relationships, VersaD captions provide detailed information about image properties, object attributes, and the overall scene. This comprehensive captioning enables VHM to thoroughly understand remote sensing images and perform diverse remote sensing tasks. Moreover, different from existing remote sensing instruction datasets that only include factual questions, HnstD contains additional deceptive questions stemming from the non-existence of objects. This feature prevents VHM from producing affirmative answers to nonsense queries, thereby ensuring its honesty. In our experiments, VHM significantly outperforms various vision language models on common tasks of scene classification, visual question answering, and visual grounding. Additionally, VHM achieves competent performance on several unexplored tasks, such as building vectorizing, multi-label classification and honest question answering. We will release the code, data and model weights at https://github.com/opendatalab/VHM .'}",oai:arXiv.org:2403.20213v4,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Chao Pang, Xingxing Weng, Jiang Wu, Jiayu Li, Yi Liu, Jiaxing Sun, Weijia Li, Shuai Wang, Litong Feng, Gui-Song Xia, Conghui He'}]","Chao Pang, Xingxing Weng, Jiang Wu, Jiayu Li, Yi Liu, Jiaxing Sun, Weijia Li, Shuai Wang, Litong Feng, Gui-Song Xia, Conghui He","{'name': 'Chao Pang, Xingxing Weng, Jiang Wu, Jiayu Li, Yi Liu, Jiaxing Sun, Weijia Li, Shuai Wang, Litong Feng, Gui-Song Xia, Conghui He'}",,
535,Fairness in Large Language Models: A Taxonomic Survey,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Fairness in Large Language Models: A Taxonomic Survey'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2404.01349'}]",https://arxiv.org/abs/2404.01349,"arXiv:2404.01349v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have demonstrated remarkable success across various domains. However, despite their promising performance in numerous real-world applications, most of these algorithms lack fairness considerations. Consequently, they may lead to discriminatory outcomes against certain communities, particularly marginalized populations, prompting extensive study in fair LLMs. On the other hand, fairness in LLMs, in contrast to fairness in traditional machine learning, entails exclusive backgrounds, taxonomies, and fulfillment techniques. To this end, this survey presents a comprehensive overview of recent advances in the existing literature concerning fair LLMs. Specifically, a brief introduction to LLMs is provided, followed by an analysis of factors contributing to bias in LLMs. Additionally, the concept of fairness in LLMs is discussed categorically, summarizing metrics for evaluating bias in LLMs and existing algorithms for promoting fairness. Furthermore, resources for evaluating bias in LLMs, including toolkits and datasets, are summarized. Finally, existing research challenges and open questions are discussed.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2404.01349v2 Announce Type: replace \nAbstract: Large Language Models (LLMs) have demonstrated remarkable success across various domains. However, despite their promising performance in numerous real-world applications, most of these algorithms lack fairness considerations. Consequently, they may lead to discriminatory outcomes against certain communities, particularly marginalized populations, prompting extensive study in fair LLMs. On the other hand, fairness in LLMs, in contrast to fairness in traditional machine learning, entails exclusive backgrounds, taxonomies, and fulfillment techniques. To this end, this survey presents a comprehensive overview of recent advances in the existing literature concerning fair LLMs. Specifically, a brief introduction to LLMs is provided, followed by an analysis of factors contributing to bias in LLMs. Additionally, the concept of fairness in LLMs is discussed categorically, summarizing metrics for evaluating bias in LLMs and existing algorithms for promoting fairness. Furthermore, resources for evaluating bias in LLMs, including toolkits and datasets, are summarized. Finally, existing research challenges and open questions are discussed.'}",oai:arXiv.org:2404.01349v2,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Zhibo Chu, Zichong Wang, Wenbin Zhang'}]","Zhibo Chu, Zichong Wang, Wenbin Zhang","{'name': 'Zhibo Chu, Zichong Wang, Wenbin Zhang'}",,
536,Reduction of Joule Losses in Memristive Switching Using Optimal Control,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Reduction of Joule Losses in Memristive Switching Using Optimal Control'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2404.01507'}]",https://arxiv.org/abs/2404.01507,"arXiv:2404.01507v5 Announce Type: replace 
Abstract: This study investigates strategies for minimizing Joule losses in resistive random access memory (ReRAM) cells, which are also referred to as memristive devices. Typically, the structure of ReRAM cells involves a nanoscale layer of resistance-switching material sandwiched between two metal electrodes. The basic question that we ask is what is the optimal driving protocol to switch a memristive device from one state to another. In the case of ideal memristors, in the most basic scenario, the optimal protocol is determined by solving a variational problem without constraints with the help of the Euler-Lagrange equation. In the case of memristive systems, for the same situation, the optimal protocol is found using the method of Lagrange multipliers. We demonstrate the advantages of our approaches through specific examples and compare our results with those of switching with constant voltage or current. Our findings suggest that voltage or current control can be used to reduce Joule losses in emerging memory devices.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2404.01507v5 Announce Type: replace \nAbstract: This study investigates strategies for minimizing Joule losses in resistive random access memory (ReRAM) cells, which are also referred to as memristive devices. Typically, the structure of ReRAM cells involves a nanoscale layer of resistance-switching material sandwiched between two metal electrodes. The basic question that we ask is what is the optimal driving protocol to switch a memristive device from one state to another. In the case of ideal memristors, in the most basic scenario, the optimal protocol is determined by solving a variational problem without constraints with the help of the Euler-Lagrange equation. In the case of memristive systems, for the same situation, the optimal protocol is found using the method of Lagrange multipliers. We demonstrate the advantages of our approaches through specific examples and compare our results with those of switching with constant voltage or current. Our findings suggest that voltage or current control can be used to reduce Joule losses in emerging memory devices.'}",oai:arXiv.org:2404.01507v5,False,"[{'term': 'cs.ET', 'scheme': None, 'label': None}, {'term': 'cond-mat.mes-hall', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Valeriy A. Slipko, Yuriy V. Pershin'}]","Valeriy A. Slipko, Yuriy V. Pershin","{'name': 'Valeriy A. Slipko, Yuriy V. Pershin'}",,
537,Hypothesis Generation with Large Language Models,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Hypothesis Generation with Large Language Models'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2404.04326'}]",https://arxiv.org/abs/2404.04326,"arXiv:2404.04326v3 Announce Type: replace 
Abstract: Effective generation of novel hypotheses is instrumental to scientific progress. So far, researchers have been the main powerhouse behind hypothesis generation by painstaking data analysis and thinking (also known as the Eureka moment). In this paper, we examine the potential of large language models (LLMs) to generate hypotheses. We focus on hypothesis generation based on data (i.e., labeled examples). To enable LLMs to handle arbitrarily long contexts, we generate initial hypotheses from a small number of examples and then update them iteratively to improve the quality of hypotheses. Inspired by multi-armed bandits, we design a reward function to inform the exploitation-exploration tradeoff in the update process. Our algorithm is able to generate hypotheses that enable much better predictive performance than few-shot prompting in classification tasks, improving accuracy by 31.7% on a synthetic dataset and by 13.9%, 3.3% and, 24.9% on three real-world datasets. We also outperform supervised learning by 12.8% and 11.2% on two challenging real-world datasets. Furthermore, we find that the generated hypotheses not only corroborate human-verified theories but also uncover new insights for the tasks.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2404.04326v3 Announce Type: replace \nAbstract: Effective generation of novel hypotheses is instrumental to scientific progress. So far, researchers have been the main powerhouse behind hypothesis generation by painstaking data analysis and thinking (also known as the Eureka moment). In this paper, we examine the potential of large language models (LLMs) to generate hypotheses. We focus on hypothesis generation based on data (i.e., labeled examples). To enable LLMs to handle arbitrarily long contexts, we generate initial hypotheses from a small number of examples and then update them iteratively to improve the quality of hypotheses. Inspired by multi-armed bandits, we design a reward function to inform the exploitation-exploration tradeoff in the update process. Our algorithm is able to generate hypotheses that enable much better predictive performance than few-shot prompting in classification tasks, improving accuracy by 31.7% on a synthetic dataset and by 13.9%, 3.3% and, 24.9% on three real-world datasets. We also outperform supervised learning by 12.8% and 11.2% on two challenging real-world datasets. Furthermore, we find that the generated hypotheses not only corroborate human-verified theories but also uncover new insights for the tasks.'}",oai:arXiv.org:2404.04326v3,False,"[{'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.CL', 'scheme': None, 'label': None}, {'term': 'cs.CY', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by-nc-sa/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-sa/4.0/'}","[{'name': 'Yangqiaoyu Zhou, Haokun Liu, Tejes Srivastava, Hongyuan Mei, Chenhao Tan'}]","Yangqiaoyu Zhou, Haokun Liu, Tejes Srivastava, Hongyuan Mei, Chenhao Tan","{'name': 'Yangqiaoyu Zhou, Haokun Liu, Tejes Srivastava, Hongyuan Mei, Chenhao Tan'}",10.18653/v1/2024.nlp4science-1.10,
538,Stable Learning Using Spiking Neural Networks Equipped With Affine Encoders and Decoders,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Stable Learning Using Spiking Neural Networks Equipped With Affine Encoders and Decoders'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2404.04549'}]",https://arxiv.org/abs/2404.04549,"arXiv:2404.04549v2 Announce Type: replace 
Abstract: We study the learning problem associated with spiking neural networks. Specifically, we focus on spiking neural networks composed of simple spiking neurons having only positive synaptic weights, equipped with an affine encoder and decoder. These neural networks are shown to depend continuously on their parameters, which facilitates classical covering number-based generalization statements and supports stable gradient-based training. We demonstrate that the positivity of the weights continues to enable a wide range of expressivity results, including rate-optimal approximation of smooth functions and dimension-independent approximation of Barron regular functions. In particular, we show in theory and simulations that affine spiking neural networks are capable of approximating shallow ReLU neural networks. Furthermore, we apply these neural networks to standard machine learning benchmarks, reaching competitive results. Finally, and remarkably, we observe that from a generalization perspective, contrary to feedforward neural networks or previous results for general spiking neural networks, the depth has little to no adverse effect on the generalization capabilities.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2404.04549v2 Announce Type: replace \nAbstract: We study the learning problem associated with spiking neural networks. Specifically, we focus on spiking neural networks composed of simple spiking neurons having only positive synaptic weights, equipped with an affine encoder and decoder. These neural networks are shown to depend continuously on their parameters, which facilitates classical covering number-based generalization statements and supports stable gradient-based training. We demonstrate that the positivity of the weights continues to enable a wide range of expressivity results, including rate-optimal approximation of smooth functions and dimension-independent approximation of Barron regular functions. In particular, we show in theory and simulations that affine spiking neural networks are capable of approximating shallow ReLU neural networks. Furthermore, we apply these neural networks to standard machine learning benchmarks, reaching competitive results. Finally, and remarkably, we observe that from a generalization perspective, contrary to feedforward neural networks or previous results for general spiking neural networks, the depth has little to no adverse effect on the generalization capabilities.'}",oai:arXiv.org:2404.04549v2,False,"[{'term': 'cs.NE', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'math.FA', 'scheme': None, 'label': None}, {'term': 'stat.ML', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'A. Martina Neuman, Dominik Dold, Philipp Christian Petersen'}]","A. Martina Neuman, Dominik Dold, Philipp Christian Petersen","{'name': 'A. Martina Neuman, Dominik Dold, Philipp Christian Petersen'}",,
539,The Church Synthesis Problem over Continuous Time,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'The Church Synthesis Problem over Continuous Time'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2404.04782'}]",https://arxiv.org/abs/2404.04782,"arXiv:2404.04782v2 Announce Type: replace 
Abstract: The Church Problem asks for the construction of a procedure which, given a logical specification A(I,O) between input omega-strings I and output omega-strings O, determines whether there exists an operator F that implements the specification in the sense that A(I, F(I)) holds for all inputs I. Buchi and Landweber provided a procedure to solve the Church problem for MSO specifications and operators computable by finite-state automata. We investigate a generalization of the Church synthesis problem to the continuous time domain of the non-negative reals.
  We show that in the continuous time domain there are phenomena which are very different from the canonical discrete time domain of the natural numbers.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2404.04782v2 Announce Type: replace \nAbstract: The Church Problem asks for the construction of a procedure which, given a logical specification A(I,O) between input omega-strings I and output omega-strings O, determines whether there exists an operator F that implements the specification in the sense that A(I, F(I)) holds for all inputs I. Buchi and Landweber provided a procedure to solve the Church problem for MSO specifications and operators computable by finite-state automata. We investigate a generalization of the Church synthesis problem to the continuous time domain of the non-negative reals.\n  We show that in the continuous time domain there are phenomena which are very different from the canonical discrete time domain of the natural numbers.'}",oai:arXiv.org:2404.04782v2,False,"[{'term': 'cs.LO', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Alexander Rabinovich, Daniel Fattal'}]","Alexander Rabinovich, Daniel Fattal","{'name': 'Alexander Rabinovich, Daniel Fattal'}",,
540,MoCha-Stereo: Motif Channel Attention Network for Stereo Matching,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'MoCha-Stereo: Motif Channel Attention Network for Stereo Matching'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2404.06842'}]",https://arxiv.org/abs/2404.06842,"arXiv:2404.06842v4 Announce Type: replace 
Abstract: Learning-based stereo matching techniques have made significant progress. However, existing methods inevitably lose geometrical structure information during the feature channel generation process, resulting in edge detail mismatches. In this paper, the Motif Cha}nnel Attention Stereo Matching Network (MoCha-Stereo) is designed to address this problem. We provide the Motif Channel Correlation Volume (MCCV) to determine more accurate edge matching costs. MCCV is achieved by projecting motif channels, which capture common geometric structures in feature channels, onto feature maps and cost volumes. In addition, edge variations in %potential feature channels of the reconstruction error map also affect details matching, we propose the Reconstruction Error Motif Penalty (REMP) module to further refine the full-resolution disparity estimation. REMP integrates the frequency information of typical channel features from the reconstruction error. MoCha-Stereo ranks 1st on the KITTI-2015 and KITTI-2012 Reflective leaderboards. Our structure also shows excellent performance in Multi-View Stereo. Code is avaliable at https://github.com/ZYangChen/MoCha-Stereo.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2404.06842v4 Announce Type: replace \nAbstract: Learning-based stereo matching techniques have made significant progress. However, existing methods inevitably lose geometrical structure information during the feature channel generation process, resulting in edge detail mismatches. In this paper, the Motif Cha}nnel Attention Stereo Matching Network (MoCha-Stereo) is designed to address this problem. We provide the Motif Channel Correlation Volume (MCCV) to determine more accurate edge matching costs. MCCV is achieved by projecting motif channels, which capture common geometric structures in feature channels, onto feature maps and cost volumes. In addition, edge variations in %potential feature channels of the reconstruction error map also affect details matching, we propose the Reconstruction Error Motif Penalty (REMP) module to further refine the full-resolution disparity estimation. REMP integrates the frequency information of typical channel features from the reconstruction error. MoCha-Stereo ranks 1st on the KITTI-2015 and KITTI-2012 Reflective leaderboards. Our structure also shows excellent performance in Multi-View Stereo. Code is avaliable at https://github.com/ZYangChen/MoCha-Stereo.'}",oai:arXiv.org:2404.06842v4,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Ziyang Chen, Wei Long, He Yao, Yongjun Zhang, Bingshu Wang, Yongbin Qin, Jia Wu'}]","Ziyang Chen, Wei Long, He Yao, Yongjun Zhang, Bingshu Wang, Yongbin Qin, Jia Wu","{'name': 'Ziyang Chen, Wei Long, He Yao, Yongjun Zhang, Bingshu Wang, Yongbin Qin, Jia Wu'}",,The IEEE/CVF Conference on Computer Vision and Pattern Recognition 2024
541,Clustering of timed sequences -- Application to the analysis of care pathways,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Clustering of timed sequences -- Application to the analysis of care pathways'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2404.15379'}]",https://arxiv.org/abs/2404.15379,"arXiv:2404.15379v3 Announce Type: replace 
Abstract: Improving the future of healthcare starts by better understanding the current actual practices in hospital settings. This motivates the objective of discovering typical care pathways from patient data. Revealing typical care pathways can be achieved through clustering. The difficulty in clustering care pathways, represented by sequences of timestamped events, lies in defining a semantically appropriate metric and clustering algorithms. In this article, we adapt two methods developed for time series to the clustering of timed sequences: the drop-DTW metric and the DBA approach for the construction of averaged time sequences. These methods are then applied in clustering algorithms to propose original and sound clustering algorithms for timed sequences. This approach is experimented with and evaluated on synthetic and real-world data.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2404.15379v3 Announce Type: replace \nAbstract: Improving the future of healthcare starts by better understanding the current actual practices in hospital settings. This motivates the objective of discovering typical care pathways from patient data. Revealing typical care pathways can be achieved through clustering. The difficulty in clustering care pathways, represented by sequences of timestamped events, lies in defining a semantically appropriate metric and clustering algorithms. In this article, we adapt two methods developed for time series to the clustering of timed sequences: the drop-DTW metric and the DBA approach for the construction of averaged time sequences. These methods are then applied in clustering algorithms to propose original and sound clustering algorithms for timed sequences. This approach is experimented with and evaluated on synthetic and real-world data.'}",oai:arXiv.org:2404.15379v3,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Thomas Guyet, Pierre Pinson, Enoal Gesny'}]","Thomas Guyet, Pierre Pinson, Enoal Gesny","{'name': 'Thomas Guyet, Pierre Pinson, Enoal Gesny'}",10.1016/j.datak.2024.102401,"Data & Knowledge Engineering, 156:102401, 2025"
542,MonoPCC: Photometric-invariant Cycle Constraint for Monocular Depth Estimation of Endoscopic Images,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'MonoPCC: Photometric-invariant Cycle Constraint for Monocular Depth Estimation of Endoscopic Images'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2404.16571'}]",https://arxiv.org/abs/2404.16571,"arXiv:2404.16571v4 Announce Type: replace 
Abstract: Photometric constraint is indispensable for self-supervised monocular depth estimation. It involves warping a source image onto a target view using estimated depth&amp;pose, and then minimizing the difference between the warped and target images. However, the endoscopic built-in light causes significant brightness fluctuations, and thus makes the photometric constraint unreliable. Previous efforts only mitigate this relying on extra models to calibrate image brightness. In this paper, we propose MonoPCC to address the brightness inconsistency radically by reshaping the photometric constraint into a cycle form. Instead of only warping the source image, MonoPCC constructs a closed loop consisting of two opposite forward-backward warping paths: from target to source and then back to target. Thus, the target image finally receives an image cycle-warped from itself, which naturally makes the constraint invariant to brightness changes. Moreover, MonoPCC transplants the source image's phase-frequency into the intermediate warped image to avoid structure lost, and also stabilizes the training via an exponential moving average (EMA) strategy to avoid frequent changes in the forward warping. The comprehensive and extensive experimental results on four endoscopic datasets demonstrate that our proposed MonoPCC shows a great robustness to the brightness inconsistency, and exceeds other state-of-the-arts by reducing the absolute relative error by at least 7.27%, 9.38%, 9.90% and 3.17%, respectively.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2404.16571v4 Announce Type: replace \nAbstract: Photometric constraint is indispensable for self-supervised monocular depth estimation. It involves warping a source image onto a target view using estimated depth&amp;pose, and then minimizing the difference between the warped and target images. However, the endoscopic built-in light causes significant brightness fluctuations, and thus makes the photometric constraint unreliable. Previous efforts only mitigate this relying on extra models to calibrate image brightness. In this paper, we propose MonoPCC to address the brightness inconsistency radically by reshaping the photometric constraint into a cycle form. Instead of only warping the source image, MonoPCC constructs a closed loop consisting of two opposite forward-backward warping paths: from target to source and then back to target. Thus, the target image finally receives an image cycle-warped from itself, which naturally makes the constraint invariant to brightness changes. Moreover, MonoPCC transplants the source image's phase-frequency into the intermediate warped image to avoid structure lost, and also stabilizes the training via an exponential moving average (EMA) strategy to avoid frequent changes in the forward warping. The comprehensive and extensive experimental results on four endoscopic datasets demonstrate that our proposed MonoPCC shows a great robustness to the brightness inconsistency, and exceeds other state-of-the-arts by reducing the absolute relative error by at least 7.27%, 9.38%, 9.90% and 3.17%, respectively.""}",oai:arXiv.org:2404.16571v4,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Zhiwei Wang, Ying Zhou, Shiquan He, Ting Li, Fan Huang, Qiang Ding, Xinxia Feng, Mei Liu, Qiang Li'}]","Zhiwei Wang, Ying Zhou, Shiquan He, Ting Li, Fan Huang, Qiang Ding, Xinxia Feng, Mei Liu, Qiang Li","{'name': 'Zhiwei Wang, Ying Zhou, Shiquan He, Ting Li, Fan Huang, Qiang Ding, Xinxia Feng, Mei Liu, Qiang Li'}",,
543,Chameleon: A Data-Efficient Generalist for Dense Visual Prediction in the Wild,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Chameleon: A Data-Efficient Generalist for Dense Visual Prediction in the Wild'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2404.18459'}]",https://arxiv.org/abs/2404.18459,"arXiv:2404.18459v3 Announce Type: replace 
Abstract: Large language models have evolved data-efficient generalists, benefiting from the universal language interface and large-scale pre-training. However, constructing a data-efficient generalist for dense visual prediction presents a distinct challenge due to the variation in label structures across different tasks. Consequently, generalization to unseen dense prediction tasks in the low-data regime is not straightforward and has received less attention from previous vision generalists. In this study, we explore a universal model that can flexibly adapt to unseen dense label structures with a few examples, enabling it to serve as a data-efficient vision generalist in diverse real-world scenarios. To this end, we base our method on a powerful meta-learning framework and explore several axes to improve its performance and versatility for real-world problems, such as flexible adaptation mechanisms and scalability. We evaluate our model across a spectrum of unseen real-world scenarios where low-shot learning is desirable, including video, 3D, medical, biological, and user-interactive tasks. Equipped with a generic architecture and an effective adaptation mechanism, our model flexibly adapts to all of these tasks with at most 50 labeled images, showcasing a significant advancement over existing data-efficient generalist approaches. Codes are available at https://github.com/GitGyun/chameleon.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2404.18459v3 Announce Type: replace \nAbstract: Large language models have evolved data-efficient generalists, benefiting from the universal language interface and large-scale pre-training. However, constructing a data-efficient generalist for dense visual prediction presents a distinct challenge due to the variation in label structures across different tasks. Consequently, generalization to unseen dense prediction tasks in the low-data regime is not straightforward and has received less attention from previous vision generalists. In this study, we explore a universal model that can flexibly adapt to unseen dense label structures with a few examples, enabling it to serve as a data-efficient vision generalist in diverse real-world scenarios. To this end, we base our method on a powerful meta-learning framework and explore several axes to improve its performance and versatility for real-world problems, such as flexible adaptation mechanisms and scalability. We evaluate our model across a spectrum of unseen real-world scenarios where low-shot learning is desirable, including video, 3D, medical, biological, and user-interactive tasks. Equipped with a generic architecture and an effective adaptation mechanism, our model flexibly adapts to all of these tasks with at most 50 labeled images, showcasing a significant advancement over existing data-efficient generalist approaches. Codes are available at https://github.com/GitGyun/chameleon.'}",oai:arXiv.org:2404.18459v3,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by-nc-sa/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-sa/4.0/'}","[{'name': 'Donggyun Kim, Seongwoong Cho, Semin Kim, Chong Luo, Seunghoon Hong'}]","Donggyun Kim, Seongwoong Cho, Semin Kim, Chong Luo, Seunghoon Hong","{'name': 'Donggyun Kim, Seongwoong Cho, Semin Kim, Chong Luo, Seunghoon Hong'}",,
544,Markovian Transformers for Informative Language Modeling,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Markovian Transformers for Informative Language Modeling'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2404.18988'}]",https://arxiv.org/abs/2404.18988,"arXiv:2404.18988v4 Announce Type: replace 
Abstract: Chain-of-Thought (CoT) reasoning holds great promise for explaining language model outputs, but recent studies have highlighted significant challenges in its practical application for interpretability. We propose to address this issue by making CoT causally essential to prediction through two key components: factoring next-token prediction through intermediate CoT text, and training CoT to predict future tokens independently of other context. This results in ""Markovian"" language models, where CoT serves as a fixed-size state for future token prediction. Our approach optimizes for ""informativeness"" - the improvement in next-token predictions using a trained CoT compared to a baseline. Using Proximal Policy Optimization (PPO) for arithmetic problems and policy gradient for GSM8K, we demonstrate effectiveness on both arithmetic problems with Mistral 7B and the GSM8K benchmark with Llama 3.1 8B, where the model learns to produce CoTs that are 33.20% more effective at predicting answers than the pre-trained baseline. The increased sensitivity of model performance to CoT perturbations provides strong evidence of CoT reliance. Furthermore, we show that CoTs trained for one model generalize to help other models predict answers, suggesting these CoTs capture reasoning patterns that transfer across different interpreters. This work advances the development of more interpretable language models, potentially enabling their extension to arbitrarily long contexts and enhancing AI reasoning capabilities across various domains.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2404.18988v4 Announce Type: replace \nAbstract: Chain-of-Thought (CoT) reasoning holds great promise for explaining language model outputs, but recent studies have highlighted significant challenges in its practical application for interpretability. We propose to address this issue by making CoT causally essential to prediction through two key components: factoring next-token prediction through intermediate CoT text, and training CoT to predict future tokens independently of other context. This results in ""Markovian"" language models, where CoT serves as a fixed-size state for future token prediction. Our approach optimizes for ""informativeness"" - the improvement in next-token predictions using a trained CoT compared to a baseline. Using Proximal Policy Optimization (PPO) for arithmetic problems and policy gradient for GSM8K, we demonstrate effectiveness on both arithmetic problems with Mistral 7B and the GSM8K benchmark with Llama 3.1 8B, where the model learns to produce CoTs that are 33.20% more effective at predicting answers than the pre-trained baseline. The increased sensitivity of model performance to CoT perturbations provides strong evidence of CoT reliance. Furthermore, we show that CoTs trained for one model generalize to help other models predict answers, suggesting these CoTs capture reasoning patterns that transfer across different interpreters. This work advances the development of more interpretable language models, potentially enabling their extension to arbitrarily long contexts and enhancing AI reasoning capabilities across various domains.'}",oai:arXiv.org:2404.18988v4,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Scott Viteri, Max Lamparth, Peter Chatain, Clark Barrett'}]","Scott Viteri, Max Lamparth, Peter Chatain, Clark Barrett","{'name': 'Scott Viteri, Max Lamparth, Peter Chatain, Clark Barrett'}",,
545,Towards trustable SHAP scores,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Towards trustable SHAP scores'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2405.00076'}]",https://arxiv.org/abs/2405.00076,"arXiv:2405.00076v2 Announce Type: replace 
Abstract: SHAP scores represent the proposed use of the well-known Shapley values in eXplainable Artificial Intelligence (XAI). Recent work has shown that the exact computation of SHAP scores can produce unsatisfactory results. Concretely, for some ML models, SHAP scores will mislead with respect to relative feature influence. To address these limitations, recently proposed alternatives exploit different axiomatic aggregations, all of which are defined in terms of abductive explanations. However, the proposed axiomatic aggregations are not Shapley values. This paper investigates how SHAP scores can be modified so as to extend axiomatic aggregations to the case of Shapley values in XAI. More importantly, the proposed new definition of SHAP scores avoids all the known cases where unsatisfactory results have been identified. The paper also characterizes the complexity of computing the novel definition of SHAP scores, highlighting families of classifiers for which computing these scores is tractable. Furthermore, the paper proposes modifications to the existing implementations of SHAP scores. These modifications eliminate some of the known limitations of SHAP scores, and have negligible impact in terms of performance.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2405.00076v2 Announce Type: replace \nAbstract: SHAP scores represent the proposed use of the well-known Shapley values in eXplainable Artificial Intelligence (XAI). Recent work has shown that the exact computation of SHAP scores can produce unsatisfactory results. Concretely, for some ML models, SHAP scores will mislead with respect to relative feature influence. To address these limitations, recently proposed alternatives exploit different axiomatic aggregations, all of which are defined in terms of abductive explanations. However, the proposed axiomatic aggregations are not Shapley values. This paper investigates how SHAP scores can be modified so as to extend axiomatic aggregations to the case of Shapley values in XAI. More importantly, the proposed new definition of SHAP scores avoids all the known cases where unsatisfactory results have been identified. The paper also characterizes the complexity of computing the novel definition of SHAP scores, highlighting families of classifiers for which computing these scores is tractable. Furthermore, the paper proposes modifications to the existing implementations of SHAP scores. These modifications eliminate some of the known limitations of SHAP scores, and have negligible impact in terms of performance.'}",oai:arXiv.org:2405.00076v2,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Olivier Letoffe, Xuanxiang Huang, Joao Marques-Silva'}]","Olivier Letoffe, Xuanxiang Huang, Joao Marques-Silva","{'name': 'Olivier Letoffe, Xuanxiang Huang, Joao Marques-Silva'}",,
546,SCONE: A Novel Stochastic Sampling to Generate Contrastive Views and Hard Negative Samples for Recommendation,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'SCONE: A Novel Stochastic Sampling to Generate Contrastive Views and Hard Negative Samples for Recommendation'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2405.00287'}]",https://arxiv.org/abs/2405.00287,"arXiv:2405.00287v2 Announce Type: replace 
Abstract: Graph-based collaborative filtering (CF) has emerged as a promising approach in recommender systems. Despite its achievements, graph-based CF models face challenges due to data sparsity and negative sampling. In this paper, we propose a novel Stochastic sampling for i) COntrastive views and ii) hard NEgative samples (SCONE) to overcome these issues. SCONE generates dynamic augmented views and diverse hard negative samples via a unified stochastic sampling approach based on score-based generative models. Our extensive experiments on 6 benchmark datasets show that SCONE consistently outperforms state-of-the-art baselines. SCONE shows efficacy in addressing user sparsity and item popularity issues, while enhancing performance for both cold-start users and long-tail items. Furthermore, our approach improves the diversity of the recommendation and the uniformity of the representations. The code is available at https://github.com/jeongwhanchoi/SCONE.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2405.00287v2 Announce Type: replace \nAbstract: Graph-based collaborative filtering (CF) has emerged as a promising approach in recommender systems. Despite its achievements, graph-based CF models face challenges due to data sparsity and negative sampling. In this paper, we propose a novel Stochastic sampling for i) COntrastive views and ii) hard NEgative samples (SCONE) to overcome these issues. SCONE generates dynamic augmented views and diverse hard negative samples via a unified stochastic sampling approach based on score-based generative models. Our extensive experiments on 6 benchmark datasets show that SCONE consistently outperforms state-of-the-art baselines. SCONE shows efficacy in addressing user sparsity and item popularity issues, while enhancing performance for both cold-start users and long-tail items. Furthermore, our approach improves the diversity of the recommendation and the uniformity of the representations. The code is available at https://github.com/jeongwhanchoi/SCONE.'}",oai:arXiv.org:2405.00287v2,False,"[{'term': 'cs.IR', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Chaejeong Lee, Jeongwhan Choi, Hyowon Wi, Sung-Bae Cho, Noseong Park'}]","Chaejeong Lee, Jeongwhan Choi, Hyowon Wi, Sung-Bae Cho, Noseong Park","{'name': 'Chaejeong Lee, Jeongwhan Choi, Hyowon Wi, Sung-Bae Cho, Noseong Park'}",,
547,A Unified Framework for Human-Allied Learning of Probabilistic Circuits,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'A Unified Framework for Human-Allied Learning of Probabilistic Circuits'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2405.02413'}]",https://arxiv.org/abs/2405.02413,"arXiv:2405.02413v2 Announce Type: replace 
Abstract: Probabilistic Circuits (PCs) have emerged as an efficient framework for representing and learning complex probability distributions. Nevertheless, the existing body of research on PCs predominantly concentrates on data-driven parameter learning, often neglecting the potential of knowledge-intensive learning, a particular issue in data-scarce/knowledge-rich domains such as healthcare. To bridge this gap, we propose a novel unified framework that can systematically integrate diverse domain knowledge into the parameter learning process of PCs. Experiments on several benchmarks as well as real world datasets show that our proposed framework can both effectively and efficiently leverage domain knowledge to achieve superior performance compared to purely data-driven learning approaches.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2405.02413v2 Announce Type: replace \nAbstract: Probabilistic Circuits (PCs) have emerged as an efficient framework for representing and learning complex probability distributions. Nevertheless, the existing body of research on PCs predominantly concentrates on data-driven parameter learning, often neglecting the potential of knowledge-intensive learning, a particular issue in data-scarce/knowledge-rich domains such as healthcare. To bridge this gap, we propose a novel unified framework that can systematically integrate diverse domain knowledge into the parameter learning process of PCs. Experiments on several benchmarks as well as real world datasets show that our proposed framework can both effectively and efficiently leverage domain knowledge to achieve superior performance compared to purely data-driven learning approaches.'}",oai:arXiv.org:2405.02413v2,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Athresh Karanam, Saurabh Mathur, Sahil Sidheekh, Sriraam Natarajan'}]","Athresh Karanam, Saurabh Mathur, Sahil Sidheekh, Sriraam Natarajan","{'name': 'Athresh Karanam, Saurabh Mathur, Sahil Sidheekh, Sriraam Natarajan'}",,
548,Mitigating Side Effects in Multi-Agent Systems Using Blame Assignment,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Mitigating Side Effects in Multi-Agent Systems Using Blame Assignment'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2405.04702'}]",https://arxiv.org/abs/2405.04702,"arXiv:2405.04702v3 Announce Type: replace 
Abstract: When independently trained or designed robots are deployed in a shared environment, their combined actions can lead to unintended negative side effects (NSEs). To ensure safe and efficient operation, robots must optimize task performance while minimizing the penalties associated with NSEs, balancing individual objectives with collective impact. We model the problem of mitigating NSEs in a cooperative multi-agent system as a bi-objective lexicographic decentralized Markov decision process. We assume independence of transitions and rewards with respect to the robots' tasks, but the joint NSE penalty creates a form of dependence in this setting. To improve scalability, the joint NSE penalty is decomposed into individual penalties for each robot using credit assignment, which facilitates decentralized policy computation. We empirically demonstrate, using mobile robots and in simulation, the effectiveness and scalability of our approach in mitigating NSEs.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2405.04702v3 Announce Type: replace \nAbstract: When independently trained or designed robots are deployed in a shared environment, their combined actions can lead to unintended negative side effects (NSEs). To ensure safe and efficient operation, robots must optimize task performance while minimizing the penalties associated with NSEs, balancing individual objectives with collective impact. We model the problem of mitigating NSEs in a cooperative multi-agent system as a bi-objective lexicographic decentralized Markov decision process. We assume independence of transitions and rewards with respect to the robots' tasks, but the joint NSE penalty creates a form of dependence in this setting. To improve scalability, the joint NSE penalty is decomposed into individual penalties for each robot using credit assignment, which facilitates decentralized policy computation. We empirically demonstrate, using mobile robots and in simulation, the effectiveness and scalability of our approach in mitigating NSEs.""}",oai:arXiv.org:2405.04702v3,False,"[{'term': 'cs.MA', 'scheme': None, 'label': None}, {'term': 'cs.RO', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Pulkit Rustagi, Sandhya Saisubramanian'}]","Pulkit Rustagi, Sandhya Saisubramanian","{'name': 'Pulkit Rustagi, Sandhya Saisubramanian'}",,
549,Mitigating federated learning contribution allocation instability through randomized aggregation,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Mitigating federated learning contribution allocation instability through randomized aggregation'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2405.08044'}]",https://arxiv.org/abs/2405.08044,"arXiv:2405.08044v2 Announce Type: replace 
Abstract: Federated learning (FL) is a collaborative and privacy-preserving Machine Learning paradigm, allowing the development of robust models without the need to centralise sensitive data. A critical challenge in FL lies in fairly and accurately allocating contributions from diverse participants. Inaccurate allocation can undermine trust, lead to unfair compensation, and thus participants may lack the incentive to join or actively contribute to the federation.
  Various remuneration strategies have been proposed to date, including auction-based approaches and Shapley-value based methods, the latter offering a means to quantify the contribution of each participant. However, little to no work has studied the stability of these contribution evaluation methods.
  In this paper, we focus on calculating contributions using gradient-based model reconstruction techniques with Shapley values. We first show that baseline Shapley values do not accurately reflect clients' contributions, leading to unstable reward allocations amongst participants in a cross-silo federation. We then introduce \textsc{FedRandom}, a new method that mitigates these shortcomings with additional data samplings, and show its efficacy at increasing the stability of contribution evaluation in federated learning.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2405.08044v2 Announce Type: replace \nAbstract: Federated learning (FL) is a collaborative and privacy-preserving Machine Learning paradigm, allowing the development of robust models without the need to centralise sensitive data. A critical challenge in FL lies in fairly and accurately allocating contributions from diverse participants. Inaccurate allocation can undermine trust, lead to unfair compensation, and thus participants may lack the incentive to join or actively contribute to the federation.\n  Various remuneration strategies have been proposed to date, including auction-based approaches and Shapley-value based methods, the latter offering a means to quantify the contribution of each participant. However, little to no work has studied the stability of these contribution evaluation methods.\n  In this paper, we focus on calculating contributions using gradient-based model reconstruction techniques with Shapley values. We first show that baseline Shapley values do not accurately reflect clients' contributions, leading to unstable reward allocations amongst participants in a cross-silo federation. We then introduce \\textsc{FedRandom}, a new method that mitigates these shortcomings with additional data samplings, and show its efficacy at increasing the stability of contribution evaluation in federated learning.""}",oai:arXiv.org:2405.08044v2,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Arno Geimer, Beltran Fiz, Radu State'}]","Arno Geimer, Beltran Fiz, Radu State","{'name': 'Arno Geimer, Beltran Fiz, Radu State'}",,
550,No Free Lunch: Research Software Testing in Teaching,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'No Free Lunch: Research Software Testing in Teaching'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2405.11965'}]",https://arxiv.org/abs/2405.11965,"arXiv:2405.11965v2 Announce Type: replace 
Abstract: Software is at the core of most scientific discoveries today. Therefore, the quality of research results highly depends on the quality of the research software. Rigorous testing, as we know it from software engineering in the industry, could ensure the quality of the research software but it also requires a substantial effort that is often not rewarded in academia. Therefore, this research explores the effects of research software testing integrated into teaching on research software. In an in-vivo experiment, we integrated the engineering of a test suite for a large-scale network simulation as group projects into a course on software testing at the Blekinge Institute of Technology, Sweden, and qualitatively measured the effects of this integration on the research software. We found that the research software benefited from the integration through substantially improved documentation and fewer hardware and software dependencies. However, this integration was effortful and although the student teams developed elegant and thoughtful test suites, no code by students went directly into the research software since we were not able to make the integration back into the research software obligatory or even remunerative. Although we strongly believe that integrating research software engineering such as testing into teaching is not only valuable for the research software itself but also for students, the research of the next generation, as they get in touch with research software engineering and bleeding-edge research in their field as part of their education, the uncertainty about the intellectual properties of students' code substantially limits the potential of integrating research software testing into teaching.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2405.11965v2 Announce Type: replace \nAbstract: Software is at the core of most scientific discoveries today. Therefore, the quality of research results highly depends on the quality of the research software. Rigorous testing, as we know it from software engineering in the industry, could ensure the quality of the research software but it also requires a substantial effort that is often not rewarded in academia. Therefore, this research explores the effects of research software testing integrated into teaching on research software. In an in-vivo experiment, we integrated the engineering of a test suite for a large-scale network simulation as group projects into a course on software testing at the Blekinge Institute of Technology, Sweden, and qualitatively measured the effects of this integration on the research software. We found that the research software benefited from the integration through substantially improved documentation and fewer hardware and software dependencies. However, this integration was effortful and although the student teams developed elegant and thoughtful test suites, no code by students went directly into the research software since we were not able to make the integration back into the research software obligatory or even remunerative. Although we strongly believe that integrating research software engineering such as testing into teaching is not only valuable for the research software itself but also for students, the research of the next generation, as they get in touch with research software engineering and bleeding-edge research in their field as part of their education, the uncertainty about the intellectual properties of students' code substantially limits the potential of integrating research software testing into teaching.""}",oai:arXiv.org:2405.11965v2,False,"[{'term': 'cs.SE', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Michael Dorner, Andreas Bauer, Florian Angermeir'}]","Michael Dorner, Andreas Bauer, Florian Angermeir","{'name': 'Michael Dorner, Andreas Bauer, Florian Angermeir'}",,
551,Agent Planning with World Knowledge Model,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Agent Planning with World Knowledge Model'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2405.14205'}]",https://arxiv.org/abs/2405.14205,"arXiv:2405.14205v3 Announce Type: replace 
Abstract: Recent endeavors towards directly using large language models (LLMs) as agent models to execute interactive planning tasks have shown commendable results. Despite their achievements, however, they still struggle with brainless trial-and-error in global planning and generating hallucinatory actions in local planning due to their poor understanding of the ``real'' physical world. Imitating humans' mental world knowledge model which provides global prior knowledge before the task and maintains local dynamic knowledge during the task, in this paper, we introduce parametric World Knowledge Model (WKM) to facilitate agent planning. Concretely, we steer the agent model to self-synthesize knowledge from both expert and sampled trajectories. Then we develop WKM, providing prior task knowledge to guide the global planning and dynamic state knowledge to assist the local planning. Experimental results on three complex real-world simulated datasets with three state-of-the-art open-source LLMs, Mistral-7B, Gemma-7B, and Llama-3-8B, demonstrate that our method can achieve superior performance compared to various strong baselines. Besides, we analyze to illustrate that our WKM can effectively alleviate the blind trial-and-error and hallucinatory action issues, providing strong support for the agent's understanding of the world. Other interesting findings include: 1) our instance-level task knowledge can generalize better to unseen tasks, 2) weak WKM can guide strong agent model planning, and 3) unified WKM training has promising potential for further development. The code is available at https://github.com/zjunlp/WKM.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2405.14205v3 Announce Type: replace \nAbstract: Recent endeavors towards directly using large language models (LLMs) as agent models to execute interactive planning tasks have shown commendable results. Despite their achievements, however, they still struggle with brainless trial-and-error in global planning and generating hallucinatory actions in local planning due to their poor understanding of the ``real'' physical world. Imitating humans' mental world knowledge model which provides global prior knowledge before the task and maintains local dynamic knowledge during the task, in this paper, we introduce parametric World Knowledge Model (WKM) to facilitate agent planning. Concretely, we steer the agent model to self-synthesize knowledge from both expert and sampled trajectories. Then we develop WKM, providing prior task knowledge to guide the global planning and dynamic state knowledge to assist the local planning. Experimental results on three complex real-world simulated datasets with three state-of-the-art open-source LLMs, Mistral-7B, Gemma-7B, and Llama-3-8B, demonstrate that our method can achieve superior performance compared to various strong baselines. Besides, we analyze to illustrate that our WKM can effectively alleviate the blind trial-and-error and hallucinatory action issues, providing strong support for the agent's understanding of the world. Other interesting findings include: 1) our instance-level task knowledge can generalize better to unseen tasks, 2) weak WKM can guide strong agent model planning, and 3) unified WKM training has promising potential for further development. The code is available at https://github.com/zjunlp/WKM.""}",oai:arXiv.org:2405.14205v3,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'cs.MA', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Shuofei Qiao, Runnan Fang, Ningyu Zhang, Yuqi Zhu, Xiang Chen, Shumin Deng, Yong Jiang, Pengjun Xie, Fei Huang, Huajun Chen'}]","Shuofei Qiao, Runnan Fang, Ningyu Zhang, Yuqi Zhu, Xiang Chen, Shumin Deng, Yong Jiang, Pengjun Xie, Fei Huang, Huajun Chen","{'name': 'Shuofei Qiao, Runnan Fang, Ningyu Zhang, Yuqi Zhu, Xiang Chen, Shumin Deng, Yong Jiang, Pengjun Xie, Fei Huang, Huajun Chen'}",,
552,SLIFER: Investigating Performance and Robustness of Malware Detection Pipelines,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'SLIFER: Investigating Performance and Robustness of Malware Detection Pipelines'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2405.14478'}]",https://arxiv.org/abs/2405.14478,"arXiv:2405.14478v3 Announce Type: replace 
Abstract: As a result of decades of research, Windows malware detection is approached through a plethora of techniques. However, there is an ongoing mismatch between academia -- which pursues an optimal performances in terms of detection rate and low false alarms -- and the requirements of real-world scenarios. In particular, academia focuses on combining static and dynamic analysis within a single or ensemble of models, falling into several pitfalls like (i) firing dynamic analysis without considering the computational burden it requires; (ii) discarding impossible-to-analyze samples; and (iii) analyzing robustness against adversarial attacks without considering that malware detectors are complemented with more non-machine-learning components. Thus, in this paper we bridge these gaps, by investigating the properties of malware detectors built with multiple and different types of analysis. To do so, we develop SLIFER, a Windows malware detection pipeline sequentially leveraging both static and dynamic analysis, interrupting computations as soon as one module triggers an alarm, requiring dynamic analysis only when needed. Contrary to the state of the art, we investigate how to deal with samples that impede analyzes, showing how much they impact performances, concluding that it is better to flag them as legitimate to not drastically increase false alarms. Lastly, we perform a robustness evaluation of SLIFER. Counter-intuitively, the injection of new content is either blocked more by signatures than dynamic analysis, due to byte artifacts created by the attack, or it is able to avoid detection from signatures, as they rely on constraints on file size disrupted by attacks. As far as we know, we are the first to investigate the properties of sequential malware detectors, shedding light on their behavior in real production environment.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2405.14478v3 Announce Type: replace \nAbstract: As a result of decades of research, Windows malware detection is approached through a plethora of techniques. However, there is an ongoing mismatch between academia -- which pursues an optimal performances in terms of detection rate and low false alarms -- and the requirements of real-world scenarios. In particular, academia focuses on combining static and dynamic analysis within a single or ensemble of models, falling into several pitfalls like (i) firing dynamic analysis without considering the computational burden it requires; (ii) discarding impossible-to-analyze samples; and (iii) analyzing robustness against adversarial attacks without considering that malware detectors are complemented with more non-machine-learning components. Thus, in this paper we bridge these gaps, by investigating the properties of malware detectors built with multiple and different types of analysis. To do so, we develop SLIFER, a Windows malware detection pipeline sequentially leveraging both static and dynamic analysis, interrupting computations as soon as one module triggers an alarm, requiring dynamic analysis only when needed. Contrary to the state of the art, we investigate how to deal with samples that impede analyzes, showing how much they impact performances, concluding that it is better to flag them as legitimate to not drastically increase false alarms. Lastly, we perform a robustness evaluation of SLIFER. Counter-intuitively, the injection of new content is either blocked more by signatures than dynamic analysis, due to byte artifacts created by the attack, or it is able to avoid detection from signatures, as they rely on constraints on file size disrupted by attacks. As far as we know, we are the first to investigate the properties of sequential malware detectors, shedding light on their behavior in real production environment.'}",oai:arXiv.org:2405.14478v3,False,"[{'term': 'cs.CR', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Andrea Ponte, Dmitrijs Trizna, Luca Demetrio, Battista Biggio, Ivan Tesfai Ogbu, Fabio Roli'}]","Andrea Ponte, Dmitrijs Trizna, Luca Demetrio, Battista Biggio, Ivan Tesfai Ogbu, Fabio Roli","{'name': 'Andrea Ponte, Dmitrijs Trizna, Luca Demetrio, Battista Biggio, Ivan Tesfai Ogbu, Fabio Roli'}",10.1016/j.cose.2024.104264,
553,AndroidWorld: A Dynamic Benchmarking Environment for Autonomous Agents,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'AndroidWorld: A Dynamic Benchmarking Environment for Autonomous Agents'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2405.14573'}]",https://arxiv.org/abs/2405.14573,"arXiv:2405.14573v4 Announce Type: replace 
Abstract: Autonomous agents that execute human tasks by controlling computers can enhance human productivity and application accessibility. However, progress in this field will be driven by realistic and reproducible benchmarks. We present AndroidWorld, a fully functional Android environment that provides reward signals for 116 programmatic tasks across 20 real-world Android apps. Unlike existing interactive environments, which provide a static test set, AndroidWorld dynamically constructs tasks that are parameterized and expressed in natural language in unlimited ways, thus enabling testing on a much larger and more realistic suite of tasks. To ensure reproducibility, each task includes dedicated initialization, success-checking, and tear-down logic, which modifies and inspects the device's system state. We experiment with baseline agents to test AndroidWorld and provide initial results on the benchmark. Our best agent can complete 30.6% of AndroidWorld's tasks, leaving ample room for future work. Furthermore, we adapt a popular desktop web agent to work on Android, which we find to be less effective on mobile, suggesting future research is needed to achieve universal, cross-platform agents. Finally, we also conduct a robustness analysis, showing that task variations can significantly affect agent performance, demonstrating that without such testing, agent performance metrics may not fully reflect practical challenges. AndroidWorld and the experiments in this paper are available at github.com/google-research/android_world.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2405.14573v4 Announce Type: replace \nAbstract: Autonomous agents that execute human tasks by controlling computers can enhance human productivity and application accessibility. However, progress in this field will be driven by realistic and reproducible benchmarks. We present AndroidWorld, a fully functional Android environment that provides reward signals for 116 programmatic tasks across 20 real-world Android apps. Unlike existing interactive environments, which provide a static test set, AndroidWorld dynamically constructs tasks that are parameterized and expressed in natural language in unlimited ways, thus enabling testing on a much larger and more realistic suite of tasks. To ensure reproducibility, each task includes dedicated initialization, success-checking, and tear-down logic, which modifies and inspects the device's system state. We experiment with baseline agents to test AndroidWorld and provide initial results on the benchmark. Our best agent can complete 30.6% of AndroidWorld's tasks, leaving ample room for future work. Furthermore, we adapt a popular desktop web agent to work on Android, which we find to be less effective on mobile, suggesting future research is needed to achieve universal, cross-platform agents. Finally, we also conduct a robustness analysis, showing that task variations can significantly affect agent performance, demonstrating that without such testing, agent performance metrics may not fully reflect practical challenges. AndroidWorld and the experiments in this paper are available at github.com/google-research/android_world.""}",oai:arXiv.org:2405.14573v4,False,"[{'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Christopher Rawles, Sarah Clinckemaillie, Yifan Chang, Jonathan Waltz, Gabrielle Lau, Marybeth Fair, Alice Li, William Bishop, Wei Li, Folawiyo Campbell-Ajala, Daniel Toyama, Robert Berry, Divya Tyamagundlu, Timothy Lillicrap, Oriana Riva'}]","Christopher Rawles, Sarah Clinckemaillie, Yifan Chang, Jonathan Waltz, Gabrielle Lau, Marybeth Fair, Alice Li, William Bishop, Wei Li, Folawiyo Campbell-Ajala, Daniel Toyama, Robert Berry, Divya Tyamagundlu, Timothy Lillicrap, Oriana Riva","{'name': 'Christopher Rawles, Sarah Clinckemaillie, Yifan Chang, Jonathan Waltz, Gabrielle Lau, Marybeth Fair, Alice Li, William Bishop, Wei Li, Folawiyo Campbell-Ajala, Daniel Toyama, Robert Berry, Divya Tyamagundlu, Timothy Lillicrap, Oriana Riva'}",,
554,WISE: Rethinking the Knowledge Memory for Lifelong Model Editing of Large Language Models,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'WISE: Rethinking the Knowledge Memory for Lifelong Model Editing of Large Language Models'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2405.14768'}]",https://arxiv.org/abs/2405.14768,"arXiv:2405.14768v3 Announce Type: replace 
Abstract: Large language models (LLMs) need knowledge updates to meet the ever-growing world facts and correct the hallucinated responses, facilitating the methods of lifelong model editing. Where the updated knowledge resides in memories is a fundamental question for model editing. In this paper, we find that editing either long-term memory (direct model parameters) or working memory (non-parametric knowledge of neural network activations/representations by retrieval) will result in an impossible triangle -- reliability, generalization, and locality can not be realized together in the lifelong editing settings. For long-term memory, directly editing the parameters will cause conflicts with irrelevant pretrained knowledge or previous edits (poor reliability and locality). For working memory, retrieval-based activations can hardly make the model understand the edits and generalize (poor generalization). Therefore, we propose WISE to bridge the gap between memories. In WISE, we design a dual parametric memory scheme, which consists of the main memory for the pretrained knowledge and a side memory for the edited knowledge. We only edit the knowledge in the side memory and train a router to decide which memory to go through when given a query. For continual editing, we devise a knowledge-sharding mechanism where different sets of edits reside in distinct subspaces of parameters, and are subsequently merged into a shared memory without conflicts. Extensive experiments show that WISE can outperform previous model editing methods and overcome the impossible triangle under lifelong model editing of question answering, hallucination, and out-of-distribution settings across trending LLM architectures, e.g., GPT, LLaMA, and Mistral. Code is available at https://github.com/zjunlp/EasyEdit.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2405.14768v3 Announce Type: replace \nAbstract: Large language models (LLMs) need knowledge updates to meet the ever-growing world facts and correct the hallucinated responses, facilitating the methods of lifelong model editing. Where the updated knowledge resides in memories is a fundamental question for model editing. In this paper, we find that editing either long-term memory (direct model parameters) or working memory (non-parametric knowledge of neural network activations/representations by retrieval) will result in an impossible triangle -- reliability, generalization, and locality can not be realized together in the lifelong editing settings. For long-term memory, directly editing the parameters will cause conflicts with irrelevant pretrained knowledge or previous edits (poor reliability and locality). For working memory, retrieval-based activations can hardly make the model understand the edits and generalize (poor generalization). Therefore, we propose WISE to bridge the gap between memories. In WISE, we design a dual parametric memory scheme, which consists of the main memory for the pretrained knowledge and a side memory for the edited knowledge. We only edit the knowledge in the side memory and train a router to decide which memory to go through when given a query. For continual editing, we devise a knowledge-sharding mechanism where different sets of edits reside in distinct subspaces of parameters, and are subsequently merged into a shared memory without conflicts. Extensive experiments show that WISE can outperform previous model editing methods and overcome the impossible triangle under lifelong model editing of question answering, hallucination, and out-of-distribution settings across trending LLM architectures, e.g., GPT, LLaMA, and Mistral. Code is available at https://github.com/zjunlp/EasyEdit.'}",oai:arXiv.org:2405.14768v3,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.IR', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Peng Wang, Zexi Li, Ningyu Zhang, Ziwen Xu, Yunzhi Yao, Yong Jiang, Pengjun Xie, Fei Huang, Huajun Chen'}]","Peng Wang, Zexi Li, Ningyu Zhang, Ziwen Xu, Yunzhi Yao, Yong Jiang, Pengjun Xie, Fei Huang, Huajun Chen","{'name': 'Peng Wang, Zexi Li, Ningyu Zhang, Ziwen Xu, Yunzhi Yao, Yong Jiang, Pengjun Xie, Fei Huang, Huajun Chen'}",,
555,Learning from Linear Algebra: A Graph Neural Network Approach to Preconditioner Design for Conjugate Gradient Solvers,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Learning from Linear Algebra: A Graph Neural Network Approach to Preconditioner Design for Conjugate Gradient Solvers'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2405.15557'}]",https://arxiv.org/abs/2405.15557,"arXiv:2405.15557v2 Announce Type: replace 
Abstract: Large linear systems are ubiquitous in modern computational science and engineering. The main recipe for solving them is the use of Krylov subspace iterative methods with well-designed preconditioners. Deep learning models can be used as nonlinear preconditioners during the iteration of linear solvers such as the conjugate gradient (CG) method. Neural network models require an enormous number of parameters to approximate well in this setup. Another approach is to take advantage of small graph neural networks (GNNs) to construct preconditioners with predefined sparsity patterns. Recently, GNNs have been shown to be a promising tool for designing preconditioners to reduce the overall computational cost of iterative methods by constructing them more efficiently than with classical linear algebra techniques. However, preconditioners designed with these approaches cannot outperform those designed with classical methods in terms of the number of iterations in CG. In our work, we recall well-established preconditioners from linear algebra and use them as a starting point for training the GNN to obtain preconditioners that reduce the condition number of the system more significantly. Numerical experiments show that our approach outperforms both classical and neural network-based methods for an important class of parametric partial differential equations. We also provide a heuristic justification for the loss function used and show that preconditioners obtained by learning with this loss function reduce the condition number in a more desirable way for CG.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2405.15557v2 Announce Type: replace \nAbstract: Large linear systems are ubiquitous in modern computational science and engineering. The main recipe for solving them is the use of Krylov subspace iterative methods with well-designed preconditioners. Deep learning models can be used as nonlinear preconditioners during the iteration of linear solvers such as the conjugate gradient (CG) method. Neural network models require an enormous number of parameters to approximate well in this setup. Another approach is to take advantage of small graph neural networks (GNNs) to construct preconditioners with predefined sparsity patterns. Recently, GNNs have been shown to be a promising tool for designing preconditioners to reduce the overall computational cost of iterative methods by constructing them more efficiently than with classical linear algebra techniques. However, preconditioners designed with these approaches cannot outperform those designed with classical methods in terms of the number of iterations in CG. In our work, we recall well-established preconditioners from linear algebra and use them as a starting point for training the GNN to obtain preconditioners that reduce the condition number of the system more significantly. Numerical experiments show that our approach outperforms both classical and neural network-based methods for an important class of parametric partial differential equations. We also provide a heuristic justification for the loss function used and show that preconditioners obtained by learning with this loss function reduce the condition number in a more desirable way for CG.'}",oai:arXiv.org:2405.15557v2,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'cs.NA', 'scheme': None, 'label': None}, {'term': 'math.NA', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Vladislav Trifonov, Alexander Rudikov, Oleg Iliev, Yuri M. Laevsky, Ivan Oseledets, Ekaterina Muravleva'}]","Vladislav Trifonov, Alexander Rudikov, Oleg Iliev, Yuri M. Laevsky, Ivan Oseledets, Ekaterina Muravleva","{'name': 'Vladislav Trifonov, Alexander Rudikov, Oleg Iliev, Yuri M. Laevsky, Ivan Oseledets, Ekaterina Muravleva'}",,
556,Knowledge Circuits in Pretrained Transformers,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Knowledge Circuits in Pretrained Transformers'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2405.17969'}]",https://arxiv.org/abs/2405.17969,"arXiv:2405.17969v3 Announce Type: replace 
Abstract: The remarkable capabilities of modern large language models are rooted in their vast repositories of knowledge encoded within their parameters, enabling them to perceive the world and engage in reasoning. The inner workings of how these models store knowledge have long been a subject of intense interest and investigation among researchers. To date, most studies have concentrated on isolated components within these models, such as the Multilayer Perceptrons and attention head. In this paper, we delve into the computation graph of the language model to uncover the knowledge circuits that are instrumental in articulating specific knowledge. The experiments, conducted with GPT2 and TinyLLAMA, have allowed us to observe how certain information heads, relation heads, and Multilayer Perceptrons collaboratively encode knowledge within the model. Moreover, we evaluate the impact of current knowledge editing techniques on these knowledge circuits, providing deeper insights into the functioning and constraints of these editing methodologies. Finally, we utilize knowledge circuits to analyze and interpret language model behaviors such as hallucinations and in-context learning. We believe the knowledge circuits hold potential for advancing our understanding of Transformers and guiding the improved design of knowledge editing. Code and data are available in https://github.com/zjunlp/KnowledgeCircuits.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2405.17969v3 Announce Type: replace \nAbstract: The remarkable capabilities of modern large language models are rooted in their vast repositories of knowledge encoded within their parameters, enabling them to perceive the world and engage in reasoning. The inner workings of how these models store knowledge have long been a subject of intense interest and investigation among researchers. To date, most studies have concentrated on isolated components within these models, such as the Multilayer Perceptrons and attention head. In this paper, we delve into the computation graph of the language model to uncover the knowledge circuits that are instrumental in articulating specific knowledge. The experiments, conducted with GPT2 and TinyLLAMA, have allowed us to observe how certain information heads, relation heads, and Multilayer Perceptrons collaboratively encode knowledge within the model. Moreover, we evaluate the impact of current knowledge editing techniques on these knowledge circuits, providing deeper insights into the functioning and constraints of these editing methodologies. Finally, we utilize knowledge circuits to analyze and interpret language model behaviors such as hallucinations and in-context learning. We believe the knowledge circuits hold potential for advancing our understanding of Transformers and guiding the improved design of knowledge editing. Code and data are available in https://github.com/zjunlp/KnowledgeCircuits.'}",oai:arXiv.org:2405.17969v3,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.IR', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Yunzhi Yao, Ningyu Zhang, Zekun Xi, Mengru Wang, Ziwen Xu, Shumin Deng, Huajun Chen'}]","Yunzhi Yao, Ningyu Zhang, Zekun Xi, Mengru Wang, Ziwen Xu, Shumin Deng, Huajun Chen","{'name': 'Yunzhi Yao, Ningyu Zhang, Zekun Xi, Mengru Wang, Ziwen Xu, Shumin Deng, Huajun Chen'}",,
557,Skeleton-OOD: An End-to-End Skeleton-Based Model for Robust Out-of-Distribution Human Action Detection,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Skeleton-OOD: An End-to-End Skeleton-Based Model for Robust Out-of-Distribution Human Action Detection'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2405.20633'}]",https://arxiv.org/abs/2405.20633,"arXiv:2405.20633v3 Announce Type: replace 
Abstract: Human action recognition is crucial in computer vision systems. However, in real-world scenarios, human actions often fall outside the distribution of training data, requiring a model to both recognize in-distribution (ID) actions and reject out-of-distribution (OOD) ones. Despite its importance, there has been limited research on OOD detection in human actions. Existing works on OOD detection mainly focus on image data with RGB structure, and many methods are post-hoc in nature. While these methods are convenient and computationally efficient, they often lack sufficient accuracy, fail to consider the exposure of OOD samples, and ignore the application in skeleton structure data. To address these challenges, we propose a novel end-to-end skeleton-based model called Skeleton-OOD, which is committed to improving the effectiveness of OOD tasks while ensuring the accuracy of ID recognition. Through extensive experiments conducted on NTU-RGB+D 60, NTU-RGB+D 120, and Kinetics-400 datasets, Skeleton-OOD demonstrates the superior performance of our proposed approach compared to state-of-the-art methods. Our findings underscore the effectiveness of classic OOD detection techniques in the context of skeleton-based action recognition tasks, offering promising avenues for future research in this field. Code is available at https://github.com/YilliaJing/Skeleton-OOD.git.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2405.20633v3 Announce Type: replace \nAbstract: Human action recognition is crucial in computer vision systems. However, in real-world scenarios, human actions often fall outside the distribution of training data, requiring a model to both recognize in-distribution (ID) actions and reject out-of-distribution (OOD) ones. Despite its importance, there has been limited research on OOD detection in human actions. Existing works on OOD detection mainly focus on image data with RGB structure, and many methods are post-hoc in nature. While these methods are convenient and computationally efficient, they often lack sufficient accuracy, fail to consider the exposure of OOD samples, and ignore the application in skeleton structure data. To address these challenges, we propose a novel end-to-end skeleton-based model called Skeleton-OOD, which is committed to improving the effectiveness of OOD tasks while ensuring the accuracy of ID recognition. Through extensive experiments conducted on NTU-RGB+D 60, NTU-RGB+D 120, and Kinetics-400 datasets, Skeleton-OOD demonstrates the superior performance of our proposed approach compared to state-of-the-art methods. Our findings underscore the effectiveness of classic OOD detection techniques in the context of skeleton-based action recognition tasks, offering promising avenues for future research in this field. Code is available at https://github.com/YilliaJing/Skeleton-OOD.git.'}",oai:arXiv.org:2405.20633v3,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Jing Xu, Anqi Zhu, Jingyu Lin, Qiuhong Ke, Cunjian Chen'}]","Jing Xu, Anqi Zhu, Jingyu Lin, Qiuhong Ke, Cunjian Chen","{'name': 'Jing Xu, Anqi Zhu, Jingyu Lin, Qiuhong Ke, Cunjian Chen'}",,
558,Diversifying Query: Region-Guided Transformer for Temporal Sentence Grounding,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Diversifying Query: Region-Guided Transformer for Temporal Sentence Grounding'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2406.00143'}]",https://arxiv.org/abs/2406.00143,"arXiv:2406.00143v2 Announce Type: replace 
Abstract: Temporal sentence grounding is a challenging task that aims to localize the moment spans relevant to a language description. Although recent DETR-based models have achieved notable progress by leveraging multiple learnable moment queries, they suffer from overlapped and redundant proposals, leading to inaccurate predictions. We attribute this limitation to the lack of task-related guidance for the learnable queries to serve a specific mode. Furthermore, the complex solution space generated by variable and open-vocabulary language descriptions complicates optimization, making it harder for learnable queries to distinguish each other adaptively. To tackle this limitation, we present a Region-Guided TRansformer (RGTR) for temporal sentence grounding, which diversifies moment queries to eliminate overlapped and redundant predictions. Instead of using learnable queries, RGTR adopts a set of anchor pairs as moment queries to introduce explicit regional guidance. Each anchor pair takes charge of moment prediction for a specific temporal region, which reduces the optimization difficulty and ensures the diversity of the final predictions. In addition, we design an IoU-aware scoring head to improve proposal quality. Extensive experiments demonstrate the effectiveness of RGTR, outperforming state-of-the-art methods on QVHighlights, Charades-STA and TACoS datasets. Codes are available at https://github.com/TensorsSun/RGTR","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2406.00143v2 Announce Type: replace \nAbstract: Temporal sentence grounding is a challenging task that aims to localize the moment spans relevant to a language description. Although recent DETR-based models have achieved notable progress by leveraging multiple learnable moment queries, they suffer from overlapped and redundant proposals, leading to inaccurate predictions. We attribute this limitation to the lack of task-related guidance for the learnable queries to serve a specific mode. Furthermore, the complex solution space generated by variable and open-vocabulary language descriptions complicates optimization, making it harder for learnable queries to distinguish each other adaptively. To tackle this limitation, we present a Region-Guided TRansformer (RGTR) for temporal sentence grounding, which diversifies moment queries to eliminate overlapped and redundant predictions. Instead of using learnable queries, RGTR adopts a set of anchor pairs as moment queries to introduce explicit regional guidance. Each anchor pair takes charge of moment prediction for a specific temporal region, which reduces the optimization difficulty and ensures the diversity of the final predictions. In addition, we design an IoU-aware scoring head to improve proposal quality. Extensive experiments demonstrate the effectiveness of RGTR, outperforming state-of-the-art methods on QVHighlights, Charades-STA and TACoS datasets. Codes are available at https://github.com/TensorsSun/RGTR'}",oai:arXiv.org:2406.00143v2,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Xiaolong Sun, Liushuai Shi, Le Wang, Sanping Zhou, Kun Xia, Yabing Wang, Gang Hua'}]","Xiaolong Sun, Liushuai Shi, Le Wang, Sanping Zhou, Kun Xia, Yabing Wang, Gang Hua","{'name': 'Xiaolong Sun, Liushuai Shi, Le Wang, Sanping Zhou, Kun Xia, Yabing Wang, Gang Hua'}",,
559,Benchmarking the Communication Competence of Code Generation for LLMs and LLM Agent,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Benchmarking the Communication Competence of Code Generation for LLMs and LLM Agent'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2406.00215'}]",https://arxiv.org/abs/2406.00215,"arXiv:2406.00215v2 Announce Type: replace 
Abstract: Large language models (LLMs) have significantly improved their ability to perform tasks in the field of code generation. However, there is still a gap between LLMs being capable coders and being top-tier software engineers. Based on the observation that top-level software engineers often ask clarifying questions to reduce ambiguity in both requirements and coding solutions, we argue that the same should be applied to LLMs for code generation tasks.
  In this work, we conducted an empirical study on the benchmark and analysis of the communication skills of LLMs for code generation. We define communication skills of LLMs as ``being able to ask clarifying questions when the description of the code generation problem has issues''. We created a new benchmark, HumanEvalComm, by modifying problem descriptions according to three issues: inconsistency, ambiguity, incompleteness. We defined new evaluation metrics such as Communication Rate and Good Question Rate, and then experimented on HumanEvalComm with different Code LLMs, and a new LLM agent approach, Okanagan, to identify and ask questions in ambiguous parts from code and descriptions for further refining the generated code. Finally, we discussed evaluation results by comparing Code LLMs and Okanagan with our findings.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2406.00215v2 Announce Type: replace \nAbstract: Large language models (LLMs) have significantly improved their ability to perform tasks in the field of code generation. However, there is still a gap between LLMs being capable coders and being top-tier software engineers. Based on the observation that top-level software engineers often ask clarifying questions to reduce ambiguity in both requirements and coding solutions, we argue that the same should be applied to LLMs for code generation tasks.\n  In this work, we conducted an empirical study on the benchmark and analysis of the communication skills of LLMs for code generation. We define communication skills of LLMs as ``being able to ask clarifying questions when the description of the code generation problem has issues''. We created a new benchmark, HumanEvalComm, by modifying problem descriptions according to three issues: inconsistency, ambiguity, incompleteness. We defined new evaluation metrics such as Communication Rate and Good Question Rate, and then experimented on HumanEvalComm with different Code LLMs, and a new LLM agent approach, Okanagan, to identify and ask questions in ambiguous parts from code and descriptions for further refining the generated code. Finally, we discussed evaluation results by comparing Code LLMs and Okanagan with our findings.""}",oai:arXiv.org:2406.00215v2,False,"[{'term': 'cs.SE', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Jie JW Wu, Fatemeh H. Fard'}]","Jie JW Wu, Fatemeh H. Fard","{'name': 'Jie JW Wu, Fatemeh H. Fard'}",,
560,Guiding a Diffusion Model with a Bad Version of Itself,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Guiding a Diffusion Model with a Bad Version of Itself'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2406.02507'}]",https://arxiv.org/abs/2406.02507,"arXiv:2406.02507v3 Announce Type: replace 
Abstract: The primary axes of interest in image-generating diffusion models are image quality, the amount of variation in the results, and how well the results align with a given condition, e.g., a class label or a text prompt. The popular classifier-free guidance approach uses an unconditional model to guide a conditional model, leading to simultaneously better prompt alignment and higher-quality images at the cost of reduced variation. These effects seem inherently entangled, and thus hard to control. We make the surprising observation that it is possible to obtain disentangled control over image quality without compromising the amount of variation by guiding generation using a smaller, less-trained version of the model itself rather than an unconditional model. This leads to significant improvements in ImageNet generation, setting record FIDs of 1.01 for 64x64 and 1.25 for 512x512, using publicly available networks. Furthermore, the method is also applicable to unconditional diffusion models, drastically improving their quality.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2406.02507v3 Announce Type: replace \nAbstract: The primary axes of interest in image-generating diffusion models are image quality, the amount of variation in the results, and how well the results align with a given condition, e.g., a class label or a text prompt. The popular classifier-free guidance approach uses an unconditional model to guide a conditional model, leading to simultaneously better prompt alignment and higher-quality images at the cost of reduced variation. These effects seem inherently entangled, and thus hard to control. We make the surprising observation that it is possible to obtain disentangled control over image quality without compromising the amount of variation by guiding generation using a smaller, less-trained version of the model itself rather than an unconditional model. This leads to significant improvements in ImageNet generation, setting record FIDs of 1.01 for 64x64 and 1.25 for 512x512, using publicly available networks. Furthermore, the method is also applicable to unconditional diffusion models, drastically improving their quality.'}",oai:arXiv.org:2406.02507v3,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'cs.NE', 'scheme': None, 'label': None}, {'term': 'stat.ML', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Tero Karras, Miika Aittala, Tuomas Kynk\\""a\\""anniemi, Jaakko Lehtinen, Timo Aila, Samuli Laine'}]","Tero Karras, Miika Aittala, Tuomas Kynk\""a\""anniemi, Jaakko Lehtinen, Timo Aila, Samuli Laine","{'name': 'Tero Karras, Miika Aittala, Tuomas Kynk\\""a\\""anniemi, Jaakko Lehtinen, Timo Aila, Samuli Laine'}",,
561,Discovering Continuous-Time Memory-Based Symbolic Policies using Genetic Programming,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Discovering Continuous-Time Memory-Based Symbolic Policies using Genetic Programming'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2406.02765'}]",https://arxiv.org/abs/2406.02765,"arXiv:2406.02765v5 Announce Type: replace 
Abstract: Artificial intelligence techniques are increasingly being applied to solve control problems, but often rely on black-box methods without transparent output generation. To improve the interpretability and transparency in control systems, models can be defined as white-box symbolic policies described by mathematical expressions. For better performance in partially observable and volatile environments, the symbolic policies are extended with memory represented by continuous-time latent variables, governed by differential equations. Genetic programming is used for optimisation, resulting in interpretable policies consisting of symbolic expressions. Our results show that symbolic policies with memory compare with black-box policies on a variety of control tasks. Furthermore, the benefit of the memory in symbolic policies is demonstrated on experiments where memory-less policies fall short. Overall, we present a method for evolving high-performing symbolic policies that offer interpretability and transparency, which lacks in black-box models.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2406.02765v5 Announce Type: replace \nAbstract: Artificial intelligence techniques are increasingly being applied to solve control problems, but often rely on black-box methods without transparent output generation. To improve the interpretability and transparency in control systems, models can be defined as white-box symbolic policies described by mathematical expressions. For better performance in partially observable and volatile environments, the symbolic policies are extended with memory represented by continuous-time latent variables, governed by differential equations. Genetic programming is used for optimisation, resulting in interpretable policies consisting of symbolic expressions. Our results show that symbolic policies with memory compare with black-box policies on a variety of control tasks. Furthermore, the benefit of the memory in symbolic policies is demonstrated on experiments where memory-less policies fall short. Overall, we present a method for evolving high-performing symbolic policies that offer interpretability and transparency, which lacks in black-box models.'}",oai:arXiv.org:2406.02765v5,False,"[{'term': 'cs.NE', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Sigur de Vries, Sander Keemink, Marcel van Gerven'}]","Sigur de Vries, Sander Keemink, Marcel van Gerven","{'name': 'Sigur de Vries, Sander Keemink, Marcel van Gerven'}",,
562,RU-AI: A Large Multimodal Dataset for Machine-Generated Content Detection,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'RU-AI: A Large Multimodal Dataset for Machine-Generated Content Detection'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2406.04906'}]",https://arxiv.org/abs/2406.04906,"arXiv:2406.04906v2 Announce Type: replace 
Abstract: The recent generative AI models' capability of creating realistic and human-like content is significantly transforming the ways in which people communicate, create and work. The appropriate use of generative AI models can benefit society, while their misuse poses threats to the society. However, the lack of aligned multimodal datasets has inhibited the development of effective and robust methods for detecting machine-generated content, particularly in triple-modality settings (e.g., text, image, and voice). In this paper, we introduce RU-AI, a new large-scale multimodal dataset for robust and efficient detection of machine-generated content in text, image and voice. Our dataset is constructed on the basis of three large publicly available datasets: Flickr8K, COCO and Places205, by adding their corresponding AI duplicates, resulting total of 1,475,370 data instances. In addition, we create a noise variant of each modality of the datasets aiming to analyse the models' robustness. Given our dataset, we conduct extensive experiments with the current SOTA detection methods. The results reveal that existing models still struggle to achieve accurate and robust classification after training on our dataset. The RU-AI dataset is designed to support the development of detection methods across modalities and can be effectively utilised for identifying machine-generated content. The source code and dataset are available at https://github.com/ZhihaoZhang97/RU-AI.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2406.04906v2 Announce Type: replace \nAbstract: The recent generative AI models' capability of creating realistic and human-like content is significantly transforming the ways in which people communicate, create and work. The appropriate use of generative AI models can benefit society, while their misuse poses threats to the society. However, the lack of aligned multimodal datasets has inhibited the development of effective and robust methods for detecting machine-generated content, particularly in triple-modality settings (e.g., text, image, and voice). In this paper, we introduce RU-AI, a new large-scale multimodal dataset for robust and efficient detection of machine-generated content in text, image and voice. Our dataset is constructed on the basis of three large publicly available datasets: Flickr8K, COCO and Places205, by adding their corresponding AI duplicates, resulting total of 1,475,370 data instances. In addition, we create a noise variant of each modality of the datasets aiming to analyse the models' robustness. Given our dataset, we conduct extensive experiments with the current SOTA detection methods. The results reveal that existing models still struggle to achieve accurate and robust classification after training on our dataset. The RU-AI dataset is designed to support the development of detection methods across modalities and can be effectively utilised for identifying machine-generated content. The source code and dataset are available at https://github.com/ZhihaoZhang97/RU-AI.""}",oai:arXiv.org:2406.04906v2,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by-nc-sa/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-sa/4.0/'}","[{'name': 'Liting Huang, Zhihao Zhang, Yiran Zhang, Xiyue Zhou, Shoujin Wang'}]","Liting Huang, Zhihao Zhang, Yiran Zhang, Xiyue Zhou, Shoujin Wang","{'name': 'Liting Huang, Zhihao Zhang, Yiran Zhang, Xiyue Zhou, Shoujin Wang'}",,
563,Identifying Query-Relevant Neurons in Large Language Models for Long-Form Texts,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Identifying Query-Relevant Neurons in Large Language Models for Long-Form Texts'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2406.10868'}]",https://arxiv.org/abs/2406.10868,"arXiv:2406.10868v4 Announce Type: replace 
Abstract: Large Language Models (LLMs) possess vast amounts of knowledge within their parameters, prompting research into methods for locating and editing this knowledge. Previous work has largely focused on locating entity-related (often single-token) facts in smaller models. However, several key questions remain unanswered: (1) How can we effectively locate query-relevant neurons in decoder-only LLMs, such as Llama and Mistral? (2) How can we address the challenge of long-form (or free-form) text generation? (3) Are there localized knowledge regions in LLMs? In this study, we introduce Query-Relevant Neuron Cluster Attribution (QRNCA), a novel architecture-agnostic framework capable of identifying query-relevant neurons in LLMs. QRNCA allows for the examination of long-form answers beyond triplet facts by employing the proxy task of multi-choice question answering. To evaluate the effectiveness of our detected neurons, we build two multi-choice QA datasets spanning diverse domains and languages. Empirical evaluations demonstrate that our method outperforms baseline methods significantly. Further, analysis of neuron distributions reveals the presence of visible localized regions, particularly within different domains. Finally, we show potential applications of our detected neurons in knowledge editing and neuron-based prediction.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2406.10868v4 Announce Type: replace \nAbstract: Large Language Models (LLMs) possess vast amounts of knowledge within their parameters, prompting research into methods for locating and editing this knowledge. Previous work has largely focused on locating entity-related (often single-token) facts in smaller models. However, several key questions remain unanswered: (1) How can we effectively locate query-relevant neurons in decoder-only LLMs, such as Llama and Mistral? (2) How can we address the challenge of long-form (or free-form) text generation? (3) Are there localized knowledge regions in LLMs? In this study, we introduce Query-Relevant Neuron Cluster Attribution (QRNCA), a novel architecture-agnostic framework capable of identifying query-relevant neurons in LLMs. QRNCA allows for the examination of long-form answers beyond triplet facts by employing the proxy task of multi-choice question answering. To evaluate the effectiveness of our detected neurons, we build two multi-choice QA datasets spanning diverse domains and languages. Empirical evaluations demonstrate that our method outperforms baseline methods significantly. Further, analysis of neuron distributions reveals the presence of visible localized regions, particularly within different domains. Finally, we show potential applications of our detected neurons in knowledge editing and neuron-based prediction.'}",oai:arXiv.org:2406.10868v4,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Lihu Chen, Adam Dejl, Francesca Toni'}]","Lihu Chen, Adam Dejl, Francesca Toni","{'name': 'Lihu Chen, Adam Dejl, Francesca Toni'}",,
564,Do Parameters Reveal More than Loss for Membership Inference?,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Do Parameters Reveal More than Loss for Membership Inference?'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2406.11544'}]",https://arxiv.org/abs/2406.11544,"arXiv:2406.11544v4 Announce Type: replace 
Abstract: Membership inference attacks are used as a key tool for disclosure auditing. They aim to infer whether an individual record was used to train a model. While such evaluations are useful to demonstrate risk, they are computationally expensive and often make strong assumptions about potential adversaries' access to models and training environments, and thus do not provide tight bounds on leakage from potential attacks. We show how prior claims around black-box access being sufficient for optimal membership inference do not hold for stochastic gradient descent, and that optimal membership inference indeed requires white-box access. Our theoretical results lead to a new white-box inference attack, IHA (Inverse Hessian Attack), that explicitly uses model parameters by taking advantage of computing inverse-Hessian vector products. Our results show that both auditors and adversaries may be able to benefit from access to model parameters, and we advocate for further research into white-box methods for membership inference.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2406.11544v4 Announce Type: replace \nAbstract: Membership inference attacks are used as a key tool for disclosure auditing. They aim to infer whether an individual record was used to train a model. While such evaluations are useful to demonstrate risk, they are computationally expensive and often make strong assumptions about potential adversaries' access to models and training environments, and thus do not provide tight bounds on leakage from potential attacks. We show how prior claims around black-box access being sufficient for optimal membership inference do not hold for stochastic gradient descent, and that optimal membership inference indeed requires white-box access. Our theoretical results lead to a new white-box inference attack, IHA (Inverse Hessian Attack), that explicitly uses model parameters by taking advantage of computing inverse-Hessian vector products. Our results show that both auditors and adversaries may be able to benefit from access to model parameters, and we advocate for further research into white-box methods for membership inference.""}",oai:arXiv.org:2406.11544v4,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.CR', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Anshuman Suri, Xiao Zhang, David Evans'}]","Anshuman Suri, Xiao Zhang, David Evans","{'name': 'Anshuman Suri, Xiao Zhang, David Evans'}",,
565,DialSim: A Real-Time Simulator for Evaluating Long-Term Multi-Party Dialogue Understanding of Conversational Agents,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'DialSim: A Real-Time Simulator for Evaluating Long-Term Multi-Party Dialogue Understanding of Conversational Agents'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2406.13144'}]",https://arxiv.org/abs/2406.13144,"arXiv:2406.13144v3 Announce Type: replace 
Abstract: Recent advancements in Large Language Models (LLMs) have significantly enhanced the capabilities of conversational agents, making them applicable to various fields (e.g., education). Despite their progress, the evaluation of the agents often overlooks the complexities of real-world conversations, such as real-time interactions, multi-party dialogues, and extended contextual dependencies. To bridge this gap, we introduce DialSim, a real-time dialogue simulator. In this simulator, an agent is assigned the role of a character from popular TV shows, requiring it to respond to spontaneous questions using past dialogue information and to distinguish between known and unknown information. Key features of DialSim include assessing the agent's ability to respond within a reasonable time limit, handling long-term multi-party dialogues, and evaluating performance under randomized questioning with LongDialQA, a novel, high-quality question-answering dataset. Our experiments using DialSim reveal the strengths and weaknesses of the latest conversational agents, offering valuable insights for future advancements in conversational AI. DialSim is available at https://dialsim.github.io/.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2406.13144v3 Announce Type: replace \nAbstract: Recent advancements in Large Language Models (LLMs) have significantly enhanced the capabilities of conversational agents, making them applicable to various fields (e.g., education). Despite their progress, the evaluation of the agents often overlooks the complexities of real-world conversations, such as real-time interactions, multi-party dialogues, and extended contextual dependencies. To bridge this gap, we introduce DialSim, a real-time dialogue simulator. In this simulator, an agent is assigned the role of a character from popular TV shows, requiring it to respond to spontaneous questions using past dialogue information and to distinguish between known and unknown information. Key features of DialSim include assessing the agent's ability to respond within a reasonable time limit, handling long-term multi-party dialogues, and evaluating performance under randomized questioning with LongDialQA, a novel, high-quality question-answering dataset. Our experiments using DialSim reveal the strengths and weaknesses of the latest conversational agents, offering valuable insights for future advancements in conversational AI. DialSim is available at https://dialsim.github.io/.""}",oai:arXiv.org:2406.13144v3,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Jiho Kim, Woosog Chay, Hyeonji Hwang, Daeun Kyung, Hyunseung Chung, Eunbyeol Cho, Yohan Jo, Edward Choi'}]","Jiho Kim, Woosog Chay, Hyeonji Hwang, Daeun Kyung, Hyunseung Chung, Eunbyeol Cho, Yohan Jo, Edward Choi","{'name': 'Jiho Kim, Woosog Chay, Hyeonji Hwang, Daeun Kyung, Hyunseung Chung, Eunbyeol Cho, Yohan Jo, Edward Choi'}",,
566,VisualRWKV: Exploring Recurrent Neural Networks for Visual Language Models,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'VisualRWKV: Exploring Recurrent Neural Networks for Visual Language Models'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2406.13362'}]",https://arxiv.org/abs/2406.13362,"arXiv:2406.13362v3 Announce Type: replace 
Abstract: Visual Language Models (VLMs) have rapidly progressed with the recent success of large language models. However, there have been few attempts to incorporate efficient linear Recurrent Neural Networks (RNNs) architectures into VLMs. In this study, we introduce VisualRWKV, the first application of a linear RNN model to multimodal learning tasks, leveraging the pre-trained RWKV language model. We propose a data-dependent recurrence and sandwich prompts to enhance our modeling capabilities, along with a 2D image scanning mechanism to enrich the processing of visual sequences. Extensive experiments demonstrate that VisualRWKV achieves competitive performance compared to Transformer-based models like LLaVA-1.5 on various benchmarks. Compared to LLaVA-1.5, VisualRWKV has a speed advantage of 3.98 times and can save 54% of GPU memory when reaching an inference length of 24K tokens. To facilitate further research and analysis, we have made the checkpoints and the associated code publicly accessible at the following GitHub repository: see https://github.com/howard-hou/VisualRWKV.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2406.13362v3 Announce Type: replace \nAbstract: Visual Language Models (VLMs) have rapidly progressed with the recent success of large language models. However, there have been few attempts to incorporate efficient linear Recurrent Neural Networks (RNNs) architectures into VLMs. In this study, we introduce VisualRWKV, the first application of a linear RNN model to multimodal learning tasks, leveraging the pre-trained RWKV language model. We propose a data-dependent recurrence and sandwich prompts to enhance our modeling capabilities, along with a 2D image scanning mechanism to enrich the processing of visual sequences. Extensive experiments demonstrate that VisualRWKV achieves competitive performance compared to Transformer-based models like LLaVA-1.5 on various benchmarks. Compared to LLaVA-1.5, VisualRWKV has a speed advantage of 3.98 times and can save 54% of GPU memory when reaching an inference length of 24K tokens. To facilitate further research and analysis, we have made the checkpoints and the associated code publicly accessible at the following GitHub repository: see https://github.com/howard-hou/VisualRWKV.'}",oai:arXiv.org:2406.13362v3,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.CL', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Haowen Hou, Peigen Zeng, Fei Ma, Fei Richard Yu'}]","Haowen Hou, Peigen Zeng, Fei Ma, Fei Richard Yu","{'name': 'Haowen Hou, Peigen Zeng, Fei Ma, Fei Richard Yu'}",,
567,Latent Variable Sequence Identification for Cognitive Models with Neural Network Estimators,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Latent Variable Sequence Identification for Cognitive Models with Neural Network Estimators'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2406.14742'}]",https://arxiv.org/abs/2406.14742,"arXiv:2406.14742v2 Announce Type: replace 
Abstract: Extracting time-varying latent variables from computational cognitive models is a key step in model-based neural analysis, which aims to understand the neural correlates of cognitive processes. However, existing methods only allow researchers to infer latent variables that explain subjects' behavior in a relatively small class of cognitive models. For example, a broad class of relevant cognitive models with analytically intractable likelihood is currently out of reach from standard techniques, based on Maximum a Posteriori parameter estimation. Here, we present an approach that extends neural Bayes estimation to learn a direct mapping between experimental data and the targeted latent variable space using recurrent neural networks and simulated datasets. We show that our approach achieves competitive performance in inferring latent variable sequences in both tractable and intractable models. Furthermore, the approach is generalizable across different computational models and is adaptable for both continuous and discrete latent spaces. We then demonstrate its applicability in real world datasets. Our work underscores that combining recurrent neural networks and simulation-based inference to identify latent variable sequences can enable researchers to access a wider class of cognitive models for model-based neural analyses, and thus test a broader set of theories.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2406.14742v2 Announce Type: replace \nAbstract: Extracting time-varying latent variables from computational cognitive models is a key step in model-based neural analysis, which aims to understand the neural correlates of cognitive processes. However, existing methods only allow researchers to infer latent variables that explain subjects' behavior in a relatively small class of cognitive models. For example, a broad class of relevant cognitive models with analytically intractable likelihood is currently out of reach from standard techniques, based on Maximum a Posteriori parameter estimation. Here, we present an approach that extends neural Bayes estimation to learn a direct mapping between experimental data and the targeted latent variable space using recurrent neural networks and simulated datasets. We show that our approach achieves competitive performance in inferring latent variable sequences in both tractable and intractable models. Furthermore, the approach is generalizable across different computational models and is adaptable for both continuous and discrete latent spaces. We then demonstrate its applicability in real world datasets. Our work underscores that combining recurrent neural networks and simulation-based inference to identify latent variable sequences can enable researchers to access a wider class of cognitive models for model-based neural analyses, and thus test a broader set of theories.""}",oai:arXiv.org:2406.14742v2,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'stat.ML', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Ti-Fen Pan, Jing-Jing Li, Bill Thompson, Anne Collins'}]","Ti-Fen Pan, Jing-Jing Li, Bill Thompson, Anne Collins","{'name': 'Ti-Fen Pan, Jing-Jing Li, Bill Thompson, Anne Collins'}",,
568,Resilience of the Electric Grid through Trustable IoT-Coordinated Assets (Extended version),"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Resilience of the Electric Grid through Trustable IoT-Coordinated Assets (Extended version)'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2406.14861'}]",https://arxiv.org/abs/2406.14861,"arXiv:2406.14861v3 Announce Type: replace 
Abstract: The electricity grid has evolved from a physical system to a cyber-physical system with digital devices that perform measurement, control, communication, computation, and actuation. The increased penetration of distributed energy resources (DERs) including renewable generation, flexible loads, and storage provides extraordinary opportunities for improvements in efficiency and sustainability. However, they can introduce new vulnerabilities in the form of cyberattacks, which can cause significant challenges in ensuring grid resilience. We propose a framework in this paper for achieving grid resilience through suitably coordinated assets including a network of Internet of Things (IoT) devices. A local electricity market is proposed to identify trustable assets and carry out this coordination. Situational Awareness (SA) of locally available DERs with the ability to inject power or reduce consumption is enabled by the market, together with a monitoring procedure for their trustability and commitment. With this SA, we show that a variety of cyberattacks can be mitigated using local trustable resources without stressing the bulk grid. Multiple demonstrations are carried out using a high-fidelity co-simulation platform, real-time hardware-in-the-loop validation, and a utility-friendly simulator.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2406.14861v3 Announce Type: replace \nAbstract: The electricity grid has evolved from a physical system to a cyber-physical system with digital devices that perform measurement, control, communication, computation, and actuation. The increased penetration of distributed energy resources (DERs) including renewable generation, flexible loads, and storage provides extraordinary opportunities for improvements in efficiency and sustainability. However, they can introduce new vulnerabilities in the form of cyberattacks, which can cause significant challenges in ensuring grid resilience. We propose a framework in this paper for achieving grid resilience through suitably coordinated assets including a network of Internet of Things (IoT) devices. A local electricity market is proposed to identify trustable assets and carry out this coordination. Situational Awareness (SA) of locally available DERs with the ability to inject power or reduce consumption is enabled by the market, together with a monitoring procedure for their trustability and commitment. With this SA, we show that a variety of cyberattacks can be mitigated using local trustable resources without stressing the bulk grid. Multiple demonstrations are carried out using a high-fidelity co-simulation platform, real-time hardware-in-the-loop validation, and a utility-friendly simulator.'}",oai:arXiv.org:2406.14861v3,False,"[{'term': 'eess.SY', 'scheme': None, 'label': None}, {'term': 'cs.ET', 'scheme': None, 'label': None}, {'term': 'cs.SY', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Vineet J. Nair, Venkatesh Venkataramanan, Priyank Srivastava, Partha S. Sarker, Anurag Srivastava, Laurentiu D. Marinovici, Jun Zha, Christopher Irwin, Prateek Mittal, John Williams, Jayant Kumar, H. Vincent Poor, Anuradha M. Annaswamy'}]","Vineet J. Nair, Venkatesh Venkataramanan, Priyank Srivastava, Partha S. Sarker, Anurag Srivastava, Laurentiu D. Marinovici, Jun Zha, Christopher Irwin, Prateek Mittal, John Williams, Jayant Kumar, H. Vincent Poor, Anuradha M. Annaswamy","{'name': 'Vineet J. Nair, Venkatesh Venkataramanan, Priyank Srivastava, Partha S. Sarker, Anurag Srivastava, Laurentiu D. Marinovici, Jun Zha, Christopher Irwin, Prateek Mittal, John Williams, Jayant Kumar, H. Vincent Poor, Anuradha M. Annaswamy'}",,
569,Cherry on the Cake: Fairness is NOT an Optimization Problem,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Cherry on the Cake: Fairness is NOT an Optimization Problem'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2406.16606'}]",https://arxiv.org/abs/2406.16606,"arXiv:2406.16606v2 Announce Type: replace 
Abstract: In Fair AI literature, the practice of maliciously creating unfair models that nevertheless satisfy fairness constraints is known as ""cherry-picking"". A cherry-picking model is a model that makes mistakes on purpose, selecting bad individuals from a minority class instead of better candidates from the same minority. The model literally cherry-picks whom to select to superficially meet the fairness constraints while making minimal changes to the unfair model. This practice has been described as ""blatantly unfair"" and has a negative impact on already marginalized communities, undermining the intended purpose of fairness measures specifically designed to protect these communities. A common assumption is that cherry-picking arises solely from malicious intent and that models designed only to optimize fairness metrics would avoid this behavior. We show that this is not the case: models optimized to minimize fairness metrics while maximizing performance are often forced to cherry-pick to some degree. In other words, cherry-picking might be an inevitable outcome of the optimization process itself. To demonstrate this, we use tools from fair cake-cutting, a mathematical subfield that studies the problem of fairly dividing a resource, referred to as the ""cake,"" among a number of participants. This concept is connected to supervised multi-label classification: any dataset can be thought of as a cake that needs to be distributed among different labels, and the model is the function that divides the cake. We adapt these classical results for machine learning and demonstrate how this connection can be prolifically used for fairness and classification in general.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2406.16606v2 Announce Type: replace \nAbstract: In Fair AI literature, the practice of maliciously creating unfair models that nevertheless satisfy fairness constraints is known as ""cherry-picking"". A cherry-picking model is a model that makes mistakes on purpose, selecting bad individuals from a minority class instead of better candidates from the same minority. The model literally cherry-picks whom to select to superficially meet the fairness constraints while making minimal changes to the unfair model. This practice has been described as ""blatantly unfair"" and has a negative impact on already marginalized communities, undermining the intended purpose of fairness measures specifically designed to protect these communities. A common assumption is that cherry-picking arises solely from malicious intent and that models designed only to optimize fairness metrics would avoid this behavior. We show that this is not the case: models optimized to minimize fairness metrics while maximizing performance are often forced to cherry-pick to some degree. In other words, cherry-picking might be an inevitable outcome of the optimization process itself. To demonstrate this, we use tools from fair cake-cutting, a mathematical subfield that studies the problem of fairly dividing a resource, referred to as the ""cake,"" among a number of participants. This concept is connected to supervised multi-label classification: any dataset can be thought of as a cake that needs to be distributed among different labels, and the model is the function that divides the cake. We adapt these classical results for machine learning and demonstrate how this connection can be prolifically used for fairness and classification in general.'}",oai:arXiv.org:2406.16606v2,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'cs.CY', 'scheme': None, 'label': None}, {'term': 'cs.GT', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Marco Favier, Toon Calders'}]","Marco Favier, Toon Calders","{'name': 'Marco Favier, Toon Calders'}",,
570,ID-Sculpt: ID-aware 3D Head Generation from Single In-the-wild Portrait Image,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'ID-Sculpt: ID-aware 3D Head Generation from Single In-the-wild Portrait Image'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2406.16710'}]",https://arxiv.org/abs/2406.16710,"arXiv:2406.16710v2 Announce Type: replace 
Abstract: While recent works have achieved great success on image-to-3D object generation, high quality and fidelity 3D head generation from a single image remains a great challenge. Previous text-based methods for generating 3D heads were limited by text descriptions and image-based methods struggled to produce high-quality head geometry. To handle this challenging problem, we propose a novel framework, ID-Sculpt, to generate high-quality 3D heads while preserving their identities. Our work incorporates the identity information of the portrait image into three parts: 1) geometry initialization, 2) geometry sculpting, and 3) texture generation stages. Given a reference portrait image, we first align the identity features with text features to realize ID-aware guidance enhancement, which contains the control signals representing the face information. We then use the canny map, ID features of the portrait image, and a pre-trained text-to-normal/depth diffusion model to generate ID-aware geometry supervision, and 3D-GAN inversion is employed to generate ID-aware geometry initialization. Furthermore, with the ability to inject identity information into 3D head generation, we use ID-aware guidance to calculate ID-aware Score Distillation (ISD) for geometry sculpting. For texture generation, we adopt the ID Consistent Texture Inpainting and Refinement which progressively expands the view for texture inpainting to obtain an initialization UV texture map. We then use the ID-aware guidance to provide image-level supervision for noisy multi-view images to obtain a refined texture map. Extensive experiments demonstrate that we can generate high-quality 3D heads with accurate geometry and texture from a single in-the-wild portrait image.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2406.16710v2 Announce Type: replace \nAbstract: While recent works have achieved great success on image-to-3D object generation, high quality and fidelity 3D head generation from a single image remains a great challenge. Previous text-based methods for generating 3D heads were limited by text descriptions and image-based methods struggled to produce high-quality head geometry. To handle this challenging problem, we propose a novel framework, ID-Sculpt, to generate high-quality 3D heads while preserving their identities. Our work incorporates the identity information of the portrait image into three parts: 1) geometry initialization, 2) geometry sculpting, and 3) texture generation stages. Given a reference portrait image, we first align the identity features with text features to realize ID-aware guidance enhancement, which contains the control signals representing the face information. We then use the canny map, ID features of the portrait image, and a pre-trained text-to-normal/depth diffusion model to generate ID-aware geometry supervision, and 3D-GAN inversion is employed to generate ID-aware geometry initialization. Furthermore, with the ability to inject identity information into 3D head generation, we use ID-aware guidance to calculate ID-aware Score Distillation (ISD) for geometry sculpting. For texture generation, we adopt the ID Consistent Texture Inpainting and Refinement which progressively expands the view for texture inpainting to obtain an initialization UV texture map. We then use the ID-aware guidance to provide image-level supervision for noisy multi-view images to obtain a refined texture map. Extensive experiments demonstrate that we can generate high-quality 3D heads with accurate geometry and texture from a single in-the-wild portrait image.'}",oai:arXiv.org:2406.16710v2,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Jinkun Hao, Junshu Tang, Jiangning Zhang, Ran Yi, Yijia Hong, Moran Li, Weijian Cao, Yating Wang, Chengjie Wang, Lizhuang Ma'}]","Jinkun Hao, Junshu Tang, Jiangning Zhang, Ran Yi, Yijia Hong, Moran Li, Weijian Cao, Yating Wang, Chengjie Wang, Lizhuang Ma","{'name': 'Jinkun Hao, Junshu Tang, Jiangning Zhang, Ran Yi, Yijia Hong, Moran Li, Weijian Cao, Yating Wang, Chengjie Wang, Lizhuang Ma'}",,
571,Disentangled Motion Modeling for Video Frame Interpolation,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Disentangled Motion Modeling for Video Frame Interpolation'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2406.17256'}]",https://arxiv.org/abs/2406.17256,"arXiv:2406.17256v2 Announce Type: replace 
Abstract: Video Frame Interpolation (VFI) aims to synthesize intermediate frames between existing frames to enhance visual smoothness and quality. Beyond the conventional methods based on the reconstruction loss, recent works have employed generative models for improved perceptual quality. However, they require complex training and large computational costs for pixel space modeling. In this paper, we introduce disentangled Motion Modeling (MoMo), a diffusion-based approach for VFI that enhances visual quality by focusing on intermediate motion modeling. We propose a disentangled two-stage training process. In the initial stage, frame synthesis and flow models are trained to generate accurate frames and flows optimal for synthesis. In the subsequent stage, we introduce a motion diffusion model, which incorporates our novel U-Net architecture specifically designed for optical flow, to generate bi-directional flows between frames. By learning the simpler low-frequency representation of motions, MoMo achieves superior perceptual quality with reduced computational demands compared to the generative modeling methods on the pixel space. MoMo surpasses state-of-the-art methods in perceptual metrics across various benchmarks, demonstrating its efficacy and efficiency in VFI.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2406.17256v2 Announce Type: replace \nAbstract: Video Frame Interpolation (VFI) aims to synthesize intermediate frames between existing frames to enhance visual smoothness and quality. Beyond the conventional methods based on the reconstruction loss, recent works have employed generative models for improved perceptual quality. However, they require complex training and large computational costs for pixel space modeling. In this paper, we introduce disentangled Motion Modeling (MoMo), a diffusion-based approach for VFI that enhances visual quality by focusing on intermediate motion modeling. We propose a disentangled two-stage training process. In the initial stage, frame synthesis and flow models are trained to generate accurate frames and flows optimal for synthesis. In the subsequent stage, we introduce a motion diffusion model, which incorporates our novel U-Net architecture specifically designed for optical flow, to generate bi-directional flows between frames. By learning the simpler low-frequency representation of motions, MoMo achieves superior perceptual quality with reduced computational demands compared to the generative modeling methods on the pixel space. MoMo surpasses state-of-the-art methods in perceptual metrics across various benchmarks, demonstrating its efficacy and efficiency in VFI.'}",oai:arXiv.org:2406.17256v2,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by-nc-sa/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-sa/4.0/'}","[{'name': 'Jaihyun Lew, Jooyoung Choi, Chaehun Shin, Dahuin Jung, Sungroh Yoon'}]","Jaihyun Lew, Jooyoung Choi, Chaehun Shin, Dahuin Jung, Sungroh Yoon","{'name': 'Jaihyun Lew, Jooyoung Choi, Chaehun Shin, Dahuin Jung, Sungroh Yoon'}",,
572,SafeAligner: Safety Alignment against Jailbreak Attacks via Response Disparity Guidance,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'SafeAligner: Safety Alignment against Jailbreak Attacks via Response Disparity Guidance'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2406.18118'}]",https://arxiv.org/abs/2406.18118,"arXiv:2406.18118v3 Announce Type: replace 
Abstract: As the development of large language models (LLMs) rapidly advances, securing these models effectively without compromising their utility has become a pivotal area of research. However, current defense strategies against jailbreak attacks (i.e., efforts to bypass security protocols) often suffer from limited adaptability, restricted general capability, and high cost. To address these challenges, we introduce SafeAligner, a methodology implemented at the decoding stage to fortify defenses against jailbreak attacks. We begin by developing two specialized models: the Sentinel Model, which is trained to foster safety, and the Intruder Model, designed to generate riskier responses. SafeAligner leverages the disparity in security levels between the responses from these models to differentiate between harmful and beneficial tokens, effectively guiding the safety alignment by altering the output token distribution of the target model. Extensive experiments show that SafeAligner can increase the likelihood of beneficial tokens, while reducing the occurrence of harmful ones, thereby ensuring secure alignment with minimal loss to generality.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2406.18118v3 Announce Type: replace \nAbstract: As the development of large language models (LLMs) rapidly advances, securing these models effectively without compromising their utility has become a pivotal area of research. However, current defense strategies against jailbreak attacks (i.e., efforts to bypass security protocols) often suffer from limited adaptability, restricted general capability, and high cost. To address these challenges, we introduce SafeAligner, a methodology implemented at the decoding stage to fortify defenses against jailbreak attacks. We begin by developing two specialized models: the Sentinel Model, which is trained to foster safety, and the Intruder Model, designed to generate riskier responses. SafeAligner leverages the disparity in security levels between the responses from these models to differentiate between harmful and beneficial tokens, effectively guiding the safety alignment by altering the output token distribution of the target model. Extensive experiments show that SafeAligner can increase the likelihood of beneficial tokens, while reducing the occurrence of harmful ones, thereby ensuring secure alignment with minimal loss to generality.'}",oai:arXiv.org:2406.18118v3,False,"[{'term': 'cs.CR', 'scheme': None, 'label': None}, {'term': 'cs.CL', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Caishuang Huang, Wanxu Zhao, Rui Zheng, Huijie Lv, Shihan Dou, Sixian Li, Xiao Wang, Enyu Zhou, Junjie Ye, Yuming Yang, Tao Gui, Qi Zhang, Xuanjing Huang'}]","Caishuang Huang, Wanxu Zhao, Rui Zheng, Huijie Lv, Shihan Dou, Sixian Li, Xiao Wang, Enyu Zhou, Junjie Ye, Yuming Yang, Tao Gui, Qi Zhang, Xuanjing Huang","{'name': 'Caishuang Huang, Wanxu Zhao, Rui Zheng, Huijie Lv, Shihan Dou, Sixian Li, Xiao Wang, Enyu Zhou, Junjie Ye, Yuming Yang, Tao Gui, Qi Zhang, Xuanjing Huang'}",,
573,LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2406.18403'}]",https://arxiv.org/abs/2406.18403,"arXiv:2406.18403v2 Announce Type: replace 
Abstract: There is an increasing trend towards evaluating NLP models with LLMs instead of human judgments, raising questions about the validity of these evaluations, as well as their reproducibility in the case of proprietary models. We provide JUDGE-BENCH, an extensible collection of 20 NLP datasets with human annotations covering a broad range of evaluated properties and types of data, and comprehensively evaluate 11 current LLMs, covering both open-weight and proprietary models, for their ability to replicate the annotations. Our evaluations show substantial variance across models and datasets. Models are reliable evaluators on some tasks, but overall display substantial variability depending on the property being evaluated, the expertise level of the human judges, and whether the language is human or model-generated. We conclude that LLMs should be carefully validated against human judgments before being used as evaluators.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2406.18403v2 Announce Type: replace \nAbstract: There is an increasing trend towards evaluating NLP models with LLMs instead of human judgments, raising questions about the validity of these evaluations, as well as their reproducibility in the case of proprietary models. We provide JUDGE-BENCH, an extensible collection of 20 NLP datasets with human annotations covering a broad range of evaluated properties and types of data, and comprehensively evaluate 11 current LLMs, covering both open-weight and proprietary models, for their ability to replicate the annotations. Our evaluations show substantial variance across models and datasets. Models are reliable evaluators on some tasks, but overall display substantial variability depending on the property being evaluated, the expertise level of the human judges, and whether the language is human or model-generated. We conclude that LLMs should be carefully validated against human judgments before being used as evaluators.'}",oai:arXiv.org:2406.18403v2,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': ""Anna Bavaresco, Raffaella Bernardi, Leonardo Bertolazzi, Desmond Elliott, Raquel Fern\\'andez, Albert Gatt, Esam Ghaleb, Mario Giulianelli, Michael Hanna, Alexander Koller, Andr\\'e F. T. Martins, Philipp Mondorf, Vera Neplenbroek, Sandro Pezzelle, Barbara Plank, David Schlangen, Alessandro Suglia, Aditya K Surikuchi, Ece Takmaz, Alberto Testoni""}]","Anna Bavaresco, Raffaella Bernardi, Leonardo Bertolazzi, Desmond Elliott, Raquel Fern\'andez, Albert Gatt, Esam Ghaleb, Mario Giulianelli, Michael Hanna, Alexander Koller, Andr\'e F. T. Martins, Philipp Mondorf, Vera Neplenbroek, Sandro Pezzelle, Barbara Plank, David Schlangen, Alessandro Suglia, Aditya K Surikuchi, Ece Takmaz, Alberto Testoni","{'name': ""Anna Bavaresco, Raffaella Bernardi, Leonardo Bertolazzi, Desmond Elliott, Raquel Fern\\'andez, Albert Gatt, Esam Ghaleb, Mario Giulianelli, Michael Hanna, Alexander Koller, Andr\\'e F. T. Martins, Philipp Mondorf, Vera Neplenbroek, Sandro Pezzelle, Barbara Plank, David Schlangen, Alessandro Suglia, Aditya K Surikuchi, Ece Takmaz, Alberto Testoni""}",,
574,DocKylin: A Large Multimodal Model for Visual Document Understanding with Efficient Visual Slimming,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'DocKylin: A Large Multimodal Model for Visual Document Understanding with Efficient Visual Slimming'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2406.19101'}]",https://arxiv.org/abs/2406.19101,"arXiv:2406.19101v4 Announce Type: replace 
Abstract: Current multimodal large language models (MLLMs) face significant challenges in visual document understanding (VDU) tasks due to the high resolution, dense text, and complex layouts typical of document images. These characteristics demand a high level of detail perception ability from MLLMs. While increasing input resolution improves detail perception capability, it also leads to longer sequences of visual tokens, increasing computational costs and straining the models' ability to handle long contexts. To address these challenges, we introduce DocKylin, a document-centric MLLM that performs visual content slimming at both the pixel and token levels, thereby reducing token sequence length in VDU scenarios. We introduce an Adaptive Pixel Slimming (APS) preprocessing module to perform pixel-level slimming, increasing the proportion of informative pixels. Moreover, we propose a novel Dynamic Token Slimming (DTS) module to conduct token-level slimming, filtering essential tokens and removing others to adaptively create a more compact visual sequence. Experiments demonstrate DocKylin's promising performance across various VDU benchmarks and the effectiveness of each component.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2406.19101v4 Announce Type: replace \nAbstract: Current multimodal large language models (MLLMs) face significant challenges in visual document understanding (VDU) tasks due to the high resolution, dense text, and complex layouts typical of document images. These characteristics demand a high level of detail perception ability from MLLMs. While increasing input resolution improves detail perception capability, it also leads to longer sequences of visual tokens, increasing computational costs and straining the models' ability to handle long contexts. To address these challenges, we introduce DocKylin, a document-centric MLLM that performs visual content slimming at both the pixel and token levels, thereby reducing token sequence length in VDU scenarios. We introduce an Adaptive Pixel Slimming (APS) preprocessing module to perform pixel-level slimming, increasing the proportion of informative pixels. Moreover, we propose a novel Dynamic Token Slimming (DTS) module to conduct token-level slimming, filtering essential tokens and removing others to adaptively create a more compact visual sequence. Experiments demonstrate DocKylin's promising performance across various VDU benchmarks and the effectiveness of each component.""}",oai:arXiv.org:2406.19101v4,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Jiaxin Zhang, Wentao Yang, Songxuan Lai, Zecheng Xie, Lianwen Jin'}]","Jiaxin Zhang, Wentao Yang, Songxuan Lai, Zecheng Xie, Lianwen Jin","{'name': 'Jiaxin Zhang, Wentao Yang, Songxuan Lai, Zecheng Xie, Lianwen Jin'}",,
575,To Word Senses and Beyond: Inducing Concepts with Contextualized Language Models,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'To Word Senses and Beyond: Inducing Concepts with Contextualized Language Models'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2406.20054'}]",https://arxiv.org/abs/2406.20054,"arXiv:2406.20054v2 Announce Type: replace 
Abstract: Polysemy and synonymy are two crucial interrelated facets of lexical ambiguity. While both phenomena are widely documented in lexical resources and have been studied extensively in NLP, leading to dedicated systems, they are often being considered independently in practical problems. While many tasks dealing with polysemy (e.g. Word Sense Disambiguiation or Induction) highlight the role of word's senses, the study of synonymy is rooted in the study of concepts, i.e. meanings shared across the lexicon. In this paper, we introduce Concept Induction, the unsupervised task of learning a soft clustering among words that defines a set of concepts directly from data. This task generalizes Word Sense Induction. We propose a bi-level approach to Concept Induction that leverages both a local lemma-centric view and a global cross-lexicon view to induce concepts. We evaluate the obtained clustering on SemCor's annotated data and obtain good performance (BCubed F1 above 0.60). We find that the local and the global levels are mutually beneficial to induce concepts and also senses in our setting. Finally, we create static embeddings representing our induced concepts and use them on the Word-in-Context task, obtaining competitive performance with the State-of-the-Art.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2406.20054v2 Announce Type: replace \nAbstract: Polysemy and synonymy are two crucial interrelated facets of lexical ambiguity. While both phenomena are widely documented in lexical resources and have been studied extensively in NLP, leading to dedicated systems, they are often being considered independently in practical problems. While many tasks dealing with polysemy (e.g. Word Sense Disambiguiation or Induction) highlight the role of word's senses, the study of synonymy is rooted in the study of concepts, i.e. meanings shared across the lexicon. In this paper, we introduce Concept Induction, the unsupervised task of learning a soft clustering among words that defines a set of concepts directly from data. This task generalizes Word Sense Induction. We propose a bi-level approach to Concept Induction that leverages both a local lemma-centric view and a global cross-lexicon view to induce concepts. We evaluate the obtained clustering on SemCor's annotated data and obtain good performance (BCubed F1 above 0.60). We find that the local and the global levels are mutually beneficial to induce concepts and also senses in our setting. Finally, we create static embeddings representing our induced concepts and use them on the Word-in-Context task, obtaining competitive performance with the State-of-the-Art.""}",oai:arXiv.org:2406.20054v2,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by-sa/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-sa/4.0/'}","[{'name': ""Bastien Li\\'etard, Pascal Denis, Mikaella Keller""}]","Bastien Li\'etard, Pascal Denis, Mikaella Keller","{'name': ""Bastien Li\\'etard, Pascal Denis, Mikaella Keller""}",10.18653/v1/2024.emnlp-main.156,"In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 2684-2696 (2024)"
576,Physics-augmented neural networks for constitutive modeling of hyperelastic geometrically exact beams,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Physics-augmented neural networks for constitutive modeling of hyperelastic geometrically exact beams'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2407.00640'}]",https://arxiv.org/abs/2407.00640,"arXiv:2407.00640v2 Announce Type: replace 
Abstract: We present neural network-based constitutive models for hyperelastic geometrically exact beams. The proposed models are physics-augmented, i.e., formulated to fulfill important mechanical conditions by construction, which improves accuracy and generalization. Strains and curvatures of the beam are used as input for feed-forward neural networks that represent the effective hyperelastic beam potential. Forces and moments are received as the gradients of the beam potential, ensuring thermodynamic consistency. Normalization conditions are considered via additional projection terms. Symmetry conditions are implemented by an invariant-based approach for transverse isotropy and a more flexible point symmetry constraint, which is included in transverse isotropy but poses fewer restrictions on the constitutive response. Furthermore, a data augmentation approach is proposed to improve the scaling behavior of the models for varying cross-section radii. Additionally, we introduce a parameterization with a scalar parameter to represent ring-shaped cross-sections with different ratios between the inner and outer radii. Formulating the beam potential as a neural network provides a highly flexible model. This enables efficient constitutive surrogate modeling for geometrically exact beams with nonlinear material behavior and cross-sectional deformation, which otherwise would require computationally much more expensive methods. The models are calibrated and tested with data generated for beams with circular and ring-shaped hyperelastic deformable cross-sections at varying inner and outer radii, showing excellent accuracy and generalization. The applicability of the proposed point symmetric model is further demonstrated by applying it in beam simulations. In all studied cases, the proposed model shows excellent performance.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2407.00640v2 Announce Type: replace \nAbstract: We present neural network-based constitutive models for hyperelastic geometrically exact beams. The proposed models are physics-augmented, i.e., formulated to fulfill important mechanical conditions by construction, which improves accuracy and generalization. Strains and curvatures of the beam are used as input for feed-forward neural networks that represent the effective hyperelastic beam potential. Forces and moments are received as the gradients of the beam potential, ensuring thermodynamic consistency. Normalization conditions are considered via additional projection terms. Symmetry conditions are implemented by an invariant-based approach for transverse isotropy and a more flexible point symmetry constraint, which is included in transverse isotropy but poses fewer restrictions on the constitutive response. Furthermore, a data augmentation approach is proposed to improve the scaling behavior of the models for varying cross-section radii. Additionally, we introduce a parameterization with a scalar parameter to represent ring-shaped cross-sections with different ratios between the inner and outer radii. Formulating the beam potential as a neural network provides a highly flexible model. This enables efficient constitutive surrogate modeling for geometrically exact beams with nonlinear material behavior and cross-sectional deformation, which otherwise would require computationally much more expensive methods. The models are calibrated and tested with data generated for beams with circular and ring-shaped hyperelastic deformable cross-sections at varying inner and outer radii, showing excellent accuracy and generalization. The applicability of the proposed point symmetric model is further demonstrated by applying it in beam simulations. In all studied cases, the proposed model shows excellent performance.'}",oai:arXiv.org:2407.00640v2,False,"[{'term': 'cs.CE', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Jasper O. Schommartz, Dominik K. Klein, Juan C. Alzate Cobo, Oliver Weeger'}]","Jasper O. Schommartz, Dominik K. Klein, Juan C. Alzate Cobo, Oliver Weeger","{'name': 'Jasper O. Schommartz, Dominik K. Klein, Juan C. Alzate Cobo, Oliver Weeger'}",10.1016/j.cma.2024.117592,Computer Methods in Applied Mechanics and Engineering 435:117592 (2025)
577,Fast and Efficient: Mask Neural Fields for 3D Scene Segmentation,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Fast and Efficient: Mask Neural Fields for 3D Scene Segmentation'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2407.01220'}]",https://arxiv.org/abs/2407.01220,"arXiv:2407.01220v3 Announce Type: replace 
Abstract: Understanding 3D scenes is a crucial challenge in computer vision research with applications spanning multiple domains. Recent advancements in distilling 2D vision-language foundation models into neural fields, like NeRF and 3DGS, enable open-vocabulary segmentation of 3D scenes from 2D multi-view images without the need for precise 3D annotations. However, while effective, these methods typically rely on the per-pixel distillation of high-dimensional CLIP features, introducing ambiguity and necessitating complex regularization strategies, which adds inefficiency during training. This paper presents MaskField, which enables efficient 3D open-vocabulary segmentation with neural fields from a novel perspective. Unlike previous methods, MaskField decomposes the distillation of mask and semantic features from foundation models by formulating a mask feature field and queries. MaskField overcomes ambiguous object boundaries by naturally introducing SAM segmented object shapes without extra regularization during training. By circumventing the direct handling of dense high-dimensional CLIP features during training, MaskField is particularly compatible with explicit scene representations like 3DGS. Our extensive experiments show that MaskField not only surpasses prior state-of-the-art methods but also achieves remarkably fast convergence. We hope that MaskField will inspire further exploration into how neural fields can be trained to comprehend 3D scenes from 2D models.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2407.01220v3 Announce Type: replace \nAbstract: Understanding 3D scenes is a crucial challenge in computer vision research with applications spanning multiple domains. Recent advancements in distilling 2D vision-language foundation models into neural fields, like NeRF and 3DGS, enable open-vocabulary segmentation of 3D scenes from 2D multi-view images without the need for precise 3D annotations. However, while effective, these methods typically rely on the per-pixel distillation of high-dimensional CLIP features, introducing ambiguity and necessitating complex regularization strategies, which adds inefficiency during training. This paper presents MaskField, which enables efficient 3D open-vocabulary segmentation with neural fields from a novel perspective. Unlike previous methods, MaskField decomposes the distillation of mask and semantic features from foundation models by formulating a mask feature field and queries. MaskField overcomes ambiguous object boundaries by naturally introducing SAM segmented object shapes without extra regularization during training. By circumventing the direct handling of dense high-dimensional CLIP features during training, MaskField is particularly compatible with explicit scene representations like 3DGS. Our extensive experiments show that MaskField not only surpasses prior state-of-the-art methods but also achieves remarkably fast convergence. We hope that MaskField will inspire further exploration into how neural fields can be trained to comprehend 3D scenes from 2D models.'}",oai:arXiv.org:2407.01220v3,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Zihan Gao, Lingling Li, Licheng Jiao, Fang Liu, Xu Liu, Wenping Ma, Yuwei Guo, Shuyuan Yang'}]","Zihan Gao, Lingling Li, Licheng Jiao, Fang Liu, Xu Liu, Wenping Ma, Yuwei Guo, Shuyuan Yang","{'name': 'Zihan Gao, Lingling Li, Licheng Jiao, Fang Liu, Xu Liu, Wenping Ma, Yuwei Guo, Shuyuan Yang'}",,
578,Solution of parameter-dependent diffusion equation in layered media,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Solution of parameter-dependent diffusion equation in layered media'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2407.02257'}]",https://arxiv.org/abs/2407.02257,"arXiv:2407.02257v2 Announce Type: replace 
Abstract: This work studies the parameter-dependent diffusion equation in a two-dimensional domain consisting of locally mirror symmetric layers. It is assumed that the diffusion coefficient is a constant in each layer. The goal is to find approximate parameter-to-solution maps that have a small number of terms. It is shown that in the case of two layers one can find a solution formula consisting of three terms with explicit dependencies on the diffusion coefficient. The formula is based on decomposing the solution into orthogonal parts related to both of the layers and the interface between them. This formula is then expanded to an approximate one for the multi-layer case. We give an analytical formula for square layers and use the finite element formulation for more general layers. The results are illustrated with numerical examples and have applications for reduced basis methods by analyzing the Kolmogorov n-width.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2407.02257v2 Announce Type: replace \nAbstract: This work studies the parameter-dependent diffusion equation in a two-dimensional domain consisting of locally mirror symmetric layers. It is assumed that the diffusion coefficient is a constant in each layer. The goal is to find approximate parameter-to-solution maps that have a small number of terms. It is shown that in the case of two layers one can find a solution formula consisting of three terms with explicit dependencies on the diffusion coefficient. The formula is based on decomposing the solution into orthogonal parts related to both of the layers and the interface between them. This formula is then expanded to an approximate one for the multi-layer case. We give an analytical formula for square layers and use the finite element formulation for more general layers. The results are illustrated with numerical examples and have applications for reduced basis methods by analyzing the Kolmogorov n-width.'}",oai:arXiv.org:2407.02257v2,False,"[{'term': 'math.NA', 'scheme': None, 'label': None}, {'term': 'cs.NA', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by-nc-nd/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-nd/4.0/'}","[{'name': 'Antti Autio, Antti Hannukainen'}]","Antti Autio, Antti Hannukainen","{'name': 'Antti Autio, Antti Hannukainen'}",,
579,UnSeenTimeQA: Time-Sensitive Question-Answering Beyond LLMs' Memorization,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""UnSeenTimeQA: Time-Sensitive Question-Answering Beyond LLMs' Memorization""}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2407.03525'}]",https://arxiv.org/abs/2407.03525,"arXiv:2407.03525v3 Announce Type: replace 
Abstract: This paper introduces UnSeenTimeQA, a novel data contamination-free time-sensitive question-answering (TSQA) benchmark. It differs from existing TSQA benchmarks by avoiding web-searchable queries grounded in the real-world. We present a series of time-sensitive event scenarios based on synthetically generated facts. It requires large language models (LLMs) to engage in genuine temporal reasoning without depending on the factual knowledge acquired during the pre-training phase. We designed three types of time-sensitive questions to test LLMs' temporal reasoning abilities over sequential and parallel event occurrences. Our evaluation of five LLMs on synthetic fact-based TSQA reveals mixed results: while they perform well on simpler subsets, their overall performance remains inferior as compared to real-world fact-based TSQA. Error analysis of LLM-generated reasoning chains indicates that LLMs face difficulties in reasoning over long-range event dependencies and parallel event timelines that unfold concurrently.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2407.03525v3 Announce Type: replace \nAbstract: This paper introduces UnSeenTimeQA, a novel data contamination-free time-sensitive question-answering (TSQA) benchmark. It differs from existing TSQA benchmarks by avoiding web-searchable queries grounded in the real-world. We present a series of time-sensitive event scenarios based on synthetically generated facts. It requires large language models (LLMs) to engage in genuine temporal reasoning without depending on the factual knowledge acquired during the pre-training phase. We designed three types of time-sensitive questions to test LLMs' temporal reasoning abilities over sequential and parallel event occurrences. Our evaluation of five LLMs on synthetic fact-based TSQA reveals mixed results: while they perform well on simpler subsets, their overall performance remains inferior as compared to real-world fact-based TSQA. Error analysis of LLM-generated reasoning chains indicates that LLMs face difficulties in reasoning over long-range event dependencies and parallel event timelines that unfold concurrently.""}",oai:arXiv.org:2407.03525v3,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Md Nayem Uddin, Amir Saeidi, Divij Handa, Agastya Seth, Tran Cao Son, Eduardo Blanco, Steven R. Corman, Chitta Baral'}]","Md Nayem Uddin, Amir Saeidi, Divij Handa, Agastya Seth, Tran Cao Son, Eduardo Blanco, Steven R. Corman, Chitta Baral","{'name': 'Md Nayem Uddin, Amir Saeidi, Divij Handa, Agastya Seth, Tran Cao Son, Eduardo Blanco, Steven R. Corman, Chitta Baral'}",,
580,FDS: Feedback-guided Domain Synthesis with Multi-Source Conditional Diffusion Models for Domain Generalization,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'FDS: Feedback-guided Domain Synthesis with Multi-Source Conditional Diffusion Models for Domain Generalization'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2407.03588'}]",https://arxiv.org/abs/2407.03588,"arXiv:2407.03588v3 Announce Type: replace 
Abstract: Domain Generalization techniques aim to enhance model robustness by simulating novel data distributions during training, typically through various augmentation or stylization strategies. However, these methods frequently suffer from limited control over the diversity of generated images and lack assurance that these images span distinct distributions. To address these challenges, we propose FDS, Feedback-guided Domain Synthesis, a novel strategy that employs diffusion models to synthesize novel, pseudo-domains by training a single model on all source domains and performing domain mixing based on learned features. By incorporating images that pose classification challenges to models trained on original samples, alongside the original dataset, we ensure the generation of a training set that spans a broad distribution spectrum. Our comprehensive evaluations demonstrate that this methodology sets new benchmarks in domain generalization performance across a range of challenging datasets, effectively managing diverse types of domain shifts. The code can be found at: \url{https://github.com/Mehrdad-Noori/FDS.git}.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2407.03588v3 Announce Type: replace \nAbstract: Domain Generalization techniques aim to enhance model robustness by simulating novel data distributions during training, typically through various augmentation or stylization strategies. However, these methods frequently suffer from limited control over the diversity of generated images and lack assurance that these images span distinct distributions. To address these challenges, we propose FDS, Feedback-guided Domain Synthesis, a novel strategy that employs diffusion models to synthesize novel, pseudo-domains by training a single model on all source domains and performing domain mixing based on learned features. By incorporating images that pose classification challenges to models trained on original samples, alongside the original dataset, we ensure the generation of a training set that spans a broad distribution spectrum. Our comprehensive evaluations demonstrate that this methodology sets new benchmarks in domain generalization performance across a range of challenging datasets, effectively managing diverse types of domain shifts. The code can be found at: \\url{https://github.com/Mehrdad-Noori/FDS.git}.'}",oai:arXiv.org:2407.03588v3,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by-nc-sa/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-sa/4.0/'}","[{'name': 'Mehrdad Noori, Milad Cheraghalikhani, Ali Bahri, Gustavo Adolfo Vargas Hakim, David Osowiechi, Moslem Yazdanpanah, Ismail Ben Ayed, Christian Desrosiers'}]","Mehrdad Noori, Milad Cheraghalikhani, Ali Bahri, Gustavo Adolfo Vargas Hakim, David Osowiechi, Moslem Yazdanpanah, Ismail Ben Ayed, Christian Desrosiers","{'name': 'Mehrdad Noori, Milad Cheraghalikhani, Ali Bahri, Gustavo Adolfo Vargas Hakim, David Osowiechi, Moslem Yazdanpanah, Ismail Ben Ayed, Christian Desrosiers'}",,
581,ANAH-v2: Scaling Analytical Hallucination Annotation of Large Language Models,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'ANAH-v2: Scaling Analytical Hallucination Annotation of Large Language Models'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2407.04693'}]",https://arxiv.org/abs/2407.04693,"arXiv:2407.04693v2 Announce Type: replace 
Abstract: Large language models (LLMs) exhibit hallucinations in long-form question-answering tasks across various domains and wide applications. Current hallucination detection and mitigation datasets are limited in domains and sizes, which struggle to scale due to prohibitive labor costs and insufficient reliability of existing hallucination annotators. To facilitate the scalable oversight of LLM hallucinations, this paper introduces an iterative self-training framework that simultaneously and progressively scales up the hallucination annotation dataset and improves the accuracy of the hallucination annotator. Based on the Expectation Maximization (EM) algorithm, in each iteration, the framework first applies a hallucination annotation pipeline to annotate a scaled dataset and then trains a more accurate hallucination annotator on the dataset. This new hallucination annotator is adopted in the hallucination annotation pipeline used for the next iteration. Extensive experimental results demonstrate that the finally obtained hallucination annotator with only 7B parameters surpasses the performance of GPT-4 and obtains new state-of-the-art hallucination detection results on HaluEval and HalluQA by zero-shot inference. Such an annotator can not only evaluate the hallucination levels of various LLMs on the large-scale dataset but also help to mitigate the hallucination of LLMs generations, with the Natural Language Inference (NLI) metric increasing from 25% to 37% on HaluEval.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2407.04693v2 Announce Type: replace \nAbstract: Large language models (LLMs) exhibit hallucinations in long-form question-answering tasks across various domains and wide applications. Current hallucination detection and mitigation datasets are limited in domains and sizes, which struggle to scale due to prohibitive labor costs and insufficient reliability of existing hallucination annotators. To facilitate the scalable oversight of LLM hallucinations, this paper introduces an iterative self-training framework that simultaneously and progressively scales up the hallucination annotation dataset and improves the accuracy of the hallucination annotator. Based on the Expectation Maximization (EM) algorithm, in each iteration, the framework first applies a hallucination annotation pipeline to annotate a scaled dataset and then trains a more accurate hallucination annotator on the dataset. This new hallucination annotator is adopted in the hallucination annotation pipeline used for the next iteration. Extensive experimental results demonstrate that the finally obtained hallucination annotator with only 7B parameters surpasses the performance of GPT-4 and obtains new state-of-the-art hallucination detection results on HaluEval and HalluQA by zero-shot inference. Such an annotator can not only evaluate the hallucination levels of various LLMs on the large-scale dataset but also help to mitigate the hallucination of LLMs generations, with the Natural Language Inference (NLI) metric increasing from 25% to 37% on HaluEval.'}",oai:arXiv.org:2407.04693v2,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Yuzhe Gu, Ziwei Ji, Wenwei Zhang, Chengqi Lyu, Dahua Lin, Kai Chen'}]","Yuzhe Gu, Ziwei Ji, Wenwei Zhang, Chengqi Lyu, Dahua Lin, Kai Chen","{'name': 'Yuzhe Gu, Ziwei Ji, Wenwei Zhang, Chengqi Lyu, Dahua Lin, Kai Chen'}",,
582,UltraEdit: Instruction-based Fine-Grained Image Editing at Scale,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'UltraEdit: Instruction-based Fine-Grained Image Editing at Scale'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2407.05282'}]",https://arxiv.org/abs/2407.05282,"arXiv:2407.05282v2 Announce Type: replace 
Abstract: This paper presents UltraEdit, a large-scale (approximately 4 million editing samples), automatically generated dataset for instruction-based image editing. Our key idea is to address the drawbacks in existing image editing datasets like InstructPix2Pix and MagicBrush, and provide a systematic approach to producing massive and high-quality image editing samples. UltraEdit offers several distinct advantages: 1) It features a broader range of editing instructions by leveraging the creativity of large language models (LLMs) alongside in-context editing examples from human raters; 2) Its data sources are based on real images, including photographs and artworks, which provide greater diversity and reduced bias compared to datasets solely generated by text-to-image models; 3) It also supports region-based editing, enhanced by high-quality, automatically produced region annotations. Our experiments show that canonical diffusion-based editing baselines trained on UltraEdit set new records on MagicBrush and Emu-Edit benchmarks. Our analysis further confirms the crucial role of real image anchors and region-based editing data. The dataset, code, and models can be found in https://ultra-editing.github.io.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2407.05282v2 Announce Type: replace \nAbstract: This paper presents UltraEdit, a large-scale (approximately 4 million editing samples), automatically generated dataset for instruction-based image editing. Our key idea is to address the drawbacks in existing image editing datasets like InstructPix2Pix and MagicBrush, and provide a systematic approach to producing massive and high-quality image editing samples. UltraEdit offers several distinct advantages: 1) It features a broader range of editing instructions by leveraging the creativity of large language models (LLMs) alongside in-context editing examples from human raters; 2) Its data sources are based on real images, including photographs and artworks, which provide greater diversity and reduced bias compared to datasets solely generated by text-to-image models; 3) It also supports region-based editing, enhanced by high-quality, automatically produced region annotations. Our experiments show that canonical diffusion-based editing baselines trained on UltraEdit set new records on MagicBrush and Emu-Edit benchmarks. Our analysis further confirms the crucial role of real image anchors and region-based editing data. The dataset, code, and models can be found in https://ultra-editing.github.io.'}",oai:arXiv.org:2407.05282v2,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Haozhe Zhao, Xiaojian Ma, Liang Chen, Shuzheng Si, Rujie Wu, Kaikai An, Peiyu Yu, Minjia Zhang, Qing Li, Baobao Chang'}]","Haozhe Zhao, Xiaojian Ma, Liang Chen, Shuzheng Si, Rujie Wu, Kaikai An, Peiyu Yu, Minjia Zhang, Qing Li, Baobao Chang","{'name': 'Haozhe Zhao, Xiaojian Ma, Liang Chen, Shuzheng Si, Rujie Wu, Kaikai An, Peiyu Yu, Minjia Zhang, Qing Li, Baobao Chang'}",,
583,An automata-based approach for synchronizable mailbox communication,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'An automata-based approach for synchronizable mailbox communication'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2407.06968'}]",https://arxiv.org/abs/2407.06968,"arXiv:2407.06968v5 Announce Type: replace 
Abstract: We revisit finite-state communicating systems with round-based communication under mailbox semantics. Mailboxes correspond to one FIFO buffer per process (instead of one buffer per pair of processes in peer-to-peer systems). Round-based communication corresponds to sequences of rounds in which processes can first send messages, then only receive (and receives must be in the same round as their sends). A system is called synchronizable if every execution can be re-scheduled into an equivalent execution that is a sequence of rounds. Previous work mostly considered the setting where rounds have fixed size. Our main contribution shows that the problem whether a mailbox communication system complies with the round-based policy, with no size limitation on rounds, is Pspace-complete. For this we use a novel automata-based approach, that also allows to determine the precise complexity (Pspace) of several questions considered in previous literature.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2407.06968v5 Announce Type: replace \nAbstract: We revisit finite-state communicating systems with round-based communication under mailbox semantics. Mailboxes correspond to one FIFO buffer per process (instead of one buffer per pair of processes in peer-to-peer systems). Round-based communication corresponds to sequences of rounds in which processes can first send messages, then only receive (and receives must be in the same round as their sends). A system is called synchronizable if every execution can be re-scheduled into an equivalent execution that is a sequence of rounds. Previous work mostly considered the setting where rounds have fixed size. Our main contribution shows that the problem whether a mailbox communication system complies with the round-based policy, with no size limitation on rounds, is Pspace-complete. For this we use a novel automata-based approach, that also allows to determine the precise complexity (Pspace) of several questions considered in previous literature.'}",oai:arXiv.org:2407.06968v5,False,"[{'term': 'cs.LO', 'scheme': None, 'label': None}, {'term': 'cs.FL', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': ""Romain Delpy, Anca Muscholl, Gr\\'egoire Sutre""}]","Romain Delpy, Anca Muscholl, Gr\'egoire Sutre","{'name': ""Romain Delpy, Anca Muscholl, Gr\\'egoire Sutre""}",,
584,Exploring Scalability of Self-Training for Open-Vocabulary Temporal Action Localization,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Exploring Scalability of Self-Training for Open-Vocabulary Temporal Action Localization'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2407.07024'}]",https://arxiv.org/abs/2407.07024,"arXiv:2407.07024v3 Announce Type: replace 
Abstract: The vocabulary size in temporal action localization (TAL) is limited by the scarcity of large-scale annotated datasets. To overcome this, recent works integrate vision-language models (VLMs), such as CLIP, for open-vocabulary TAL (OV-TAL). However, despite the success of VLMs trained on extensive datasets, existing OV-TAL methods still rely on human-labeled TAL datasets of limited size to train action localizers, limiting their generalizability. In this paper, we explore the scalability of self-training with unlabeled YouTube videos for OV-TAL. Our approach consists of two stages: (1) a class-agnostic action localizer is trained on a human-labeled TAL dataset to generate pseudo-labels for unlabeled videos, and (2) the large-scale pseudo-labeled dataset is then used to train the localizer. Extensive experiments demonstrate that leveraging web-scale videos in self-training significantly enhances the generalizability of an action localizer. Additionally, we identify limitations in existing OV-TAL evaluation schemes and propose a new benchmark for thorough assessment. Finally, we showcase the TAL performance of the large multimodal model Gemini-1.5 on our new benchmark. Code is released at https://github.com/HYUNJS/STOV-TAL.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2407.07024v3 Announce Type: replace \nAbstract: The vocabulary size in temporal action localization (TAL) is limited by the scarcity of large-scale annotated datasets. To overcome this, recent works integrate vision-language models (VLMs), such as CLIP, for open-vocabulary TAL (OV-TAL). However, despite the success of VLMs trained on extensive datasets, existing OV-TAL methods still rely on human-labeled TAL datasets of limited size to train action localizers, limiting their generalizability. In this paper, we explore the scalability of self-training with unlabeled YouTube videos for OV-TAL. Our approach consists of two stages: (1) a class-agnostic action localizer is trained on a human-labeled TAL dataset to generate pseudo-labels for unlabeled videos, and (2) the large-scale pseudo-labeled dataset is then used to train the localizer. Extensive experiments demonstrate that leveraging web-scale videos in self-training significantly enhances the generalizability of an action localizer. Additionally, we identify limitations in existing OV-TAL evaluation schemes and propose a new benchmark for thorough assessment. Finally, we showcase the TAL performance of the large multimodal model Gemini-1.5 on our new benchmark. Code is released at https://github.com/HYUNJS/STOV-TAL.'}",oai:arXiv.org:2407.07024v3,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Jeongseok Hyun, Su Ho Han, Hyolim Kang, Joon-Young Lee, Seon Joo Kim'}]","Jeongseok Hyun, Su Ho Han, Hyolim Kang, Joon-Young Lee, Seon Joo Kim","{'name': 'Jeongseok Hyun, Su Ho Han, Hyolim Kang, Joon-Young Lee, Seon Joo Kim'}",,
585,Almost-linear Time Approximation Algorithm to Euclidean $k$-median and $k$-means,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Almost-linear Time Approximation Algorithm to Euclidean $k$-median and $k$-means'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2407.11217'}]",https://arxiv.org/abs/2407.11217,"arXiv:2407.11217v2 Announce Type: replace 
Abstract: Clustering is one of the staples of data analysis and unsupervised learning. As such, clustering algorithms are often used on massive data sets, and they need to be extremely fast. We focus on the Euclidean $k$-median and $k$-means problems, two of the standard ways to model the task of clustering.
  For these, the go-to algorithm is $k$-means++, which yields an $O(\log k)$-approximation in time $\tilde O(nkd)$. While it is possible to improve either the approximation factor [Lattanzi and Sohler, ICML19] or the running time [Cohen-Addad et al., NeurIPS 20], it is unknown how precise a linear-time algorithm can be.
  In this paper, we almost answer this question by presenting an almost linear-time algorithm to compute a constant-factor approximation.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2407.11217v2 Announce Type: replace \nAbstract: Clustering is one of the staples of data analysis and unsupervised learning. As such, clustering algorithms are often used on massive data sets, and they need to be extremely fast. We focus on the Euclidean $k$-median and $k$-means problems, two of the standard ways to model the task of clustering.\n  For these, the go-to algorithm is $k$-means++, which yields an $O(\\log k)$-approximation in time $\\tilde O(nkd)$. While it is possible to improve either the approximation factor [Lattanzi and Sohler, ICML19] or the running time [Cohen-Addad et al., NeurIPS 20], it is unknown how precise a linear-time algorithm can be.\n  In this paper, we almost answer this question by presenting an almost linear-time algorithm to compute a constant-factor approximation.'}",oai:arXiv.org:2407.11217v2,False,"[{'term': 'cs.DS', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': ""Max Dupr\\'e la Tour, David Saulpic""}]","Max Dupr\'e la Tour, David Saulpic","{'name': ""Max Dupr\\'e la Tour, David Saulpic""}",,
586,Diff-Shadow: Global-guided Diffusion Model for Shadow Removal,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Diff-Shadow: Global-guided Diffusion Model for Shadow Removal'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2407.16214'}]",https://arxiv.org/abs/2407.16214,"arXiv:2407.16214v2 Announce Type: replace 
Abstract: We propose Diff-Shadow, a global-guided diffusion model for shadow removal. Previous transformer-based approaches can utilize global information to relate shadow and non-shadow regions but are limited in their synthesis ability and recover images with obvious boundaries. In contrast, diffusion-based methods can generate better content but they are not exempt from issues related to inconsistent illumination. In this work, we combine the advantages of diffusion models and global guidance to achieve shadow-free restoration. Specifically, we propose a parallel UNets architecture: 1) the local branch performs the patch-based noise estimation in the diffusion process, and 2) the global branch recovers the low-resolution shadow-free images. A Reweight Cross Attention (RCA) module is designed to integrate global contextual information of non-shadow regions into the local branch. We further design a Global-guided Sampling Strategy (GSS) that mitigates patch boundary issues and ensures consistent illumination across shaded and unshaded regions in the recovered image. Comprehensive experiments on datasets ISTD, ISTD+, and SRD have demonstrated the effectiveness of Diff-Shadow. Compared to state-of-the-art methods, our method achieves a significant improvement in terms of PSNR, increasing from 32.33dB to 33.69dB on the ISTD dataset.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2407.16214v2 Announce Type: replace \nAbstract: We propose Diff-Shadow, a global-guided diffusion model for shadow removal. Previous transformer-based approaches can utilize global information to relate shadow and non-shadow regions but are limited in their synthesis ability and recover images with obvious boundaries. In contrast, diffusion-based methods can generate better content but they are not exempt from issues related to inconsistent illumination. In this work, we combine the advantages of diffusion models and global guidance to achieve shadow-free restoration. Specifically, we propose a parallel UNets architecture: 1) the local branch performs the patch-based noise estimation in the diffusion process, and 2) the global branch recovers the low-resolution shadow-free images. A Reweight Cross Attention (RCA) module is designed to integrate global contextual information of non-shadow regions into the local branch. We further design a Global-guided Sampling Strategy (GSS) that mitigates patch boundary issues and ensures consistent illumination across shaded and unshaded regions in the recovered image. Comprehensive experiments on datasets ISTD, ISTD+, and SRD have demonstrated the effectiveness of Diff-Shadow. Compared to state-of-the-art methods, our method achieves a significant improvement in terms of PSNR, increasing from 32.33dB to 33.69dB on the ISTD dataset.'}",oai:arXiv.org:2407.16214v2,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by-nc-nd/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-nd/4.0/'}","[{'name': 'Jinting Luo, Ru Li, Chengzhi Jiang, Xiaoming Zhang, Mingyan Han, Ting Jiang, Haoqiang Fan, Shuaicheng Liu'}]","Jinting Luo, Ru Li, Chengzhi Jiang, Xiaoming Zhang, Mingyan Han, Ting Jiang, Haoqiang Fan, Shuaicheng Liu","{'name': 'Jinting Luo, Ru Li, Chengzhi Jiang, Xiaoming Zhang, Mingyan Han, Ting Jiang, Haoqiang Fan, Shuaicheng Liu'}",,
587,DeepClean: Integrated Distortion Identification and Algorithm Selection for Rectifying Image Corruptions,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'DeepClean: Integrated Distortion Identification and Algorithm Selection for Rectifying Image Corruptions'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2407.16302'}]",https://arxiv.org/abs/2407.16302,"arXiv:2407.16302v2 Announce Type: replace 
Abstract: Distortion identification and rectification in images and videos is vital for achieving good performance in downstream vision applications. Instead of relying on fixed trial-and-error based image processing pipelines, we propose a two-level sequential planning approach for automated image distortion classification and rectification. At the higher level it detects the class of corruptions present in the input image, if any. The lower level selects a specific algorithm to be applied, from a set of externally provided candidate algorithms. The entire two-level setup runs in the form of a single forward pass during inference and it is to be queried iteratively until the retrieval of the original image. We demonstrate improvements compared to three baselines on the object detection task on COCO image dataset with rich set of distortions. The advantage of our approach is its dynamic reconfiguration, conditioned on the input image and generalisability to unseen candidate algorithms at inference time, since it relies only on the comparison of their output of the image embeddings.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2407.16302v2 Announce Type: replace \nAbstract: Distortion identification and rectification in images and videos is vital for achieving good performance in downstream vision applications. Instead of relying on fixed trial-and-error based image processing pipelines, we propose a two-level sequential planning approach for automated image distortion classification and rectification. At the higher level it detects the class of corruptions present in the input image, if any. The lower level selects a specific algorithm to be applied, from a set of externally provided candidate algorithms. The entire two-level setup runs in the form of a single forward pass during inference and it is to be queried iteratively until the retrieval of the original image. We demonstrate improvements compared to three baselines on the object detection task on COCO image dataset with rich set of distortions. The advantage of our approach is its dynamic reconfiguration, conditioned on the input image and generalisability to unseen candidate algorithms at inference time, since it relies only on the comparison of their output of the image embeddings.'}",oai:arXiv.org:2407.16302v2,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'eess.IV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Aditya Kapoor, Harshad Khadilkar, Jayvardhana Gubbi'}]","Aditya Kapoor, Harshad Khadilkar, Jayvardhana Gubbi","{'name': 'Aditya Kapoor, Harshad Khadilkar, Jayvardhana Gubbi'}",,
588,Revisiting Machine Unlearning with Dimensional Alignment,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Revisiting Machine Unlearning with Dimensional Alignment'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2407.17710'}]",https://arxiv.org/abs/2407.17710,"arXiv:2407.17710v2 Announce Type: replace 
Abstract: Machine unlearning, an emerging research topic focusing on compliance with data privacy regulations, enables trained models to remove the information learned from specific data. While many existing methods indirectly address this issue by intentionally injecting incorrect supervisions, they can drastically and unpredictably alter the decision boundaries and feature spaces, leading to training instability and undesired side effects. To fundamentally approach this task, we first analyze the changes in latent feature spaces between original and retrained models, and observe that the feature representations of samples not involved in training are closely aligned with the feature manifolds of previously seen samples in training. Based on these findings, we introduce a novel evaluation metric for machine unlearning, coined dimensional alignment, which measures the alignment between the eigenspaces of the forget and retain set samples. We employ this metric as a regularizer loss to build a robust and stable unlearning framework, which is further enhanced by integrating a self-distillation loss and an alternating training scheme. Our framework effectively eliminates information from the forget set and preserves knowledge from the retain set. Lastly, we identify critical flaws in established evaluation metrics for machine unlearning, and introduce new evaluation tools that more accurately reflect the fundamental goals of machine unlearning.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2407.17710v2 Announce Type: replace \nAbstract: Machine unlearning, an emerging research topic focusing on compliance with data privacy regulations, enables trained models to remove the information learned from specific data. While many existing methods indirectly address this issue by intentionally injecting incorrect supervisions, they can drastically and unpredictably alter the decision boundaries and feature spaces, leading to training instability and undesired side effects. To fundamentally approach this task, we first analyze the changes in latent feature spaces between original and retrained models, and observe that the feature representations of samples not involved in training are closely aligned with the feature manifolds of previously seen samples in training. Based on these findings, we introduce a novel evaluation metric for machine unlearning, coined dimensional alignment, which measures the alignment between the eigenspaces of the forget and retain set samples. We employ this metric as a regularizer loss to build a robust and stable unlearning framework, which is further enhanced by integrating a self-distillation loss and an alternating training scheme. Our framework effectively eliminates information from the forget set and preserves knowledge from the retain set. Lastly, we identify critical flaws in established evaluation metrics for machine unlearning, and introduce new evaluation tools that more accurately reflect the fundamental goals of machine unlearning.'}",oai:arXiv.org:2407.17710v2,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Seonguk Seo, Dongwan Kim, Bohyung Han'}]","Seonguk Seo, Dongwan Kim, Bohyung Han","{'name': 'Seonguk Seo, Dongwan Kim, Bohyung Han'}",,
589,Multi-Agent Trajectory Prediction with Difficulty-Guided Feature Enhancement Network,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Multi-Agent Trajectory Prediction with Difficulty-Guided Feature Enhancement Network'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2407.18551'}]",https://arxiv.org/abs/2407.18551,"arXiv:2407.18551v3 Announce Type: replace 
Abstract: Trajectory prediction is crucial for autonomous driving as it aims to forecast the future movements of traffic participants. Traditional methods usually perform holistic inference on the trajectories of agents, neglecting the differences in prediction difficulty among agents. This paper proposes a novel Difficulty-Guided Feature Enhancement Network (DGFNet), which leverages the prediction difficulty differences among agents for multi-agent trajectory prediction. Firstly, we employ spatio-temporal feature encoding and interaction to capture rich spatio-temporal features. Secondly, a difficulty-guided decoder controls the flow of future trajectories into subsequent modules, obtaining reliable future trajectories. Then, feature interaction and fusion are performed through the future feature interaction module. Finally, the fused agent features are fed into the final predictor to generate the predicted trajectory distributions for multiple participants. Experimental results demonstrate that our DGFNet achieves state-of-the-art performance on the Argoverse 1\&2 motion forecasting benchmarks. Ablation studies further validate the effectiveness of each module. Moreover, compared with SOTA methods, our method balances trajectory prediction accuracy and real-time inference speed.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2407.18551v3 Announce Type: replace \nAbstract: Trajectory prediction is crucial for autonomous driving as it aims to forecast the future movements of traffic participants. Traditional methods usually perform holistic inference on the trajectories of agents, neglecting the differences in prediction difficulty among agents. This paper proposes a novel Difficulty-Guided Feature Enhancement Network (DGFNet), which leverages the prediction difficulty differences among agents for multi-agent trajectory prediction. Firstly, we employ spatio-temporal feature encoding and interaction to capture rich spatio-temporal features. Secondly, a difficulty-guided decoder controls the flow of future trajectories into subsequent modules, obtaining reliable future trajectories. Then, feature interaction and fusion are performed through the future feature interaction module. Finally, the fused agent features are fed into the final predictor to generate the predicted trajectory distributions for multiple participants. Experimental results demonstrate that our DGFNet achieves state-of-the-art performance on the Argoverse 1\\&2 motion forecasting benchmarks. Ablation studies further validate the effectiveness of each module. Moreover, compared with SOTA methods, our method balances trajectory prediction accuracy and real-time inference speed.'}",oai:arXiv.org:2407.18551v3,False,"[{'term': 'cs.RO', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Guipeng Xin, Duanfeng Chu, Liping Lu, Zejian Deng, Yuang Lu, Xigang Wu'}]","Guipeng Xin, Duanfeng Chu, Liping Lu, Zejian Deng, Yuang Lu, Xigang Wu","{'name': 'Guipeng Xin, Duanfeng Chu, Liping Lu, Zejian Deng, Yuang Lu, Xigang Wu'}",,
590,Improving Retrieval Augmented Language Model with Self-Reasoning,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Improving Retrieval Augmented Language Model with Self-Reasoning'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2407.19813'}]",https://arxiv.org/abs/2407.19813,"arXiv:2407.19813v3 Announce Type: replace 
Abstract: The Retrieval-Augmented Language Model (RALM) has shown remarkable performance on knowledge-intensive tasks by incorporating external knowledge during inference, which mitigates the factual hallucinations inherited in large language models (LLMs). Despite these advancements, challenges persist in the implementation of RALMs, particularly concerning their reliability and traceability. To be specific, the irrelevant document retrieval may result in unhelpful response generation or even deteriorate the performance of LLMs, while the lack of proper citations in generated outputs complicates efforts to verify the trustworthiness of the models. To this end, we propose a novel self-reasoning framework aimed at improving the reliability and traceability of RALMs, whose core idea is to leverage reasoning trajectories generated by the LLM itself. The framework involves constructing self-reason trajectories with three processes: a relevance-aware process, an evidence-aware selective process, and a trajectory analysis process. We have evaluated our framework across four public datasets (two short-form QA datasets, one long-form QA dataset, and one fact verification dataset) to demonstrate the superiority of our method, which can outperform existing state-of-the-art models and can achieve comparable performance with GPT-4, while only using 2,000 training samples.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2407.19813v3 Announce Type: replace \nAbstract: The Retrieval-Augmented Language Model (RALM) has shown remarkable performance on knowledge-intensive tasks by incorporating external knowledge during inference, which mitigates the factual hallucinations inherited in large language models (LLMs). Despite these advancements, challenges persist in the implementation of RALMs, particularly concerning their reliability and traceability. To be specific, the irrelevant document retrieval may result in unhelpful response generation or even deteriorate the performance of LLMs, while the lack of proper citations in generated outputs complicates efforts to verify the trustworthiness of the models. To this end, we propose a novel self-reasoning framework aimed at improving the reliability and traceability of RALMs, whose core idea is to leverage reasoning trajectories generated by the LLM itself. The framework involves constructing self-reason trajectories with three processes: a relevance-aware process, an evidence-aware selective process, and a trajectory analysis process. We have evaluated our framework across four public datasets (two short-form QA datasets, one long-form QA dataset, and one fact verification dataset) to demonstrate the superiority of our method, which can outperform existing state-of-the-art models and can achieve comparable performance with GPT-4, while only using 2,000 training samples.'}",oai:arXiv.org:2407.19813v3,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Yuan Xia, Jingbo Zhou, Zhenhui Shi, Jun Chen, Haifeng Huang'}]","Yuan Xia, Jingbo Zhou, Zhenhui Shi, Jun Chen, Haifeng Huang","{'name': 'Yuan Xia, Jingbo Zhou, Zhenhui Shi, Jun Chen, Haifeng Huang'}",,
591,Counterfactual rewards promote collective transport using individually controlled swarm microrobots,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Counterfactual rewards promote collective transport using individually controlled swarm microrobots'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2407.20041'}]",https://arxiv.org/abs/2407.20041,"arXiv:2407.20041v2 Announce Type: replace 
Abstract: Swarm robots offer fascinating opportunities to perform complex tasks beyond the capabilities of individual machines. Just as a swarm of ants collectively moves a large object, similar functions can emerge within a group of robots through individual strategies based on local sensing. However, realizing collective functions with individually controlled microrobots is particularly challenging due to their micrometer size, large number of degrees of freedom, strong thermal noise relative to the propulsion speed, complex physical coupling between neighboring microrobots, and surface collisions. Here, we implement Multi-Agent Reinforcement Learning (MARL) to generate a control strategy for up to 200 microrobots whose motions are individually controlled by laser spots. During the learning process, we employ so-called counterfactual rewards that automatically assign credit to the individual microrobots, which allows for fast and unbiased training. With the help of this efficient reward scheme, swarm microrobots learn to collectively transport a large cargo object to an arbitrary position and orientation, similar to ant swarms. We demonstrate that this flexible and versatile swarm robotic system is robust to variations in group size, the presence of malfunctioning units, and environmental noise. Such control strategies can potentially enable complex and automated assembly of mobile micromachines, programmable drug delivery capsules, and other advanced lab-on-a-chip applications.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2407.20041v2 Announce Type: replace \nAbstract: Swarm robots offer fascinating opportunities to perform complex tasks beyond the capabilities of individual machines. Just as a swarm of ants collectively moves a large object, similar functions can emerge within a group of robots through individual strategies based on local sensing. However, realizing collective functions with individually controlled microrobots is particularly challenging due to their micrometer size, large number of degrees of freedom, strong thermal noise relative to the propulsion speed, complex physical coupling between neighboring microrobots, and surface collisions. Here, we implement Multi-Agent Reinforcement Learning (MARL) to generate a control strategy for up to 200 microrobots whose motions are individually controlled by laser spots. During the learning process, we employ so-called counterfactual rewards that automatically assign credit to the individual microrobots, which allows for fast and unbiased training. With the help of this efficient reward scheme, swarm microrobots learn to collectively transport a large cargo object to an arbitrary position and orientation, similar to ant swarms. We demonstrate that this flexible and versatile swarm robotic system is robust to variations in group size, the presence of malfunctioning units, and environmental noise. Such control strategies can potentially enable complex and automated assembly of mobile micromachines, programmable drug delivery capsules, and other advanced lab-on-a-chip applications.'}",oai:arXiv.org:2407.20041v2,False,"[{'term': 'cs.RO', 'scheme': None, 'label': None}, {'term': 'cond-mat.soft', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Veit-Lorenz Heuthe, Emanuele Panizon, Hongri Gu, Clemens Bechinger'}]","Veit-Lorenz Heuthe, Emanuele Panizon, Hongri Gu, Clemens Bechinger","{'name': 'Veit-Lorenz Heuthe, Emanuele Panizon, Hongri Gu, Clemens Bechinger'}",10.1126/scirobotics.ado5888,
592,Stable Rank and Intrinsic Dimension of Real and Complex Matrices,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Stable Rank and Intrinsic Dimension of Real and Complex Matrices'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2407.21594'}]",https://arxiv.org/abs/2407.21594,"arXiv:2407.21594v2 Announce Type: replace 
Abstract: The notion of `stable rank' of a matrix is central to the analysis of randomized matrix algorithms, covariance estimation, deep neural networks, and recommender systems. We compare the properties of the stable rank and intrinsic dimension of real and complex matrices to those of the classical rank. Basic proofs and examples illustrate that the stable rank does not satisfy any of the fundamental rank properties, while the intrinsic dimension satisfies a few. In particular, the stable rank and intrinsic dimension of a submatrix can exceed those of the original matrix; adding a Hermitian positive semi-definite matrix can lower the intrinsic dimension of the sum; and multiplication by a nonsingular matrix can drastically change the stable rank and the intrinsic dimension. We generalize the concept of stable rank to the p-stable in any Schatten p-norm, thereby unifying the concepts of stable rank and intrinsic dimension: The stable rank is the 2-stable rank, while the intrinsic dimension is the 1-stable rank of a Hermitian positive semi-definite matrix. We derive sum and product inequalities for the pth root of the p-stable rank, and show that it is well-conditioned in the norm-wise absolute sense. The conditioning improves if the matrix and the perturbation are Hermitian positive semi-definite.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2407.21594v2 Announce Type: replace \nAbstract: The notion of `stable rank' of a matrix is central to the analysis of randomized matrix algorithms, covariance estimation, deep neural networks, and recommender systems. We compare the properties of the stable rank and intrinsic dimension of real and complex matrices to those of the classical rank. Basic proofs and examples illustrate that the stable rank does not satisfy any of the fundamental rank properties, while the intrinsic dimension satisfies a few. In particular, the stable rank and intrinsic dimension of a submatrix can exceed those of the original matrix; adding a Hermitian positive semi-definite matrix can lower the intrinsic dimension of the sum; and multiplication by a nonsingular matrix can drastically change the stable rank and the intrinsic dimension. We generalize the concept of stable rank to the p-stable in any Schatten p-norm, thereby unifying the concepts of stable rank and intrinsic dimension: The stable rank is the 2-stable rank, while the intrinsic dimension is the 1-stable rank of a Hermitian positive semi-definite matrix. We derive sum and product inequalities for the pth root of the p-stable rank, and show that it is well-conditioned in the norm-wise absolute sense. The conditioning improves if the matrix and the perturbation are Hermitian positive semi-definite.""}",oai:arXiv.org:2407.21594v2,False,"[{'term': 'math.NA', 'scheme': None, 'label': None}, {'term': 'cs.NA', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Ilse C. F. Ipsen, Arvind K. Saibaba'}]","Ilse C. F. Ipsen, Arvind K. Saibaba","{'name': 'Ilse C. F. Ipsen, Arvind K. Saibaba'}",,
593,Safetywashing: Do AI Safety Benchmarks Actually Measure Safety Progress?,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Safetywashing: Do AI Safety Benchmarks Actually Measure Safety Progress?'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2407.21792'}]",https://arxiv.org/abs/2407.21792,"arXiv:2407.21792v2 Announce Type: replace 
Abstract: As artificial intelligence systems grow more powerful, there has been increasing interest in ""AI safety"" research to address emerging and future risks. However, the field of AI safety remains poorly defined and inconsistently measured, leading to confusion about how researchers can contribute. This lack of clarity is compounded by the unclear relationship between AI safety benchmarks and upstream general capabilities (e.g., general knowledge and reasoning). To address these issues, we conduct a comprehensive meta-analysis of AI safety benchmarks, empirically analyzing their correlation with general capabilities across dozens of models and providing a survey of existing directions in AI safety. Our findings reveal that many safety benchmarks highly correlate with both upstream model capabilities and training compute, potentially enabling ""safetywashing"" -- where capability improvements are misrepresented as safety advancements. Based on these findings, we propose an empirical foundation for developing more meaningful safety metrics and define AI safety in a machine learning research context as a set of clearly delineated research goals that are empirically separable from generic capabilities advancements. In doing so, we aim to provide a more rigorous framework for AI safety research, advancing the science of safety evaluations and clarifying the path towards measurable progress.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2407.21792v2 Announce Type: replace \nAbstract: As artificial intelligence systems grow more powerful, there has been increasing interest in ""AI safety"" research to address emerging and future risks. However, the field of AI safety remains poorly defined and inconsistently measured, leading to confusion about how researchers can contribute. This lack of clarity is compounded by the unclear relationship between AI safety benchmarks and upstream general capabilities (e.g., general knowledge and reasoning). To address these issues, we conduct a comprehensive meta-analysis of AI safety benchmarks, empirically analyzing their correlation with general capabilities across dozens of models and providing a survey of existing directions in AI safety. Our findings reveal that many safety benchmarks highly correlate with both upstream model capabilities and training compute, potentially enabling ""safetywashing"" -- where capability improvements are misrepresented as safety advancements. Based on these findings, we propose an empirical foundation for developing more meaningful safety metrics and define AI safety in a machine learning research context as a set of clearly delineated research goals that are empirically separable from generic capabilities advancements. In doing so, we aim to provide a more rigorous framework for AI safety research, advancing the science of safety evaluations and clarifying the path towards measurable progress.'}",oai:arXiv.org:2407.21792v2,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.CL', 'scheme': None, 'label': None}, {'term': 'cs.CY', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Richard Ren, Steven Basart, Adam Khoja, Alice Gatti, Long Phan, Xuwang Yin, Mantas Mazeika, Alexander Pan, Gabriel Mukobi, Ryan H. Kim, Stephen Fitz, Dan Hendrycks'}]","Richard Ren, Steven Basart, Adam Khoja, Alice Gatti, Long Phan, Xuwang Yin, Mantas Mazeika, Alexander Pan, Gabriel Mukobi, Ryan H. Kim, Stephen Fitz, Dan Hendrycks","{'name': 'Richard Ren, Steven Basart, Adam Khoja, Alice Gatti, Long Phan, Xuwang Yin, Mantas Mazeika, Alexander Pan, Gabriel Mukobi, Ryan H. Kim, Stephen Fitz, Dan Hendrycks'}",,
594,Enhancing Ethereum Fraud Detection via Generative and Contrastive Self-supervision,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Enhancing Ethereum Fraud Detection via Generative and Contrastive Self-supervision'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2408.00641'}]",https://arxiv.org/abs/2408.00641,"arXiv:2408.00641v2 Announce Type: replace 
Abstract: The rampant fraudulent activities on Ethereum hinder the healthy development of the blockchain ecosystem, necessitating the reinforcement of regulations. However, multiple imbalances involving account interaction frequencies and interaction types in the Ethereum transaction environment pose significant challenges to data mining-based fraud detection research. To address this, we first propose the concept of meta-interactions to refine interaction behaviors in Ethereum, and based on this, we present a dual self-supervision enhanced Ethereum fraud detection framework, named Meta-IFD. This framework initially introduces a generative self-supervision mechanism to augment the interaction features of accounts, followed by a contrastive self-supervision mechanism to differentiate various behavior patterns, and ultimately characterizes the behavioral representations of accounts and mines potential fraud risks through multi-view interaction feature learning. Extensive experiments on real Ethereum datasets demonstrate the effectiveness and superiority of our framework in detecting common Ethereum fraud behaviors such as Ponzi schemes and phishing scams. Additionally, the generative module can effectively alleviate the interaction distribution imbalance in Ethereum data, while the contrastive module significantly enhances the framework's ability to distinguish different behavior patterns. The source code will be available in https://github.com/GISec-Team/Meta-IFD.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2408.00641v2 Announce Type: replace \nAbstract: The rampant fraudulent activities on Ethereum hinder the healthy development of the blockchain ecosystem, necessitating the reinforcement of regulations. However, multiple imbalances involving account interaction frequencies and interaction types in the Ethereum transaction environment pose significant challenges to data mining-based fraud detection research. To address this, we first propose the concept of meta-interactions to refine interaction behaviors in Ethereum, and based on this, we present a dual self-supervision enhanced Ethereum fraud detection framework, named Meta-IFD. This framework initially introduces a generative self-supervision mechanism to augment the interaction features of accounts, followed by a contrastive self-supervision mechanism to differentiate various behavior patterns, and ultimately characterizes the behavioral representations of accounts and mines potential fraud risks through multi-view interaction feature learning. Extensive experiments on real Ethereum datasets demonstrate the effectiveness and superiority of our framework in detecting common Ethereum fraud behaviors such as Ponzi schemes and phishing scams. Additionally, the generative module can effectively alleviate the interaction distribution imbalance in Ethereum data, while the contrastive module significantly enhances the framework's ability to distinguish different behavior patterns. The source code will be available in https://github.com/GISec-Team/Meta-IFD.""}",oai:arXiv.org:2408.00641v2,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Chenxiang Jin, Jiajun Zhou, Chenxuan Xie, Shanqing Yu, Qi Xuan, Xiaoniu Yang'}]","Chenxiang Jin, Jiajun Zhou, Chenxuan Xie, Shanqing Yu, Qi Xuan, Xiaoniu Yang","{'name': 'Chenxiang Jin, Jiajun Zhou, Chenxuan Xie, Shanqing Yu, Qi Xuan, Xiaoniu Yang'}",,
595,SkyDiffusion: Ground-to-Aerial Image Synthesis with Diffusion Models and BEV Paradigm,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'SkyDiffusion: Ground-to-Aerial Image Synthesis with Diffusion Models and BEV Paradigm'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2408.01812'}]",https://arxiv.org/abs/2408.01812,"arXiv:2408.01812v3 Announce Type: replace 
Abstract: Ground-to-aerial image synthesis focuses on generating realistic aerial images from corresponding ground street view images while maintaining consistent content layout, simulating a top-down view. The significant viewpoint difference leads to domain gaps between views, and dense urban scenes limit the visible range of street views, making this cross-view generation task particularly challenging. In this paper, we introduce SkyDiffusion, a novel cross-view generation method for synthesizing aerial images from street view images, utilizing a diffusion model and the Bird's-Eye View (BEV) paradigm. The Curved-BEV method in SkyDiffusion converts street-view images into a BEV perspective, effectively bridging the domain gap, and employs a ""multi-to-one"" mapping strategy to address occlusion issues in dense urban scenes. Next, SkyDiffusion designed a BEV-guided diffusion model to generate content-consistent and realistic aerial images. Additionally, we introduce a novel dataset, Ground2Aerial-3, designed for diverse ground-to-aerial image synthesis applications, including disaster scene aerial synthesis, historical high-resolution satellite image synthesis, and low-altitude UAV image synthesis tasks. Experimental results demonstrate that SkyDiffusion outperforms state-of-the-art methods on cross-view datasets across natural (CVUSA), suburban (CVACT), urban (VIGOR-Chicago), and various application scenarios (G2A-3), achieving realistic and content-consistent aerial image generation. More result and dataset information can be found at https://opendatalab.github.io/skydiffusion/ .","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2408.01812v3 Announce Type: replace \nAbstract: Ground-to-aerial image synthesis focuses on generating realistic aerial images from corresponding ground street view images while maintaining consistent content layout, simulating a top-down view. The significant viewpoint difference leads to domain gaps between views, and dense urban scenes limit the visible range of street views, making this cross-view generation task particularly challenging. In this paper, we introduce SkyDiffusion, a novel cross-view generation method for synthesizing aerial images from street view images, utilizing a diffusion model and the Bird\'s-Eye View (BEV) paradigm. The Curved-BEV method in SkyDiffusion converts street-view images into a BEV perspective, effectively bridging the domain gap, and employs a ""multi-to-one"" mapping strategy to address occlusion issues in dense urban scenes. Next, SkyDiffusion designed a BEV-guided diffusion model to generate content-consistent and realistic aerial images. Additionally, we introduce a novel dataset, Ground2Aerial-3, designed for diverse ground-to-aerial image synthesis applications, including disaster scene aerial synthesis, historical high-resolution satellite image synthesis, and low-altitude UAV image synthesis tasks. Experimental results demonstrate that SkyDiffusion outperforms state-of-the-art methods on cross-view datasets across natural (CVUSA), suburban (CVACT), urban (VIGOR-Chicago), and various application scenarios (G2A-3), achieving realistic and content-consistent aerial image generation. More result and dataset information can be found at https://opendatalab.github.io/skydiffusion/ .'}",oai:arXiv.org:2408.01812v3,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Junyan Ye, Jun He, Weijia Li, Zhutao Lv, Yi Lin, Jinhua Yu, Haote Yang, Conghui He'}]","Junyan Ye, Jun He, Weijia Li, Zhutao Lv, Yi Lin, Jinhua Yu, Haote Yang, Conghui He","{'name': 'Junyan Ye, Jun He, Weijia Li, Zhutao Lv, Yi Lin, Jinhua Yu, Haote Yang, Conghui He'}",,
596,Generative AI as a Service in 6G Edge-Cloud: Generation Task Offloading by In-context Learning,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Generative AI as a Service in 6G Edge-Cloud: Generation Task Offloading by In-context Learning'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2408.02549'}]",https://arxiv.org/abs/2408.02549,"arXiv:2408.02549v2 Announce Type: replace 
Abstract: Generative artificial intelligence (GAI) is a promising technique towards 6G networks, and generative foundation models such as large language models (LLMs) have attracted considerable interest from academia and telecom industry. This work considers a novel edge-cloud deployment of foundation models in 6G networks. Specifically, it aims to minimize the service delay of foundation models by radio resource allocation and task offloading, i.e., offloading diverse content generation tasks to proper LLMs at the network edge or cloud. In particular, we first introduce the communication system model, i.e., allocating radio resources and calculating link capacity to support generated content transmission, and then we present the LLM inference model to calculate the delay of content generation. After that, we propose a novel in-context learning method to optimize the task offloading decisions. It utilizes LLM's inference capabilities, and avoids the difficulty of dedicated model training or fine-tuning as in conventional machine learning algorithms. Finally, the simulations demonstrate that the proposed edge-cloud deployment and in-context learning task offloading method can achieve satisfactory generation service quality without dedicated model training or fine-tuning.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2408.02549v2 Announce Type: replace \nAbstract: Generative artificial intelligence (GAI) is a promising technique towards 6G networks, and generative foundation models such as large language models (LLMs) have attracted considerable interest from academia and telecom industry. This work considers a novel edge-cloud deployment of foundation models in 6G networks. Specifically, it aims to minimize the service delay of foundation models by radio resource allocation and task offloading, i.e., offloading diverse content generation tasks to proper LLMs at the network edge or cloud. In particular, we first introduce the communication system model, i.e., allocating radio resources and calculating link capacity to support generated content transmission, and then we present the LLM inference model to calculate the delay of content generation. After that, we propose a novel in-context learning method to optimize the task offloading decisions. It utilizes LLM's inference capabilities, and avoids the difficulty of dedicated model training or fine-tuning as in conventional machine learning algorithms. Finally, the simulations demonstrate that the proposed edge-cloud deployment and in-context learning task offloading method can achieve satisfactory generation service quality without dedicated model training or fine-tuning.""}",oai:arXiv.org:2408.02549v2,False,"[{'term': 'eess.SY', 'scheme': None, 'label': None}, {'term': 'cs.SY', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/publicdomain/zero/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/publicdomain/zero/1.0/'}","[{'name': 'Hao Zhou, Chengming Hu, Dun Yuan, Ye Yuan, Di Wu, Xue Liu, Zhu Han, Charlie Zhang'}]","Hao Zhou, Chengming Hu, Dun Yuan, Ye Yuan, Di Wu, Xue Liu, Zhu Han, Charlie Zhang","{'name': 'Hao Zhou, Chengming Hu, Dun Yuan, Ye Yuan, Di Wu, Xue Liu, Zhu Han, Charlie Zhang'}",,
597,Img-Diff: Contrastive Data Synthesis for Multimodal Large Language Models,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Img-Diff: Contrastive Data Synthesis for Multimodal Large Language Models'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2408.04594'}]",https://arxiv.org/abs/2408.04594,"arXiv:2408.04594v3 Announce Type: replace 
Abstract: High-performance Multimodal Large Language Models (MLLMs) are heavily dependent on data quality. To advance fine-grained image recognition within MLLMs, we introduce a novel data synthesis method inspired by contrastive learning and image difference captioning. Our key idea involves challenging the model to discern both matching and distinct elements by scrutinizing object differences in detailed regions across similar images. We begin by generating pairs of similar images that emphasize object variations. Following this, we employ a Difference Area Generator to pinpoint object differences, and subsequently, a Difference Captions Generator to articulate these differences. This process results in a high-quality dataset of ""object replacement"" samples, termed Img-Diff, which can be scaled as needed due to its automated nature. We leverage this generated dataset to fine-tune state-of-the-art (SOTA) MLLMs, such as InternVL2, achieving substantial improvements across various image difference and Visual Question Answering tasks. Notably, the trained models significantly outperform existing SOTA models like GPT-4V and Gemini on the MMVP benchmark. Additionally, we conduct comprehensive evaluations to validate the dataset's diversity, quality, and robustness, offering several insights into the synthesis of such contrastive datasets. We release our codes and dataset to encourage further research on multimodal data synthesis and MLLMs' fundamental capabilities for image understanding.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2408.04594v3 Announce Type: replace \nAbstract: High-performance Multimodal Large Language Models (MLLMs) are heavily dependent on data quality. To advance fine-grained image recognition within MLLMs, we introduce a novel data synthesis method inspired by contrastive learning and image difference captioning. Our key idea involves challenging the model to discern both matching and distinct elements by scrutinizing object differences in detailed regions across similar images. We begin by generating pairs of similar images that emphasize object variations. Following this, we employ a Difference Area Generator to pinpoint object differences, and subsequently, a Difference Captions Generator to articulate these differences. This process results in a high-quality dataset of ""object replacement"" samples, termed Img-Diff, which can be scaled as needed due to its automated nature. We leverage this generated dataset to fine-tune state-of-the-art (SOTA) MLLMs, such as InternVL2, achieving substantial improvements across various image difference and Visual Question Answering tasks. Notably, the trained models significantly outperform existing SOTA models like GPT-4V and Gemini on the MMVP benchmark. Additionally, we conduct comprehensive evaluations to validate the dataset\'s diversity, quality, and robustness, offering several insights into the synthesis of such contrastive datasets. We release our codes and dataset to encourage further research on multimodal data synthesis and MLLMs\' fundamental capabilities for image understanding.'}",oai:arXiv.org:2408.04594v3,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Qirui Jiao, Daoyuan Chen, Yilun Huang, Bolin Ding, Yaliang Li, Ying Shen'}]","Qirui Jiao, Daoyuan Chen, Yilun Huang, Bolin Ding, Yaliang Li, Ying Shen","{'name': 'Qirui Jiao, Daoyuan Chen, Yilun Huang, Bolin Ding, Yaliang Li, Ying Shen'}",,
598,Generalized Encouragement-Based Instrumental Variables for Counterfactual Regression,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Generalized Encouragement-Based Instrumental Variables for Counterfactual Regression'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2408.05428'}]",https://arxiv.org/abs/2408.05428,"arXiv:2408.05428v2 Announce Type: replace 
Abstract: In causal inference, encouragement designs (EDs) are widely used to analyze causal effects, when randomized controlled trials (RCTs) are impractical or compliance to treatment cannot be perfectly enforced. Unlike RCTs, which directly allocate treatments, EDs randomly assign encouragement policies that positively motivate individuals to engage in a specific treatment. These random encouragements act as instrumental variables (IVs), facilitating the identification of causal effects through leveraging exogenous perturbations in discrete treatment scenarios. However, real-world applications of encouragement designs often face challenges such as incomplete randomization, limited experimental data, and significantly fewer encouragements compared to treatments, hindering precise causal effect estimation. To address this, this paper introduces novel theories and algorithms for identifying the Conditional Average Treatment Effect (CATE) using variations in encouragement. Further, by leveraging both observational and encouragement data, we propose a generalized IV estimator, named Encouragement-based Counterfactual Regression (EnCounteR), to effectively estimate the causal effects. Extensive experiments on both synthetic and real-world datasets demonstrate the superiority of EnCounteR over existing methods.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2408.05428v2 Announce Type: replace \nAbstract: In causal inference, encouragement designs (EDs) are widely used to analyze causal effects, when randomized controlled trials (RCTs) are impractical or compliance to treatment cannot be perfectly enforced. Unlike RCTs, which directly allocate treatments, EDs randomly assign encouragement policies that positively motivate individuals to engage in a specific treatment. These random encouragements act as instrumental variables (IVs), facilitating the identification of causal effects through leveraging exogenous perturbations in discrete treatment scenarios. However, real-world applications of encouragement designs often face challenges such as incomplete randomization, limited experimental data, and significantly fewer encouragements compared to treatments, hindering precise causal effect estimation. To address this, this paper introduces novel theories and algorithms for identifying the Conditional Average Treatment Effect (CATE) using variations in encouragement. Further, by leveraging both observational and encouragement data, we propose a generalized IV estimator, named Encouragement-based Counterfactual Regression (EnCounteR), to effectively estimate the causal effects. Extensive experiments on both synthetic and real-world datasets demonstrate the superiority of EnCounteR over existing methods.'}",oai:arXiv.org:2408.05428v2,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'stat.ME', 'scheme': None, 'label': None}, {'term': 'stat.ML', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Anpeng Wu, Kun Kuang, Ruoxuan Xiong, Xiangwei Chen, Zexu Sun, Fei Wu, Kun Zhang'}]","Anpeng Wu, Kun Kuang, Ruoxuan Xiong, Xiangwei Chen, Zexu Sun, Fei Wu, Kun Zhang","{'name': 'Anpeng Wu, Kun Kuang, Ruoxuan Xiong, Xiangwei Chen, Zexu Sun, Fei Wu, Kun Zhang'}",,
599,Super-intelligence or Superstition? Exploring Psychological Factors Influencing Belief in AI Predictions about Personal Behavior,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Super-intelligence or Superstition? Exploring Psychological Factors Influencing Belief in AI Predictions about Personal Behavior'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2408.06602'}]",https://arxiv.org/abs/2408.06602,"arXiv:2408.06602v3 Announce Type: replace 
Abstract: Could belief in AI predictions be just another form of superstition? This study investigates psychological factors that influence belief in AI predictions about personal behavior, comparing it to belief in astrology- and personality-based predictions. Through an experiment with 238 participants, we examined how cognitive style, paranormal beliefs, AI attitudes, personality traits, and other factors affect perceived validity, reliability, usefulness, and personalization of predictions from different sources. Our findings reveal that belief in AI predictions is positively correlated with belief in predictions based on astrology and personality psychology. Notably, paranormal beliefs and positive attitudes about AI significantly increased perceived validity, reliability, usefulness, and personalization of AI predictions. Conscientiousness was negatively correlated with belief in predictions across all sources, and interest in the prediction topic increased believability across predictions. Surprisingly, we found no evidence that cognitive style has an impact on belief in fictitious AI-generated predictions. These results highlight the ""rational superstition"" phenomenon in AI, where belief is driven more by mental heuristics and intuition than critical evaluation. This research advances our understanding of the psychology of human-AI interaction, offering insights into designing and promoting AI systems that foster appropriate trust and skepticism, critical for responsible integration in an increasingly AI-driven world.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2408.06602v3 Announce Type: replace \nAbstract: Could belief in AI predictions be just another form of superstition? This study investigates psychological factors that influence belief in AI predictions about personal behavior, comparing it to belief in astrology- and personality-based predictions. Through an experiment with 238 participants, we examined how cognitive style, paranormal beliefs, AI attitudes, personality traits, and other factors affect perceived validity, reliability, usefulness, and personalization of predictions from different sources. Our findings reveal that belief in AI predictions is positively correlated with belief in predictions based on astrology and personality psychology. Notably, paranormal beliefs and positive attitudes about AI significantly increased perceived validity, reliability, usefulness, and personalization of AI predictions. Conscientiousness was negatively correlated with belief in predictions across all sources, and interest in the prediction topic increased believability across predictions. Surprisingly, we found no evidence that cognitive style has an impact on belief in fictitious AI-generated predictions. These results highlight the ""rational superstition"" phenomenon in AI, where belief is driven more by mental heuristics and intuition than critical evaluation. This research advances our understanding of the psychology of human-AI interaction, offering insights into designing and promoting AI systems that foster appropriate trust and skepticism, critical for responsible integration in an increasingly AI-driven world.'}",oai:arXiv.org:2408.06602v3,False,"[{'term': 'cs.HC', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Eunhae Lee, Pat Pataranutaporn, Judith Amores, Pattie Maes'}]","Eunhae Lee, Pat Pataranutaporn, Judith Amores, Pattie Maes","{'name': 'Eunhae Lee, Pat Pataranutaporn, Judith Amores, Pattie Maes'}",,
600,Alignment-Enhanced Decoding:Defending via Token-Level Adaptive Refining of Probability Distributions,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Alignment-Enhanced Decoding:Defending via Token-Level Adaptive Refining of Probability Distributions'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2408.07663'}]",https://arxiv.org/abs/2408.07663,"arXiv:2408.07663v2 Announce Type: replace 
Abstract: Large language models are susceptible to jailbreak attacks, which can result in the generation of harmful content. While prior defenses mitigate these risks by perturbing or inspecting inputs, they ignore competing objectives, the underlying cause of alignment failures. In this paper, we propose Alignment-Enhanced Decoding (AED), a novel defense that employs adaptive decoding to address the root causes of jailbreak issues. We first define the Competitive Index to quantify alignment failures and utilize feedback from self-evaluation to compute post-alignment logits. Then, AED adaptively combines AED and post-alignment logits with the original logits to obtain harmless and helpful distributions. Consequently, our method enhances safety alignment while maintaining helpfulness. We conduct experiments across five models and four common jailbreaks, with the results validating the effectiveness of our approach. Code is available at https://github.com/GIGABaozi/AED.git.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2408.07663v2 Announce Type: replace \nAbstract: Large language models are susceptible to jailbreak attacks, which can result in the generation of harmful content. While prior defenses mitigate these risks by perturbing or inspecting inputs, they ignore competing objectives, the underlying cause of alignment failures. In this paper, we propose Alignment-Enhanced Decoding (AED), a novel defense that employs adaptive decoding to address the root causes of jailbreak issues. We first define the Competitive Index to quantify alignment failures and utilize feedback from self-evaluation to compute post-alignment logits. Then, AED adaptively combines AED and post-alignment logits with the original logits to obtain harmless and helpful distributions. Consequently, our method enhances safety alignment while maintaining helpfulness. We conduct experiments across five models and four common jailbreaks, with the results validating the effectiveness of our approach. Code is available at https://github.com/GIGABaozi/AED.git.'}",oai:arXiv.org:2408.07663v2,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Quan Liu, Zhenhong Zhou, Longzhu He, Yi Liu, Wei Zhang, Sen Su'}]","Quan Liu, Zhenhong Zhou, Longzhu He, Yi Liu, Wei Zhang, Sen Su","{'name': 'Quan Liu, Zhenhong Zhou, Longzhu He, Yi Liu, Wei Zhang, Sen Su'}",,
601,"IDEA: Enhancing the Rule Learning Ability of Large Language Model Agent through Induction, Deduction, and Abduction","{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'IDEA: Enhancing the Rule Learning Ability of Large Language Model Agent through Induction, Deduction, and Abduction'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2408.10455'}]",https://arxiv.org/abs/2408.10455,"arXiv:2408.10455v5 Announce Type: replace 
Abstract: While large language models (LLMs) have been thoroughly evaluated for deductive and inductive reasoning, their proficiency in holistic rule learning in interactive environments remains less explored. We introduce RULEARN, a novel benchmark to assess the rule-learning abilities of LLM agents in interactive settings. In RULEARN, agents strategically interact with simulated environments to gather observations, discern patterns, and solve complex problems. To enhance the rule-learning capabilities for LLM agents, we propose IDEA, a novel reasoning framework that integrates the process of Induction, Deduction, and Abduction. The IDEA agent generates initial hypotheses from limited observations through abduction, devises plans to validate these hypotheses or leverages them to solve problems via deduction, and refines previous hypotheses through induction, dynamically establishing and applying rules that mimic human rule-learning behaviors. Our evaluation of the IDEA framework, which involves five representative LLMs, demonstrates significant improvements over the baseline. Furthermore, our study with human participants reveals notable discrepancies in rule-learning behaviors between humans and LLMs. We believe our benchmark will serve as a valuable and challenging resource, and IDEA will provide crucial insights for the development of LLM agents capable of human-like rule learning in real-world scenarios. Our code and data is publicly available.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2408.10455v5 Announce Type: replace \nAbstract: While large language models (LLMs) have been thoroughly evaluated for deductive and inductive reasoning, their proficiency in holistic rule learning in interactive environments remains less explored. We introduce RULEARN, a novel benchmark to assess the rule-learning abilities of LLM agents in interactive settings. In RULEARN, agents strategically interact with simulated environments to gather observations, discern patterns, and solve complex problems. To enhance the rule-learning capabilities for LLM agents, we propose IDEA, a novel reasoning framework that integrates the process of Induction, Deduction, and Abduction. The IDEA agent generates initial hypotheses from limited observations through abduction, devises plans to validate these hypotheses or leverages them to solve problems via deduction, and refines previous hypotheses through induction, dynamically establishing and applying rules that mimic human rule-learning behaviors. Our evaluation of the IDEA framework, which involves five representative LLMs, demonstrates significant improvements over the baseline. Furthermore, our study with human participants reveals notable discrepancies in rule-learning behaviors between humans and LLMs. We believe our benchmark will serve as a valuable and challenging resource, and IDEA will provide crucial insights for the development of LLM agents capable of human-like rule learning in real-world scenarios. Our code and data is publicly available.'}",oai:arXiv.org:2408.10455v5,False,"[{'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Kaiyu He, Mian Zhang, Shuo Yan, Peilin Wu, Zhiyu Zoey Chen'}]","Kaiyu He, Mian Zhang, Shuo Yan, Peilin Wu, Zhiyu Zoey Chen","{'name': 'Kaiyu He, Mian Zhang, Shuo Yan, Peilin Wu, Zhiyu Zoey Chen'}",,
602,ICSD: An Open-source Dataset for Infant Cry and Snoring Detection,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'ICSD: An Open-source Dataset for Infant Cry and Snoring Detection'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2408.10561'}]",https://arxiv.org/abs/2408.10561,"arXiv:2408.10561v2 Announce Type: replace 
Abstract: The detection and analysis of infant cry and snoring events are crucial tasks within the field of audio signal processing. While existing datasets for general sound event detection are plentiful, they often fall short in providing sufficient, strongly labeled data specific to infant cries and snoring. To provide a benchmark dataset and thus foster the research of infant cry and snoring detection, this paper introduces the Infant Cry and Snoring Detection (ICSD) dataset, a novel, publicly available dataset specially designed for ICSD tasks. The ICSD comprises three types of subsets: a real strongly labeled subset with event-based labels annotated manually, a weakly labeled subset with only clip-level event annotations, and a synthetic subset generated and labeled with strong annotations. This paper provides a detailed description of the ICSD creation process, including the challenges encountered and the solutions adopted. We offer a comprehensive characterization of the dataset, discussing its limitations and key factors for ICSD usage. Additionally, we conduct extensive experiments on the ICSD dataset to establish baseline systems and offer insights into the main factors when using this dataset for ICSD research. Our goal is to develop a dataset that will be widely adopted by the community as a new open benchmark for future ICSD research.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2408.10561v2 Announce Type: replace \nAbstract: The detection and analysis of infant cry and snoring events are crucial tasks within the field of audio signal processing. While existing datasets for general sound event detection are plentiful, they often fall short in providing sufficient, strongly labeled data specific to infant cries and snoring. To provide a benchmark dataset and thus foster the research of infant cry and snoring detection, this paper introduces the Infant Cry and Snoring Detection (ICSD) dataset, a novel, publicly available dataset specially designed for ICSD tasks. The ICSD comprises three types of subsets: a real strongly labeled subset with event-based labels annotated manually, a weakly labeled subset with only clip-level event annotations, and a synthetic subset generated and labeled with strong annotations. This paper provides a detailed description of the ICSD creation process, including the challenges encountered and the solutions adopted. We offer a comprehensive characterization of the dataset, discussing its limitations and key factors for ICSD usage. Additionally, we conduct extensive experiments on the ICSD dataset to establish baseline systems and offer insights into the main factors when using this dataset for ICSD research. Our goal is to develop a dataset that will be widely adopted by the community as a new open benchmark for future ICSD research.'}",oai:arXiv.org:2408.10561v2,False,"[{'term': 'cs.SD', 'scheme': None, 'label': None}, {'term': 'eess.AS', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Qingyu Liu, Longfei Song, Dongxing Xu, Yanhua Long'}]","Qingyu Liu, Longfei Song, Dongxing Xu, Yanhua Long","{'name': 'Qingyu Liu, Longfei Song, Dongxing Xu, Yanhua Long'}",,
603,Benchmarking Large Language Models for Math Reasoning Tasks,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Benchmarking Large Language Models for Math Reasoning Tasks'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2408.10839'}]",https://arxiv.org/abs/2408.10839,"arXiv:2408.10839v2 Announce Type: replace 
Abstract: The use of Large Language Models (LLMs) in mathematical reasoning has become a cornerstone of related research, demonstrating the intelligence of these models and enabling potential practical applications through their advanced performance, such as in educational settings. Despite the variety of datasets and in-context learning algorithms designed to improve the ability of LLMs to automate mathematical problem solving, the lack of comprehensive benchmarking across different datasets makes it complicated to select an appropriate model for specific tasks. In this project, we present a benchmark that fairly compares seven state-of-the-art in-context learning algorithms for mathematical problem solving across five widely used mathematical datasets on four powerful foundation models. Furthermore, we explore the trade-off between efficiency and performance, highlighting the practical applications of LLMs for mathematical reasoning. Our results indicate that larger foundation models like GPT-4o and LLaMA 3-70B can solve mathematical reasoning independently from the concrete prompting strategy, while for smaller models the in-context learning approach significantly influences the performance. Moreover, the optimal prompt depends on the chosen foundation model. We open-source our benchmark code to support the integration of additional models in future research.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2408.10839v2 Announce Type: replace \nAbstract: The use of Large Language Models (LLMs) in mathematical reasoning has become a cornerstone of related research, demonstrating the intelligence of these models and enabling potential practical applications through their advanced performance, such as in educational settings. Despite the variety of datasets and in-context learning algorithms designed to improve the ability of LLMs to automate mathematical problem solving, the lack of comprehensive benchmarking across different datasets makes it complicated to select an appropriate model for specific tasks. In this project, we present a benchmark that fairly compares seven state-of-the-art in-context learning algorithms for mathematical problem solving across five widely used mathematical datasets on four powerful foundation models. Furthermore, we explore the trade-off between efficiency and performance, highlighting the practical applications of LLMs for mathematical reasoning. Our results indicate that larger foundation models like GPT-4o and LLaMA 3-70B can solve mathematical reasoning independently from the concrete prompting strategy, while for smaller models the in-context learning approach significantly influences the performance. Moreover, the optimal prompt depends on the chosen foundation model. We open-source our benchmark code to support the integration of additional models in future research.'}",oai:arXiv.org:2408.10839v2,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Kathrin Se{\\ss}ler, Yao Rong, Emek G\\""ozl\\""ukl\\""u, Enkelejda Kasneci'}]","Kathrin Se{\ss}ler, Yao Rong, Emek G\""ozl\""ukl\""u, Enkelejda Kasneci","{'name': 'Kathrin Se{\\ss}ler, Yao Rong, Emek G\\""ozl\\""ukl\\""u, Enkelejda Kasneci'}",,
604,Cage: Hardware-Accelerated Safe WebAssembly,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Cage: Hardware-Accelerated Safe WebAssembly'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2408.11456'}]",https://arxiv.org/abs/2408.11456,"arXiv:2408.11456v3 Announce Type: replace 
Abstract: WebAssembly (WASM) is an immensely versatile and increasingly popular compilation target. It executes applications written in several languages (e.g., C/C++) with near-native performance in various domains (e.g., mobile, edge, cloud). Despite WASM's sandboxing feature, which isolates applications from other instances and the host platform, WASM does not inherently provide any memory safety guarantees for applications written in low-level, unsafe languages. To this end, we propose Cage, a hardware-accelerated toolchain for WASM that supports unmodified applications compiled to WASM and utilizes diverse Arm hardware features aiming to enrich the memory safety properties of WASM. Precisely, Cage leverages Arm's Memory Tagging Extension (MTE) to (i) provide spatial and temporal memory safety for heap and stack allocations and (ii) improve the performance of WASM's sandboxing mechanism. Cage further employs Arm's Pointer Authentication (PAC) to prevent leaked pointers from being reused by other WASM instances, thus enhancing WASM's security properties. We implement our system based on 64-bit WASM. We provide a WASM compiler and runtime with support for Arm's MTE and PAC. On top of that, Cage's LLVM-based compiler toolchain transforms unmodified applications to provide spatial and temporal memory safety for stack and heap allocations and prevent function pointer reuse. Our evaluation on real hardware shows that Cage incurs minimal runtime (<5.8%) and memory (<3.7%) overheads and can improve the performance of WASM's sandboxing mechanism, achieving a speedup of over 5.1%, while offering efficient memory safety guarantees.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2408.11456v3 Announce Type: replace \nAbstract: WebAssembly (WASM) is an immensely versatile and increasingly popular compilation target. It executes applications written in several languages (e.g., C/C++) with near-native performance in various domains (e.g., mobile, edge, cloud). Despite WASM's sandboxing feature, which isolates applications from other instances and the host platform, WASM does not inherently provide any memory safety guarantees for applications written in low-level, unsafe languages. To this end, we propose Cage, a hardware-accelerated toolchain for WASM that supports unmodified applications compiled to WASM and utilizes diverse Arm hardware features aiming to enrich the memory safety properties of WASM. Precisely, Cage leverages Arm's Memory Tagging Extension (MTE) to (i) provide spatial and temporal memory safety for heap and stack allocations and (ii) improve the performance of WASM's sandboxing mechanism. Cage further employs Arm's Pointer Authentication (PAC) to prevent leaked pointers from being reused by other WASM instances, thus enhancing WASM's security properties. We implement our system based on 64-bit WASM. We provide a WASM compiler and runtime with support for Arm's MTE and PAC. On top of that, Cage's LLVM-based compiler toolchain transforms unmodified applications to provide spatial and temporal memory safety for stack and heap allocations and prevent function pointer reuse. Our evaluation on real hardware shows that Cage incurs minimal runtime (<5.8%) and memory (<3.7%) overheads and can improve the performance of WASM's sandboxing mechanism, achieving a speedup of over 5.1%, while offering efficient memory safety guarantees.""}",oai:arXiv.org:2408.11456v3,False,"[{'term': 'cs.PL', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Martin Fink, Dimitrios Stavrakakis, Dennis Sprokholt, Soham Chakraborty, Jan-Erik Ekberg, Pramod Bhatotia'}]","Martin Fink, Dimitrios Stavrakakis, Dennis Sprokholt, Soham Chakraborty, Jan-Erik Ekberg, Pramod Bhatotia","{'name': 'Martin Fink, Dimitrios Stavrakakis, Dennis Sprokholt, Soham Chakraborty, Jan-Erik Ekberg, Pramod Bhatotia'}",10.1145/3696443.3708920,"23rd ACM/IEEE International Symposium on Code Generation and Optimization (CGO '25), March 2025, Las Vegas, NV, USA"
605,Learning Deep Dissipative Dynamics,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Learning Deep Dissipative Dynamics'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2408.11479'}]",https://arxiv.org/abs/2408.11479,"arXiv:2408.11479v2 Announce Type: replace 
Abstract: This study challenges strictly guaranteeing ``dissipativity'' of a dynamical system represented by neural networks learned from given time-series data. Dissipativity is a crucial indicator for dynamical systems that generalizes stability and input-output stability, known to be valid across various systems including robotics, biological systems, and molecular dynamics. By analytically proving the general solution to the nonlinear Kalman-Yakubovich-Popov (KYP) lemma, which is the necessary and sufficient condition for dissipativity, we propose a differentiable projection that transforms any dynamics represented by neural networks into dissipative ones and a learning method for the transformed dynamics. Utilizing the generality of dissipativity, our method strictly guarantee stability, input-output stability, and energy conservation of trained dynamical systems. Finally, we demonstrate the robustness of our method against out-of-domain input through applications to robotic arms and fluid dynamics. Code is https://github.com/kojima-r/DeepDissipativeModel","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2408.11479v2 Announce Type: replace \nAbstract: This study challenges strictly guaranteeing ``dissipativity'' of a dynamical system represented by neural networks learned from given time-series data. Dissipativity is a crucial indicator for dynamical systems that generalizes stability and input-output stability, known to be valid across various systems including robotics, biological systems, and molecular dynamics. By analytically proving the general solution to the nonlinear Kalman-Yakubovich-Popov (KYP) lemma, which is the necessary and sufficient condition for dissipativity, we propose a differentiable projection that transforms any dynamics represented by neural networks into dissipative ones and a learning method for the transformed dynamics. Utilizing the generality of dissipativity, our method strictly guarantee stability, input-output stability, and energy conservation of trained dynamical systems. Finally, we demonstrate the robustness of our method against out-of-domain input through applications to robotic arms and fluid dynamics. Code is https://github.com/kojima-r/DeepDissipativeModel""}",oai:arXiv.org:2408.11479v2,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'cs.SY', 'scheme': None, 'label': None}, {'term': 'eess.SY', 'scheme': None, 'label': None}, {'term': 'math.DS', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Yuji Okamoto, Ryosuke Kojima'}]","Yuji Okamoto, Ryosuke Kojima","{'name': 'Yuji Okamoto, Ryosuke Kojima'}",,
606,Sum of Squares Circuits,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Sum of Squares Circuits'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2408.11778'}]",https://arxiv.org/abs/2408.11778,"arXiv:2408.11778v2 Announce Type: replace 
Abstract: Designing expressive generative models that support exact and efficient inference is a core question in probabilistic ML. Probabilistic circuits (PCs) offer a framework where this tractability-vs-expressiveness trade-off can be analyzed theoretically. Recently, squared PCs encoding subtractive mixtures via negative parameters have emerged as tractable models that can be exponentially more expressive than monotonic PCs, i.e., PCs with positive parameters only. In this paper, we provide a more precise theoretical characterization of the expressiveness relationships among these models. First, we prove that squared PCs can be less expressive than monotonic ones. Second, we formalize a novel class of PCs -- sum of squares PCs -- that can be exponentially more expressive than both squared and monotonic PCs. Around sum of squares PCs, we build an expressiveness hierarchy that allows us to precisely unify and separate different tractable model classes such as Born Machines and PSD models, and other recently introduced tractable probabilistic models by using complex parameters. Finally, we empirically show the effectiveness of sum of squares circuits in performing distribution estimation.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2408.11778v2 Announce Type: replace \nAbstract: Designing expressive generative models that support exact and efficient inference is a core question in probabilistic ML. Probabilistic circuits (PCs) offer a framework where this tractability-vs-expressiveness trade-off can be analyzed theoretically. Recently, squared PCs encoding subtractive mixtures via negative parameters have emerged as tractable models that can be exponentially more expressive than monotonic PCs, i.e., PCs with positive parameters only. In this paper, we provide a more precise theoretical characterization of the expressiveness relationships among these models. First, we prove that squared PCs can be less expressive than monotonic ones. Second, we formalize a novel class of PCs -- sum of squares PCs -- that can be exponentially more expressive than both squared and monotonic PCs. Around sum of squares PCs, we build an expressiveness hierarchy that allows us to precisely unify and separate different tractable model classes such as Born Machines and PSD models, and other recently introduced tractable probabilistic models by using complex parameters. Finally, we empirically show the effectiveness of sum of squares circuits in performing distribution estimation.'}",oai:arXiv.org:2408.11778v2,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.CC', 'scheme': None, 'label': None}, {'term': 'math.AG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by-nc-nd/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-nd/4.0/'}","[{'name': 'Lorenzo Loconte, Stefan Mengel, Antonio Vergari'}]","Lorenzo Loconte, Stefan Mengel, Antonio Vergari","{'name': 'Lorenzo Loconte, Stefan Mengel, Antonio Vergari'}",,
607,DLCRec: A Novel Approach for Managing Diversity in LLM-Based Recommender Systems,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'DLCRec: A Novel Approach for Managing Diversity in LLM-Based Recommender Systems'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2408.12470'}]",https://arxiv.org/abs/2408.12470,"arXiv:2408.12470v2 Announce Type: replace 
Abstract: The integration of Large Language Models (LLMs) into recommender systems has led to substantial performance improvements. However, this often comes at the cost of diminished recommendation diversity, which can negatively impact user satisfaction. To address this issue, controllable recommendation has emerged as a promising approach, allowing users to specify their preferences and receive recommendations that meet their diverse needs. Despite its potential, existing controllable recommender systems frequently rely on simplistic mechanisms, such as a single prompt, to regulate diversity-an approach that falls short of capturing the full complexity of user preferences. In response to these limitations, we propose DLCRec, a novel framework designed to enable fine-grained control over diversity in LLM-based recommendations. Unlike traditional methods, DLCRec adopts a fine-grained task decomposition strategy, breaking down the recommendation process into three sequential sub-tasks: genre prediction, genre filling, and item prediction. These sub-tasks are trained independently and inferred sequentially according to user-defined control numbers, ensuring more precise control over diversity. Furthermore, the scarcity and uneven distribution of diversity-related user behavior data pose significant challenges for fine-tuning. To overcome these obstacles, we introduce two data augmentation techniques that enhance the model's robustness to noisy and out-of-distribution data. These techniques expose the model to a broader range of patterns, improving its adaptability in generating recommendations with varying levels of diversity. Our extensive empirical evaluation demonstrates that DLCRec not only provides precise control over diversity but also outperforms state-of-the-art baselines across multiple recommendation scenarios.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2408.12470v2 Announce Type: replace \nAbstract: The integration of Large Language Models (LLMs) into recommender systems has led to substantial performance improvements. However, this often comes at the cost of diminished recommendation diversity, which can negatively impact user satisfaction. To address this issue, controllable recommendation has emerged as a promising approach, allowing users to specify their preferences and receive recommendations that meet their diverse needs. Despite its potential, existing controllable recommender systems frequently rely on simplistic mechanisms, such as a single prompt, to regulate diversity-an approach that falls short of capturing the full complexity of user preferences. In response to these limitations, we propose DLCRec, a novel framework designed to enable fine-grained control over diversity in LLM-based recommendations. Unlike traditional methods, DLCRec adopts a fine-grained task decomposition strategy, breaking down the recommendation process into three sequential sub-tasks: genre prediction, genre filling, and item prediction. These sub-tasks are trained independently and inferred sequentially according to user-defined control numbers, ensuring more precise control over diversity. Furthermore, the scarcity and uneven distribution of diversity-related user behavior data pose significant challenges for fine-tuning. To overcome these obstacles, we introduce two data augmentation techniques that enhance the model's robustness to noisy and out-of-distribution data. These techniques expose the model to a broader range of patterns, improving its adaptability in generating recommendations with varying levels of diversity. Our extensive empirical evaluation demonstrates that DLCRec not only provides precise control over diversity but also outperforms state-of-the-art baselines across multiple recommendation scenarios.""}",oai:arXiv.org:2408.12470v2,False,"[{'term': 'cs.IR', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Jiaju Chen, Chongming Gao, Shuai Yuan, Shuchang Liu, Qingpeng Cai, Peng Jiang'}]","Jiaju Chen, Chongming Gao, Shuai Yuan, Shuchang Liu, Qingpeng Cai, Peng Jiang","{'name': 'Jiaju Chen, Chongming Gao, Shuai Yuan, Shuchang Liu, Qingpeng Cai, Peng Jiang'}",,
608,LLMs as Zero-shot Graph Learners: Alignment of GNN Representations with LLM Token Embeddings,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'LLMs as Zero-shot Graph Learners: Alignment of GNN Representations with LLM Token Embeddings'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2408.14512'}]",https://arxiv.org/abs/2408.14512,"arXiv:2408.14512v3 Announce Type: replace 
Abstract: Zero-shot graph machine learning, especially with graph neural networks (GNNs), has garnered significant interest due to the challenge of scarce labeled data. While methods like self-supervised learning and graph prompt learning have been extensively explored, they often rely on fine-tuning with task-specific labels, limiting their effectiveness in zero-shot scenarios. Inspired by the zero-shot capabilities of instruction-fine-tuned large language models (LLMs), we introduce a novel framework named Token Embedding-Aligned Graph Language Model (TEA-GLM) that leverages LLMs as cross-dataset and cross-task zero-shot learners for graph machine learning. Concretely, we pretrain a GNN, aligning its representations with token embeddings of an LLM. We then train a linear projector that transforms the GNN's representations into a fixed number of graph token embeddings without tuning the LLM. A unified instruction is designed for various graph tasks at different levels, such as node classification (node-level) and link prediction (edge-level). These design choices collectively enhance our method's effectiveness in zero-shot learning, setting it apart from existing methods. Experiments show that our graph token embeddings help the LLM predictor achieve state-of-the-art performance on unseen datasets and tasks compared to other methods using LLMs as predictors.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2408.14512v3 Announce Type: replace \nAbstract: Zero-shot graph machine learning, especially with graph neural networks (GNNs), has garnered significant interest due to the challenge of scarce labeled data. While methods like self-supervised learning and graph prompt learning have been extensively explored, they often rely on fine-tuning with task-specific labels, limiting their effectiveness in zero-shot scenarios. Inspired by the zero-shot capabilities of instruction-fine-tuned large language models (LLMs), we introduce a novel framework named Token Embedding-Aligned Graph Language Model (TEA-GLM) that leverages LLMs as cross-dataset and cross-task zero-shot learners for graph machine learning. Concretely, we pretrain a GNN, aligning its representations with token embeddings of an LLM. We then train a linear projector that transforms the GNN's representations into a fixed number of graph token embeddings without tuning the LLM. A unified instruction is designed for various graph tasks at different levels, such as node classification (node-level) and link prediction (edge-level). These design choices collectively enhance our method's effectiveness in zero-shot learning, setting it apart from existing methods. Experiments show that our graph token embeddings help the LLM predictor achieve state-of-the-art performance on unseen datasets and tasks compared to other methods using LLMs as predictors.""}",oai:arXiv.org:2408.14512v3,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.CL', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Duo Wang, Yuan Zuo, Fengzhi Li, Junjie Wu'}]","Duo Wang, Yuan Zuo, Fengzhi Li, Junjie Wu","{'name': 'Duo Wang, Yuan Zuo, Fengzhi Li, Junjie Wu'}",,
609,Latent Ewald summation for machine learning of long-range interactions,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Latent Ewald summation for machine learning of long-range interactions'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2408.15165'}]",https://arxiv.org/abs/2408.15165,"arXiv:2408.15165v2 Announce Type: replace 
Abstract: Machine learning interatomic potentials (MLIPs) often neglect long-range interactions, such as electrostatic and dispersion forces. In this work, we introduce a straightforward and efficient method to account for long-range interactions by learning a latent variable from local atomic descriptors and applying an Ewald summation to this variable. We demonstrate that in systems including charged and polar molecular dimers, bulk water, and water-vapor interface, standard short-ranged MLIPs can lead to unphysical predictions even when employing message passing. The long-range models effectively eliminate these artifacts, with only about twice the computational cost of short-range MLIPs.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2408.15165v2 Announce Type: replace \nAbstract: Machine learning interatomic potentials (MLIPs) often neglect long-range interactions, such as electrostatic and dispersion forces. In this work, we introduce a straightforward and efficient method to account for long-range interactions by learning a latent variable from local atomic descriptors and applying an Ewald summation to this variable. We demonstrate that in systems including charged and polar molecular dimers, bulk water, and water-vapor interface, standard short-ranged MLIPs can lead to unphysical predictions even when employing message passing. The long-range models effectively eliminate these artifacts, with only about twice the computational cost of short-range MLIPs.'}",oai:arXiv.org:2408.15165v2,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'cond-mat.mtrl-sci', 'scheme': None, 'label': None}, {'term': 'physics.chem-ph', 'scheme': None, 'label': None}, {'term': 'physics.comp-ph', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}",[{'name': 'Bingqing Cheng'}],Bingqing Cheng,{'name': 'Bingqing Cheng'},,
610,Prediction-Feedback DETR for Temporal Action Detection,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Prediction-Feedback DETR for Temporal Action Detection'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2408.16729'}]",https://arxiv.org/abs/2408.16729,"arXiv:2408.16729v3 Announce Type: replace 
Abstract: Temporal Action Detection (TAD) is fundamental yet challenging for real-world video applications. Leveraging the unique benefits of transformers, various DETR-based approaches have been adopted in TAD. However, it has recently been identified that the attention collapse in self-attention causes the performance degradation of DETR for TAD. Building upon previous research, this paper newly addresses the attention collapse problem in cross-attention within DETR-based TAD methods. Moreover, our findings reveal that cross-attention exhibits patterns distinct from predictions, indicating a short-cut phenomenon. To resolve this, we propose a new framework, Prediction-Feedback DETR (Pred-DETR), which utilizes predictions to restore the collapse and align the cross- and self-attention with predictions. Specifically, we devise novel prediction-feedback objectives using guidance from the relations of the predictions. As a result, Pred-DETR significantly alleviates the collapse and achieves state-of-the-art performance among DETR-based methods on various challenging benchmarks including THUMOS14, ActivityNet-v1.3, HACS, and FineAction.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2408.16729v3 Announce Type: replace \nAbstract: Temporal Action Detection (TAD) is fundamental yet challenging for real-world video applications. Leveraging the unique benefits of transformers, various DETR-based approaches have been adopted in TAD. However, it has recently been identified that the attention collapse in self-attention causes the performance degradation of DETR for TAD. Building upon previous research, this paper newly addresses the attention collapse problem in cross-attention within DETR-based TAD methods. Moreover, our findings reveal that cross-attention exhibits patterns distinct from predictions, indicating a short-cut phenomenon. To resolve this, we propose a new framework, Prediction-Feedback DETR (Pred-DETR), which utilizes predictions to restore the collapse and align the cross- and self-attention with predictions. Specifically, we devise novel prediction-feedback objectives using guidance from the relations of the predictions. As a result, Pred-DETR significantly alleviates the collapse and achieves state-of-the-art performance among DETR-based methods on various challenging benchmarks including THUMOS14, ActivityNet-v1.3, HACS, and FineAction.'}",oai:arXiv.org:2408.16729v3,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Jihwan Kim, Miso Lee, Cheol-Ho Cho, Jihyun Lee, Jae-Pil Heo'}]","Jihwan Kim, Miso Lee, Cheol-Ho Cho, Jihyun Lee, Jae-Pil Heo","{'name': 'Jihwan Kim, Miso Lee, Cheol-Ho Cho, Jihyun Lee, Jae-Pil Heo'}",,
611,MaFeRw: Query Rewriting with Multi-Aspect Feedbacks for Retrieval-Augmented Large Language Models,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'MaFeRw: Query Rewriting with Multi-Aspect Feedbacks for Retrieval-Augmented Large Language Models'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2408.17072'}]",https://arxiv.org/abs/2408.17072,"arXiv:2408.17072v2 Announce Type: replace 
Abstract: In a real-world RAG system, the current query often involves spoken ellipses and ambiguous references from dialogue contexts, necessitating query rewriting to better describe user's information needs. However, traditional context-based rewriting has minimal enhancement on downstream generation tasks due to the lengthy process from query rewriting to response generation. Some researchers try to utilize reinforcement learning with generation feedback to assist the rewriter, but these sparse rewards provide little guidance in most cases, leading to unstable training and generation results. We find that user's needs are also reflected in the gold document, retrieved documents and ground truth. Therefore, by feeding back these multi-aspect dense rewards to query rewriting, more stable and satisfactory responses can be achieved. In this paper, we propose a novel query rewriting method MaFeRw, which improves RAG performance by integrating multi-aspect feedback from both the retrieval process and generated results. Specifically, we first use manual data to train a T5 model for the rewriter initialization. Next, we design three metrics as reinforcement learning feedback: the similarity between the rewritten query and the gold document, the ranking metrics, and ROUGE between the generation and the ground truth. Inspired by RLAIF, we train three kinds of reward models for the above metrics to achieve more efficient training. Finally, we combine the scores of these reward models as feedback, and use PPO algorithm to explore the optimal query rewriting strategy. Experimental results on two conversational RAG datasets demonstrate that MaFeRw achieves superior generation metrics and more stable training compared to baselines.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2408.17072v2 Announce Type: replace \nAbstract: In a real-world RAG system, the current query often involves spoken ellipses and ambiguous references from dialogue contexts, necessitating query rewriting to better describe user's information needs. However, traditional context-based rewriting has minimal enhancement on downstream generation tasks due to the lengthy process from query rewriting to response generation. Some researchers try to utilize reinforcement learning with generation feedback to assist the rewriter, but these sparse rewards provide little guidance in most cases, leading to unstable training and generation results. We find that user's needs are also reflected in the gold document, retrieved documents and ground truth. Therefore, by feeding back these multi-aspect dense rewards to query rewriting, more stable and satisfactory responses can be achieved. In this paper, we propose a novel query rewriting method MaFeRw, which improves RAG performance by integrating multi-aspect feedback from both the retrieval process and generated results. Specifically, we first use manual data to train a T5 model for the rewriter initialization. Next, we design three metrics as reinforcement learning feedback: the similarity between the rewritten query and the gold document, the ranking metrics, and ROUGE between the generation and the ground truth. Inspired by RLAIF, we train three kinds of reward models for the above metrics to achieve more efficient training. Finally, we combine the scores of these reward models as feedback, and use PPO algorithm to explore the optimal query rewriting strategy. Experimental results on two conversational RAG datasets demonstrate that MaFeRw achieves superior generation metrics and more stable training compared to baselines.""}",oai:arXiv.org:2408.17072v2,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Yujing Wang, Hainan Zhang, Liang Pang, Binghui Guo, Hongwei Zheng, Zhiming Zheng'}]","Yujing Wang, Hainan Zhang, Liang Pang, Binghui Guo, Hongwei Zheng, Zhiming Zheng","{'name': 'Yujing Wang, Hainan Zhang, Liang Pang, Binghui Guo, Hongwei Zheng, Zhiming Zheng'}",,
612,Development and Validation of a Modular Sensor-Based System for Gait Analysis and Control in Lower-Limb Exoskeletons,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Development and Validation of a Modular Sensor-Based System for Gait Analysis and Control in Lower-Limb Exoskeletons'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2409.01174'}]",https://arxiv.org/abs/2409.01174,"arXiv:2409.01174v2 Announce Type: replace 
Abstract: With rapid advancements in exoskeleton hardware technologies, successful assessment and accurate control remain challenging. This study introduces a modular sensor-based system to enhance biomechanical evaluation and control in lower-limb exoskeletons, utilizing advanced sensor technologies and fuzzy logic. We aim to surpass the limitations of current biomechanical evaluation methods confined to laboratories and to address the high costs and complexity of exoskeleton control systems. The system integrates inertial measurement units, force-sensitive resistors, and load cells into instrumented crutches and 3D-printed insoles. These components function both independently and collectively to capture comprehensive biomechanical data, including the anteroposterior center of pressure and crutch ground reaction forces. This data is processed through a central unit using fuzzy logic algorithms for real-time gait phase estimation and exoskeleton control. Validation experiments with three participants, benchmarked against gold-standard motion capture and force plate technologies, demonstrate our system's capability for reliable gait phase detection and precise biomechanical measurements. By offering our designs open-source and integrating cost-effective technologies, this study advances wearable robotics and promotes broader innovation and adoption in exoskeleton research.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2409.01174v2 Announce Type: replace \nAbstract: With rapid advancements in exoskeleton hardware technologies, successful assessment and accurate control remain challenging. This study introduces a modular sensor-based system to enhance biomechanical evaluation and control in lower-limb exoskeletons, utilizing advanced sensor technologies and fuzzy logic. We aim to surpass the limitations of current biomechanical evaluation methods confined to laboratories and to address the high costs and complexity of exoskeleton control systems. The system integrates inertial measurement units, force-sensitive resistors, and load cells into instrumented crutches and 3D-printed insoles. These components function both independently and collectively to capture comprehensive biomechanical data, including the anteroposterior center of pressure and crutch ground reaction forces. This data is processed through a central unit using fuzzy logic algorithms for real-time gait phase estimation and exoskeleton control. Validation experiments with three participants, benchmarked against gold-standard motion capture and force plate technologies, demonstrate our system's capability for reliable gait phase detection and precise biomechanical measurements. By offering our designs open-source and integrating cost-effective technologies, this study advances wearable robotics and promotes broader innovation and adoption in exoskeleton research.""}",oai:arXiv.org:2409.01174v2,False,"[{'term': 'cs.RO', 'scheme': None, 'label': None}, {'term': 'cs.HC', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Giorgos Marinou, Ibrahima Kourouma, Katja Mombaur'}]","Giorgos Marinou, Ibrahima Kourouma, Katja Mombaur","{'name': 'Giorgos Marinou, Ibrahima Kourouma, Katja Mombaur'}",,
613,Recoverable Compression: A Multimodal Vision Token Recovery Mechanism Guided by Text Information,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Recoverable Compression: A Multimodal Vision Token Recovery Mechanism Guided by Text Information'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2409.01179'}]",https://arxiv.org/abs/2409.01179,"arXiv:2409.01179v3 Announce Type: replace 
Abstract: With the advancement of large-scale language modeling techniques, large multimodal models combining visual encoders with large language models have demonstrated exceptional performance in various visual tasks. Most of the current large-scale multimodal models achieve this by mapping visual features obtained from the visual encoder into a large language model and using them as inputs alongside text for downstream tasks. Therefore, the number of visual tokens directly affects the training and inference speed of the model. There has been significant work on token pruning for visual transformers, but for large multimodal models, only relying on visual information for token pruning or compression may lead to significant loss of important information. On the other hand, the textual input in the form of a question may contain valuable information that can aid in answering the question, providing additional knowledge to the model. To address the potential oversimplification and excessive pruning that can occur with most purely visual token pruning methods, we propose a text information-guided dynamic visual token recovery mechanism that does not require training. This mechanism leverages the similarity between the question text and visual tokens to recover visually meaningful tokens with important text information while merging other less important tokens. Experimental results demonstrate that our proposed method achieves comparable performance to the original approach while compressing the visual tokens to an average of 10% of the original quantity. Our source code will be made publicly available following acceptance.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2409.01179v3 Announce Type: replace \nAbstract: With the advancement of large-scale language modeling techniques, large multimodal models combining visual encoders with large language models have demonstrated exceptional performance in various visual tasks. Most of the current large-scale multimodal models achieve this by mapping visual features obtained from the visual encoder into a large language model and using them as inputs alongside text for downstream tasks. Therefore, the number of visual tokens directly affects the training and inference speed of the model. There has been significant work on token pruning for visual transformers, but for large multimodal models, only relying on visual information for token pruning or compression may lead to significant loss of important information. On the other hand, the textual input in the form of a question may contain valuable information that can aid in answering the question, providing additional knowledge to the model. To address the potential oversimplification and excessive pruning that can occur with most purely visual token pruning methods, we propose a text information-guided dynamic visual token recovery mechanism that does not require training. This mechanism leverages the similarity between the question text and visual tokens to recover visually meaningful tokens with important text information while merging other less important tokens. Experimental results demonstrate that our proposed method achieves comparable performance to the original approach while compressing the visual tokens to an average of 10% of the original quantity. Our source code will be made publicly available following acceptance.'}",oai:arXiv.org:2409.01179v3,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Yi Chen, Jian Xu, Xu-Yao Zhang, Wen-Zhuo Liu, Yang-Yang Liu, Cheng-Lin Liu'}]","Yi Chen, Jian Xu, Xu-Yao Zhang, Wen-Zhuo Liu, Yang-Yang Liu, Cheng-Lin Liu","{'name': 'Yi Chen, Jian Xu, Xu-Yao Zhang, Wen-Zhuo Liu, Yang-Yang Liu, Cheng-Lin Liu'}",,
614,Prompt Compression with Context-Aware Sentence Encoding for Fast and Improved LLM Inference,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Prompt Compression with Context-Aware Sentence Encoding for Fast and Improved LLM Inference'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2409.01227'}]",https://arxiv.org/abs/2409.01227,"arXiv:2409.01227v3 Announce Type: replace 
Abstract: Large language models (LLMs) have triggered a new stream of research focusing on compressing the context length to reduce the computational cost while ensuring the retention of helpful information for LLMs to answer the given question. Token-based removal methods are one of the most prominent approaches in this direction, but risk losing the semantics of the context caused by intermediate token removal, especially under high compression ratios, while also facing challenges in computational efficiency. In this work, we propose context-aware prompt compression (CPC), a sentence-level prompt compression technique where its key innovation is a novel context-aware sentence encoder that provides a relevance score for each sentence for a given question. To train this encoder, we generate a new dataset consisting of questions, positives, and negative pairs where positives are sentences relevant to the question, while negatives are irrelevant context sentences. We train the encoder in a contrastive setup to learn context-aware sentence representations. Our method considerably outperforms prior works on prompt compression on benchmark datasets and is up to 10.93x faster at inference compared to the best token-level compression method. We also find better improvement for shorter length constraints in most benchmarks, showing the effectiveness of our proposed solution in the compression of relevant information in a shorter context. Finally, we release the code and the dataset for quick reproducibility and further development: https://github.com/Workday/cpc.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2409.01227v3 Announce Type: replace \nAbstract: Large language models (LLMs) have triggered a new stream of research focusing on compressing the context length to reduce the computational cost while ensuring the retention of helpful information for LLMs to answer the given question. Token-based removal methods are one of the most prominent approaches in this direction, but risk losing the semantics of the context caused by intermediate token removal, especially under high compression ratios, while also facing challenges in computational efficiency. In this work, we propose context-aware prompt compression (CPC), a sentence-level prompt compression technique where its key innovation is a novel context-aware sentence encoder that provides a relevance score for each sentence for a given question. To train this encoder, we generate a new dataset consisting of questions, positives, and negative pairs where positives are sentences relevant to the question, while negatives are irrelevant context sentences. We train the encoder in a contrastive setup to learn context-aware sentence representations. Our method considerably outperforms prior works on prompt compression on benchmark datasets and is up to 10.93x faster at inference compared to the best token-level compression method. We also find better improvement for shorter length constraints in most benchmarks, showing the effectiveness of our proposed solution in the compression of relevant information in a shorter context. Finally, we release the code and the dataset for quick reproducibility and further development: https://github.com/Workday/cpc.'}",oai:arXiv.org:2409.01227v3,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Barys Liskavets, Maxim Ushakov, Shuvendu Roy, Mark Klibanov, Ali Etemad, Shane Luke'}]","Barys Liskavets, Maxim Ushakov, Shuvendu Roy, Mark Klibanov, Ali Etemad, Shane Luke","{'name': 'Barys Liskavets, Maxim Ushakov, Shuvendu Roy, Mark Klibanov, Ali Etemad, Shane Luke'}",,
615,A Deployed Online Reinforcement Learning Algorithm In An Oral Health Clinical Trial,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'A Deployed Online Reinforcement Learning Algorithm In An Oral Health Clinical Trial'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2409.02069'}]",https://arxiv.org/abs/2409.02069,"arXiv:2409.02069v2 Announce Type: replace 
Abstract: Dental disease is a prevalent chronic condition associated with substantial financial burden, personal suffering, and increased risk of systemic diseases. Despite widespread recommendations for twice-daily tooth brushing, adherence to recommended oral self-care behaviors remains sub-optimal due to factors such as forgetfulness and disengagement. To address this, we developed Oralytics, a mHealth intervention system designed to complement clinician-delivered preventative care for marginalized individuals at risk for dental disease. Oralytics incorporates an online reinforcement learning algorithm to determine optimal times to deliver intervention prompts that encourage oral self-care behaviors. We have deployed Oralytics in a registered clinical trial. The deployment required careful design to manage challenges specific to the clinical trials setting in the U.S. In this paper, we (1) highlight key design decisions of the RL algorithm that address these challenges and (2) conduct a re-sampling analysis to evaluate algorithm design decisions. A second phase (randomized control trial) of Oralytics is planned to start in spring 2025.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2409.02069v2 Announce Type: replace \nAbstract: Dental disease is a prevalent chronic condition associated with substantial financial burden, personal suffering, and increased risk of systemic diseases. Despite widespread recommendations for twice-daily tooth brushing, adherence to recommended oral self-care behaviors remains sub-optimal due to factors such as forgetfulness and disengagement. To address this, we developed Oralytics, a mHealth intervention system designed to complement clinician-delivered preventative care for marginalized individuals at risk for dental disease. Oralytics incorporates an online reinforcement learning algorithm to determine optimal times to deliver intervention prompts that encourage oral self-care behaviors. We have deployed Oralytics in a registered clinical trial. The deployment required careful design to manage challenges specific to the clinical trials setting in the U.S. In this paper, we (1) highlight key design decisions of the RL algorithm that address these challenges and (2) conduct a re-sampling analysis to evaluate algorithm design decisions. A second phase (randomized control trial) of Oralytics is planned to start in spring 2025.'}",oai:arXiv.org:2409.02069v2,False,"[{'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.HC', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Anna L. Trella, Kelly W. Zhang, Hinal Jajal, Inbal Nahum-Shani, Vivek Shetty, Finale Doshi-Velez, Susan A. Murphy'}]","Anna L. Trella, Kelly W. Zhang, Hinal Jajal, Inbal Nahum-Shani, Vivek Shetty, Finale Doshi-Velez, Susan A. Murphy","{'name': 'Anna L. Trella, Kelly W. Zhang, Hinal Jajal, Inbal Nahum-Shani, Vivek Shetty, Finale Doshi-Velez, Susan A. Murphy'}",,
616,Cycle Pixel Difference Network for Crisp Edge Detection,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Cycle Pixel Difference Network for Crisp Edge Detection'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2409.04272'}]",https://arxiv.org/abs/2409.04272,"arXiv:2409.04272v2 Announce Type: replace 
Abstract: Edge detection, as a fundamental task in computer vision, has garnered increasing attention. The advent of deep learning has significantly advanced this field. However, recent deep learning-based methods generally face two significant issues: 1) reliance on large-scale pre-trained weights, and 2) generation of thick edges. We construct a U-shape encoder-decoder model named CPD-Net that successfully addresses these two issues simultaneously. In response to issue 1), we propose a novel cycle pixel difference convolution (CPDC), which effectively integrates edge prior knowledge with modern convolution operations, consequently successfully eliminating the dependence on large-scale pre-trained weights. As for issue 2), we construct a multi-scale information enhancement module (MSEM) and a dual residual connection-based (DRC) decoder to enhance the edge location ability of the model, thereby generating crisp and clean contour maps. Comprehensive experiments conducted on four standard benchmarks demonstrate that our method achieves competitive performance on the BSDS500 dataset (ODS=0.813 and AC=0.352), NYUD-V2 (ODS=0.760 and AC=0.223), BIPED dataset (ODS=0.898 and AC=0.426), and CID (ODS=0.59). Our approach provides a novel perspective for addressing these challenges in edge detection.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2409.04272v2 Announce Type: replace \nAbstract: Edge detection, as a fundamental task in computer vision, has garnered increasing attention. The advent of deep learning has significantly advanced this field. However, recent deep learning-based methods generally face two significant issues: 1) reliance on large-scale pre-trained weights, and 2) generation of thick edges. We construct a U-shape encoder-decoder model named CPD-Net that successfully addresses these two issues simultaneously. In response to issue 1), we propose a novel cycle pixel difference convolution (CPDC), which effectively integrates edge prior knowledge with modern convolution operations, consequently successfully eliminating the dependence on large-scale pre-trained weights. As for issue 2), we construct a multi-scale information enhancement module (MSEM) and a dual residual connection-based (DRC) decoder to enhance the edge location ability of the model, thereby generating crisp and clean contour maps. Comprehensive experiments conducted on four standard benchmarks demonstrate that our method achieves competitive performance on the BSDS500 dataset (ODS=0.813 and AC=0.352), NYUD-V2 (ODS=0.760 and AC=0.223), BIPED dataset (ODS=0.898 and AC=0.426), and CID (ODS=0.59). Our approach provides a novel perspective for addressing these challenges in edge detection.'}",oai:arXiv.org:2409.04272v2,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Changsong Liu, Wei Zhang, Yanyan Liu, Mingyang Li, Wenlin Li, Yimeng Fan, Xiangnan Bai, Liang Zhang'}]","Changsong Liu, Wei Zhang, Yanyan Liu, Mingyang Li, Wenlin Li, Yimeng Fan, Xiangnan Bai, Liang Zhang","{'name': 'Changsong Liu, Wei Zhang, Yanyan Liu, Mingyang Li, Wenlin Li, Yimeng Fan, Xiangnan Bai, Liang Zhang'}",,
617,CONNECTOR: Enhancing the Traceability of Decentralized Bridge Applications via Automatic Cross-chain Transaction Association,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'CONNECTOR: Enhancing the Traceability of Decentralized Bridge Applications via Automatic Cross-chain Transaction Association'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2409.04937'}]",https://arxiv.org/abs/2409.04937,"arXiv:2409.04937v2 Announce Type: replace 
Abstract: Decentralized bridge applications are important software that connects various blockchains and facilitates cross-chain asset transfer in the decentralized finance (DeFi) ecosystem which currently operates in a multi-chain environment. Cross-chain transaction association identifies and matches unique transactions executed by bridge DApps, which is important research to enhance the traceability of cross-chain bridge DApps. However, existing methods rely entirely on unobservable internal ledgers or APIs, violating the open and decentralized properties of blockchain. In this paper, we analyze the challenges of this issue and then present CONNECTOR, an automated cross-chain transaction association analysis method based on bridge smart contracts. Specifically, CONNECTOR first identifies deposit transactions by extracting distinctive and generic features from the transaction traces of bridge contracts. With the accurate deposit transactions, CONNECTOR mines the execution logs of bridge contracts to achieve withdrawal transaction matching. We conduct real-world experiments on different types of bridges to demonstrate the effectiveness of CONNECTOR. The experiment demonstrates that CONNECTOR successfully identifies 100% deposit transactions, associates 95.81% withdrawal transactions, and surpasses methods for CeFi bridges. Based on the association results, we obtain interesting findings about cross-chain transaction behaviors in DeFi bridges and analyze the tracing abilities of CONNECTOR to assist the DeFi bridge apps.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2409.04937v2 Announce Type: replace \nAbstract: Decentralized bridge applications are important software that connects various blockchains and facilitates cross-chain asset transfer in the decentralized finance (DeFi) ecosystem which currently operates in a multi-chain environment. Cross-chain transaction association identifies and matches unique transactions executed by bridge DApps, which is important research to enhance the traceability of cross-chain bridge DApps. However, existing methods rely entirely on unobservable internal ledgers or APIs, violating the open and decentralized properties of blockchain. In this paper, we analyze the challenges of this issue and then present CONNECTOR, an automated cross-chain transaction association analysis method based on bridge smart contracts. Specifically, CONNECTOR first identifies deposit transactions by extracting distinctive and generic features from the transaction traces of bridge contracts. With the accurate deposit transactions, CONNECTOR mines the execution logs of bridge contracts to achieve withdrawal transaction matching. We conduct real-world experiments on different types of bridges to demonstrate the effectiveness of CONNECTOR. The experiment demonstrates that CONNECTOR successfully identifies 100% deposit transactions, associates 95.81% withdrawal transactions, and surpasses methods for CeFi bridges. Based on the association results, we obtain interesting findings about cross-chain transaction behaviors in DeFi bridges and analyze the tracing abilities of CONNECTOR to assist the DeFi bridge apps.'}",oai:arXiv.org:2409.04937v2,False,"[{'term': 'cs.SE', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Dan Lin, Jiajing Wu, Yuxin Su, Ziye Zheng, Yuhong Nan, Qinnan Zhang, Bowen Song, Zibin Zheng'}]","Dan Lin, Jiajing Wu, Yuxin Su, Ziye Zheng, Yuhong Nan, Qinnan Zhang, Bowen Song, Zibin Zheng","{'name': 'Dan Lin, Jiajing Wu, Yuxin Su, Ziye Zheng, Yuhong Nan, Qinnan Zhang, Bowen Song, Zibin Zheng'}",,
618,Alt-MoE: Multimodal Alignment via Alternating Optimization of Multi-directional MoE with Unimodal Models,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Alt-MoE: Multimodal Alignment via Alternating Optimization of Multi-directional MoE with Unimodal Models'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2409.05929'}]",https://arxiv.org/abs/2409.05929,"arXiv:2409.05929v2 Announce Type: replace 
Abstract: Recent Large Multi-Modal Models (LMMs) have made significant advancements in multi-modal alignment by employing lightweight connection modules to facilitate the representation and fusion of knowledge from existing pre-trained uni-modal models. However, these methods still rely on modality-specific and direction-specific connectors, leading to compartmentalized knowledge representations and reduced computational efficiency, which limits the model's ability to form unified multi-modal representations. To address these issues, we introduce a novel training framework, Alt-MoE, which employs the Mixture of Experts (MoE) as a unified multi-directional connector across modalities, and employs a multi-step sequential alternating unidirectional alignment strategy, which converges to bidirectional alignment over iterations. The extensive empirical studies revealed the following key points: 1) Alt-MoE achieves competitive results by integrating diverse knowledge representations from uni-modal models. This approach seamlessly fuses the specialized expertise of existing high-performance uni-modal models, effectively synthesizing their domain-specific knowledge into a cohesive multi-modal representation. 2) Alt-MoE efficiently scales to new tasks and modalities without altering its model architecture or training strategy. Furthermore, Alt-MoE operates in latent space, supporting vector pre-storage and real-time retrieval via lightweight multi-directional MoE, thereby facilitating massive data processing. Our methodology has been validated on several well-performing uni-modal models (LLAMA3, Qwen2, and DINOv2), achieving competitive results on a wide range of downstream tasks and datasets.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2409.05929v2 Announce Type: replace \nAbstract: Recent Large Multi-Modal Models (LMMs) have made significant advancements in multi-modal alignment by employing lightweight connection modules to facilitate the representation and fusion of knowledge from existing pre-trained uni-modal models. However, these methods still rely on modality-specific and direction-specific connectors, leading to compartmentalized knowledge representations and reduced computational efficiency, which limits the model's ability to form unified multi-modal representations. To address these issues, we introduce a novel training framework, Alt-MoE, which employs the Mixture of Experts (MoE) as a unified multi-directional connector across modalities, and employs a multi-step sequential alternating unidirectional alignment strategy, which converges to bidirectional alignment over iterations. The extensive empirical studies revealed the following key points: 1) Alt-MoE achieves competitive results by integrating diverse knowledge representations from uni-modal models. This approach seamlessly fuses the specialized expertise of existing high-performance uni-modal models, effectively synthesizing their domain-specific knowledge into a cohesive multi-modal representation. 2) Alt-MoE efficiently scales to new tasks and modalities without altering its model architecture or training strategy. Furthermore, Alt-MoE operates in latent space, supporting vector pre-storage and real-time retrieval via lightweight multi-directional MoE, thereby facilitating massive data processing. Our methodology has been validated on several well-performing uni-modal models (LLAMA3, Qwen2, and DINOv2), achieving competitive results on a wide range of downstream tasks and datasets.""}",oai:arXiv.org:2409.05929v2,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Hongyang Lei, Xiaolong Cheng, Dan Wang, Kun Fan, Qi Qin, Huazhen Huang, Yetao Wu, Qingqing Gu, Zhonglin Jiang, Yong Chen, Luo Ji'}]","Hongyang Lei, Xiaolong Cheng, Dan Wang, Kun Fan, Qi Qin, Huazhen Huang, Yetao Wu, Qingqing Gu, Zhonglin Jiang, Yong Chen, Luo Ji","{'name': 'Hongyang Lei, Xiaolong Cheng, Dan Wang, Kun Fan, Qi Qin, Huazhen Huang, Yetao Wu, Qingqing Gu, Zhonglin Jiang, Yong Chen, Luo Ji'}",,
619,Design of Distributed Controller for Discrete-Time Systems Via the Integration of Extended LMI and Clique-Wise Decomposition,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Design of Distributed Controller for Discrete-Time Systems Via the Integration of Extended LMI and Clique-Wise Decomposition'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2409.07666'}]",https://arxiv.org/abs/2409.07666,"arXiv:2409.07666v3 Announce Type: replace 
Abstract: This study addresses the centralized synthesis of distributed controllers using linear matrix inequalities (LMIs). Sparsity constraints on control gains of distributed controllers result in conservatism via the convexification of the existing methods such as the extended LMI method. In order to mitigate the conservatism, we introduce a novel LMI formulation for this problem, utilizing the clique-wise decomposition method from our previous work on continuous-time systems. By reformulating the sparsity constraint on the gain matrix within cliques, this method achieves a broader solution set. Also, the analytical superiority of our method is confirmed through numerical examples.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2409.07666v3 Announce Type: replace \nAbstract: This study addresses the centralized synthesis of distributed controllers using linear matrix inequalities (LMIs). Sparsity constraints on control gains of distributed controllers result in conservatism via the convexification of the existing methods such as the extended LMI method. In order to mitigate the conservatism, we introduce a novel LMI formulation for this problem, utilizing the clique-wise decomposition method from our previous work on continuous-time systems. By reformulating the sparsity constraint on the gain matrix within cliques, this method achieves a broader solution set. Also, the analytical superiority of our method is confirmed through numerical examples.'}",oai:arXiv.org:2409.07666v3,False,"[{'term': 'eess.SY', 'scheme': None, 'label': None}, {'term': 'cs.SY', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Sotaro Fushimi, Yuto Watanabe, Kazunori Sakurama'}]","Sotaro Fushimi, Yuto Watanabe, Kazunori Sakurama","{'name': 'Sotaro Fushimi, Yuto Watanabe, Kazunori Sakurama'}",,
620,Knowledge Tagging with Large Language Model based Multi-Agent System,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Knowledge Tagging with Large Language Model based Multi-Agent System'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2409.08406'}]",https://arxiv.org/abs/2409.08406,"arXiv:2409.08406v2 Announce Type: replace 
Abstract: Knowledge tagging for questions is vital in modern intelligent educational applications, including learning progress diagnosis, practice question recommendations, and course content organization. Traditionally, these annotations have been performed by pedagogical experts, as the task demands not only a deep semantic understanding of question stems and knowledge definitions but also a strong ability to link problem-solving logic with relevant knowledge concepts. With the advent of advanced natural language processing (NLP) algorithms, such as pre-trained language models and large language models (LLMs), pioneering studies have explored automating the knowledge tagging process using various machine learning models. In this paper, we investigate the use of a multi-agent system to address the limitations of previous algorithms, particularly in handling complex cases involving intricate knowledge definitions and strict numerical constraints. By demonstrating its superior performance on the publicly available math question knowledge tagging dataset, MathKnowCT, we highlight the significant potential of an LLM-based multi-agent system in overcoming the challenges that previous methods have encountered. Finally, through an in-depth discussion of the implications of automating knowledge tagging, we underscore the promising results of deploying LLM-based algorithms in educational contexts.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2409.08406v2 Announce Type: replace \nAbstract: Knowledge tagging for questions is vital in modern intelligent educational applications, including learning progress diagnosis, practice question recommendations, and course content organization. Traditionally, these annotations have been performed by pedagogical experts, as the task demands not only a deep semantic understanding of question stems and knowledge definitions but also a strong ability to link problem-solving logic with relevant knowledge concepts. With the advent of advanced natural language processing (NLP) algorithms, such as pre-trained language models and large language models (LLMs), pioneering studies have explored automating the knowledge tagging process using various machine learning models. In this paper, we investigate the use of a multi-agent system to address the limitations of previous algorithms, particularly in handling complex cases involving intricate knowledge definitions and strict numerical constraints. By demonstrating its superior performance on the publicly available math question knowledge tagging dataset, MathKnowCT, we highlight the significant potential of an LLM-based multi-agent system in overcoming the challenges that previous methods have encountered. Finally, through an in-depth discussion of the implications of automating knowledge tagging, we underscore the promising results of deploying LLM-based algorithms in educational contexts.'}",oai:arXiv.org:2409.08406v2,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Hang Li, Tianlong Xu, Ethan Chang, Qingsong Wen'}]","Hang Li, Tianlong Xu, Ethan Chang, Qingsong Wen","{'name': 'Hang Li, Tianlong Xu, Ethan Chang, Qingsong Wen'}",,
621,RT-DETRv3: Real-time End-to-End Object Detection with Hierarchical Dense Positive Supervision,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'RT-DETRv3: Real-time End-to-End Object Detection with Hierarchical Dense Positive Supervision'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2409.08475'}]",https://arxiv.org/abs/2409.08475,"arXiv:2409.08475v3 Announce Type: replace 
Abstract: RT-DETR is the first real-time end-to-end transformer-based object detector. Its efficiency comes from the framework design and the Hungarian matching. However, compared to dense supervision detectors like the YOLO series, the Hungarian matching provides much sparser supervision, leading to insufficient model training and difficult to achieve optimal results. To address these issues, we proposed a hierarchical dense positive supervision method based on RT-DETR, named RT-DETRv3. Firstly, we introduce a CNN-based auxiliary branch that provides dense supervision that collaborates with the original decoder to enhance the encoder feature representation. Secondly, to address insufficient decoder training, we propose a novel learning strategy involving self-attention perturbation. This strategy diversifies label assignment for positive samples across multiple query groups, thereby enriching positive supervisions. Additionally, we introduce a shared-weight decoder branch for dense positive supervision to ensure more high-quality queries matching each ground truth. Notably, all aforementioned modules are training-only. We conduct extensive experiments to demonstrate the effectiveness of our approach on COCO val2017. RT-DETRv3 significantly outperforms existing real-time detectors, including the RT-DETR series and the YOLO series. For example, RT-DETRv3-R18 achieves 48.1% AP (+1.6%/+1.4%) compared to RT-DETR-R18/RT-DETRv2-R18, while maintaining the same latency. Furthermore, RT-DETRv3-R101 can attain an impressive 54.6% AP outperforming YOLOv10-X. The code will be released at https://github.com/clxia12/RT-DETRv3.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2409.08475v3 Announce Type: replace \nAbstract: RT-DETR is the first real-time end-to-end transformer-based object detector. Its efficiency comes from the framework design and the Hungarian matching. However, compared to dense supervision detectors like the YOLO series, the Hungarian matching provides much sparser supervision, leading to insufficient model training and difficult to achieve optimal results. To address these issues, we proposed a hierarchical dense positive supervision method based on RT-DETR, named RT-DETRv3. Firstly, we introduce a CNN-based auxiliary branch that provides dense supervision that collaborates with the original decoder to enhance the encoder feature representation. Secondly, to address insufficient decoder training, we propose a novel learning strategy involving self-attention perturbation. This strategy diversifies label assignment for positive samples across multiple query groups, thereby enriching positive supervisions. Additionally, we introduce a shared-weight decoder branch for dense positive supervision to ensure more high-quality queries matching each ground truth. Notably, all aforementioned modules are training-only. We conduct extensive experiments to demonstrate the effectiveness of our approach on COCO val2017. RT-DETRv3 significantly outperforms existing real-time detectors, including the RT-DETR series and the YOLO series. For example, RT-DETRv3-R18 achieves 48.1% AP (+1.6%/+1.4%) compared to RT-DETR-R18/RT-DETRv2-R18, while maintaining the same latency. Furthermore, RT-DETRv3-R101 can attain an impressive 54.6% AP outperforming YOLOv10-X. The code will be released at https://github.com/clxia12/RT-DETRv3.'}",oai:arXiv.org:2409.08475v3,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Shuo Wang, Chunlong Xia, Feng Lv, Yifeng Shi'}]","Shuo Wang, Chunlong Xia, Feng Lv, Yifeng Shi","{'name': 'Shuo Wang, Chunlong Xia, Feng Lv, Yifeng Shi'}",,
622,Incorporating Procedural Fairness in Flag Submissions on Social Media Platforms,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Incorporating Procedural Fairness in Flag Submissions on Social Media Platforms'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2409.08498'}]",https://arxiv.org/abs/2409.08498,"arXiv:2409.08498v2 Announce Type: replace 
Abstract: Flagging mechanisms on social media platforms allow users to report inappropriate posts/accounts for review by content moderators. These reports are pivotal to platforms' efforts toward regulating norm violations. This paper examines how platforms' design choices in implementing flagging mechanisms influence flaggers' perceptions of content moderation. We conducted a survey experiment asking US respondents (N=2,936) to flag inappropriate posts using one of 54 randomly assigned flagging implementations. After flagging, participants rated their fairness perceptions of the flag submission process along the dimensions of consistency, transparency, and voice (agency). We found that participants perceived greater transparency when flagging interfaces included community guidelines and greater voice when they incorporated a text box for open-ended feedback. Our qualitative analysis highlights user needs for improved accessibility, educational support for reporting, and protections against false flags. We offer design recommendations for building fairer flagging systems without exacerbating the cognitive burden of submitting flags.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2409.08498v2 Announce Type: replace \nAbstract: Flagging mechanisms on social media platforms allow users to report inappropriate posts/accounts for review by content moderators. These reports are pivotal to platforms' efforts toward regulating norm violations. This paper examines how platforms' design choices in implementing flagging mechanisms influence flaggers' perceptions of content moderation. We conducted a survey experiment asking US respondents (N=2,936) to flag inappropriate posts using one of 54 randomly assigned flagging implementations. After flagging, participants rated their fairness perceptions of the flag submission process along the dimensions of consistency, transparency, and voice (agency). We found that participants perceived greater transparency when flagging interfaces included community guidelines and greater voice when they incorporated a text box for open-ended feedback. Our qualitative analysis highlights user needs for improved accessibility, educational support for reporting, and protections against false flags. We offer design recommendations for building fairer flagging systems without exacerbating the cognitive burden of submitting flags.""}",oai:arXiv.org:2409.08498v2,False,"[{'term': 'cs.HC', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Yunhee Shim, Shagun Jhaver'}]","Yunhee Shim, Shagun Jhaver","{'name': 'Yunhee Shim, Shagun Jhaver'}",,
623,Training Datasets Generation for Machine Learning: Application to Vision Based Navigation,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Training Datasets Generation for Machine Learning: Application to Vision Based Navigation'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2409.11383'}]",https://arxiv.org/abs/2409.11383,"arXiv:2409.11383v2 Announce Type: replace 
Abstract: Vision Based Navigation consists in utilizing cameras as precision sensors for GNC after extracting information from images. To enable the adoption of machine learning for space applications, one of obstacles is the demonstration that available training datasets are adequate to validate the algorithms. The objective of the study is to generate datasets of images and metadata suitable for training machine learning algorithms. Two use cases were selected and a robust methodology was developed to validate the datasets including the ground truth. The first use case is in-orbit rendezvous with a man-made object: a mockup of satellite ENVISAT. The second use case is a Lunar landing scenario. Datasets were produced from archival datasets (Chang'e 3), from the laboratory at DLR TRON facility and at Airbus Robotic laboratory, from SurRender software high fidelity image simulator using Model Capture and from Generative Adversarial Networks. The use case definition included the selection of algorithms as benchmark: an AI-based pose estimation algorithm and a dense optical flow algorithm were selected. Eventually it is demonstrated that datasets produced with SurRender and selected laboratory facilities are adequate to train machine learning algorithms.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2409.11383v2 Announce Type: replace \nAbstract: Vision Based Navigation consists in utilizing cameras as precision sensors for GNC after extracting information from images. To enable the adoption of machine learning for space applications, one of obstacles is the demonstration that available training datasets are adequate to validate the algorithms. The objective of the study is to generate datasets of images and metadata suitable for training machine learning algorithms. Two use cases were selected and a robust methodology was developed to validate the datasets including the ground truth. The first use case is in-orbit rendezvous with a man-made object: a mockup of satellite ENVISAT. The second use case is a Lunar landing scenario. Datasets were produced from archival datasets (Chang'e 3), from the laboratory at DLR TRON facility and at Airbus Robotic laboratory, from SurRender software high fidelity image simulator using Model Capture and from Generative Adversarial Networks. The use case definition included the selection of algorithms as benchmark: an AI-based pose estimation algorithm and a dense optical flow algorithm were selected. Eventually it is demonstrated that datasets produced with SurRender and selected laboratory facilities are adequate to train machine learning algorithms.""}",oai:arXiv.org:2409.11383v2,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'astro-ph.EP', 'scheme': None, 'label': None}, {'term': 'cs.GR', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'J\\\'er\\\'emy Lebreton, Ingo Ahrns, Roland Brochard, Christoph Haskamp, Hans Kr\\""uger, Matthieu Le Goff, Nicolas Menga, Nicolas Ollagnier, Ralf Regele, Francesco Capolupo, Massimo Casasco'}]","J\'er\'emy Lebreton, Ingo Ahrns, Roland Brochard, Christoph Haskamp, Hans Kr\""uger, Matthieu Le Goff, Nicolas Menga, Nicolas Ollagnier, Ralf Regele, Francesco Capolupo, Massimo Casasco","{'name': 'J\\\'er\\\'emy Lebreton, Ingo Ahrns, Roland Brochard, Christoph Haskamp, Hans Kr\\""uger, Matthieu Le Goff, Nicolas Menga, Nicolas Ollagnier, Ralf Regele, Francesco Capolupo, Massimo Casasco'}",,
624,Why Is Anything Conscious?,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Why Is Anything Conscious?'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2409.14545'}]",https://arxiv.org/abs/2409.14545,"arXiv:2409.14545v4 Announce Type: replace 
Abstract: We tackle the hard problem of consciousness taking the naturally selected, embodied organism as our starting point. We provide a formalism describing how biological systems self-organise to hierarchically interpret unlabelled sensory information according to valence. Such interpretations imply behavioural policies which are differentiated from each other only by the qualitative aspect of information processing. Natural selection favours systems that intervene in the world to achieve homeostatic and reproductive goals. Quality is a property arising in such systems to link cause to affect to motivate interventions. This produces interoceptive and exteroceptive classifiers and determines priorities. In formalising the seminal distinction between access and phenomenal consciousness, we claim that access consciousness at the human level requires the ability to hierarchically model i) the self, ii) the world/others and iii) the self as modelled by others, and that this requires phenomenal consciousness. Phenomenal without access consciousness is likely common, but the reverse is implausible. To put it provocatively: death grounds meaning, and Nature does not like zombies. We then describe the multilayered architecture of self-organisation from rocks to Einstein, illustrating how our argument applies. Our proposal lays the foundation of a formal science of consciousness, closer to human fact than zombie fiction.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2409.14545v4 Announce Type: replace \nAbstract: We tackle the hard problem of consciousness taking the naturally selected, embodied organism as our starting point. We provide a formalism describing how biological systems self-organise to hierarchically interpret unlabelled sensory information according to valence. Such interpretations imply behavioural policies which are differentiated from each other only by the qualitative aspect of information processing. Natural selection favours systems that intervene in the world to achieve homeostatic and reproductive goals. Quality is a property arising in such systems to link cause to affect to motivate interventions. This produces interoceptive and exteroceptive classifiers and determines priorities. In formalising the seminal distinction between access and phenomenal consciousness, we claim that access consciousness at the human level requires the ability to hierarchically model i) the self, ii) the world/others and iii) the self as modelled by others, and that this requires phenomenal consciousness. Phenomenal without access consciousness is likely common, but the reverse is implausible. To put it provocatively: death grounds meaning, and Nature does not like zombies. We then describe the multilayered architecture of self-organisation from rocks to Einstein, illustrating how our argument applies. Our proposal lays the foundation of a formal science of consciousness, closer to human fact than zombie fiction.'}",oai:arXiv.org:2409.14545v4,False,"[{'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Michael Timothy Bennett, Sean Welsh, Anna Ciaunica'}]","Michael Timothy Bennett, Sean Welsh, Anna Ciaunica","{'name': 'Michael Timothy Bennett, Sean Welsh, Anna Ciaunica'}",,
625,Distribution-Level Feature Distancing for Machine Unlearning: Towards a Better Trade-off Between Model Utility and Forgetting,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Distribution-Level Feature Distancing for Machine Unlearning: Towards a Better Trade-off Between Model Utility and Forgetting'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2409.14747'}]",https://arxiv.org/abs/2409.14747,"arXiv:2409.14747v5 Announce Type: replace 
Abstract: With the explosive growth of deep learning applications and increasing privacy concerns, the right to be forgotten has become a critical requirement in various AI industries. For example, given a facial recognition system, some individuals may wish to remove their personal data that might have been used in the training phase. Unfortunately, deep neural networks sometimes unexpectedly leak personal identities, making this removal challenging. While recent machine unlearning algorithms aim to enable models to forget specific data, we identify an unintended utility drop-correlation collapse-in which the essential correlations between image features and true labels weaken during the forgetting process. To address this challenge, we propose Distribution-Level Feature Distancing (DLFD), a novel method that efficiently forgets instances while preserving task-relevant feature correlations. Our method synthesizes data samples by optimizing the feature distribution to be distinctly different from that of forget samples, achieving effective results within a single training epoch. Through extensive experiments on facial recognition datasets, we demonstrate that our approach significantly outperforms state-of-the-art machine unlearning methods in both forgetting performance and model utility preservation.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2409.14747v5 Announce Type: replace \nAbstract: With the explosive growth of deep learning applications and increasing privacy concerns, the right to be forgotten has become a critical requirement in various AI industries. For example, given a facial recognition system, some individuals may wish to remove their personal data that might have been used in the training phase. Unfortunately, deep neural networks sometimes unexpectedly leak personal identities, making this removal challenging. While recent machine unlearning algorithms aim to enable models to forget specific data, we identify an unintended utility drop-correlation collapse-in which the essential correlations between image features and true labels weaken during the forgetting process. To address this challenge, we propose Distribution-Level Feature Distancing (DLFD), a novel method that efficiently forgets instances while preserving task-relevant feature correlations. Our method synthesizes data samples by optimizing the feature distribution to be distinctly different from that of forget samples, achieving effective results within a single training epoch. Through extensive experiments on facial recognition datasets, we demonstrate that our approach significantly outperforms state-of-the-art machine unlearning methods in both forgetting performance and model utility preservation.'}",oai:arXiv.org:2409.14747v5,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by-nc-sa/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-sa/4.0/'}","[{'name': 'Dasol Choi, Dongbin Na'}]","Dasol Choi, Dongbin Na","{'name': 'Dasol Choi, Dongbin Na'}",,
626,Erase then Rectify: A Training-Free Parameter Editing Approach for Cost-Effective Graph Unlearning,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Erase then Rectify: A Training-Free Parameter Editing Approach for Cost-Effective Graph Unlearning'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2409.16684'}]",https://arxiv.org/abs/2409.16684,"arXiv:2409.16684v2 Announce Type: replace 
Abstract: Graph unlearning, which aims to eliminate the influence of specific nodes, edges, or attributes from a trained Graph Neural Network (GNN), is essential in applications where privacy, bias, or data obsolescence is a concern. However, existing graph unlearning techniques often necessitate additional training on the remaining data, leading to significant computational costs, particularly with large-scale graphs. To address these challenges, we propose a two-stage training-free approach, Erase then Rectify (ETR), designed for efficient and scalable graph unlearning while preserving the model utility. Specifically, we first build a theoretical foundation showing that masking parameters critical for unlearned samples enables effective unlearning. Building on this insight, the Erase stage strategically edits model parameters to eliminate the impact of unlearned samples and their propagated influence on intercorrelated nodes. To further ensure the GNN's utility, the Rectify stage devises a gradient approximation method to estimate the model's gradient on the remaining dataset, which is then used to enhance model performance. Overall, ETR achieves graph unlearning without additional training or full training data access, significantly reducing computational overhead and preserving data privacy. Extensive experiments on seven public datasets demonstrate the consistent superiority of ETR in model utility, unlearning efficiency, and unlearning effectiveness, establishing it as a promising solution for real-world graph unlearning challenges.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2409.16684v2 Announce Type: replace \nAbstract: Graph unlearning, which aims to eliminate the influence of specific nodes, edges, or attributes from a trained Graph Neural Network (GNN), is essential in applications where privacy, bias, or data obsolescence is a concern. However, existing graph unlearning techniques often necessitate additional training on the remaining data, leading to significant computational costs, particularly with large-scale graphs. To address these challenges, we propose a two-stage training-free approach, Erase then Rectify (ETR), designed for efficient and scalable graph unlearning while preserving the model utility. Specifically, we first build a theoretical foundation showing that masking parameters critical for unlearned samples enables effective unlearning. Building on this insight, the Erase stage strategically edits model parameters to eliminate the impact of unlearned samples and their propagated influence on intercorrelated nodes. To further ensure the GNN's utility, the Rectify stage devises a gradient approximation method to estimate the model's gradient on the remaining dataset, which is then used to enhance model performance. Overall, ETR achieves graph unlearning without additional training or full training data access, significantly reducing computational overhead and preserving data privacy. Extensive experiments on seven public datasets demonstrate the consistent superiority of ETR in model utility, unlearning efficiency, and unlearning effectiveness, establishing it as a promising solution for real-world graph unlearning challenges.""}",oai:arXiv.org:2409.16684v2,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Zhe-Rui Yang, Jindong Han, Chang-Dong Wang, Hao Liu'}]","Zhe-Rui Yang, Jindong Han, Chang-Dong Wang, Hao Liu","{'name': 'Zhe-Rui Yang, Jindong Han, Chang-Dong Wang, Hao Liu'}",,
627,"Deep CLAS: Deep Contextual Listen, Attend and Spell","{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Deep CLAS: Deep Contextual Listen, Attend and Spell'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2409.17603'}]",https://arxiv.org/abs/2409.17603,"arXiv:2409.17603v2 Announce Type: replace 
Abstract: Contextual-LAS (CLAS) has been shown effective in improving Automatic Speech Recognition (ASR) of rare words. It relies on phrase-level contextual modeling and attention-based relevance scoring without explicit contextual constraint which lead to insufficient use of contextual information. In this work, we propose deep CLAS to use contextual information better. We introduce bias loss forcing model to focus on contextual information. The query of bias attention is also enriched to improve the accuracy of the bias attention score. To get fine-grained contextual information, we replace phrase-level encoding with character-level encoding and encode contextual information with conformer rather than LSTM. Moreover, we directly use the bias attention score to correct the output probability distribution of the model. Experiments using the public AISHELL-1 and AISHELL-NER. On AISHELL-1, compared to CLAS baselines, deep CLAS obtains a 65.78% relative recall and a 53.49% relative F1-score increase in the named entity recognition scene.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2409.17603v2 Announce Type: replace \nAbstract: Contextual-LAS (CLAS) has been shown effective in improving Automatic Speech Recognition (ASR) of rare words. It relies on phrase-level contextual modeling and attention-based relevance scoring without explicit contextual constraint which lead to insufficient use of contextual information. In this work, we propose deep CLAS to use contextual information better. We introduce bias loss forcing model to focus on contextual information. The query of bias attention is also enriched to improve the accuracy of the bias attention score. To get fine-grained contextual information, we replace phrase-level encoding with character-level encoding and encode contextual information with conformer rather than LSTM. Moreover, we directly use the bias attention score to correct the output probability distribution of the model. Experiments using the public AISHELL-1 and AISHELL-NER. On AISHELL-1, compared to CLAS baselines, deep CLAS obtains a 65.78% relative recall and a 53.49% relative F1-score increase in the named entity recognition scene.'}",oai:arXiv.org:2409.17603v2,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}, {'term': 'cs.SD', 'scheme': None, 'label': None}, {'term': 'eess.AS', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Mengzhi Wang, Shifu Xiong, Genshun Wan, Hang Chen, Jianqing Gao, Lirong Dai'}]","Mengzhi Wang, Shifu Xiong, Genshun Wan, Hang Chen, Jianqing Gao, Lirong Dai","{'name': 'Mengzhi Wang, Shifu Xiong, Genshun Wan, Hang Chen, Jianqing Gao, Lirong Dai'}",,
628,Leveraging Anthropometric Measurements to Improve Human Mesh Estimation and Ensure Consistent Body Shapes,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Leveraging Anthropometric Measurements to Improve Human Mesh Estimation and Ensure Consistent Body Shapes'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2409.17671'}]",https://arxiv.org/abs/2409.17671,"arXiv:2409.17671v3 Announce Type: replace 
Abstract: The basic body shape (i.e., the body shape in T-pose) of a person does not change within a single video. However, most SOTA human mesh estimation (HME) models output a slightly different, thus inconsistent basic body shape for each video frame. Furthermore, we find that SOTA 3D human pose estimation (HPE) models outperform HME models regarding the precision of the estimated 3D keypoint positions. We solve the problem of inconsistent body shapes by leveraging anthropometric measurements like taken by tailors from humans. We create a model called A2B that converts given anthropometric measurements to basic body shape parameters of human mesh models. We obtain superior and consistent human meshes by combining the A2B model results with the keypoints of 3D HPE models using inverse kinematics. We evaluate our approach on challenging datasets like ASPset or fit3D, where we can lower the MPJPE by over 30 mm compared to SOTA HME models. Further, replacing estimates of the body shape parameters from existing HME models with A2B results not only increases the performance of these HME models, but also guarantees consistent body shapes.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2409.17671v3 Announce Type: replace \nAbstract: The basic body shape (i.e., the body shape in T-pose) of a person does not change within a single video. However, most SOTA human mesh estimation (HME) models output a slightly different, thus inconsistent basic body shape for each video frame. Furthermore, we find that SOTA 3D human pose estimation (HPE) models outperform HME models regarding the precision of the estimated 3D keypoint positions. We solve the problem of inconsistent body shapes by leveraging anthropometric measurements like taken by tailors from humans. We create a model called A2B that converts given anthropometric measurements to basic body shape parameters of human mesh models. We obtain superior and consistent human meshes by combining the A2B model results with the keypoints of 3D HPE models using inverse kinematics. We evaluate our approach on challenging datasets like ASPset or fit3D, where we can lower the MPJPE by over 30 mm compared to SOTA HME models. Further, replacing estimates of the body shape parameters from existing HME models with A2B results not only increases the performance of these HME models, but also guarantees consistent body shapes.'}",oai:arXiv.org:2409.17671v3,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Katja Ludwig, Julian Lorenz, Daniel Kienzle, Tuan Bui, Rainer Lienhart'}]","Katja Ludwig, Julian Lorenz, Daniel Kienzle, Tuan Bui, Rainer Lienhart","{'name': 'Katja Ludwig, Julian Lorenz, Daniel Kienzle, Tuan Bui, Rainer Lienhart'}",,
629,URIEL+: Enhancing Linguistic Inclusion and Usability in a Typological and Multilingual Knowledge Base,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'URIEL+: Enhancing Linguistic Inclusion and Usability in a Typological and Multilingual Knowledge Base'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2409.18472'}]",https://arxiv.org/abs/2409.18472,"arXiv:2409.18472v2 Announce Type: replace 
Abstract: URIEL is a knowledge base offering geographical, phylogenetic, and typological vector representations for 7970 languages. It includes distance measures between these vectors for 4005 languages, which are accessible via the lang2vec tool. Despite being frequently cited, URIEL is limited in terms of linguistic inclusion and overall usability. To tackle these challenges, we introduce URIEL+, an enhanced version of URIEL and lang2vec that addresses these limitations. In addition to expanding typological feature coverage for 2898 languages, URIEL+ improves the user experience with robust, customizable distance calculations to better suit the needs of users. These upgrades also offer competitive performance on downstream tasks and provide distances that better align with linguistic distance studies.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2409.18472v2 Announce Type: replace \nAbstract: URIEL is a knowledge base offering geographical, phylogenetic, and typological vector representations for 7970 languages. It includes distance measures between these vectors for 4005 languages, which are accessible via the lang2vec tool. Despite being frequently cited, URIEL is limited in terms of linguistic inclusion and overall usability. To tackle these challenges, we introduce URIEL+, an enhanced version of URIEL and lang2vec that addresses these limitations. In addition to expanding typological feature coverage for 2898 languages, URIEL+ improves the user experience with robust, customizable distance calculations to better suit the needs of users. These upgrades also offer competitive performance on downstream tasks and provide distances that better align with linguistic distance studies.'}",oai:arXiv.org:2409.18472v2,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Aditya Khan, Mason Shipton, David Anugraha, Kaiyao Duan, Phuong H. Hoang, Eric Khiu, A. Seza Do\\u{g}ru\\""oz, En-Shiun Annie Lee'}]","Aditya Khan, Mason Shipton, David Anugraha, Kaiyao Duan, Phuong H. Hoang, Eric Khiu, A. Seza Do\u{g}ru\""oz, En-Shiun Annie Lee","{'name': 'Aditya Khan, Mason Shipton, David Anugraha, Kaiyao Duan, Phuong H. Hoang, Eric Khiu, A. Seza Do\\u{g}ru\\""oz, En-Shiun Annie Lee'}",,
630,Neural network approaches for variance reduction in fluctuation formulas,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Neural network approaches for variance reduction in fluctuation formulas'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2410.00278'}]",https://arxiv.org/abs/2410.00278,"arXiv:2410.00278v2 Announce Type: replace 
Abstract: We propose a method utilizing physics-informed neural networks (PINNs) to solve Poisson equations that serve as control variates in the computation of transport coefficients via fluctuation formulas, such as the Green--Kubo and generalized Einstein-like formulas. By leveraging approximate solutions to the Poisson equation constructed through neural networks, our approach significantly reduces the variance of the estimator at hand. We provide an extensive numerical analysis of the estimators and detail a methodology for training neural networks to solve these Poisson equations. The approximate solutions are then incorporated into Monte Carlo simulations as effective control variates, demonstrating the suitability of the method for moderately high-dimensional problems where fully deterministic solutions are computationally infeasible.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2410.00278v2 Announce Type: replace \nAbstract: We propose a method utilizing physics-informed neural networks (PINNs) to solve Poisson equations that serve as control variates in the computation of transport coefficients via fluctuation formulas, such as the Green--Kubo and generalized Einstein-like formulas. By leveraging approximate solutions to the Poisson equation constructed through neural networks, our approach significantly reduces the variance of the estimator at hand. We provide an extensive numerical analysis of the estimators and detail a methodology for training neural networks to solve these Poisson equations. The approximate solutions are then incorporated into Monte Carlo simulations as effective control variates, demonstrating the suitability of the method for moderately high-dimensional problems where fully deterministic solutions are computationally infeasible.'}",oai:arXiv.org:2410.00278v2,False,"[{'term': 'math.NA', 'scheme': None, 'label': None}, {'term': 'cs.NA', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Grigorios Pavliotis, Renato Spacek, Gabriel Stoltz, Urbain Vaes'}]","Grigorios Pavliotis, Renato Spacek, Gabriel Stoltz, Urbain Vaes","{'name': 'Grigorios Pavliotis, Renato Spacek, Gabriel Stoltz, Urbain Vaes'}",,
631,Unleashing the Unseen: Harnessing Benign Datasets for Jailbreaking Large Language Models,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Unleashing the Unseen: Harnessing Benign Datasets for Jailbreaking Large Language Models'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2410.00451'}]",https://arxiv.org/abs/2410.00451,"arXiv:2410.00451v3 Announce Type: replace 
Abstract: Despite significant ongoing efforts in safety alignment, large language models (LLMs) such as GPT-4 and LLaMA 3 remain vulnerable to jailbreak attacks that can induce harmful behaviors, including through the use of adversarial suffixes. Building on prior research, we hypothesize that these adversarial suffixes are not mere bugs but may represent features that can dominate the LLM's behavior. To evaluate this hypothesis, we conduct several experiments. First, we demonstrate that benign features can be effectively made to function as adversarial suffixes, i.e., we develop a feature extraction method to extract sample-agnostic features from benign dataset in the form of suffixes and show that these suffixes may effectively compromise safety alignment. Second, we show that adversarial suffixes generated from jailbreak attacks may contain meaningful features, i.e., appending the same suffix to different prompts results in responses exhibiting specific characteristics. Third, we show that such benign-yet-safety-compromising features can be easily introduced through fine-tuning using only benign datasets. As a result, we are able to completely eliminate GPT's safety alignment in a blackbox setting through finetuning with only benign data. Our code and data is available at \url{https://github.com/suffix-maybe-feature/adver-suffix-maybe-features}.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2410.00451v3 Announce Type: replace \nAbstract: Despite significant ongoing efforts in safety alignment, large language models (LLMs) such as GPT-4 and LLaMA 3 remain vulnerable to jailbreak attacks that can induce harmful behaviors, including through the use of adversarial suffixes. Building on prior research, we hypothesize that these adversarial suffixes are not mere bugs but may represent features that can dominate the LLM's behavior. To evaluate this hypothesis, we conduct several experiments. First, we demonstrate that benign features can be effectively made to function as adversarial suffixes, i.e., we develop a feature extraction method to extract sample-agnostic features from benign dataset in the form of suffixes and show that these suffixes may effectively compromise safety alignment. Second, we show that adversarial suffixes generated from jailbreak attacks may contain meaningful features, i.e., appending the same suffix to different prompts results in responses exhibiting specific characteristics. Third, we show that such benign-yet-safety-compromising features can be easily introduced through fine-tuning using only benign datasets. As a result, we are able to completely eliminate GPT's safety alignment in a blackbox setting through finetuning with only benign data. Our code and data is available at \\url{https://github.com/suffix-maybe-feature/adver-suffix-maybe-features}.""}",oai:arXiv.org:2410.00451v3,False,"[{'term': 'cs.CR', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.CL', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Wei Zhao, Zhe Li, Yige Li, Jun Sun'}]","Wei Zhao, Zhe Li, Yige Li, Jun Sun","{'name': 'Wei Zhao, Zhe Li, Yige Li, Jun Sun'}",,
632,Dynamic Planning for LLM-based Graphical User Interface Automation,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Dynamic Planning for LLM-based Graphical User Interface Automation'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2410.00467'}]",https://arxiv.org/abs/2410.00467,"arXiv:2410.00467v3 Announce Type: replace 
Abstract: The advent of large language models (LLMs) has spurred considerable interest in advancing autonomous LLMs-based agents, particularly in intriguing applications within smartphone graphical user interfaces (GUIs). When presented with a task goal, these agents typically emulate human actions within a GUI environment until the task is completed. However, a key challenge lies in devising effective plans to guide action prediction in GUI tasks, though planning have been widely recognized as effective for decomposing complex tasks into a series of steps. Specifically, given the dynamic nature of environmental GUIs following action execution, it is crucial to dynamically adapt plans based on environmental feedback and action history.We show that the widely-used ReAct approach fails due to the excessively long historical dialogues. To address this challenge, we propose a novel approach called Dynamic Planning of Thoughts (D-PoT) for LLM-based GUI agents.D-PoT involves the dynamic adjustment of planning based on the environmental feedback and execution history. Experimental results reveal that the proposed D-PoT significantly surpassed the strong GPT-4V baseline by +12.7% (34.66% $\rightarrow$ 47.36%) in accuracy. The analysis highlights the generality of dynamic planning in different backbone LLMs, as well as the benefits in mitigating hallucinations and adapting to unseen tasks. Code is available at https://github.com/sqzhang-lazy/D-PoT.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2410.00467v3 Announce Type: replace \nAbstract: The advent of large language models (LLMs) has spurred considerable interest in advancing autonomous LLMs-based agents, particularly in intriguing applications within smartphone graphical user interfaces (GUIs). When presented with a task goal, these agents typically emulate human actions within a GUI environment until the task is completed. However, a key challenge lies in devising effective plans to guide action prediction in GUI tasks, though planning have been widely recognized as effective for decomposing complex tasks into a series of steps. Specifically, given the dynamic nature of environmental GUIs following action execution, it is crucial to dynamically adapt plans based on environmental feedback and action history.We show that the widely-used ReAct approach fails due to the excessively long historical dialogues. To address this challenge, we propose a novel approach called Dynamic Planning of Thoughts (D-PoT) for LLM-based GUI agents.D-PoT involves the dynamic adjustment of planning based on the environmental feedback and execution history. Experimental results reveal that the proposed D-PoT significantly surpassed the strong GPT-4V baseline by +12.7% (34.66% $\\rightarrow$ 47.36%) in accuracy. The analysis highlights the generality of dynamic planning in different backbone LLMs, as well as the benefits in mitigating hallucinations and adapting to unseen tasks. Code is available at https://github.com/sqzhang-lazy/D-PoT.'}",oai:arXiv.org:2410.00467v3,False,"[{'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.HC', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by-nc-sa/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-sa/4.0/'}","[{'name': 'Shaoqing Zhang, Zhuosheng Zhang, Kehai Chen, Xinbei Ma, Muyun Yang, Tiejun Zhao, Min Zhang'}]","Shaoqing Zhang, Zhuosheng Zhang, Kehai Chen, Xinbei Ma, Muyun Yang, Tiejun Zhao, Min Zhang","{'name': 'Shaoqing Zhang, Zhuosheng Zhang, Kehai Chen, Xinbei Ma, Muyun Yang, Tiejun Zhao, Min Zhang'}",,
633,LLaVA Needs More Knowledge: Retrieval Augmented Natural Language Generation with Knowledge Graph for Explaining Thoracic Pathologies,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'LLaVA Needs More Knowledge: Retrieval Augmented Natural Language Generation with Knowledge Graph for Explaining Thoracic Pathologies'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2410.04749'}]",https://arxiv.org/abs/2410.04749,"arXiv:2410.04749v2 Announce Type: replace 
Abstract: Generating Natural Language Explanations (NLEs) for model predictions on medical images, particularly those depicting thoracic pathologies, remains a critical and challenging task. Existing methodologies often struggle due to general models' insufficient domain-specific medical knowledge and privacy concerns associated with retrieval-based augmentation techniques. To address these issues, we propose a novel Vision-Language framework augmented with a Knowledge Graph (KG)-based datastore, which enhances the model's understanding by incorporating additional domain-specific medical knowledge essential for generating accurate and informative NLEs. Our framework employs a KG-based retrieval mechanism that not only improves the precision of the generated explanations but also preserves data privacy by avoiding direct data retrieval. The KG datastore is designed as a plug-and-play module, allowing for seamless integration with various model architectures. We introduce and evaluate three distinct frameworks within this paradigm: KG-LLaVA, which integrates the pre-trained LLaVA model with KG-RAG; Med-XPT, a custom framework combining MedCLIP, a transformer-based projector, and GPT-2; and Bio-LLaVA, which adapts LLaVA by incorporating the Bio-ViT-L vision model. These frameworks are validated on the MIMIC-NLE dataset, where they achieve state-of-the-art results, underscoring the effectiveness of KG augmentation in generating high-quality NLEs for thoracic pathologies.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2410.04749v2 Announce Type: replace \nAbstract: Generating Natural Language Explanations (NLEs) for model predictions on medical images, particularly those depicting thoracic pathologies, remains a critical and challenging task. Existing methodologies often struggle due to general models' insufficient domain-specific medical knowledge and privacy concerns associated with retrieval-based augmentation techniques. To address these issues, we propose a novel Vision-Language framework augmented with a Knowledge Graph (KG)-based datastore, which enhances the model's understanding by incorporating additional domain-specific medical knowledge essential for generating accurate and informative NLEs. Our framework employs a KG-based retrieval mechanism that not only improves the precision of the generated explanations but also preserves data privacy by avoiding direct data retrieval. The KG datastore is designed as a plug-and-play module, allowing for seamless integration with various model architectures. We introduce and evaluate three distinct frameworks within this paradigm: KG-LLaVA, which integrates the pre-trained LLaVA model with KG-RAG; Med-XPT, a custom framework combining MedCLIP, a transformer-based projector, and GPT-2; and Bio-LLaVA, which adapts LLaVA by incorporating the Bio-ViT-L vision model. These frameworks are validated on the MIMIC-NLE dataset, where they achieve state-of-the-art results, underscoring the effectiveness of KG augmentation in generating high-quality NLEs for thoracic pathologies.""}",oai:arXiv.org:2410.04749v2,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by-nc-nd/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-nd/4.0/'}","[{'name': 'Ameer Hamza,  Abdullah, Yong Hyun Ahn, Sungyoung Lee, Seong Tae Kim'}]","Ameer Hamza,  Abdullah, Yong Hyun Ahn, Sungyoung Lee, Seong Tae Kim","{'name': 'Ameer Hamza,  Abdullah, Yong Hyun Ahn, Sungyoung Lee, Seong Tae Kim'}",,
634,T-JEPA: Augmentation-Free Self-Supervised Learning for Tabular Data,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'T-JEPA: Augmentation-Free Self-Supervised Learning for Tabular Data'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2410.05016'}]",https://arxiv.org/abs/2410.05016,"arXiv:2410.05016v2 Announce Type: replace 
Abstract: Self-supervision is often used for pre-training to foster performance on a downstream task by constructing meaningful representations of samples. Self-supervised learning (SSL) generally involves generating different views of the same sample and thus requires data augmentations that are challenging to construct for tabular data. This constitutes one of the main challenges of self-supervision for structured data. In the present work, we propose a novel augmentation-free SSL method for tabular data. Our approach, T-JEPA, relies on a Joint Embedding Predictive Architecture (JEPA) and is akin to mask reconstruction in the latent space. It involves predicting the latent representation of one subset of features from the latent representation of a different subset within the same sample, thereby learning rich representations without augmentations. We use our method as a pre-training technique and train several deep classifiers on the obtained representation. Our experimental results demonstrate a substantial improvement in both classification and regression tasks, outperforming models trained directly on samples in their original data space. Moreover, T-JEPA enables some methods to consistently outperform or match the performance of traditional methods likes Gradient Boosted Decision Trees. To understand why, we extensively characterize the obtained representations and show that T-JEPA effectively identifies relevant features for downstream tasks without access to the labels. Additionally, we introduce regularization tokens, a novel regularization method critical for training of JEPA-based models on structured data.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2410.05016v2 Announce Type: replace \nAbstract: Self-supervision is often used for pre-training to foster performance on a downstream task by constructing meaningful representations of samples. Self-supervised learning (SSL) generally involves generating different views of the same sample and thus requires data augmentations that are challenging to construct for tabular data. This constitutes one of the main challenges of self-supervision for structured data. In the present work, we propose a novel augmentation-free SSL method for tabular data. Our approach, T-JEPA, relies on a Joint Embedding Predictive Architecture (JEPA) and is akin to mask reconstruction in the latent space. It involves predicting the latent representation of one subset of features from the latent representation of a different subset within the same sample, thereby learning rich representations without augmentations. We use our method as a pre-training technique and train several deep classifiers on the obtained representation. Our experimental results demonstrate a substantial improvement in both classification and regression tasks, outperforming models trained directly on samples in their original data space. Moreover, T-JEPA enables some methods to consistently outperform or match the performance of traditional methods likes Gradient Boosted Decision Trees. To understand why, we extensively characterize the obtained representations and show that T-JEPA effectively identifies relevant features for downstream tasks without access to the labels. Additionally, we introduce regularization tokens, a novel regularization method critical for training of JEPA-based models on structured data.'}",oai:arXiv.org:2410.05016v2,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'stat.ML', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': ""Hugo Thimonier, Jos\\'e Lucas De Melo Costa, Fabrice Popineau, Arpad Rimmel, Bich-Li\\^en Doan""}]","Hugo Thimonier, Jos\'e Lucas De Melo Costa, Fabrice Popineau, Arpad Rimmel, Bich-Li\^en Doan","{'name': ""Hugo Thimonier, Jos\\'e Lucas De Melo Costa, Fabrice Popineau, Arpad Rimmel, Bich-Li\\^en Doan""}",,
635,Accelerating Diffusion Transformers with Token-wise Feature Caching,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Accelerating Diffusion Transformers with Token-wise Feature Caching'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2410.05317'}]",https://arxiv.org/abs/2410.05317,"arXiv:2410.05317v3 Announce Type: replace 
Abstract: Diffusion transformers have shown significant effectiveness in both image and video synthesis at the expense of huge computation costs. To address this problem, feature caching methods have been introduced to accelerate diffusion transformers by caching the features in previous timesteps and reusing them in the following timesteps. However, previous caching methods ignore that different tokens exhibit different sensitivities to feature caching, and feature caching on some tokens may lead to 10$\times$ more destruction to the overall generation quality compared with other tokens. In this paper, we introduce token-wise feature caching, allowing us to adaptively select the most suitable tokens for caching, and further enable us to apply different caching ratios to neural layers in different types and depths. Extensive experiments on PixArt-$\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image and video generation with no requirements for training. For instance, 2.36$\times$ and 1.93$\times$ acceleration are achieved on OpenSora and PixArt-$\alpha$ with almost no drop in generation quality.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2410.05317v3 Announce Type: replace \nAbstract: Diffusion transformers have shown significant effectiveness in both image and video synthesis at the expense of huge computation costs. To address this problem, feature caching methods have been introduced to accelerate diffusion transformers by caching the features in previous timesteps and reusing them in the following timesteps. However, previous caching methods ignore that different tokens exhibit different sensitivities to feature caching, and feature caching on some tokens may lead to 10$\\times$ more destruction to the overall generation quality compared with other tokens. In this paper, we introduce token-wise feature caching, allowing us to adaptively select the most suitable tokens for caching, and further enable us to apply different caching ratios to neural layers in different types and depths. Extensive experiments on PixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image and video generation with no requirements for training. For instance, 2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and PixArt-$\\alpha$ with almost no drop in generation quality.'}",oai:arXiv.org:2410.05317v3,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Chang Zou, Xuyang Liu, Ting Liu, Siteng Huang, Linfeng Zhang'}]","Chang Zou, Xuyang Liu, Ting Liu, Siteng Huang, Linfeng Zhang","{'name': 'Chang Zou, Xuyang Liu, Ting Liu, Siteng Huang, Linfeng Zhang'}",,
636,Human and LLM Biases in Hate Speech Annotations: A Socio-Demographic Analysis of Annotators and Targets,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Human and LLM Biases in Hate Speech Annotations: A Socio-Demographic Analysis of Annotators and Targets'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2410.07991'}]",https://arxiv.org/abs/2410.07991,"arXiv:2410.07991v4 Announce Type: replace 
Abstract: The rise of online platforms exacerbated the spread of hate speech, demanding scalable and effective detection. However, the accuracy of hate speech detection systems heavily relies on human-labeled data, which is inherently susceptible to biases. While previous work has examined the issue, the interplay between the characteristics of the annotator and those of the target of the hate are still unexplored. We fill this gap by leveraging an extensive dataset with rich socio-demographic information of both annotators and targets, uncovering how human biases manifest in relation to the target's attributes. Our analysis surfaces the presence of widespread biases, which we quantitatively describe and characterize based on their intensity and prevalence, revealing marked differences. Furthermore, we compare human biases with those exhibited by persona-based LLMs. Our findings indicate that while persona-based LLMs do exhibit biases, these differ significantly from those of human annotators. Overall, our work offers new and nuanced results on human biases in hate speech annotations, as well as fresh insights into the design of AI-driven hate speech detection systems.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2410.07991v4 Announce Type: replace \nAbstract: The rise of online platforms exacerbated the spread of hate speech, demanding scalable and effective detection. However, the accuracy of hate speech detection systems heavily relies on human-labeled data, which is inherently susceptible to biases. While previous work has examined the issue, the interplay between the characteristics of the annotator and those of the target of the hate are still unexplored. We fill this gap by leveraging an extensive dataset with rich socio-demographic information of both annotators and targets, uncovering how human biases manifest in relation to the target's attributes. Our analysis surfaces the presence of widespread biases, which we quantitatively describe and characterize based on their intensity and prevalence, revealing marked differences. Furthermore, we compare human biases with those exhibited by persona-based LLMs. Our findings indicate that while persona-based LLMs do exhibit biases, these differ significantly from those of human annotators. Overall, our work offers new and nuanced results on human biases in hate speech annotations, as well as fresh insights into the design of AI-driven hate speech detection systems.""}",oai:arXiv.org:2410.07991v4,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.HC', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Tommaso Giorgi, Lorenzo Cima, Tiziano Fagni, Marco Avvenuti, Stefano Cresci'}]","Tommaso Giorgi, Lorenzo Cima, Tiziano Fagni, Marco Avvenuti, Stefano Cresci","{'name': 'Tommaso Giorgi, Lorenzo Cima, Tiziano Fagni, Marco Avvenuti, Stefano Cresci'}",,
637,"Design of Secure, Privacy-focused, and Accessible E-Payment Applications for Older Adults","{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Design of Secure, Privacy-focused, and Accessible E-Payment Applications for Older Adults'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2410.08555'}]",https://arxiv.org/abs/2410.08555,"arXiv:2410.08555v2 Announce Type: replace 
Abstract: E-payments are essential for transactional convenience in today's digital economy and are becoming increasingly important for older adults, emphasizing the need for enhanced security, privacy, and usability. To address this, we conducted a survey-based study with 400 older adults aged 60 and above to evaluate a high-fidelity prototype of an e-payment mobile application, which included features such as multi-factor authentication (MFA) and QR code-based recipient addition. Based on our findings, we developed a tailored \b{eta} version of the application to meet the specific needs of this demographic. Notably, approximately 91% of participants preferred traditional knowledge-based and single-mode authentication compared to expert-recommended MFA. We concluded by providing recommendations aimed at developing inclusive e-payment solutions that address the security, privacy, and usability requirements of older adults.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2410.08555v2 Announce Type: replace \nAbstract: E-payments are essential for transactional convenience in today's digital economy and are becoming increasingly important for older adults, emphasizing the need for enhanced security, privacy, and usability. To address this, we conducted a survey-based study with 400 older adults aged 60 and above to evaluate a high-fidelity prototype of an e-payment mobile application, which included features such as multi-factor authentication (MFA) and QR code-based recipient addition. Based on our findings, we developed a tailored \\b{eta} version of the application to meet the specific needs of this demographic. Notably, approximately 91% of participants preferred traditional knowledge-based and single-mode authentication compared to expert-recommended MFA. We concluded by providing recommendations aimed at developing inclusive e-payment solutions that address the security, privacy, and usability requirements of older adults.""}",oai:arXiv.org:2410.08555v2,False,"[{'term': 'cs.CY', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/publicdomain/zero/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/publicdomain/zero/1.0/'}",[{'name': 'Sanchari Das'}],Sanchari Das,{'name': 'Sanchari Das'},,"BuildSEC'24 Building a Secure & Empowered Cyberspace; 19-21 December 2024, New Delhi, India"
638,POPoS: Improving Efficient and Robust Facial Landmark Detection with Parallel Optimal Position Search,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'POPoS: Improving Efficient and Robust Facial Landmark Detection with Parallel Optimal Position Search'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2410.09583'}]",https://arxiv.org/abs/2410.09583,"arXiv:2410.09583v4 Announce Type: replace 
Abstract: Achieving a balance between accuracy and efficiency is a critical challenge in facial landmark detection (FLD). This paper introduces Parallel Optimal Position Search (POPoS), a high-precision encoding-decoding framework designed to address the limitations of traditional FLD methods. POPoS employs three key contributions: (1) Pseudo-range multilateration is utilized to correct heatmap errors, improving landmark localization accuracy. By integrating multiple anchor points, it reduces the impact of individual heatmap inaccuracies, leading to robust overall positioning. (2) To enhance the pseudo-range accuracy of selected anchor points, a new loss function, named multilateration anchor loss, is proposed. This loss function enhances the accuracy of the distance map, mitigates the risk of local optima, and ensures optimal solutions. (3) A single-step parallel computation algorithm is introduced, boosting computational efficiency and reducing processing time. Extensive evaluations across five benchmark datasets demonstrate that POPoS consistently outperforms existing methods, particularly excelling in low-resolution heatmaps scenarios with minimal computational overhead. These advantages make POPoS a highly efficient and accurate tool for FLD, with broad applicability in real-world scenarios.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2410.09583v4 Announce Type: replace \nAbstract: Achieving a balance between accuracy and efficiency is a critical challenge in facial landmark detection (FLD). This paper introduces Parallel Optimal Position Search (POPoS), a high-precision encoding-decoding framework designed to address the limitations of traditional FLD methods. POPoS employs three key contributions: (1) Pseudo-range multilateration is utilized to correct heatmap errors, improving landmark localization accuracy. By integrating multiple anchor points, it reduces the impact of individual heatmap inaccuracies, leading to robust overall positioning. (2) To enhance the pseudo-range accuracy of selected anchor points, a new loss function, named multilateration anchor loss, is proposed. This loss function enhances the accuracy of the distance map, mitigates the risk of local optima, and ensures optimal solutions. (3) A single-step parallel computation algorithm is introduced, boosting computational efficiency and reducing processing time. Extensive evaluations across five benchmark datasets demonstrate that POPoS consistently outperforms existing methods, particularly excelling in low-resolution heatmaps scenarios with minimal computational overhead. These advantages make POPoS a highly efficient and accurate tool for FLD, with broad applicability in real-world scenarios.'}",oai:arXiv.org:2410.09583v4,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Chong-Yang Xiang, Jun-Yan He, Zhi-Qi Cheng, Xiao Wu, Xian-Sheng Hua'}]","Chong-Yang Xiang, Jun-Yan He, Zhi-Qi Cheng, Xiao Wu, Xian-Sheng Hua","{'name': 'Chong-Yang Xiang, Jun-Yan He, Zhi-Qi Cheng, Xiao Wu, Xian-Sheng Hua'}",,
639,Audio Captioning RAG via Generative Pair-to-Pair Retrieval with Refined Knowledge Base,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Audio Captioning RAG via Generative Pair-to-Pair Retrieval with Refined Knowledge Base'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2410.10913'}]",https://arxiv.org/abs/2410.10913,"arXiv:2410.10913v2 Announce Type: replace 
Abstract: Recent advances in audio understanding tasks leverage the reasoning capabilities of LLMs. However, adapting LLMs to learn audio concepts requires massive training data and substantial computational resources. To address these challenges, Retrieval-Augmented Generation (RAG) retrieves audio-text pairs from a knowledge base (KB) and augments them with query audio to generate accurate textual responses. In RAG, the relevance of the retrieved information plays a crucial role in effectively processing the input. In this paper, we analyze how different retrieval methods and knowledge bases impact the relevance of audio-text pairs and the performance of audio captioning with RAG. We propose generative pair-to-pair retrieval, which uses the generated caption as a text query to accurately find relevant audio-text pairs to the query audio, thereby improving the relevance and accuracy of retrieved information. Additionally, we refine the large-scale knowledge base to retain only audio-text pairs that align with the contextualized intents. Our approach achieves state-of-the-art results on benchmarks including AudioCaps, Clotho, and Auto-ACD, with detailed ablation studies validating the effectiveness of our retrieval and KB construction methods.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2410.10913v2 Announce Type: replace \nAbstract: Recent advances in audio understanding tasks leverage the reasoning capabilities of LLMs. However, adapting LLMs to learn audio concepts requires massive training data and substantial computational resources. To address these challenges, Retrieval-Augmented Generation (RAG) retrieves audio-text pairs from a knowledge base (KB) and augments them with query audio to generate accurate textual responses. In RAG, the relevance of the retrieved information plays a crucial role in effectively processing the input. In this paper, we analyze how different retrieval methods and knowledge bases impact the relevance of audio-text pairs and the performance of audio captioning with RAG. We propose generative pair-to-pair retrieval, which uses the generated caption as a text query to accurately find relevant audio-text pairs to the query audio, thereby improving the relevance and accuracy of retrieved information. Additionally, we refine the large-scale knowledge base to retain only audio-text pairs that align with the contextualized intents. Our approach achieves state-of-the-art results on benchmarks including AudioCaps, Clotho, and Auto-ACD, with detailed ablation studies validating the effectiveness of our retrieval and KB construction methods.'}",oai:arXiv.org:2410.10913v2,False,"[{'term': 'cs.SD', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'eess.AS', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by-nc-sa/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-sa/4.0/'}","[{'name': 'Choi Changin, Lim Sungjun, Rhee Wonjong'}]","Choi Changin, Lim Sungjun, Rhee Wonjong","{'name': 'Choi Changin, Lim Sungjun, Rhee Wonjong'}",,
640,3D Gaussian Splatting in Robotics: A Survey,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': '3D Gaussian Splatting in Robotics: A Survey'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2410.12262'}]",https://arxiv.org/abs/2410.12262,"arXiv:2410.12262v2 Announce Type: replace 
Abstract: Dense 3D representations of the environment have been a long-term goal in the robotics field. While previous Neural Radiance Fields (NeRF) representation have been prevalent for its implicit, coordinate-based model, the recent emergence of 3D Gaussian Splatting (3DGS) has demonstrated remarkable potential in its explicit radiance field representation. By leveraging 3D Gaussian primitives for explicit scene representation and enabling differentiable rendering, 3DGS has shown significant advantages over other radiance fields in real-time rendering and photo-realistic performance, which is beneficial for robotic applications. In this survey, we provide a comprehensive understanding of 3DGS in the field of robotics. We divide our discussion of the related works into two main categories: the application of 3DGS and the advancements in 3DGS techniques. In the application section, we explore how 3DGS has been utilized in various robotics tasks from scene understanding and interaction perspectives. The advance of 3DGS section focuses on the improvements of 3DGS own properties in its adaptability and efficiency, aiming to enhance its performance in robotics. We then summarize the most commonly used datasets and evaluation metrics in robotics. Finally, we identify the challenges and limitations of current 3DGS methods and discuss the future development of 3DGS in robotics.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2410.12262v2 Announce Type: replace \nAbstract: Dense 3D representations of the environment have been a long-term goal in the robotics field. While previous Neural Radiance Fields (NeRF) representation have been prevalent for its implicit, coordinate-based model, the recent emergence of 3D Gaussian Splatting (3DGS) has demonstrated remarkable potential in its explicit radiance field representation. By leveraging 3D Gaussian primitives for explicit scene representation and enabling differentiable rendering, 3DGS has shown significant advantages over other radiance fields in real-time rendering and photo-realistic performance, which is beneficial for robotic applications. In this survey, we provide a comprehensive understanding of 3DGS in the field of robotics. We divide our discussion of the related works into two main categories: the application of 3DGS and the advancements in 3DGS techniques. In the application section, we explore how 3DGS has been utilized in various robotics tasks from scene understanding and interaction perspectives. The advance of 3DGS section focuses on the improvements of 3DGS own properties in its adaptability and efficiency, aiming to enhance its performance in robotics. We then summarize the most commonly used datasets and evaluation metrics in robotics. Finally, we identify the challenges and limitations of current 3DGS methods and discuss the future development of 3DGS in robotics.'}",oai:arXiv.org:2410.12262v2,False,"[{'term': 'cs.RO', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Siting Zhu, Guangming Wang, Xin Kong, Dezhi Kong, Hesheng Wang'}]","Siting Zhu, Guangming Wang, Xin Kong, Dezhi Kong, Hesheng Wang","{'name': 'Siting Zhu, Guangming Wang, Xin Kong, Dezhi Kong, Hesheng Wang'}",,
641,Beamforming Optimization for Continuous Aperture Array (CAPA)-based Communications,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Beamforming Optimization for Continuous Aperture Array (CAPA)-based Communications'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2410.13677'}]",https://arxiv.org/abs/2410.13677,"arXiv:2410.13677v2 Announce Type: replace 
Abstract: The beamforming optimization in continuous aperture array (CAPA)-based multi-user communications is studied. In contrast to conventional spatially discrete antenna arrays, CAPAs can exploit the full spatial degrees of freedom (DoFs) by emitting information-bearing electromagnetic (EM) waves through continuous source current distributed across the aperture. Nevertheless, such an operation renders the beamforming optimization problem as a non-convex integral-based functional programming problem, which is challenging for conventional discrete optimization methods. A couple of low-complexity approaches are proposed to solve the functional programming problem. 1) Calculus of variations (CoV)-based approach: Closed-form structure of the optimal continuous source patterns are derived based on CoV, inspiring a low-complexity integral-free iterative algorithm for solving the functional programming problem. 2) Correlation-based zero-forcing (Corr-ZF) approach: Closed-form ZF source current patterns that completely eliminate the inter-user interference are derived based on the channel correlations. By using these patterns, the original functional programming problem is transformed to a simple power allocation problem, which can be solved using the classical water-filling approach with reduced complexity. Our numerical results validate the effectiveness of the proposed designs and reveal that: i) compared to the state-of-the-art Fourier-based discretization approach, the proposed CoV-based approach not only improves communication performance but also reduces computational complexity by up to hundreds of times for large CAPA apertures and high frequencies, and ii) the proposed Corr-ZF approach achieves asymptotically optimal performance compared to the CoV-based approach.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2410.13677v2 Announce Type: replace \nAbstract: The beamforming optimization in continuous aperture array (CAPA)-based multi-user communications is studied. In contrast to conventional spatially discrete antenna arrays, CAPAs can exploit the full spatial degrees of freedom (DoFs) by emitting information-bearing electromagnetic (EM) waves through continuous source current distributed across the aperture. Nevertheless, such an operation renders the beamforming optimization problem as a non-convex integral-based functional programming problem, which is challenging for conventional discrete optimization methods. A couple of low-complexity approaches are proposed to solve the functional programming problem. 1) Calculus of variations (CoV)-based approach: Closed-form structure of the optimal continuous source patterns are derived based on CoV, inspiring a low-complexity integral-free iterative algorithm for solving the functional programming problem. 2) Correlation-based zero-forcing (Corr-ZF) approach: Closed-form ZF source current patterns that completely eliminate the inter-user interference are derived based on the channel correlations. By using these patterns, the original functional programming problem is transformed to a simple power allocation problem, which can be solved using the classical water-filling approach with reduced complexity. Our numerical results validate the effectiveness of the proposed designs and reveal that: i) compared to the state-of-the-art Fourier-based discretization approach, the proposed CoV-based approach not only improves communication performance but also reduces computational complexity by up to hundreds of times for large CAPA apertures and high frequencies, and ii) the proposed Corr-ZF approach achieves asymptotically optimal performance compared to the CoV-based approach.'}",oai:arXiv.org:2410.13677v2,False,"[{'term': 'cs.IT', 'scheme': None, 'label': None}, {'term': 'eess.SP', 'scheme': None, 'label': None}, {'term': 'math.IT', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Zhaolin Wang, Chongjun Ouyang, Yuanwei Liu'}]","Zhaolin Wang, Chongjun Ouyang, Yuanwei Liu","{'name': 'Zhaolin Wang, Chongjun Ouyang, Yuanwei Liu'}",,
642,ETF: An Entity Tracing Framework for Hallucination Detection in Code Summaries,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'ETF: An Entity Tracing Framework for Hallucination Detection in Code Summaries'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2410.14748'}]",https://arxiv.org/abs/2410.14748,"arXiv:2410.14748v3 Announce Type: replace 
Abstract: Recent advancements in large language models (LLMs) have significantly enhanced their ability to understand both natural language and code, driving their use in tasks like natural language-to-code (NL2Code) and code summarization. However, LLMs are prone to hallucination-outputs that stray from intended meanings. Detecting hallucinations in code summarization is especially difficult due to the complex interplay between programming and natural languages. We introduce a first-of-its-kind dataset with $\sim$10K samples, curated specifically for hallucination detection in code summarization. We further propose a novel Entity Tracing Framework (ETF) that a) utilizes static program analysis to identify code entities from the program and b) uses LLMs to map and verify these entities and their intents within generated code summaries. Our experimental analysis demonstrates the effectiveness of the framework, leading to a 0.73 F1 score. This approach provides an interpretable method for detecting hallucinations by grounding entities, allowing us to evaluate summary accuracy.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2410.14748v3 Announce Type: replace \nAbstract: Recent advancements in large language models (LLMs) have significantly enhanced their ability to understand both natural language and code, driving their use in tasks like natural language-to-code (NL2Code) and code summarization. However, LLMs are prone to hallucination-outputs that stray from intended meanings. Detecting hallucinations in code summarization is especially difficult due to the complex interplay between programming and natural languages. We introduce a first-of-its-kind dataset with $\\sim$10K samples, curated specifically for hallucination detection in code summarization. We further propose a novel Entity Tracing Framework (ETF) that a) utilizes static program analysis to identify code entities from the program and b) uses LLMs to map and verify these entities and their intents within generated code summaries. Our experimental analysis demonstrates the effectiveness of the framework, leading to a 0.73 F1 score. This approach provides an interpretable method for detecting hallucinations by grounding entities, allowing us to evaluate summary accuracy.'}",oai:arXiv.org:2410.14748v3,False,"[{'term': 'cs.SE', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.CL', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Kishan Maharaj, Vitobha Munigala, Srikanth G. Tamilselvam, Prince Kumar, Sayandeep Sen, Palani Kodeswaran, Abhijit Mishra, Pushpak Bhattacharyya'}]","Kishan Maharaj, Vitobha Munigala, Srikanth G. Tamilselvam, Prince Kumar, Sayandeep Sen, Palani Kodeswaran, Abhijit Mishra, Pushpak Bhattacharyya","{'name': 'Kishan Maharaj, Vitobha Munigala, Srikanth G. Tamilselvam, Prince Kumar, Sayandeep Sen, Palani Kodeswaran, Abhijit Mishra, Pushpak Bhattacharyya'}",,
643,Activity Recognition on Avatar-Anonymized Datasets with Masked Differential Privacy,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Activity Recognition on Avatar-Anonymized Datasets with Masked Differential Privacy'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2410.17098'}]",https://arxiv.org/abs/2410.17098,"arXiv:2410.17098v2 Announce Type: replace 
Abstract: Privacy-preserving computer vision is an important emerging problem in machine learning and artificial intelligence. Prevalent methods tackling this problem use differential privacy (DP) or obfuscation techniques to protect the privacy of individuals. In both cases, the utility of the trained model is sacrificed heavily in this process. In this work, we present an anonymization pipeline that replaces sensitive human subjects in video datasets with synthetic avatars within context, employing a combined rendering and stable diffusion-based strategy. Additionally we propose masked differential privacy ({MaskDP}) to protect non-anonymized but privacy sensitive background information. MaskDP allows for controlling sensitive regions where differential privacy is applied, in contrast to applying DP on the entire input. This combined methodology provides strong privacy protection while minimizing the usual performance penalty of privacy preserving methods. Experiments on multiple challenging action recognition datasets demonstrate that our proposed techniques result in better utility-privacy trade-offs compared to standard differentially private training in the especially demanding $\epsilon<1$ regime.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2410.17098v2 Announce Type: replace \nAbstract: Privacy-preserving computer vision is an important emerging problem in machine learning and artificial intelligence. Prevalent methods tackling this problem use differential privacy (DP) or obfuscation techniques to protect the privacy of individuals. In both cases, the utility of the trained model is sacrificed heavily in this process. In this work, we present an anonymization pipeline that replaces sensitive human subjects in video datasets with synthetic avatars within context, employing a combined rendering and stable diffusion-based strategy. Additionally we propose masked differential privacy ({MaskDP}) to protect non-anonymized but privacy sensitive background information. MaskDP allows for controlling sensitive regions where differential privacy is applied, in contrast to applying DP on the entire input. This combined methodology provides strong privacy protection while minimizing the usual performance penalty of privacy preserving methods. Experiments on multiple challenging action recognition datasets demonstrate that our proposed techniques result in better utility-privacy trade-offs compared to standard differentially private training in the especially demanding $\\epsilon<1$ regime.'}",oai:arXiv.org:2410.17098v2,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by-sa/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-sa/4.0/'}","[{'name': 'David Schneider, Sina Sajadmanesh, Vikash Sehwag, Saquib Sarfraz, Rainer Stiefelhagen, Lingjuan Lyu, Vivek Sharma'}]","David Schneider, Sina Sajadmanesh, Vikash Sehwag, Saquib Sarfraz, Rainer Stiefelhagen, Lingjuan Lyu, Vivek Sharma","{'name': 'David Schneider, Sina Sajadmanesh, Vikash Sehwag, Saquib Sarfraz, Rainer Stiefelhagen, Lingjuan Lyu, Vivek Sharma'}",,
644,HardRace: A Dynamic Data Race Monitor for Production Use,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'HardRace: A Dynamic Data Race Monitor for Production Use'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2410.18412'}]",https://arxiv.org/abs/2410.18412,"arXiv:2410.18412v2 Announce Type: replace 
Abstract: Data races are critical issues in multithreaded program, leading to unpredictable, catastrophic and difficult-to-diagnose problems. Despite the extensive in-house testing, data races often escape to deployed software and manifest in production runs. Existing approaches suffer from either prohibitively high runtime overhead or incomplete detection capability. In this paper, we introduce HardRace, a data race monitor to detect races on-the-fly while with sufficiently low runtime overhead and high detection capability. HardRace firstly employs sound static analysis to determine a minimal set of essential memory accesses relevant to data races. It then leverages hardware trace instruction, i.e., Intel PTWRITE, to selectively record only these memory accesses and thread synchronization events during execution with negligible runtime overhead. Given the tracing data, HardRace performs standard data race detection algorithms to timely report potential races occurred in production runs. The experimental evaluations show that HardRace outperforms state-of-the-art tools like ProRace and Kard in terms of both runtime overhead and detection capability -- HardRace can detect all kinds of data races in read-world applications while maintaining a negligible overhead, less than 2% on average.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2410.18412v2 Announce Type: replace \nAbstract: Data races are critical issues in multithreaded program, leading to unpredictable, catastrophic and difficult-to-diagnose problems. Despite the extensive in-house testing, data races often escape to deployed software and manifest in production runs. Existing approaches suffer from either prohibitively high runtime overhead or incomplete detection capability. In this paper, we introduce HardRace, a data race monitor to detect races on-the-fly while with sufficiently low runtime overhead and high detection capability. HardRace firstly employs sound static analysis to determine a minimal set of essential memory accesses relevant to data races. It then leverages hardware trace instruction, i.e., Intel PTWRITE, to selectively record only these memory accesses and thread synchronization events during execution with negligible runtime overhead. Given the tracing data, HardRace performs standard data race detection algorithms to timely report potential races occurred in production runs. The experimental evaluations show that HardRace outperforms state-of-the-art tools like ProRace and Kard in terms of both runtime overhead and detection capability -- HardRace can detect all kinds of data races in read-world applications while maintaining a negligible overhead, less than 2% on average.'}",oai:arXiv.org:2410.18412v2,False,"[{'term': 'cs.SE', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Xudong Sun, Zhuo Chen, Jingyang Shi, Yiyu Zhang, Peng Di, Jianhua Zhao, Xuandong Li, Zhiqiang Zuo'}]","Xudong Sun, Zhuo Chen, Jingyang Shi, Yiyu Zhang, Peng Di, Jianhua Zhao, Xuandong Li, Zhiqiang Zuo","{'name': 'Xudong Sun, Zhuo Chen, Jingyang Shi, Yiyu Zhang, Peng Di, Jianhua Zhao, Xuandong Li, Zhiqiang Zuo'}",,
645,Accelerating AI Performance using Anderson Extrapolation on GPUs,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Accelerating AI Performance using Anderson Extrapolation on GPUs'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2410.19460'}]",https://arxiv.org/abs/2410.19460,"arXiv:2410.19460v2 Announce Type: replace 
Abstract: We present a novel approach for accelerating AI performance by leveraging Anderson extrapolation, a vector-to-vector mapping technique based on a window of historical iterations. By identifying the crossover point (Fig. 1) where a mixing penalty is incurred, the method focuses on reducing iterations to convergence, with fewer more compute-intensive but generally cacheable iterations, balancing speed and memory usage with accuracy and algorithmic stability, respectively. We demonstrate significant improvements, in both training and inference, motivated by scalability and efficiency extensions to the realm of high-performance computing (HPC).","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2410.19460v2 Announce Type: replace \nAbstract: We present a novel approach for accelerating AI performance by leveraging Anderson extrapolation, a vector-to-vector mapping technique based on a window of historical iterations. By identifying the crossover point (Fig. 1) where a mixing penalty is incurred, the method focuses on reducing iterations to convergence, with fewer more compute-intensive but generally cacheable iterations, balancing speed and memory usage with accuracy and algorithmic stability, respectively. We demonstrate significant improvements, in both training and inference, motivated by scalability and efficiency extensions to the realm of high-performance computing (HPC).'}",oai:arXiv.org:2410.19460v2,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.NA', 'scheme': None, 'label': None}, {'term': 'cs.PF', 'scheme': None, 'label': None}, {'term': 'math.NA', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Saleem Abdul Fattah Ahmed Al Dajani, David E. Keyes'}]","Saleem Abdul Fattah Ahmed Al Dajani, David E. Keyes","{'name': 'Saleem Abdul Fattah Ahmed Al Dajani, David E. Keyes'}",,"Neural Information Processing Systems (NeurIPS). Machine Learning with New Compute Paradigms (MLNCP) Workshop, October 2024"
646,Grid4D: 4D Decomposed Hash Encoding for High-fidelity Dynamic Gaussian Splatting,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Grid4D: 4D Decomposed Hash Encoding for High-fidelity Dynamic Gaussian Splatting'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2410.20815'}]",https://arxiv.org/abs/2410.20815,"arXiv:2410.20815v2 Announce Type: replace 
Abstract: Recently, Gaussian splatting has received more and more attention in the field of static scene rendering. Due to the low computational overhead and inherent flexibility of explicit representations, plane-based explicit methods are popular ways to predict deformations for Gaussian-based dynamic scene rendering models. However, plane-based methods rely on the inappropriate low-rank assumption and excessively decompose the space-time 4D encoding, resulting in overmuch feature overlap and unsatisfactory rendering quality. To tackle these problems, we propose Grid4D, a dynamic scene rendering model based on Gaussian splatting and employing a novel explicit encoding method for the 4D input through the hash encoding. Different from plane-based explicit representations, we decompose the 4D encoding into one spatial and three temporal 3D hash encodings without the low-rank assumption. Additionally, we design a novel attention module that generates the attention scores in a directional range to aggregate the spatial and temporal features. The directional attention enables Grid4D to more accurately fit the diverse deformations across distinct scene components based on the spatial encoded features. Moreover, to mitigate the inherent lack of smoothness in explicit representation methods, we introduce a smooth regularization term that keeps our model from the chaos of deformation prediction. Our experiments demonstrate that Grid4D significantly outperforms the state-of-the-art models in visual quality and rendering speed.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2410.20815v2 Announce Type: replace \nAbstract: Recently, Gaussian splatting has received more and more attention in the field of static scene rendering. Due to the low computational overhead and inherent flexibility of explicit representations, plane-based explicit methods are popular ways to predict deformations for Gaussian-based dynamic scene rendering models. However, plane-based methods rely on the inappropriate low-rank assumption and excessively decompose the space-time 4D encoding, resulting in overmuch feature overlap and unsatisfactory rendering quality. To tackle these problems, we propose Grid4D, a dynamic scene rendering model based on Gaussian splatting and employing a novel explicit encoding method for the 4D input through the hash encoding. Different from plane-based explicit representations, we decompose the 4D encoding into one spatial and three temporal 3D hash encodings without the low-rank assumption. Additionally, we design a novel attention module that generates the attention scores in a directional range to aggregate the spatial and temporal features. The directional attention enables Grid4D to more accurately fit the diverse deformations across distinct scene components based on the spatial encoded features. Moreover, to mitigate the inherent lack of smoothness in explicit representation methods, we introduce a smooth regularization term that keeps our model from the chaos of deformation prediction. Our experiments demonstrate that Grid4D significantly outperforms the state-of-the-art models in visual quality and rendering speed.'}",oai:arXiv.org:2410.20815v2,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Jiawei Xu, Zexin Fan, Jian Yang, Jin Xie'}]","Jiawei Xu, Zexin Fan, Jian Yang, Jin Xie","{'name': 'Jiawei Xu, Zexin Fan, Jian Yang, Jin Xie'}",,
647,Chatbot Companionship: A Mixed-Methods Study of Companion Chatbot Usage Patterns and Their Relationship to Loneliness in Active Users,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Chatbot Companionship: A Mixed-Methods Study of Companion Chatbot Usage Patterns and Their Relationship to Loneliness in Active Users'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2410.21596'}]",https://arxiv.org/abs/2410.21596,"arXiv:2410.21596v2 Announce Type: replace 
Abstract: Companion chatbots offer a potential solution to the growing epidemic of loneliness, but their impact on users' psychosocial well-being remains poorly understood. This study presents a large-scale survey (n = 404) of regular users of companion chatbots, investigating the relationship between chatbot usage and loneliness. We develop a model explaining approximately 50% of variance in loneliness; while usage does not directly predict loneliness, we identify factors including neuroticism, social network size, and problematic use. We identify seven distinct clusters of users, from socially fulfilled dependent users to lonely moderate users. Different usage patterns can lead to markedly different outcomes, with some users experiencing enhanced social confidence while others risk further isolation. Our work contributes to the ongoing dialogue about the role of AI in social and emotional support, offering insights for developing more targeted and ethical approaches to AI companionship that complement rather than replace human connections.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2410.21596v2 Announce Type: replace \nAbstract: Companion chatbots offer a potential solution to the growing epidemic of loneliness, but their impact on users' psychosocial well-being remains poorly understood. This study presents a large-scale survey (n = 404) of regular users of companion chatbots, investigating the relationship between chatbot usage and loneliness. We develop a model explaining approximately 50% of variance in loneliness; while usage does not directly predict loneliness, we identify factors including neuroticism, social network size, and problematic use. We identify seven distinct clusters of users, from socially fulfilled dependent users to lonely moderate users. Different usage patterns can lead to markedly different outcomes, with some users experiencing enhanced social confidence while others risk further isolation. Our work contributes to the ongoing dialogue about the role of AI in social and emotional support, offering insights for developing more targeted and ethical approaches to AI companionship that complement rather than replace human connections.""}",oai:arXiv.org:2410.21596v2,False,"[{'term': 'cs.HC', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Auren R. Liu, Pat Pataranutaporn, Pattie Maes'}]","Auren R. Liu, Pat Pataranutaporn, Pattie Maes","{'name': 'Auren R. Liu, Pat Pataranutaporn, Pattie Maes'}",,
648,Learning Infinitesimal Generators of Continuous Symmetries from Data,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Learning Infinitesimal Generators of Continuous Symmetries from Data'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2410.21853'}]",https://arxiv.org/abs/2410.21853,"arXiv:2410.21853v2 Announce Type: replace 
Abstract: Exploiting symmetry inherent in data can significantly improve the sample efficiency of a learning procedure and the generalization of learned models. When data clearly reveals underlying symmetry, leveraging this symmetry can naturally inform the design of model architectures or learning strategies. Yet, in numerous real-world scenarios, identifying the specific symmetry within a given data distribution often proves ambiguous. To tackle this, some existing works learn symmetry in a data-driven manner, parameterizing and learning expected symmetry through data. However, these methods often rely on explicit knowledge, such as pre-defined Lie groups, which are typically restricted to linear or affine transformations. In this paper, we propose a novel symmetry learning algorithm based on transformations defined with one-parameter groups, continuously parameterized transformations flowing along the directions of vector fields called infinitesimal generators. Our method is built upon minimal inductive biases, encompassing not only commonly utilized symmetries rooted in Lie groups but also extending to symmetries derived from nonlinear generators. To learn these symmetries, we introduce a notion of a validity score that examine whether the transformed data is still valid for the given task. The validity score is designed to be fully differentiable and easily computable, enabling effective searches for transformations that achieve symmetries innate to the data. We apply our method mainly in two domains: image data and partial differential equations, and demonstrate its advantages. Our codes are available at \url{https://github.com/kogyeonghoon/learning-symmetry-from-scratch.git}.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2410.21853v2 Announce Type: replace \nAbstract: Exploiting symmetry inherent in data can significantly improve the sample efficiency of a learning procedure and the generalization of learned models. When data clearly reveals underlying symmetry, leveraging this symmetry can naturally inform the design of model architectures or learning strategies. Yet, in numerous real-world scenarios, identifying the specific symmetry within a given data distribution often proves ambiguous. To tackle this, some existing works learn symmetry in a data-driven manner, parameterizing and learning expected symmetry through data. However, these methods often rely on explicit knowledge, such as pre-defined Lie groups, which are typically restricted to linear or affine transformations. In this paper, we propose a novel symmetry learning algorithm based on transformations defined with one-parameter groups, continuously parameterized transformations flowing along the directions of vector fields called infinitesimal generators. Our method is built upon minimal inductive biases, encompassing not only commonly utilized symmetries rooted in Lie groups but also extending to symmetries derived from nonlinear generators. To learn these symmetries, we introduce a notion of a validity score that examine whether the transformed data is still valid for the given task. The validity score is designed to be fully differentiable and easily computable, enabling effective searches for transformations that achieve symmetries innate to the data. We apply our method mainly in two domains: image data and partial differential equations, and demonstrate its advantages. Our codes are available at \\url{https://github.com/kogyeonghoon/learning-symmetry-from-scratch.git}.'}",oai:arXiv.org:2410.21853v2,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Gyeonghoon Ko, Hyunsu Kim, Juho Lee'}]","Gyeonghoon Ko, Hyunsu Kim, Juho Lee","{'name': 'Gyeonghoon Ko, Hyunsu Kim, Juho Lee'}",,
649,DECRL: A Deep Evolutionary Clustering Jointed Temporal Knowledge Graph Representation Learning Approach,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'DECRL: A Deep Evolutionary Clustering Jointed Temporal Knowledge Graph Representation Learning Approach'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2410.22631'}]",https://arxiv.org/abs/2410.22631,"arXiv:2410.22631v2 Announce Type: replace 
Abstract: Temporal Knowledge Graph (TKG) representation learning aims to map temporal evolving entities and relations to embedded representations in a continuous low-dimensional vector space. However, existing approaches cannot capture the temporal evolution of high-order correlations in TKGs. To this end, we propose a Deep Evolutionary Clustering jointed temporal knowledge graph Representation Learning approach (DECRL). Specifically, a deep evolutionary clustering module is proposed to capture the temporal evolution of high-order correlations among entities. Furthermore, a cluster-aware unsupervised alignment mechanism is introduced to ensure the precise one-to-one alignment of soft overlapping clusters across timestamps, thereby maintaining the temporal smoothness of clusters. In addition, an implicit correlation encoder is introduced to capture latent correlations between any pair of clusters under the guidance of a global graph. Extensive experiments on seven real-world datasets demonstrate that DECRL achieves the state-of-the-art performances, outperforming the best baseline by an average of 9.53%, 12.98%, 10.42%, and 14.68% in MRR, Hits@1, Hits@3, and Hits@10, respectively.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2410.22631v2 Announce Type: replace \nAbstract: Temporal Knowledge Graph (TKG) representation learning aims to map temporal evolving entities and relations to embedded representations in a continuous low-dimensional vector space. However, existing approaches cannot capture the temporal evolution of high-order correlations in TKGs. To this end, we propose a Deep Evolutionary Clustering jointed temporal knowledge graph Representation Learning approach (DECRL). Specifically, a deep evolutionary clustering module is proposed to capture the temporal evolution of high-order correlations among entities. Furthermore, a cluster-aware unsupervised alignment mechanism is introduced to ensure the precise one-to-one alignment of soft overlapping clusters across timestamps, thereby maintaining the temporal smoothness of clusters. In addition, an implicit correlation encoder is introduced to capture latent correlations between any pair of clusters under the guidance of a global graph. Extensive experiments on seven real-world datasets demonstrate that DECRL achieves the state-of-the-art performances, outperforming the best baseline by an average of 9.53%, 12.98%, 10.42%, and 14.68% in MRR, Hits@1, Hits@3, and Hits@10, respectively.'}",oai:arXiv.org:2410.22631v2,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Qian Chen, Ling Chen'}]","Qian Chen, Ling Chen","{'name': 'Qian Chen, Ling Chen'}",,
650,Conditioned quantum-assisted deep generative surrogate for particle-calorimeter interactions,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Conditioned quantum-assisted deep generative surrogate for particle-calorimeter interactions'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2410.22870'}]",https://arxiv.org/abs/2410.22870,"arXiv:2410.22870v5 Announce Type: replace 
Abstract: Particle collisions at accelerators such as the Large Hadron Collider, recorded and analyzed by experiments such as ATLAS and CMS, enable exquisite measurements of the Standard Model and searches for new phenomena. Simulations of collision events at these detectors have played a pivotal role in shaping the design of future experiments and analyzing ongoing ones. However, the quest for accuracy in Large Hadron Collider (LHC) collisions comes at an imposing computational cost, with projections estimating the need for millions of CPU-years annually during the High Luminosity LHC (HL-LHC) run \cite{collaboration2022atlas}. Simulating a single LHC event with \textsc{Geant4} currently devours around 1000 CPU seconds, with simulations of the calorimeter subdetectors in particular imposing substantial computational demands \cite{rousseau2023experimental}. To address this challenge, we propose a conditioned quantum-assisted deep generative model. Our model integrates a conditioned variational autoencoder (VAE) on the exterior with a conditioned Restricted Boltzmann Machine (RBM) in the latent space, providing enhanced expressiveness compared to conventional VAEs. The RBM nodes and connections are meticulously engineered to enable the use of qubits and couplers on D-Wave's Pegasus-structured \textit{Advantage} quantum annealer (QA) for sampling. We introduce a novel method for conditioning the quantum-assisted RBM using \textit{flux biases}. We further propose a novel adaptive mapping to estimate the effective inverse temperature in quantum annealers. The effectiveness of our framework is illustrated using Dataset 2 of the CaloChallenge \cite{calochallenge}.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2410.22870v5 Announce Type: replace \nAbstract: Particle collisions at accelerators such as the Large Hadron Collider, recorded and analyzed by experiments such as ATLAS and CMS, enable exquisite measurements of the Standard Model and searches for new phenomena. Simulations of collision events at these detectors have played a pivotal role in shaping the design of future experiments and analyzing ongoing ones. However, the quest for accuracy in Large Hadron Collider (LHC) collisions comes at an imposing computational cost, with projections estimating the need for millions of CPU-years annually during the High Luminosity LHC (HL-LHC) run \\cite{collaboration2022atlas}. Simulating a single LHC event with \\textsc{Geant4} currently devours around 1000 CPU seconds, with simulations of the calorimeter subdetectors in particular imposing substantial computational demands \\cite{rousseau2023experimental}. To address this challenge, we propose a conditioned quantum-assisted deep generative model. Our model integrates a conditioned variational autoencoder (VAE) on the exterior with a conditioned Restricted Boltzmann Machine (RBM) in the latent space, providing enhanced expressiveness compared to conventional VAEs. The RBM nodes and connections are meticulously engineered to enable the use of qubits and couplers on D-Wave's Pegasus-structured \\textit{Advantage} quantum annealer (QA) for sampling. We introduce a novel method for conditioning the quantum-assisted RBM using \\textit{flux biases}. We further propose a novel adaptive mapping to estimate the effective inverse temperature in quantum annealers. The effectiveness of our framework is illustrated using Dataset 2 of the CaloChallenge \\cite{calochallenge}.""}",oai:arXiv.org:2410.22870v5,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'hep-ph', 'scheme': None, 'label': None}, {'term': 'physics.comp-ph', 'scheme': None, 'label': None}, {'term': 'physics.ins-det', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'J. Quetzalcoatl Toledo-Marin, Sebastian Gonzalez, Hao Jia, Ian Lu, Deniz Sogutlu, Abhishek Abhishek, Colin Gay, Eric Paquet, Roger Melko, Geoffrey C. Fox, Maximilian Swiatlowski, Wojciech Fedorko'}]","J. Quetzalcoatl Toledo-Marin, Sebastian Gonzalez, Hao Jia, Ian Lu, Deniz Sogutlu, Abhishek Abhishek, Colin Gay, Eric Paquet, Roger Melko, Geoffrey C. Fox, Maximilian Swiatlowski, Wojciech Fedorko","{'name': 'J. Quetzalcoatl Toledo-Marin, Sebastian Gonzalez, Hao Jia, Ian Lu, Deniz Sogutlu, Abhishek Abhishek, Colin Gay, Eric Paquet, Roger Melko, Geoffrey C. Fox, Maximilian Swiatlowski, Wojciech Fedorko'}",,
651,CausalDiff: Causality-Inspired Disentanglement via Diffusion Model for Adversarial Defense,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'CausalDiff: Causality-Inspired Disentanglement via Diffusion Model for Adversarial Defense'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2410.23091'}]",https://arxiv.org/abs/2410.23091,"arXiv:2410.23091v5 Announce Type: replace 
Abstract: Despite ongoing efforts to defend neural classifiers from adversarial attacks, they remain vulnerable, especially to unseen attacks. In contrast, humans are difficult to be cheated by subtle manipulations, since we make judgments only based on essential factors. Inspired by this observation, we attempt to model label generation with essential label-causative factors and incorporate label-non-causative factors to assist data generation. For an adversarial example, we aim to discriminate the perturbations as non-causative factors and make predictions only based on the label-causative factors. Concretely, we propose a casual diffusion model (CausalDiff) that adapts diffusion models for conditional data generation and disentangles the two types of casual factors by learning towards a novel casual information bottleneck objective. Empirically, CausalDiff has significantly outperformed state-of-the-art defense methods on various unseen attacks, achieving an average robustness of 86.39% (+4.01%) on CIFAR-10, 56.25% (+3.13%) on CIFAR-100, and 82.62% (+4.93%) on GTSRB (German Traffic Sign Recognition Benchmark). The code is available at https://github.com/CAS-AISafetyBasicResearchGroup/CausalDiff","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2410.23091v5 Announce Type: replace \nAbstract: Despite ongoing efforts to defend neural classifiers from adversarial attacks, they remain vulnerable, especially to unseen attacks. In contrast, humans are difficult to be cheated by subtle manipulations, since we make judgments only based on essential factors. Inspired by this observation, we attempt to model label generation with essential label-causative factors and incorporate label-non-causative factors to assist data generation. For an adversarial example, we aim to discriminate the perturbations as non-causative factors and make predictions only based on the label-causative factors. Concretely, we propose a casual diffusion model (CausalDiff) that adapts diffusion models for conditional data generation and disentangles the two types of casual factors by learning towards a novel casual information bottleneck objective. Empirically, CausalDiff has significantly outperformed state-of-the-art defense methods on various unseen attacks, achieving an average robustness of 86.39% (+4.01%) on CIFAR-10, 56.25% (+3.13%) on CIFAR-100, and 82.62% (+4.93%) on GTSRB (German Traffic Sign Recognition Benchmark). The code is available at https://github.com/CAS-AISafetyBasicResearchGroup/CausalDiff'}",oai:arXiv.org:2410.23091v5,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Mingkun Zhang, Keping Bi, Wei Chen, Quanrun Chen, Jiafeng Guo, Xueqi Cheng'}]","Mingkun Zhang, Keping Bi, Wei Chen, Quanrun Chen, Jiafeng Guo, Xueqi Cheng","{'name': 'Mingkun Zhang, Keping Bi, Wei Chen, Quanrun Chen, Jiafeng Guo, Xueqi Cheng'}",,
652,"Web Scraping for Research: Legal, Ethical, Institutional, and Scientific Considerations","{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Web Scraping for Research: Legal, Ethical, Institutional, and Scientific Considerations'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2410.23432'}]",https://arxiv.org/abs/2410.23432,"arXiv:2410.23432v2 Announce Type: replace 
Abstract: Scientists across disciplines often use data from the internet to conduct research, generating valuable insights about human behavior. However, as generative AI relying on massive text corpora becomes increasingly valuable, platforms have greatly restricted access to data through official channels. As a result, researchers will likely engage in more web scraping to collect data, introducing new challenges and concerns for researchers. This paper proposes a comprehensive framework for web scraping in social science research for U.S.-based researchers, examining the legal, ethical, institutional, and scientific factors that researchers should consider when scraping the web. We present an overview of the current regulatory environment impacting when and how researchers can access, collect, store, and share data via scraping. We then provide researchers with recommendations to conduct scraping in a scientifically legitimate and ethical manner. We aim to equip researchers with the relevant information to mitigate risks and maximize the impact of their research amidst this evolving data access landscape.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2410.23432v2 Announce Type: replace \nAbstract: Scientists across disciplines often use data from the internet to conduct research, generating valuable insights about human behavior. However, as generative AI relying on massive text corpora becomes increasingly valuable, platforms have greatly restricted access to data through official channels. As a result, researchers will likely engage in more web scraping to collect data, introducing new challenges and concerns for researchers. This paper proposes a comprehensive framework for web scraping in social science research for U.S.-based researchers, examining the legal, ethical, institutional, and scientific factors that researchers should consider when scraping the web. We present an overview of the current regulatory environment impacting when and how researchers can access, collect, store, and share data via scraping. We then provide researchers with recommendations to conduct scraping in a scientifically legitimate and ethical manner. We aim to equip researchers with the relevant information to mitigate risks and maximize the impact of their research amidst this evolving data access landscape.'}",oai:arXiv.org:2410.23432v2,False,"[{'term': 'cs.CY', 'scheme': None, 'label': None}, {'term': 'cs.SI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Megan A. Brown, Andrew Gruen, Gabe Maldoff, Solomon Messing, Zeve Sanderson, Michael Zimmer'}]","Megan A. Brown, Andrew Gruen, Gabe Maldoff, Solomon Messing, Zeve Sanderson, Michael Zimmer","{'name': 'Megan A. Brown, Andrew Gruen, Gabe Maldoff, Solomon Messing, Zeve Sanderson, Michael Zimmer'}",,
653,EZ-HOI: VLM Adaptation via Guided Prompt Learning for Zero-Shot HOI Detection,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'EZ-HOI: VLM Adaptation via Guided Prompt Learning for Zero-Shot HOI Detection'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2410.23904'}]",https://arxiv.org/abs/2410.23904,"arXiv:2410.23904v3 Announce Type: replace 
Abstract: Detecting Human-Object Interactions (HOI) in zero-shot settings, where models must handle unseen classes, poses significant challenges. Existing methods that rely on aligning visual encoders with large Vision-Language Models (VLMs) to tap into the extensive knowledge of VLMs, require large, computationally expensive models and encounter training difficulties. Adapting VLMs with prompt learning offers an alternative to direct alignment. However, fine-tuning on task-specific datasets often leads to overfitting to seen classes and suboptimal performance on unseen classes, due to the absence of unseen class labels. To address these challenges, we introduce a novel prompt learning-based framework for Efficient Zero-Shot HOI detection (EZ-HOI). First, we introduce Large Language Model (LLM) and VLM guidance for learnable prompts, integrating detailed HOI descriptions and visual semantics to adapt VLMs to HOI tasks. However, because training datasets contain seen-class labels alone, fine-tuning VLMs on such datasets tends to optimize learnable prompts for seen classes instead of unseen ones. Therefore, we design prompt learning for unseen classes using information from related seen classes, with LLMs utilized to highlight the differences between unseen and related seen classes. Quantitative evaluations on benchmark datasets demonstrate that our EZ-HOI achieves state-of-the-art performance across various zero-shot settings with only 10.35% to 33.95% of the trainable parameters compared to existing methods. Code is available at https://github.com/ChelsieLei/EZ-HOI.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2410.23904v3 Announce Type: replace \nAbstract: Detecting Human-Object Interactions (HOI) in zero-shot settings, where models must handle unseen classes, poses significant challenges. Existing methods that rely on aligning visual encoders with large Vision-Language Models (VLMs) to tap into the extensive knowledge of VLMs, require large, computationally expensive models and encounter training difficulties. Adapting VLMs with prompt learning offers an alternative to direct alignment. However, fine-tuning on task-specific datasets often leads to overfitting to seen classes and suboptimal performance on unseen classes, due to the absence of unseen class labels. To address these challenges, we introduce a novel prompt learning-based framework for Efficient Zero-Shot HOI detection (EZ-HOI). First, we introduce Large Language Model (LLM) and VLM guidance for learnable prompts, integrating detailed HOI descriptions and visual semantics to adapt VLMs to HOI tasks. However, because training datasets contain seen-class labels alone, fine-tuning VLMs on such datasets tends to optimize learnable prompts for seen classes instead of unseen ones. Therefore, we design prompt learning for unseen classes using information from related seen classes, with LLMs utilized to highlight the differences between unseen and related seen classes. Quantitative evaluations on benchmark datasets demonstrate that our EZ-HOI achieves state-of-the-art performance across various zero-shot settings with only 10.35% to 33.95% of the trainable parameters compared to existing methods. Code is available at https://github.com/ChelsieLei/EZ-HOI.'}",oai:arXiv.org:2410.23904v3,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Qinqian Lei, Bo Wang, Robby T. Tan'}]","Qinqian Lei, Bo Wang, Robby T. Tan","{'name': 'Qinqian Lei, Bo Wang, Robby T. Tan'}",,
654,Mitigating Spurious Correlations via Disagreement Probability,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Mitigating Spurious Correlations via Disagreement Probability'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2411.01757'}]",https://arxiv.org/abs/2411.01757,"arXiv:2411.01757v3 Announce Type: replace 
Abstract: Models trained with empirical risk minimization (ERM) are prone to be biased towards spurious correlations between target labels and bias attributes, which leads to poor performance on data groups lacking spurious correlations. It is particularly challenging to address this problem when access to bias labels is not permitted. To mitigate the effect of spurious correlations without bias labels, we first introduce a novel training objective designed to robustly enhance model performance across all data samples, irrespective of the presence of spurious correlations. From this objective, we then derive a debiasing method, Disagreement Probability based Resampling for debiasing (DPR), which does not require bias labels. DPR leverages the disagreement between the target label and the prediction of a biased model to identify bias-conflicting samples-those without spurious correlations-and upsamples them according to the disagreement probability. Empirical evaluations on multiple benchmarks demonstrate that DPR achieves state-of-the-art performance over existing baselines that do not use bias labels. Furthermore, we provide a theoretical analysis that details how DPR reduces dependency on spurious correlations.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2411.01757v3 Announce Type: replace \nAbstract: Models trained with empirical risk minimization (ERM) are prone to be biased towards spurious correlations between target labels and bias attributes, which leads to poor performance on data groups lacking spurious correlations. It is particularly challenging to address this problem when access to bias labels is not permitted. To mitigate the effect of spurious correlations without bias labels, we first introduce a novel training objective designed to robustly enhance model performance across all data samples, irrespective of the presence of spurious correlations. From this objective, we then derive a debiasing method, Disagreement Probability based Resampling for debiasing (DPR), which does not require bias labels. DPR leverages the disagreement between the target label and the prediction of a biased model to identify bias-conflicting samples-those without spurious correlations-and upsamples them according to the disagreement probability. Empirical evaluations on multiple benchmarks demonstrate that DPR achieves state-of-the-art performance over existing baselines that do not use bias labels. Furthermore, we provide a theoretical analysis that details how DPR reduces dependency on spurious correlations.'}",oai:arXiv.org:2411.01757v3,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'stat.ML', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by-nc-nd/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-nd/4.0/'}","[{'name': 'Hyeonggeun Han, Sehwan Kim, Hyungjun Joo, Sangwoo Hong, Jungwoo Lee'}]","Hyeonggeun Han, Sehwan Kim, Hyungjun Joo, Sangwoo Hong, Jungwoo Lee","{'name': 'Hyeonggeun Han, Sehwan Kim, Hyungjun Joo, Sangwoo Hong, Jungwoo Lee'}",,
655,Explanations that reveal all through the definition of encoding,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Explanations that reveal all through the definition of encoding'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2411.02664'}]",https://arxiv.org/abs/2411.02664,"arXiv:2411.02664v2 Announce Type: replace 
Abstract: Feature attributions attempt to highlight what inputs drive predictive power. Good attributions or explanations are thus those that produce inputs that retain this predictive power; accordingly, evaluations of explanations score their quality of prediction. However, evaluations produce scores better than what appears possible from the values in the explanation for a class of explanations, called encoding explanations. Probing for encoding remains a challenge because there is no general characterization of what gives the extra predictive power. We develop a definition of encoding that identifies this extra predictive power via conditional dependence and show that the definition fits existing examples of encoding. This definition implies, in contrast to encoding explanations, that non-encoding explanations contain all the informative inputs used to produce the explanation, giving them a ""what you see is what you get"" property, which makes them transparent and simple to use. Next, we prove that existing scores (ROAR, FRESH, EVAL-X) do not rank non-encoding explanations above encoding ones, and develop STRIPE-X which ranks them correctly. After empirically demonstrating the theoretical insights, we use STRIPE-X to show that despite prompting an LLM to produce non-encoding explanations for a sentiment analysis task, the LLM-generated explanations encode.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2411.02664v2 Announce Type: replace \nAbstract: Feature attributions attempt to highlight what inputs drive predictive power. Good attributions or explanations are thus those that produce inputs that retain this predictive power; accordingly, evaluations of explanations score their quality of prediction. However, evaluations produce scores better than what appears possible from the values in the explanation for a class of explanations, called encoding explanations. Probing for encoding remains a challenge because there is no general characterization of what gives the extra predictive power. We develop a definition of encoding that identifies this extra predictive power via conditional dependence and show that the definition fits existing examples of encoding. This definition implies, in contrast to encoding explanations, that non-encoding explanations contain all the informative inputs used to produce the explanation, giving them a ""what you see is what you get"" property, which makes them transparent and simple to use. Next, we prove that existing scores (ROAR, FRESH, EVAL-X) do not rank non-encoding explanations above encoding ones, and develop STRIPE-X which ranks them correctly. After empirically demonstrating the theoretical insights, we use STRIPE-X to show that despite prompting an LLM to produce non-encoding explanations for a sentiment analysis task, the LLM-generated explanations encode.'}",oai:arXiv.org:2411.02664v2,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'stat.ML', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by-nc-sa/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-sa/4.0/'}","[{'name': 'Aahlad Puli, Nhi Nguyen, Rajesh Ranganath'}]","Aahlad Puli, Nhi Nguyen, Rajesh Ranganath","{'name': 'Aahlad Puli, Nhi Nguyen, Rajesh Ranganath'}",,38th Conference on Neural Information Processing Systems (NeurIPS 2024)
656,Lightning IR: Straightforward Fine-tuning and Inference of Transformer-based Language Models for Information Retrieval,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Lightning IR: Straightforward Fine-tuning and Inference of Transformer-based Language Models for Information Retrieval'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2411.04677'}]",https://arxiv.org/abs/2411.04677,"arXiv:2411.04677v3 Announce Type: replace 
Abstract: A wide range of transformer-based language models have been proposed for information retrieval tasks. However, including transformer-based models in retrieval pipelines is often complex and requires substantial engineering effort. In this paper, we introduce Lightning IR, an easy-to-use PyTorch Lightning-based framework for applying transformer-based language models in retrieval scenarios. Lightning IR provides a modular and extensible architecture that supports all stages of a retrieval pipeline: from fine-tuning and indexing to searching and re-ranking. Designed to be scalable and reproducible, Lightning IR is available as open-source: https://github.com/webis-de/lightning-ir.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2411.04677v3 Announce Type: replace \nAbstract: A wide range of transformer-based language models have been proposed for information retrieval tasks. However, including transformer-based models in retrieval pipelines is often complex and requires substantial engineering effort. In this paper, we introduce Lightning IR, an easy-to-use PyTorch Lightning-based framework for applying transformer-based language models in retrieval scenarios. Lightning IR provides a modular and extensible architecture that supports all stages of a retrieval pipeline: from fine-tuning and indexing to searching and re-ranking. Designed to be scalable and reproducible, Lightning IR is available as open-source: https://github.com/webis-de/lightning-ir.'}",oai:arXiv.org:2411.04677v3,False,"[{'term': 'cs.IR', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Ferdinand Schlatt, Maik Fr\\""obe, Matthias Hagen'}]","Ferdinand Schlatt, Maik Fr\""obe, Matthias Hagen","{'name': 'Ferdinand Schlatt, Maik Fr\\""obe, Matthias Hagen'}",10.1145/3701551.3704118,
657,ZAHA: Introducing the Level of Facade Generalization and the Large-Scale Point Cloud Facade Semantic Segmentation Benchmark Dataset,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'ZAHA: Introducing the Level of Facade Generalization and the Large-Scale Point Cloud Facade Semantic Segmentation Benchmark Dataset'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2411.04865'}]",https://arxiv.org/abs/2411.04865,"arXiv:2411.04865v4 Announce Type: replace 
Abstract: Facade semantic segmentation is a long-standing challenge in photogrammetry and computer vision. Although the last decades have witnessed the influx of facade segmentation methods, there is a lack of comprehensive facade classes and data covering the architectural variability. In ZAHA, we introduce Level of Facade Generalization (LoFG), novel hierarchical facade classes designed based on international urban modeling standards, ensuring compatibility with real-world challenging classes and uniform methods' comparison. Realizing the LoFG, we present to date the largest semantic 3D facade segmentation dataset, providing 601 million annotated points at five and 15 classes of LoFG2 and LoFG3, respectively. Moreover, we analyze the performance of baseline semantic segmentation methods on our introduced LoFG classes and data, complementing it with a discussion on the unresolved challenges for facade segmentation. We firmly believe that ZAHA shall facilitate further development of 3D facade semantic segmentation methods, enabling robust segmentation indispensable in creating urban digital twins.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2411.04865v4 Announce Type: replace \nAbstract: Facade semantic segmentation is a long-standing challenge in photogrammetry and computer vision. Although the last decades have witnessed the influx of facade segmentation methods, there is a lack of comprehensive facade classes and data covering the architectural variability. In ZAHA, we introduce Level of Facade Generalization (LoFG), novel hierarchical facade classes designed based on international urban modeling standards, ensuring compatibility with real-world challenging classes and uniform methods' comparison. Realizing the LoFG, we present to date the largest semantic 3D facade segmentation dataset, providing 601 million annotated points at five and 15 classes of LoFG2 and LoFG3, respectively. Moreover, we analyze the performance of baseline semantic segmentation methods on our introduced LoFG classes and data, complementing it with a discussion on the unresolved challenges for facade segmentation. We firmly believe that ZAHA shall facilitate further development of 3D facade semantic segmentation methods, enabling robust segmentation indispensable in creating urban digital twins.""}",oai:arXiv.org:2411.04865v4,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Olaf Wysocki, Yue Tan, Thomas Froech, Yan Xia, Magdalena Wysocki, Ludwig Hoegner, Daniel Cremers, Christoph Holst'}]","Olaf Wysocki, Yue Tan, Thomas Froech, Yan Xia, Magdalena Wysocki, Ludwig Hoegner, Daniel Cremers, Christoph Holst","{'name': 'Olaf Wysocki, Yue Tan, Thomas Froech, Yan Xia, Magdalena Wysocki, Ludwig Hoegner, Daniel Cremers, Christoph Holst'}",,
658,CodeLutra: Boosting LLM Code Generation via Preference-Guided Refinement,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'CodeLutra: Boosting LLM Code Generation via Preference-Guided Refinement'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2411.05199'}]",https://arxiv.org/abs/2411.05199,"arXiv:2411.05199v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have revolutionized code generation but require significant resources and often over-generalize, limiting their task-specific efficiency. Fine-tuning smaller, open-source LLMs provides a cost-effective alternative. However, standard supervised approaches rely only on correct examples, missing valuable insights from failures. We introduce CodeLutra, a framework that leverages both correct and incorrect code attempts. Instead of using only correct solutions, CodeLutra applies iterative preference-based refinement, comparing successful and failed outputs to better approximate desired results. This approach narrows the performance gap with state-of-the-art larger models without requiring massive datasets or auxiliary models. For instance, on a challenging data science coding task, using only 500 samples improved Llama-3-8B's accuracy from 28.2% to 48.6%, approaching GPT-4's level. By learning from both successes and mistakes, CodeLutra provides a scalable and efficient path to high-quality code generation, making smaller open-source models more competitive with leading closed-source alternatives.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2411.05199v2 Announce Type: replace \nAbstract: Large Language Models (LLMs) have revolutionized code generation but require significant resources and often over-generalize, limiting their task-specific efficiency. Fine-tuning smaller, open-source LLMs provides a cost-effective alternative. However, standard supervised approaches rely only on correct examples, missing valuable insights from failures. We introduce CodeLutra, a framework that leverages both correct and incorrect code attempts. Instead of using only correct solutions, CodeLutra applies iterative preference-based refinement, comparing successful and failed outputs to better approximate desired results. This approach narrows the performance gap with state-of-the-art larger models without requiring massive datasets or auxiliary models. For instance, on a challenging data science coding task, using only 500 samples improved Llama-3-8B's accuracy from 28.2% to 48.6%, approaching GPT-4's level. By learning from both successes and mistakes, CodeLutra provides a scalable and efficient path to high-quality code generation, making smaller open-source models more competitive with leading closed-source alternatives.""}",oai:arXiv.org:2411.05199v2,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Leitian Tao, Xiang Chen, Tong Yu, Tung Mai, Ryan Rossi, Yixuan Li, Saayan Mitra'}]","Leitian Tao, Xiang Chen, Tong Yu, Tung Mai, Ryan Rossi, Yixuan Li, Saayan Mitra","{'name': 'Leitian Tao, Xiang Chen, Tong Yu, Tung Mai, Ryan Rossi, Yixuan Li, Saayan Mitra'}",,
659,An $\mathbf{L^*}$ Algorithm for Deterministic Weighted Regular Languages,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'An $\\mathbf{L^*}$ Algorithm for Deterministic Weighted Regular Languages'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2411.06228'}]",https://arxiv.org/abs/2411.06228,"arXiv:2411.06228v2 Announce Type: replace 
Abstract: Extracting finite state automata (FSAs) from black-box models offers a powerful approach to gaining interpretable insights into complex model behaviors. To support this pursuit, we present a weighted variant of Angluin's (1987) $\mathbf{L^*}$ algorithm for learning FSAs. We stay faithful to the original algorithm, devising a way to exactly learn deterministic weighted FSAs whose weights support division. Furthermore, we formulate the learning process in a manner that highlights the connection with FSA minimization, showing how $\mathbf{L^*}$ directly learns a minimal automaton for the target language.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2411.06228v2 Announce Type: replace \nAbstract: Extracting finite state automata (FSAs) from black-box models offers a powerful approach to gaining interpretable insights into complex model behaviors. To support this pursuit, we present a weighted variant of Angluin's (1987) $\\mathbf{L^*}$ algorithm for learning FSAs. We stay faithful to the original algorithm, devising a way to exactly learn deterministic weighted FSAs whose weights support division. Furthermore, we formulate the learning process in a manner that highlights the connection with FSA minimization, showing how $\\mathbf{L^*}$ directly learns a minimal automaton for the target language.""}",oai:arXiv.org:2411.06228v2,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Clemente Pasti, Talu Karag\\""oz, Anej Svete, Franz Nowak, Reda Boumasmoud, Ryan Cotterell'}]","Clemente Pasti, Talu Karag\""oz, Anej Svete, Franz Nowak, Reda Boumasmoud, Ryan Cotterell","{'name': 'Clemente Pasti, Talu Karag\\""oz, Anej Svete, Franz Nowak, Reda Boumasmoud, Ryan Cotterell'}",10.18653/v1/2024.emnlp-main.468,
660,Tracing the Roots: Leveraging Temporal Dynamics in Diffusion Trajectories for Origin Attribution,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Tracing the Roots: Leveraging Temporal Dynamics in Diffusion Trajectories for Origin Attribution'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2411.07449'}]",https://arxiv.org/abs/2411.07449,"arXiv:2411.07449v2 Announce Type: replace 
Abstract: Diffusion models have revolutionized image synthesis, garnering significant research interest in recent years. Diffusion is an iterative algorithm in which samples are generated step-by-step, starting from pure noise. This process introduces the notion of diffusion trajectories, i.e., paths from the standard Gaussian distribution to the target image distribution. In this context, we study discriminative algorithms operating on these trajectories. Specifically, given a pre-trained diffusion model, we consider the problem of classifying images as part of the training dataset, generated by the model or originating from an external source. Our approach demonstrates the presence of patterns across steps that can be leveraged for classification. We also conduct ablation studies, which reveal that using higher-order gradient features to characterize the trajectories leads to significant performance gains and more robust algorithms.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2411.07449v2 Announce Type: replace \nAbstract: Diffusion models have revolutionized image synthesis, garnering significant research interest in recent years. Diffusion is an iterative algorithm in which samples are generated step-by-step, starting from pure noise. This process introduces the notion of diffusion trajectories, i.e., paths from the standard Gaussian distribution to the target image distribution. In this context, we study discriminative algorithms operating on these trajectories. Specifically, given a pre-trained diffusion model, we consider the problem of classifying images as part of the training dataset, generated by the model or originating from an external source. Our approach demonstrates the presence of patterns across steps that can be leveraged for classification. We also conduct ablation studies, which reveal that using higher-order gradient features to characterize the trajectories leads to significant performance gains and more robust algorithms.'}",oai:arXiv.org:2411.07449v2,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Andreas Floros, Seyed-Mohsen Moosavi-Dezfooli, Pier Luigi Dragotti'}]","Andreas Floros, Seyed-Mohsen Moosavi-Dezfooli, Pier Luigi Dragotti","{'name': 'Andreas Floros, Seyed-Mohsen Moosavi-Dezfooli, Pier Luigi Dragotti'}",,
661,SPICA: Retrieving Scenarios for Pluralistic In-Context Alignment,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'SPICA: Retrieving Scenarios for Pluralistic In-Context Alignment'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2411.10912'}]",https://arxiv.org/abs/2411.10912,"arXiv:2411.10912v2 Announce Type: replace 
Abstract: When different groups' values differ, one approach to model alignment is to steer models at inference time towards each group's preferences. However, techniques like in-context learning only consider similarity when drawing few-shot examples and not cross-group differences in values. We propose SPICA, a framework that accounts for group-level differences during in-context example retrieval. SPICA introduces three designs: scenario banks, group-informed retrieval metrics, and in-context alignment prompts. From an evaluation of SPICA on an alignment task collecting inputs from four demographic groups ($n = 544$), our metrics retrieve in-context examples that more closely match observed preferences, with the best prompt configuration using multiple contrastive responses to demonstrate examples. In an end-to-end evaluation ($n = 120$), we observe that SPICA is higher rated than similarity-based retrieval, with groups seeing up to a +0.16 point improvement on a 5 point scale. Additionally, gains from SPICA were more uniform, with all groups benefiting from alignment rather than only some. Finally, we find that while a group-agnostic approach can align to aggregated values, it is not most suited for divergent groups.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2411.10912v2 Announce Type: replace \nAbstract: When different groups' values differ, one approach to model alignment is to steer models at inference time towards each group's preferences. However, techniques like in-context learning only consider similarity when drawing few-shot examples and not cross-group differences in values. We propose SPICA, a framework that accounts for group-level differences during in-context example retrieval. SPICA introduces three designs: scenario banks, group-informed retrieval metrics, and in-context alignment prompts. From an evaluation of SPICA on an alignment task collecting inputs from four demographic groups ($n = 544$), our metrics retrieve in-context examples that more closely match observed preferences, with the best prompt configuration using multiple contrastive responses to demonstrate examples. In an end-to-end evaluation ($n = 120$), we observe that SPICA is higher rated than similarity-based retrieval, with groups seeing up to a +0.16 point improvement on a 5 point scale. Additionally, gains from SPICA were more uniform, with all groups benefiting from alignment rather than only some. Finally, we find that while a group-agnostic approach can align to aggregated values, it is not most suited for divergent groups.""}",oai:arXiv.org:2411.10912v2,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by-nc-sa/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-sa/4.0/'}","[{'name': 'Quan Ze Chen, K. J. Kevin Feng, Chan Young Park, Amy X. Zhang'}]","Quan Ze Chen, K. J. Kevin Feng, Chan Young Park, Amy X. Zhang","{'name': 'Quan Ze Chen, K. J. Kevin Feng, Chan Young Park, Amy X. Zhang'}",,
662,SageAttention2: Efficient Attention with Thorough Outlier Smoothing and Per-thread INT4 Quantization,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'SageAttention2: Efficient Attention with Thorough Outlier Smoothing and Per-thread INT4 Quantization'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2411.10958'}]",https://arxiv.org/abs/2411.10958,"arXiv:2411.10958v2 Announce Type: replace 
Abstract: Although quantization for linear layers has been widely used, its application to accelerate the attention process remains limited. To further enhance the efficiency of attention computation compared to SageAttention while maintaining precision, we propose SageAttention2, which utilizes significantly faster 4-bit matrix multiplication (Matmul) alongside additional precision-enhancing techniques. First, we propose to quantize matrixes $(Q, K)$ to INT4 in a hardware-friendly thread-level granularity and quantize matrixes $(\widetilde P, V)$ to FP8. Second, we propose a method to smooth $Q$, enhancing the accuracy of INT4 $QK$. Third, we propose to use an FP32 Matmul buffer for $PV$ to enhance the accuracy of FP8 $\widetilde PV$. The operations per second (OPS) of SageAttention2 surpass FlashAttention2 and xformers by about 3x and 5x on RTX4090, respectively. Comprehensive experiments confirm that our approach incurs negligible end-to-end metrics loss across diverse models, including those for large language processing, image generation, and video generation. The codes are available at https://github.com/thu-ml/SageAttention.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2411.10958v2 Announce Type: replace \nAbstract: Although quantization for linear layers has been widely used, its application to accelerate the attention process remains limited. To further enhance the efficiency of attention computation compared to SageAttention while maintaining precision, we propose SageAttention2, which utilizes significantly faster 4-bit matrix multiplication (Matmul) alongside additional precision-enhancing techniques. First, we propose to quantize matrixes $(Q, K)$ to INT4 in a hardware-friendly thread-level granularity and quantize matrixes $(\\widetilde P, V)$ to FP8. Second, we propose a method to smooth $Q$, enhancing the accuracy of INT4 $QK$. Third, we propose to use an FP32 Matmul buffer for $PV$ to enhance the accuracy of FP8 $\\widetilde PV$. The operations per second (OPS) of SageAttention2 surpass FlashAttention2 and xformers by about 3x and 5x on RTX4090, respectively. Comprehensive experiments confirm that our approach incurs negligible end-to-end metrics loss across diverse models, including those for large language processing, image generation, and video generation. The codes are available at https://github.com/thu-ml/SageAttention.'}",oai:arXiv.org:2411.10958v2,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.NE', 'scheme': None, 'label': None}, {'term': 'cs.PF', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Jintao Zhang, Haofeng Huang, Pengle Zhang, Jia Wei, Jun Zhu, Jianfei Chen'}]","Jintao Zhang, Haofeng Huang, Pengle Zhang, Jia Wei, Jun Zhu, Jianfei Chen","{'name': 'Jintao Zhang, Haofeng Huang, Pengle Zhang, Jia Wei, Jun Zhu, Jianfei Chen'}",,
663,CKGFuzzer: LLM-Based Fuzz Driver Generation Enhanced By Code Knowledge Graph,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'CKGFuzzer: LLM-Based Fuzz Driver Generation Enhanced By Code Knowledge Graph'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2411.11532'}]",https://arxiv.org/abs/2411.11532,"arXiv:2411.11532v2 Announce Type: replace 
Abstract: In recent years, the programming capabilities of large language models (LLMs) have garnered significant attention. Fuzz testing, a highly effective technique, plays a key role in enhancing software reliability and detecting vulnerabilities. However, traditional fuzz testing tools rely on manually crafted fuzz drivers, which can limit both testing efficiency and effectiveness. To address this challenge, we propose an automated fuzz testing method driven by a code knowledge graph and powered by an LLM-based intelligent agent system, referred to as CKGFuzzer. We approach fuzz driver creation as a code generation task, leveraging the knowledge graph of the code repository to automate the generation process within the fuzzing loop, while continuously refining both the fuzz driver and input seeds. The code knowledge graph is constructed through interprocedural program analysis, where each node in the graph represents a code entity, such as a function or a file. The knowledge graph-enhanced CKGFuzzer not only effectively resolves compilation errors in fuzz drivers and generates input seeds tailored to specific API usage scenarios, but also analyzes fuzz driver crash reports, assisting developers in improving code quality. By querying the knowledge graph of the code repository and learning from API usage scenarios, we can better identify testing targets and understand the specific purpose of each fuzz driver. We evaluated our approach using eight open-source software projects. The experimental results indicate that CKGFuzzer achieved an average improvement of 8.73% in code coverage compared to state-of-the-art techniques. Additionally, CKGFuzzer reduced the manual review workload in crash case analysis by 84.4% and successfully detected 11 real bugs (including nine previously unreported bugs) across the tested libraries.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2411.11532v2 Announce Type: replace \nAbstract: In recent years, the programming capabilities of large language models (LLMs) have garnered significant attention. Fuzz testing, a highly effective technique, plays a key role in enhancing software reliability and detecting vulnerabilities. However, traditional fuzz testing tools rely on manually crafted fuzz drivers, which can limit both testing efficiency and effectiveness. To address this challenge, we propose an automated fuzz testing method driven by a code knowledge graph and powered by an LLM-based intelligent agent system, referred to as CKGFuzzer. We approach fuzz driver creation as a code generation task, leveraging the knowledge graph of the code repository to automate the generation process within the fuzzing loop, while continuously refining both the fuzz driver and input seeds. The code knowledge graph is constructed through interprocedural program analysis, where each node in the graph represents a code entity, such as a function or a file. The knowledge graph-enhanced CKGFuzzer not only effectively resolves compilation errors in fuzz drivers and generates input seeds tailored to specific API usage scenarios, but also analyzes fuzz driver crash reports, assisting developers in improving code quality. By querying the knowledge graph of the code repository and learning from API usage scenarios, we can better identify testing targets and understand the specific purpose of each fuzz driver. We evaluated our approach using eight open-source software projects. The experimental results indicate that CKGFuzzer achieved an average improvement of 8.73% in code coverage compared to state-of-the-art techniques. Additionally, CKGFuzzer reduced the manual review workload in crash case analysis by 84.4% and successfully detected 11 real bugs (including nine previously unreported bugs) across the tested libraries.'}",oai:arXiv.org:2411.11532v2,False,"[{'term': 'cs.SE', 'scheme': None, 'label': None}, {'term': 'cs.CR', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Hanxiang Xu, Wei Ma, Ting Zhou, Yanjie Zhao, Kai Chen, Qiang Hu, Yang Liu, Haoyu Wang'}]","Hanxiang Xu, Wei Ma, Ting Zhou, Yanjie Zhao, Kai Chen, Qiang Hu, Yang Liu, Haoyu Wang","{'name': 'Hanxiang Xu, Wei Ma, Ting Zhou, Yanjie Zhao, Kai Chen, Qiang Hu, Yang Liu, Haoyu Wang'}",,
664,Low-resource Machine Translation: what for? who for? An observational study on a dedicated Tetun language translation service,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Low-resource Machine Translation: what for? who for? An observational study on a dedicated Tetun language translation service'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2411.12262'}]",https://arxiv.org/abs/2411.12262,"arXiv:2411.12262v2 Announce Type: replace 
Abstract: Low-resource machine translation (MT) presents a diversity of community needs and application challenges that remain poorly understood. To complement surveys and focus groups, which tend to rely on small samples of respondents, we propose an observational study on actual usage patterns of a specialized MT service for the Tetun language, which is the lingua franca in Timor-Leste. Our analysis of 100,000 translation requests reveals patterns that challenge assumptions based on existing corpora. We find that users, many of them students on mobile devices, typically translate text from a high-resource language into Tetun across diverse domains including science, healthcare, and daily life. This contrasts sharply with available Tetun corpora, which are dominated by news articles covering government and social issues. Our results suggest that MT systems for minority languages like Tetun should prioritize accuracy on domains relevant to educational contexts, in the high-resource to low-resource direction. More broadly, this study demonstrates how observational analysis can inform low-resource language technology development, by grounding research in practical community needs.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2411.12262v2 Announce Type: replace \nAbstract: Low-resource machine translation (MT) presents a diversity of community needs and application challenges that remain poorly understood. To complement surveys and focus groups, which tend to rely on small samples of respondents, we propose an observational study on actual usage patterns of a specialized MT service for the Tetun language, which is the lingua franca in Timor-Leste. Our analysis of 100,000 translation requests reveals patterns that challenge assumptions based on existing corpora. We find that users, many of them students on mobile devices, typically translate text from a high-resource language into Tetun across diverse domains including science, healthcare, and daily life. This contrasts sharply with available Tetun corpora, which are dominated by news articles covering government and social issues. Our results suggest that MT systems for minority languages like Tetun should prioritize accuracy on domains relevant to educational contexts, in the high-resource to low-resource direction. More broadly, this study demonstrates how observational analysis can inform low-resource language technology development, by grounding research in practical community needs.'}",oai:arXiv.org:2411.12262v2,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': ""Raphael Merx, Ad\\'erito Jos\\'e Guterres Correia, Hanna Suominen, Ekaterina Vylomova""}]","Raphael Merx, Ad\'erito Jos\'e Guterres Correia, Hanna Suominen, Ekaterina Vylomova","{'name': ""Raphael Merx, Ad\\'erito Jos\\'e Guterres Correia, Hanna Suominen, Ekaterina Vylomova""}",,
665,Video-RAG: Visually-aligned Retrieval-Augmented Long Video Comprehension,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Video-RAG: Visually-aligned Retrieval-Augmented Long Video Comprehension'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2411.13093'}]",https://arxiv.org/abs/2411.13093,"arXiv:2411.13093v2 Announce Type: replace 
Abstract: Existing large video-language models (LVLMs) struggle to comprehend long videos correctly due to limited context. To address this problem, fine-tuning long-context LVLMs and employing GPT-based agents have emerged as promising solutions. However, fine-tuning LVLMs would require extensive high-quality data and substantial GPU resources, while GPT-based agents would rely on proprietary models (e.g., GPT-4o). In this paper, we propose Video Retrieval-Augmented Generation (Video-RAG), a training-free and cost-effective pipeline that employs visually-aligned auxiliary texts to help facilitate cross-modality alignment while providing additional information beyond the visual content. Specifically, we leverage open-source external tools to extract visually-aligned information from pure video data (e.g., audio, optical character, and object detection), and incorporate the extracted information into an existing LVLM as auxiliary texts, alongside video frames and queries, in a plug-and-play manner. Our Video-RAG offers several key advantages: (i) lightweight with low computing overhead due to single-turn retrieval; (ii) easy implementation and compatibility with any LVLM; and (iii) significant, consistent performance gains across long video understanding benchmarks, including Video-MME, MLVU, and LongVideoBench. Notably, our model demonstrates superior performance over proprietary models like Gemini-1.5-Pro and GPT-4o when utilized with a 72B model.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2411.13093v2 Announce Type: replace \nAbstract: Existing large video-language models (LVLMs) struggle to comprehend long videos correctly due to limited context. To address this problem, fine-tuning long-context LVLMs and employing GPT-based agents have emerged as promising solutions. However, fine-tuning LVLMs would require extensive high-quality data and substantial GPU resources, while GPT-based agents would rely on proprietary models (e.g., GPT-4o). In this paper, we propose Video Retrieval-Augmented Generation (Video-RAG), a training-free and cost-effective pipeline that employs visually-aligned auxiliary texts to help facilitate cross-modality alignment while providing additional information beyond the visual content. Specifically, we leverage open-source external tools to extract visually-aligned information from pure video data (e.g., audio, optical character, and object detection), and incorporate the extracted information into an existing LVLM as auxiliary texts, alongside video frames and queries, in a plug-and-play manner. Our Video-RAG offers several key advantages: (i) lightweight with low computing overhead due to single-turn retrieval; (ii) easy implementation and compatibility with any LVLM; and (iii) significant, consistent performance gains across long video understanding benchmarks, including Video-MME, MLVU, and LongVideoBench. Notably, our model demonstrates superior performance over proprietary models like Gemini-1.5-Pro and GPT-4o when utilized with a 72B model.'}",oai:arXiv.org:2411.13093v2,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Yongdong Luo, Xiawu Zheng, Xiao Yang, Guilin Li, Haojia Lin, Jinfa Huang, Jiayi Ji, Fei Chao, Jiebo Luo, Rongrong Ji'}]","Yongdong Luo, Xiawu Zheng, Xiao Yang, Guilin Li, Haojia Lin, Jinfa Huang, Jiayi Ji, Fei Chao, Jiebo Luo, Rongrong Ji","{'name': 'Yongdong Luo, Xiawu Zheng, Xiao Yang, Guilin Li, Haojia Lin, Jinfa Huang, Jiayi Ji, Fei Chao, Jiebo Luo, Rongrong Ji'}",,
666,BayLing 2: A Multilingual Large Language Model with Efficient Language Alignment,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'BayLing 2: A Multilingual Large Language Model with Efficient Language Alignment'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2411.16300'}]",https://arxiv.org/abs/2411.16300,"arXiv:2411.16300v3 Announce Type: replace 
Abstract: Large language models (LLMs), with their powerful generative capabilities and vast knowledge, empower various tasks in everyday life. However, these abilities are primarily concentrated in high-resource languages, leaving low-resource languages with weaker generative capabilities and relatively limited knowledge. Enhancing the multilingual capabilities of LLMs is therefore crucial for serving over 100 linguistic communities worldwide. An intuitive approach to enhance the multilingual capabilities would be to construct instruction data for various languages, but constructing instruction data for over 100 languages is prohibitively costly. In this paper, we introduce BayLing 2, which efficiently transfers generative capabilities and knowledge from high-resource languages to low-resource languages through language alignment. To achieve this, we constructed a dataset of 3.2 million instructions, comprising high-resource language instructions (Chinese and English) and cross-lingual instructions for 100+ languages and performed instruction tuning based on the dataset to facilitate the capability transfer between languages. Using Llama as the foundation model, we developed BayLing-2-7B, BayLing-2-13B, and BayLing-2-8B, and conducted a comprehensive evaluation of BayLing. For multilingual translation across 100+ languages, BayLing shows superior performance compared to open-source models of similar scale. For multilingual knowledge and understanding benchmarks, BayLing achieves significant improvements across over 20 low-resource languages, demonstrating its capability of effective knowledge transfer from high-resource to low-resource languages. Furthermore, results on English benchmarks indicate that BayLing maintains high performance in highresource languages while enhancing the performance in low-resource languages. Demo, homepage, code and models of BayLing are available.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2411.16300v3 Announce Type: replace \nAbstract: Large language models (LLMs), with their powerful generative capabilities and vast knowledge, empower various tasks in everyday life. However, these abilities are primarily concentrated in high-resource languages, leaving low-resource languages with weaker generative capabilities and relatively limited knowledge. Enhancing the multilingual capabilities of LLMs is therefore crucial for serving over 100 linguistic communities worldwide. An intuitive approach to enhance the multilingual capabilities would be to construct instruction data for various languages, but constructing instruction data for over 100 languages is prohibitively costly. In this paper, we introduce BayLing 2, which efficiently transfers generative capabilities and knowledge from high-resource languages to low-resource languages through language alignment. To achieve this, we constructed a dataset of 3.2 million instructions, comprising high-resource language instructions (Chinese and English) and cross-lingual instructions for 100+ languages and performed instruction tuning based on the dataset to facilitate the capability transfer between languages. Using Llama as the foundation model, we developed BayLing-2-7B, BayLing-2-13B, and BayLing-2-8B, and conducted a comprehensive evaluation of BayLing. For multilingual translation across 100+ languages, BayLing shows superior performance compared to open-source models of similar scale. For multilingual knowledge and understanding benchmarks, BayLing achieves significant improvements across over 20 low-resource languages, demonstrating its capability of effective knowledge transfer from high-resource to low-resource languages. Furthermore, results on English benchmarks indicate that BayLing maintains high performance in highresource languages while enhancing the performance in low-resource languages. Demo, homepage, code and models of BayLing are available.'}",oai:arXiv.org:2411.16300v3,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by-nc-nd/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-nd/4.0/'}","[{'name': 'Shaolei Zhang, Kehao Zhang, Qingkai Fang, Shoutao Guo, Yan Zhou, Xiaodong Liu, Yang Feng'}]","Shaolei Zhang, Kehao Zhang, Qingkai Fang, Shoutao Guo, Yan Zhou, Xiaodong Liu, Yang Feng","{'name': 'Shaolei Zhang, Kehao Zhang, Qingkai Fang, Shoutao Guo, Yan Zhou, Xiaodong Liu, Yang Feng'}",,
667,Proxima. A DAG based cooperative distributed ledger,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Proxima. A DAG based cooperative distributed ledger'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2411.16456'}]",https://arxiv.org/abs/2411.16456,"arXiv:2411.16456v2 Announce Type: replace 
Abstract: This paper introduces a novel architecture for a distributed ledger, commonly referred to as a ""blockchain"", which is organized in the form of directed acyclic graph (DAG) with UTXO transactions as vertices, rather than as a chain of blocks. Consensus on the state of ledger assets is achieved through the cooperative consensus: a profit-driven behavior of token holders themselves, which is viable only when they cooperate by following the biggest ledger coverage rule. The cooperative behavior is facilitated by enforcing purposefully designed UTXO transaction validity constraints. Token holders are the sole category of participants authorized to make amendments to the ledger, making participation completely permissionless - without miners, validators, committees or staking - and without any need of knowledge about the composition of the set of all participants in the consensus. The setup allows to achieve high throughput and scalability alongside with low transaction costs, while preserving key aspects of high decentralization, open participation, and asynchronicity found in Bitcoin and other proof-of-work blockchains, but without unreasonable energy consumption. Sybil protection is achieved similarly to proof-of-stake blockchains, using tokens native to the ledger, yet the architecture operates in a leaderless manner without block proposers and committee selection.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2411.16456v2 Announce Type: replace \nAbstract: This paper introduces a novel architecture for a distributed ledger, commonly referred to as a ""blockchain"", which is organized in the form of directed acyclic graph (DAG) with UTXO transactions as vertices, rather than as a chain of blocks. Consensus on the state of ledger assets is achieved through the cooperative consensus: a profit-driven behavior of token holders themselves, which is viable only when they cooperate by following the biggest ledger coverage rule. The cooperative behavior is facilitated by enforcing purposefully designed UTXO transaction validity constraints. Token holders are the sole category of participants authorized to make amendments to the ledger, making participation completely permissionless - without miners, validators, committees or staking - and without any need of knowledge about the composition of the set of all participants in the consensus. The setup allows to achieve high throughput and scalability alongside with low transaction costs, while preserving key aspects of high decentralization, open participation, and asynchronicity found in Bitcoin and other proof-of-work blockchains, but without unreasonable energy consumption. Sybil protection is achieved similarly to proof-of-stake blockchains, using tokens native to the ledger, yet the architecture operates in a leaderless manner without block proposers and committee selection.'}",oai:arXiv.org:2411.16456v2,False,"[{'term': 'cs.DC', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by-sa/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-sa/4.0/'}",[{'name': 'Evaldas Drasutis'}],Evaldas Drasutis,{'name': 'Evaldas Drasutis'},,
668,Self-Generated Critiques Boost Reward Modeling for Language Models,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Self-Generated Critiques Boost Reward Modeling for Language Models'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2411.16646'}]",https://arxiv.org/abs/2411.16646,"arXiv:2411.16646v2 Announce Type: replace 
Abstract: Reward modeling is crucial for aligning large language models (LLMs) with human preferences, especially in reinforcement learning from human feedback (RLHF). However, current reward models mainly produce scalar scores and struggle to incorporate critiques in a natural language format. We hypothesize that predicting both critiques and the scalar reward would improve reward modeling ability. Motivated by this, we propose Critic-RM, a framework that improves reward models using self-generated critiques without extra supervision. Critic-RM employs a two-stage process: generating and filtering high-quality critiques, followed by joint fine-tuning on reward prediction and critique generation. Experiments across benchmarks show that Critic-RM improves reward modeling accuracy by 3.7%-7.3% compared to standard reward models and LLM judges, demonstrating strong performance and data efficiency. Additional studies further validate the effectiveness of generated critiques in rectifying flawed reasoning steps with 2.5%-3.2% gains in improving reasoning accuracy.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2411.16646v2 Announce Type: replace \nAbstract: Reward modeling is crucial for aligning large language models (LLMs) with human preferences, especially in reinforcement learning from human feedback (RLHF). However, current reward models mainly produce scalar scores and struggle to incorporate critiques in a natural language format. We hypothesize that predicting both critiques and the scalar reward would improve reward modeling ability. Motivated by this, we propose Critic-RM, a framework that improves reward models using self-generated critiques without extra supervision. Critic-RM employs a two-stage process: generating and filtering high-quality critiques, followed by joint fine-tuning on reward prediction and critique generation. Experiments across benchmarks show that Critic-RM improves reward modeling accuracy by 3.7%-7.3% compared to standard reward models and LLM judges, demonstrating strong performance and data efficiency. Additional studies further validate the effectiveness of generated critiques in rectifying flawed reasoning steps with 2.5%-3.2% gains in improving reasoning accuracy.'}",oai:arXiv.org:2411.16646v2,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by-nc-sa/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-sa/4.0/'}","[{'name': 'Yue Yu, Zhengxing Chen, Aston Zhang, Liang Tan, Chenguang Zhu, Richard Yuanzhe Pang, Yundi Qian, Xuewei Wang, Suchin Gururangan, Chao Zhang, Melanie Kambadur, Dhruv Mahajan, Rui Hou'}]","Yue Yu, Zhengxing Chen, Aston Zhang, Liang Tan, Chenguang Zhu, Richard Yuanzhe Pang, Yundi Qian, Xuewei Wang, Suchin Gururangan, Chao Zhang, Melanie Kambadur, Dhruv Mahajan, Rui Hou","{'name': 'Yue Yu, Zhengxing Chen, Aston Zhang, Liang Tan, Chenguang Zhu, Richard Yuanzhe Pang, Yundi Qian, Xuewei Wang, Suchin Gururangan, Chao Zhang, Melanie Kambadur, Dhruv Mahajan, Rui Hou'}",,
669,A Clinical Trial Design Approach to Auditing Language Models in Healthcare Setting,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'A Clinical Trial Design Approach to Auditing Language Models in Healthcare Setting'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2411.16702'}]",https://arxiv.org/abs/2411.16702,"arXiv:2411.16702v2 Announce Type: replace 
Abstract: We present an audit mechanism for language models, with a focus on models deployed in the healthcare setting. Our proposed mechanism takes inspiration from clinical trial design where we posit the language model audit as a single blind equivalence trial, with the comparison of interest being the subject matter experts. We show that using our proposed method, we can follow principled sample size and power calculations, leading to the requirement of sampling minimum number of records while maintaining the audit integrity and statistical soundness. Finally, we provide a real-world example of the audit used in a production environment in a large-scale public health network.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2411.16702v2 Announce Type: replace \nAbstract: We present an audit mechanism for language models, with a focus on models deployed in the healthcare setting. Our proposed mechanism takes inspiration from clinical trial design where we posit the language model audit as a single blind equivalence trial, with the comparison of interest being the subject matter experts. We show that using our proposed method, we can follow principled sample size and power calculations, leading to the requirement of sampling minimum number of records while maintaining the audit integrity and statistical soundness. Finally, we provide a real-world example of the audit used in a production environment in a large-scale public health network.'}",oai:arXiv.org:2411.16702v2,False,"[{'term': 'cs.CY', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Lovedeep Gondara, Jonathan Simkin'}]","Lovedeep Gondara, Jonathan Simkin","{'name': 'Lovedeep Gondara, Jonathan Simkin'}",,
670,SoK: Watermarking for AI-Generated Content,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'SoK: Watermarking for AI-Generated Content'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2411.18479'}]",https://arxiv.org/abs/2411.18479,"arXiv:2411.18479v2 Announce Type: replace 
Abstract: As the outputs of generative AI (GenAI) techniques improve in quality, it becomes increasingly challenging to distinguish them from human-created content. Watermarking schemes are a promising approach to address the problem of distinguishing between AI and human-generated content. These schemes embed hidden signals within AI-generated content to enable reliable detection. While watermarking is not a silver bullet for addressing all risks associated with GenAI, it can play a crucial role in enhancing AI safety and trustworthiness by combating misinformation and deception. This paper presents a comprehensive overview of watermarking techniques for GenAI, beginning with the need for watermarking from historical and regulatory perspectives. We formalize the definitions and desired properties of watermarking schemes and examine the key objectives and threat models for existing approaches. Practical evaluation strategies are also explored, providing insights into the development of robust watermarking techniques capable of resisting various attacks. Additionally, we review recent representative works, highlight open challenges, and discuss potential directions for this emerging field. By offering a thorough understanding of watermarking in GenAI, this work aims to guide researchers in advancing watermarking methods and applications, and support policymakers in addressing the broader implications of GenAI.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2411.18479v2 Announce Type: replace \nAbstract: As the outputs of generative AI (GenAI) techniques improve in quality, it becomes increasingly challenging to distinguish them from human-created content. Watermarking schemes are a promising approach to address the problem of distinguishing between AI and human-generated content. These schemes embed hidden signals within AI-generated content to enable reliable detection. While watermarking is not a silver bullet for addressing all risks associated with GenAI, it can play a crucial role in enhancing AI safety and trustworthiness by combating misinformation and deception. This paper presents a comprehensive overview of watermarking techniques for GenAI, beginning with the need for watermarking from historical and regulatory perspectives. We formalize the definitions and desired properties of watermarking schemes and examine the key objectives and threat models for existing approaches. Practical evaluation strategies are also explored, providing insights into the development of robust watermarking techniques capable of resisting various attacks. Additionally, we review recent representative works, highlight open challenges, and discuss potential directions for this emerging field. By offering a thorough understanding of watermarking in GenAI, this work aims to guide researchers in advancing watermarking methods and applications, and support policymakers in addressing the broader implications of GenAI.'}",oai:arXiv.org:2411.18479v2,False,"[{'term': 'cs.CR', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Xuandong Zhao, Sam Gunn, Miranda Christ, Jaiden Fairoze, Andres Fabrega, Nicholas Carlini, Sanjam Garg, Sanghyun Hong, Milad Nasr, Florian Tramer, Somesh Jha, Lei Li, Yu-Xiang Wang, Dawn Song'}]","Xuandong Zhao, Sam Gunn, Miranda Christ, Jaiden Fairoze, Andres Fabrega, Nicholas Carlini, Sanjam Garg, Sanghyun Hong, Milad Nasr, Florian Tramer, Somesh Jha, Lei Li, Yu-Xiang Wang, Dawn Song","{'name': 'Xuandong Zhao, Sam Gunn, Miranda Christ, Jaiden Fairoze, Andres Fabrega, Nicholas Carlini, Sanjam Garg, Sanghyun Hong, Milad Nasr, Florian Tramer, Somesh Jha, Lei Li, Yu-Xiang Wang, Dawn Song'}",,
671,CAT4D: Create Anything in 4D with Multi-View Video Diffusion Models,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'CAT4D: Create Anything in 4D with Multi-View Video Diffusion Models'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2411.18613'}]",https://arxiv.org/abs/2411.18613,"arXiv:2411.18613v2 Announce Type: replace 
Abstract: We present CAT4D, a method for creating 4D (dynamic 3D) scenes from monocular video. CAT4D leverages a multi-view video diffusion model trained on a diverse combination of datasets to enable novel view synthesis at any specified camera poses and timestamps. Combined with a novel sampling approach, this model can transform a single monocular video into a multi-view video, enabling robust 4D reconstruction via optimization of a deformable 3D Gaussian representation. We demonstrate competitive performance on novel view synthesis and dynamic scene reconstruction benchmarks, and highlight the creative capabilities for 4D scene generation from real or generated videos. See our project page for results and interactive demos: https://cat-4d.github.io/.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2411.18613v2 Announce Type: replace \nAbstract: We present CAT4D, a method for creating 4D (dynamic 3D) scenes from monocular video. CAT4D leverages a multi-view video diffusion model trained on a diverse combination of datasets to enable novel view synthesis at any specified camera poses and timestamps. Combined with a novel sampling approach, this model can transform a single monocular video into a multi-view video, enabling robust 4D reconstruction via optimization of a deformable 3D Gaussian representation. We demonstrate competitive performance on novel view synthesis and dynamic scene reconstruction benchmarks, and highlight the creative capabilities for 4D scene generation from real or generated videos. See our project page for results and interactive demos: https://cat-4d.github.io/.'}",oai:arXiv.org:2411.18613v2,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Rundi Wu, Ruiqi Gao, Ben Poole, Alex Trevithick, Changxi Zheng, Jonathan T. Barron, Aleksander Holynski'}]","Rundi Wu, Ruiqi Gao, Ben Poole, Alex Trevithick, Changxi Zheng, Jonathan T. Barron, Aleksander Holynski","{'name': 'Rundi Wu, Ruiqi Gao, Ben Poole, Alex Trevithick, Changxi Zheng, Jonathan T. Barron, Aleksander Holynski'}",,
672,PEFT-as-an-Attack! Jailbreaking Language Models during Federated Parameter-Efficient Fine-Tuning,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'PEFT-as-an-Attack! Jailbreaking Language Models during Federated Parameter-Efficient Fine-Tuning'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2411.19335'}]",https://arxiv.org/abs/2411.19335,"arXiv:2411.19335v2 Announce Type: replace 
Abstract: Federated Parameter-Efficient Fine-Tuning (FedPEFT) has emerged as a promising paradigm for privacy-preserving and efficient adaptation of Pre-trained Language Models (PLMs) in Federated Learning (FL) settings. It preserves data privacy by keeping the data decentralized and training the model on local devices, ensuring that raw data never leaves the user's device. Moreover, the integration of PEFT methods such as LoRA significantly reduces the number of trainable parameters compared to fine-tuning the entire model, thereby minimizing communication costs and computational overhead. Despite its potential, the security implications of FedPEFT remain underexplored. This paper introduces a novel security threat to FedPEFT, termed PEFT-as-an-Attack (PaaA), which exposes how PEFT can be exploited as an attack vector to circumvent PLMs' safety alignment and generate harmful content in response to malicious prompts. Our evaluation of PaaA reveals that with less than 1% of the model's parameters set as trainable, and a small subset of clients acting maliciously, the attack achieves an approximate 80% attack success rate using representative PEFT methods such as LoRA. To mitigate this threat, we further investigate potential defense strategies, including Robust Aggregation Schemes (RASs) and Post-PEFT Safety Alignment (PPSA). However, our empirical analysis highlights the limitations of these defenses, i.e., even the most advanced RASs, such as DnC and ClippedClustering, struggle to defend against PaaA in scenarios with highly heterogeneous data distributions. Similarly, while PPSA can reduce attack success rates to below 10%, it severely degrades the model's accuracy on the target task. Our results underscore the urgent need for more effective defense mechanisms that simultaneously ensure security and maintain the performance of the FedPEFT paradigm.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2411.19335v2 Announce Type: replace \nAbstract: Federated Parameter-Efficient Fine-Tuning (FedPEFT) has emerged as a promising paradigm for privacy-preserving and efficient adaptation of Pre-trained Language Models (PLMs) in Federated Learning (FL) settings. It preserves data privacy by keeping the data decentralized and training the model on local devices, ensuring that raw data never leaves the user's device. Moreover, the integration of PEFT methods such as LoRA significantly reduces the number of trainable parameters compared to fine-tuning the entire model, thereby minimizing communication costs and computational overhead. Despite its potential, the security implications of FedPEFT remain underexplored. This paper introduces a novel security threat to FedPEFT, termed PEFT-as-an-Attack (PaaA), which exposes how PEFT can be exploited as an attack vector to circumvent PLMs' safety alignment and generate harmful content in response to malicious prompts. Our evaluation of PaaA reveals that with less than 1% of the model's parameters set as trainable, and a small subset of clients acting maliciously, the attack achieves an approximate 80% attack success rate using representative PEFT methods such as LoRA. To mitigate this threat, we further investigate potential defense strategies, including Robust Aggregation Schemes (RASs) and Post-PEFT Safety Alignment (PPSA). However, our empirical analysis highlights the limitations of these defenses, i.e., even the most advanced RASs, such as DnC and ClippedClustering, struggle to defend against PaaA in scenarios with highly heterogeneous data distributions. Similarly, while PPSA can reduce attack success rates to below 10%, it severely degrades the model's accuracy on the target task. Our results underscore the urgent need for more effective defense mechanisms that simultaneously ensure security and maintain the performance of the FedPEFT paradigm.""}",oai:arXiv.org:2411.19335v2,False,"[{'term': 'cs.CR', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Shenghui Li, Edith C. -H. Ngai, Fanghua Ye, Thiemo Voigt'}]","Shenghui Li, Edith C. -H. Ngai, Fanghua Ye, Thiemo Voigt","{'name': 'Shenghui Li, Edith C. -H. Ngai, Fanghua Ye, Thiemo Voigt'}",,
673,KnowledgePrompts: Exploring the Abilities of Large Language Models to Solve Proportional Analogies via Knowledge-Enhanced Prompting,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'KnowledgePrompts: Exploring the Abilities of Large Language Models to Solve Proportional Analogies via Knowledge-Enhanced Prompting'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.00869'}]",https://arxiv.org/abs/2412.00869,"arXiv:2412.00869v2 Announce Type: replace 
Abstract: Making analogies is fundamental to cognition. Proportional analogies, which consist of four terms, are often used to assess linguistic and cognitive abilities. For instance, completing analogies like ""Oxygen is to Gas as  is to "" requires identifying the semantic relationship (e.g., ""type of"") between the first pair of terms (""Oxygen"" and ""Gas"") and finding a second pair that shares the same relationship (e.g., ""Aluminum"" and ""Metal""). In this work, we introduce a 15K Multiple-Choice Question Answering (MCQA) dataset for proportional analogy completion and evaluate the performance of contemporary Large Language Models (LLMs) in various knowledge-enhanced prompt settings. Specifically, we augment prompts with three types of knowledge: exemplar, structured, and targeted. Our results show that despite extensive training data, solving proportional analogies remains challenging for current LLMs, with the best model achieving an accuracy of 55%. Notably, we find that providing targeted knowledge can better assist models in completing proportional analogies compared to providing exemplars or collections of structured knowledge. Our code and data are available at: https://github.com/Thiliniiw/KnowledgePrompts/","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.00869v2 Announce Type: replace \nAbstract: Making analogies is fundamental to cognition. Proportional analogies, which consist of four terms, are often used to assess linguistic and cognitive abilities. For instance, completing analogies like ""Oxygen is to Gas as  is to "" requires identifying the semantic relationship (e.g., ""type of"") between the first pair of terms (""Oxygen"" and ""Gas"") and finding a second pair that shares the same relationship (e.g., ""Aluminum"" and ""Metal""). In this work, we introduce a 15K Multiple-Choice Question Answering (MCQA) dataset for proportional analogy completion and evaluate the performance of contemporary Large Language Models (LLMs) in various knowledge-enhanced prompt settings. Specifically, we augment prompts with three types of knowledge: exemplar, structured, and targeted. Our results show that despite extensive training data, solving proportional analogies remains challenging for current LLMs, with the best model achieving an accuracy of 55%. Notably, we find that providing targeted knowledge can better assist models in completing proportional analogies compared to providing exemplars or collections of structured knowledge. Our code and data are available at: https://github.com/Thiliniiw/KnowledgePrompts/'}",oai:arXiv.org:2412.00869v2,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by-nc-sa/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-sa/4.0/'}","[{'name': 'Thilini Wijesiriwardene, Ruwan Wickramarachchi, Sreeram Vennam, Vinija Jain, Aman Chadha, Amitava Das, Ponnurangam Kumaraguru, Amit Sheth'}]","Thilini Wijesiriwardene, Ruwan Wickramarachchi, Sreeram Vennam, Vinija Jain, Aman Chadha, Amitava Das, Ponnurangam Kumaraguru, Amit Sheth","{'name': 'Thilini Wijesiriwardene, Ruwan Wickramarachchi, Sreeram Vennam, Vinija Jain, Aman Chadha, Amitava Das, Ponnurangam Kumaraguru, Amit Sheth'}",,
674,Su-RoBERTa: A Semi-supervised Approach to Predicting Suicide Risk through Social Media using Base Language Models,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Su-RoBERTa: A Semi-supervised Approach to Predicting Suicide Risk through Social Media using Base Language Models'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.01353'}]",https://arxiv.org/abs/2412.01353,"arXiv:2412.01353v2 Announce Type: replace 
Abstract: In recent times, more and more people are posting about their mental states across various social media platforms. Leveraging this data, AI-based systems can be developed that help in assessing the mental health of individuals, such as suicide risk. This paper is a study done on suicidal risk assessments using Reddit data leveraging Base language models to identify patterns from social media posts. We have demonstrated that using smaller language models, i.e., less than 500M parameters, can also be effective in contrast to LLMs with greater than 500M parameters. We propose Su-RoBERTa, a fine-tuned RoBERTa on suicide risk prediction task that utilized both the labeled and unlabeled Reddit data and tackled class imbalance by data augmentation using GPT-2 model. Our Su-RoBERTa model attained a 69.84% weighted F1 score during the Final evaluation. This paper demonstrates the effectiveness of Base language models for the analysis of the risk factors related to mental health with an efficient computation pipeline","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.01353v2 Announce Type: replace \nAbstract: In recent times, more and more people are posting about their mental states across various social media platforms. Leveraging this data, AI-based systems can be developed that help in assessing the mental health of individuals, such as suicide risk. This paper is a study done on suicidal risk assessments using Reddit data leveraging Base language models to identify patterns from social media posts. We have demonstrated that using smaller language models, i.e., less than 500M parameters, can also be effective in contrast to LLMs with greater than 500M parameters. We propose Su-RoBERTa, a fine-tuned RoBERTa on suicide risk prediction task that utilized both the labeled and unlabeled Reddit data and tackled class imbalance by data augmentation using GPT-2 model. Our Su-RoBERTa model attained a 69.84% weighted F1 score during the Final evaluation. This paper demonstrates the effectiveness of Base language models for the analysis of the risk factors related to mental health with an efficient computation pipeline'}",oai:arXiv.org:2412.01353v2,False,"[{'term': 'cs.HC', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.SI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Chayan Tank, Shaina Mehta, Sarthak Pol, Vinayak Katoch, Avinash Anand, Raj Jaiswal, Rajiv Ratn Shah'}]","Chayan Tank, Shaina Mehta, Sarthak Pol, Vinayak Katoch, Avinash Anand, Raj Jaiswal, Rajiv Ratn Shah","{'name': 'Chayan Tank, Shaina Mehta, Sarthak Pol, Vinayak Katoch, Avinash Anand, Raj Jaiswal, Rajiv Ratn Shah'}",,
675,Task Adaptation of Reinforcement Learning-based NAS Agents through Transfer Learning,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Task Adaptation of Reinforcement Learning-based NAS Agents through Transfer Learning'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.01420'}]",https://arxiv.org/abs/2412.01420,"arXiv:2412.01420v2 Announce Type: replace 
Abstract: Recently, a novel paradigm has been proposed for reinforcement learning-based NAS agents, that revolves around the incremental improvement of a given architecture. We assess the abilities of such reinforcement learning agents to transfer between different tasks. We perform our evaluation using the Trans-NASBench-101 benchmark, and consider the efficacy of the transferred agents, as well as how quickly they can be trained. We find that pretraining an agent on one task benefits the performance of the agent in another task in all but 1 task when considering final performance. We also show that the training procedure for an agent can be shortened significantly by pretraining it on another task. Our results indicate that these effects occur regardless of the source or target task, although they are more pronounced for some tasks than for others. Our results show that transfer learning can be an effective tool in mitigating the computational cost of the initial training procedure for reinforcement learning-based NAS agents.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.01420v2 Announce Type: replace \nAbstract: Recently, a novel paradigm has been proposed for reinforcement learning-based NAS agents, that revolves around the incremental improvement of a given architecture. We assess the abilities of such reinforcement learning agents to transfer between different tasks. We perform our evaluation using the Trans-NASBench-101 benchmark, and consider the efficacy of the transferred agents, as well as how quickly they can be trained. We find that pretraining an agent on one task benefits the performance of the agent in another task in all but 1 task when considering final performance. We also show that the training procedure for an agent can be shortened significantly by pretraining it on another task. Our results indicate that these effects occur regardless of the source or target task, although they are more pronounced for some tasks than for others. Our results show that transfer learning can be an effective tool in mitigating the computational cost of the initial training procedure for reinforcement learning-based NAS agents.'}",oai:arXiv.org:2412.01420v2,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Amber Cassimon, Siegfried Mercelis, Kevin Mets'}]","Amber Cassimon, Siegfried Mercelis, Kevin Mets","{'name': 'Amber Cassimon, Siegfried Mercelis, Kevin Mets'}",,
676,Sometimes I am a Tree: Data Drives Unstable Hierarchical Generalization,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Sometimes I am a Tree: Data Drives Unstable Hierarchical Generalization'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.04619'}]",https://arxiv.org/abs/2412.04619,"arXiv:2412.04619v3 Announce Type: replace 
Abstract: Language models (LMs), like other neural networks, often favor shortcut heuristics based on surface-level patterns. Although LMs behave like n-gram models early in training, they must eventually learn hierarchical syntactic representations to correctly apply grammatical rules out-of-distribution (OOD). In this work, we use case studies of English grammar to explore how complex, diverse training data drives models to generalize OOD. We construct a framework that unifies our understanding of random variation with training dynamics, rule selection with memorization, and data diversity with complexity. We show that these factors are nuanced, and that intermediate levels of diversity and complexity lead to inconsistent behavior across random seeds and to unstable training dynamics. Our findings emphasize the critical role of training data in shaping generalization patterns and illuminate how competing model strategies lead to inconsistent generalization outcomes across random seeds. Code is available at https://github.com/sunnytqin/concept_comp.git.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.04619v3 Announce Type: replace \nAbstract: Language models (LMs), like other neural networks, often favor shortcut heuristics based on surface-level patterns. Although LMs behave like n-gram models early in training, they must eventually learn hierarchical syntactic representations to correctly apply grammatical rules out-of-distribution (OOD). In this work, we use case studies of English grammar to explore how complex, diverse training data drives models to generalize OOD. We construct a framework that unifies our understanding of random variation with training dynamics, rule selection with memorization, and data diversity with complexity. We show that these factors are nuanced, and that intermediate levels of diversity and complexity lead to inconsistent behavior across random seeds and to unstable training dynamics. Our findings emphasize the critical role of training data in shaping generalization patterns and illuminate how competing model strategies lead to inconsistent generalization outcomes across random seeds. Code is available at https://github.com/sunnytqin/concept_comp.git.'}",oai:arXiv.org:2412.04619v3,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'cs.CL', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Tian Qin, Naomi Saphra, David Alvarez-Melis'}]","Tian Qin, Naomi Saphra, David Alvarez-Melis","{'name': 'Tian Qin, Naomi Saphra, David Alvarez-Melis'}",,
677,PoLaRIS Dataset: A Maritime Object Detection and Tracking Dataset in Pohang Canal,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'PoLaRIS Dataset: A Maritime Object Detection and Tracking Dataset in Pohang Canal'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.06192'}]",https://arxiv.org/abs/2412.06192,"arXiv:2412.06192v2 Announce Type: replace 
Abstract: Maritime environments often present hazardous situations due to factors such as moving ships or buoys, which become obstacles under the influence of waves. In such challenging conditions, the ability to detect and track potentially hazardous objects is critical for the safe navigation of marine robots. To address the scarcity of comprehensive datasets capturing these dynamic scenarios, we introduce a new multi-modal dataset that includes image and point-wise annotations of maritime hazards. Our dataset provides detailed ground truth for obstacle detection and tracking, including objects as small as 10$\times$10 pixels, which are crucial for maritime safety. To validate the dataset's effectiveness as a reliable benchmark, we conducted evaluations using various methodologies, including \ac{SOTA} techniques for object detection and tracking. These evaluations are expected to contribute to performance improvements, particularly in the complex maritime environment. To the best of our knowledge, this is the first dataset offering multi-modal annotations specifically tailored to maritime environments. Our dataset is available at https://sites.google.com/view/polaris-dataset.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.06192v2 Announce Type: replace \nAbstract: Maritime environments often present hazardous situations due to factors such as moving ships or buoys, which become obstacles under the influence of waves. In such challenging conditions, the ability to detect and track potentially hazardous objects is critical for the safe navigation of marine robots. To address the scarcity of comprehensive datasets capturing these dynamic scenarios, we introduce a new multi-modal dataset that includes image and point-wise annotations of maritime hazards. Our dataset provides detailed ground truth for obstacle detection and tracking, including objects as small as 10$\\times$10 pixels, which are crucial for maritime safety. To validate the dataset's effectiveness as a reliable benchmark, we conducted evaluations using various methodologies, including \\ac{SOTA} techniques for object detection and tracking. These evaluations are expected to contribute to performance improvements, particularly in the complex maritime environment. To the best of our knowledge, this is the first dataset offering multi-modal annotations specifically tailored to maritime environments. Our dataset is available at https://sites.google.com/view/polaris-dataset.""}",oai:arXiv.org:2412.06192v2,False,"[{'term': 'cs.RO', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by-nc-sa/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-sa/4.0/'}","[{'name': 'Jiwon Choi, Dongjin Cho, Gihyeon Lee, Hogyun Kim, Geonmo Yang, Joowan Kim, Younggun Cho'}]","Jiwon Choi, Dongjin Cho, Gihyeon Lee, Hogyun Kim, Geonmo Yang, Joowan Kim, Younggun Cho","{'name': 'Jiwon Choi, Dongjin Cho, Gihyeon Lee, Hogyun Kim, Geonmo Yang, Joowan Kim, Younggun Cho'}",,
678,Fearless Unsafe. A More User-friendly Document for Unsafe Rust Programming Base on Refined Safety Properties,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Fearless Unsafe. A More User-friendly Document for Unsafe Rust Programming Base on Refined Safety Properties'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.06251'}]",https://arxiv.org/abs/2412.06251,"arXiv:2412.06251v2 Announce Type: replace 
Abstract: Rust, a popular systems-level programming language, has garnered widespread attention due to its features of achieving run-time efficiency and memory safety. With an increasing number of real-world projects adopting Rust, understanding how to assist programmers in correctly writing unsafe code poses a significant challenge. Based on our observations, the current standard library has many unsafe APIs, but their descriptions are not uniform, complete, and intuitive, especially in describing safety requirements. Therefore, we advocate establishing a systematic category of safety requirements for revising those documents.
  In this paper, we extended and refined our study in ICSE 2024. We defined a category of Safety Properties (22 items in total) that learned from the documents of unsafe APIs in the standard library. Then, we labeled all public unsafe APIs (438 in total) and analyzed their correlations. Based on the safety properties, we reorganized all the unsafe documents in the standard library and designed a consultation plugin into rust-analyzer as a complementary tool to assist Rust developers in writing unsafe code. To validate the practical significance, we categorized the root causes of all Rust CVEs up to 2024-01-31 (419 in total) into safety properties and further counted the real-world usage of unsafe APIs in the crates.io ecosystem.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.06251v2 Announce Type: replace \nAbstract: Rust, a popular systems-level programming language, has garnered widespread attention due to its features of achieving run-time efficiency and memory safety. With an increasing number of real-world projects adopting Rust, understanding how to assist programmers in correctly writing unsafe code poses a significant challenge. Based on our observations, the current standard library has many unsafe APIs, but their descriptions are not uniform, complete, and intuitive, especially in describing safety requirements. Therefore, we advocate establishing a systematic category of safety requirements for revising those documents.\n  In this paper, we extended and refined our study in ICSE 2024. We defined a category of Safety Properties (22 items in total) that learned from the documents of unsafe APIs in the standard library. Then, we labeled all public unsafe APIs (438 in total) and analyzed their correlations. Based on the safety properties, we reorganized all the unsafe documents in the standard library and designed a consultation plugin into rust-analyzer as a complementary tool to assist Rust developers in writing unsafe code. To validate the practical significance, we categorized the root causes of all Rust CVEs up to 2024-01-31 (419 in total) into safety properties and further counted the real-world usage of unsafe APIs in the crates.io ecosystem.'}",oai:arXiv.org:2412.06251v2,False,"[{'term': 'cs.SE', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Mohan Cui, Penglei Mao, Shuran Sun, Yangfan Zhou, Hui Xu'}]","Mohan Cui, Penglei Mao, Shuran Sun, Yangfan Zhou, Hui Xu","{'name': 'Mohan Cui, Penglei Mao, Shuran Sun, Yangfan Zhou, Hui Xu'}",,
679,"S$^{2}$FT: Efficient, Scalable and Generalizable LLM Fine-tuning by Structured Sparsity","{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'S$^{2}$FT: Efficient, Scalable and Generalizable LLM Fine-tuning by Structured Sparsity'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.06289'}]",https://arxiv.org/abs/2412.06289,"arXiv:2412.06289v3 Announce Type: replace 
Abstract: Current PEFT methods for LLMs can achieve either high quality, efficient training, or scalable serving, but not all three simultaneously. To address this limitation, we investigate sparse fine-tuning and observe a remarkable improvement in generalization ability. Utilizing this key insight, we propose a family of Structured Sparse Fine-Tuning (S$^{2}$FT) methods for LLMs, which concurrently achieve state-of-the-art fine-tuning performance, training efficiency, and inference scalability. S$^{2}$FT accomplishes this by ""selecting sparsely and computing densely"". It selects a few heads and channels in the MHA and FFN modules for each Transformer block, respectively. Next, it co-permutes weight matrices on both sides of the coupled structures in LLMs to connect the selected components in each layer into a dense submatrix. Finally, S$^{2}$FT performs in-place gradient updates on all submatrices. Through theoretical analysis and empirical results, our method prevents forgetting while simplifying optimization, delivers SOTA performance on both commonsense and arithmetic reasoning with 4.6% and 1.3% average improvements compared to LoRA, and surpasses full FT by 11.5% when generalizing to various domains after instruction tuning. Using our partial backpropagation algorithm, S$^{2}$FT saves training memory up to 3$\times$ and improves latency by 1.5-2.7$\times$ compared to full FT, while delivering an average 10% improvement over LoRA on both metrics. We further demonstrate that the weight updates in S$^{2}$FT can be decoupled into adapters, enabling effective fusion, fast switch, and efficient parallelism for serving multiple fine-tuned models.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.06289v3 Announce Type: replace \nAbstract: Current PEFT methods for LLMs can achieve either high quality, efficient training, or scalable serving, but not all three simultaneously. To address this limitation, we investigate sparse fine-tuning and observe a remarkable improvement in generalization ability. Utilizing this key insight, we propose a family of Structured Sparse Fine-Tuning (S$^{2}$FT) methods for LLMs, which concurrently achieve state-of-the-art fine-tuning performance, training efficiency, and inference scalability. S$^{2}$FT accomplishes this by ""selecting sparsely and computing densely"". It selects a few heads and channels in the MHA and FFN modules for each Transformer block, respectively. Next, it co-permutes weight matrices on both sides of the coupled structures in LLMs to connect the selected components in each layer into a dense submatrix. Finally, S$^{2}$FT performs in-place gradient updates on all submatrices. Through theoretical analysis and empirical results, our method prevents forgetting while simplifying optimization, delivers SOTA performance on both commonsense and arithmetic reasoning with 4.6% and 1.3% average improvements compared to LoRA, and surpasses full FT by 11.5% when generalizing to various domains after instruction tuning. Using our partial backpropagation algorithm, S$^{2}$FT saves training memory up to 3$\\times$ and improves latency by 1.5-2.7$\\times$ compared to full FT, while delivering an average 10% improvement over LoRA on both metrics. We further demonstrate that the weight updates in S$^{2}$FT can be decoupled into adapters, enabling effective fusion, fast switch, and efficient parallelism for serving multiple fine-tuned models.'}",oai:arXiv.org:2412.06289v3,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Xinyu Yang, Jixuan Leng, Geyang Guo, Jiawei Zhao, Ryumei Nakada, Linjun Zhang, Huaxiu Yao, Beidi Chen'}]","Xinyu Yang, Jixuan Leng, Geyang Guo, Jiawei Zhao, Ryumei Nakada, Linjun Zhang, Huaxiu Yao, Beidi Chen","{'name': 'Xinyu Yang, Jixuan Leng, Geyang Guo, Jiawei Zhao, Ryumei Nakada, Linjun Zhang, Huaxiu Yao, Beidi Chen'}",,
680,When Every Token Counts: Optimal Segmentation for Low-Resource Language Models,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'When Every Token Counts: Optimal Segmentation for Low-Resource Language Models'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.06926'}]",https://arxiv.org/abs/2412.06926,"arXiv:2412.06926v3 Announce Type: replace 
Abstract: Traditional greedy tokenization methods have been a critical step in Natural Language Processing (NLP), influencing how text is converted into tokens and directly impacting model performance. While subword tokenizers like Byte-Pair Encoding (BPE) are widely used, questions remain about their optimality across model scales and languages. In this work, we demonstrate through extensive experiments that an optimal BPE configuration significantly reduces token count compared to greedy segmentation, yielding improvements in token-saving percentages and performance benefits, particularly for smaller models. We evaluate tokenization performance across various intrinsic and extrinsic tasks, including generation and classification. Our findings suggest that compression-optimized tokenization strategies could provide substantial advantages for multilingual and low-resource language applications, highlighting a promising direction for further research and inclusive NLP.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.06926v3 Announce Type: replace \nAbstract: Traditional greedy tokenization methods have been a critical step in Natural Language Processing (NLP), influencing how text is converted into tokens and directly impacting model performance. While subword tokenizers like Byte-Pair Encoding (BPE) are widely used, questions remain about their optimality across model scales and languages. In this work, we demonstrate through extensive experiments that an optimal BPE configuration significantly reduces token count compared to greedy segmentation, yielding improvements in token-saving percentages and performance benefits, particularly for smaller models. We evaluate tokenization performance across various intrinsic and extrinsic tasks, including generation and classification. Our findings suggest that compression-optimized tokenization strategies could provide substantial advantages for multilingual and low-resource language applications, highlighting a promising direction for further research and inclusive NLP.'}",oai:arXiv.org:2412.06926v3,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Bharath Raj S, Garvit Suri, Vikrant Dewangan, Raghav Sonavane'}]","Bharath Raj S, Garvit Suri, Vikrant Dewangan, Raghav Sonavane","{'name': 'Bharath Raj S, Garvit Suri, Vikrant Dewangan, Raghav Sonavane'}",,
681,Optimizing Personalized Federated Learning through Adaptive Layer-Wise Learning,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Optimizing Personalized Federated Learning through Adaptive Layer-Wise Learning'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.07062'}]",https://arxiv.org/abs/2412.07062,"arXiv:2412.07062v2 Announce Type: replace 
Abstract: Real-life deployment of federated Learning (FL) often faces non-IID data, which leads to poor accuracy and slow convergence. Personalized FL (pFL) tackles these issues by tailoring local models to individual data sources and using weighted aggregation methods for client-specific learning. However, existing pFL methods often fail to provide each local model with global knowledge on demand while maintaining low computational overhead. Additionally, local models tend to over-personalize their data during the training process, potentially dropping previously acquired global information. We propose FLAYER, a novel layer-wise learning method for pFL that optimizes local model personalization performance. FLAYER considers the different roles and learning abilities of neural network layers of individual local models. It incorporates global information for each local model as needed to initialize the local model cost-effectively. It then dynamically adjusts learning rates for each layer during local training, optimizing the personalized learning process for each local model while preserving global knowledge. Additionally, to enhance global representation in pFL, FLAYER selectively uploads parameters for global aggregation in a layer-wise manner. We evaluate FLAYER on four representative datasets in computer vision and natural language processing domains. Compared to six state-of-the-art pFL methods, FLAYER improves the inference accuracy, on average, by 7.21\% (up to 14.29\%).","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.07062v2 Announce Type: replace \nAbstract: Real-life deployment of federated Learning (FL) often faces non-IID data, which leads to poor accuracy and slow convergence. Personalized FL (pFL) tackles these issues by tailoring local models to individual data sources and using weighted aggregation methods for client-specific learning. However, existing pFL methods often fail to provide each local model with global knowledge on demand while maintaining low computational overhead. Additionally, local models tend to over-personalize their data during the training process, potentially dropping previously acquired global information. We propose FLAYER, a novel layer-wise learning method for pFL that optimizes local model personalization performance. FLAYER considers the different roles and learning abilities of neural network layers of individual local models. It incorporates global information for each local model as needed to initialize the local model cost-effectively. It then dynamically adjusts learning rates for each layer during local training, optimizing the personalized learning process for each local model while preserving global knowledge. Additionally, to enhance global representation in pFL, FLAYER selectively uploads parameters for global aggregation in a layer-wise manner. We evaluate FLAYER on four representative datasets in computer vision and natural language processing domains. Compared to six state-of-the-art pFL methods, FLAYER improves the inference accuracy, on average, by 7.21\\% (up to 14.29\\%).'}",oai:arXiv.org:2412.07062v2,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Weihang Chen, Jie Ren, Zhiqiang Li, Ling Gao, Zheng Wang'}]","Weihang Chen, Jie Ren, Zhiqiang Li, Ling Gao, Zheng Wang","{'name': 'Weihang Chen, Jie Ren, Zhiqiang Li, Ling Gao, Zheng Wang'}",,
682,Multi-Scale Contrastive Learning for Video Temporal Grounding,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Multi-Scale Contrastive Learning for Video Temporal Grounding'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.07157'}]",https://arxiv.org/abs/2412.07157,"arXiv:2412.07157v2 Announce Type: replace 
Abstract: Temporal grounding, which localizes video moments related to a natural language query, is a core problem of vision-language learning and video understanding. To encode video moments of varying lengths, recent methods employ a multi-level structure known as a feature pyramid. In this structure, lower levels concentrate on short-range video moments, while higher levels address long-range moments. Because higher levels experience downsampling to accommodate increasing moment length, their capacity to capture information is reduced and consequently leads to degraded information in moment representations. To resolve this problem, we propose a contrastive learning framework to capture salient semantics among video moments. Our key methodology is to leverage samples from the feature space emanating from multiple stages of the video encoder itself requiring neither data augmentation nor online memory banks to obtain positive and negative samples. To enable such an extension, we introduce a sampling process to draw multiple video moments corresponding to a common query. Subsequently, by utilizing these moments' representations across video encoder layers, we instantiate a novel form of multi-scale and cross-scale contrastive learning that links local short-range video moments with global long-range video moments. Extensive experiments demonstrate the effectiveness of our framework for not only long-form but also short-form video grounding.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.07157v2 Announce Type: replace \nAbstract: Temporal grounding, which localizes video moments related to a natural language query, is a core problem of vision-language learning and video understanding. To encode video moments of varying lengths, recent methods employ a multi-level structure known as a feature pyramid. In this structure, lower levels concentrate on short-range video moments, while higher levels address long-range moments. Because higher levels experience downsampling to accommodate increasing moment length, their capacity to capture information is reduced and consequently leads to degraded information in moment representations. To resolve this problem, we propose a contrastive learning framework to capture salient semantics among video moments. Our key methodology is to leverage samples from the feature space emanating from multiple stages of the video encoder itself requiring neither data augmentation nor online memory banks to obtain positive and negative samples. To enable such an extension, we introduce a sampling process to draw multiple video moments corresponding to a common query. Subsequently, by utilizing these moments' representations across video encoder layers, we instantiate a novel form of multi-scale and cross-scale contrastive learning that links local short-range video moments with global long-range video moments. Extensive experiments demonstrate the effectiveness of our framework for not only long-form but also short-form video grounding.""}",oai:arXiv.org:2412.07157v2,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by-sa/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-sa/4.0/'}","[{'name': 'Thong Thanh Nguyen, Yi Bin, Xiaobao Wu, Zhiyuan Hu, Cong-Duy T Nguyen, See-Kiong Ng, Anh Tuan Luu'}]","Thong Thanh Nguyen, Yi Bin, Xiaobao Wu, Zhiyuan Hu, Cong-Duy T Nguyen, See-Kiong Ng, Anh Tuan Luu","{'name': 'Thong Thanh Nguyen, Yi Bin, Xiaobao Wu, Zhiyuan Hu, Cong-Duy T Nguyen, See-Kiong Ng, Anh Tuan Luu'}",,
683,Motion-aware Contrastive Learning for Temporal Panoptic Scene Graph Generation,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Motion-aware Contrastive Learning for Temporal Panoptic Scene Graph Generation'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.07160'}]",https://arxiv.org/abs/2412.07160,"arXiv:2412.07160v2 Announce Type: replace 
Abstract: To equip artificial intelligence with a comprehensive understanding towards a temporal world, video and 4D panoptic scene graph generation abstracts visual data into nodes to represent entities and edges to capture temporal relations. Existing methods encode entity masks tracked across temporal dimensions (mask tubes), then predict their relations with temporal pooling operation, which does not fully utilize the motion indicative of the entities' relation. To overcome this limitation, we introduce a contrastive representation learning framework that focuses on motion pattern for temporal scene graph generation. Firstly, our framework encourages the model to learn close representations for mask tubes of similar subject-relation-object triplets. Secondly, we seek to push apart mask tubes from their temporally shuffled versions. Moreover, we also learn distant representations for mask tubes belonging to the same video but different triplets. Extensive experiments show that our motion-aware contrastive framework significantly improves state-of-the-art methods on both video and 4D datasets.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.07160v2 Announce Type: replace \nAbstract: To equip artificial intelligence with a comprehensive understanding towards a temporal world, video and 4D panoptic scene graph generation abstracts visual data into nodes to represent entities and edges to capture temporal relations. Existing methods encode entity masks tracked across temporal dimensions (mask tubes), then predict their relations with temporal pooling operation, which does not fully utilize the motion indicative of the entities' relation. To overcome this limitation, we introduce a contrastive representation learning framework that focuses on motion pattern for temporal scene graph generation. Firstly, our framework encourages the model to learn close representations for mask tubes of similar subject-relation-object triplets. Secondly, we seek to push apart mask tubes from their temporally shuffled versions. Moreover, we also learn distant representations for mask tubes belonging to the same video but different triplets. Extensive experiments show that our motion-aware contrastive framework significantly improves state-of-the-art methods on both video and 4D datasets.""}",oai:arXiv.org:2412.07160v2,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Thong Thanh Nguyen, Xiaobao Wu, Yi Bin, Cong-Duy T Nguyen, See-Kiong Ng, Anh Tuan Luu'}]","Thong Thanh Nguyen, Xiaobao Wu, Yi Bin, Cong-Duy T Nguyen, See-Kiong Ng, Anh Tuan Luu","{'name': 'Thong Thanh Nguyen, Xiaobao Wu, Yi Bin, Cong-Duy T Nguyen, See-Kiong Ng, Anh Tuan Luu'}",,
684,Reconciling Human Development and Giant Panda Protection Goals: Cost-efficiency Evaluation of Farmland Reverting and Energy Substitution Programs in Wolong National Reserve,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Reconciling Human Development and Giant Panda Protection Goals: Cost-efficiency Evaluation of Farmland Reverting and Energy Substitution Programs in Wolong National Reserve'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.07275'}]",https://arxiv.org/abs/2412.07275,"arXiv:2412.07275v3 Announce Type: replace 
Abstract: Balancing human development with conservation necessitates ecological policies that optimize outcomes within limited budgets, highlighting the importance of cost-efficiency and local impact analysis. This study employs the Socio-Econ-Ecosystem Multipurpose Simulator (SEEMS), an Agent-Based Model (ABM) designed for simulating small-scale Coupled Human and Nature Systems (CHANS), to evaluate the cost-efficiency of two major ecology conservation programs: Grain-to-Green (G2G) and Firewood-to-Electricity (F2E). Focusing on China Wolong National Reserve, a worldwide hot spot for flagship species conservation, the study evaluates the direct benefits of these programs, including reverted farmland area and firewood consumption, along with their combined indirect benefits on habitat quality, carbon emissions, and gross economic benefits. The findings are as follows: (1) The G2G program achieves optimal financial efficiency at approximately 500 CNY/Mu, with diminishing returns observed beyond 1000 CNY/Mu; (2) For the F2E program, the most fiscally cost-efficient option arises when the subsidized electricity price is at 0.4-0.5 CNY/kWh, while further reductions of the prices to below 0.1 CNY/kWh result in a diminishing cost-benefit ratio; (3) Comprehensive cost-efficiency analysis reveals no significant link between financial burden and carbon emissions, but a positive correlation with habitat quality and an inverted U-shaped relationship with total economic income; (4) Pareto analysis identifies 18 optimal dual-policy combinations for balancing carbon footprint, habitat quality, and gross economic benefits; (5) Posterior Pareto optimization further refines the selection of a specific policy scheme for a given realistic scenario. The analytical framework of this paper helps policymakers design economically viable and environmentally sustainable policies, addressing global conservation challenges.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.07275v3 Announce Type: replace \nAbstract: Balancing human development with conservation necessitates ecological policies that optimize outcomes within limited budgets, highlighting the importance of cost-efficiency and local impact analysis. This study employs the Socio-Econ-Ecosystem Multipurpose Simulator (SEEMS), an Agent-Based Model (ABM) designed for simulating small-scale Coupled Human and Nature Systems (CHANS), to evaluate the cost-efficiency of two major ecology conservation programs: Grain-to-Green (G2G) and Firewood-to-Electricity (F2E). Focusing on China Wolong National Reserve, a worldwide hot spot for flagship species conservation, the study evaluates the direct benefits of these programs, including reverted farmland area and firewood consumption, along with their combined indirect benefits on habitat quality, carbon emissions, and gross economic benefits. The findings are as follows: (1) The G2G program achieves optimal financial efficiency at approximately 500 CNY/Mu, with diminishing returns observed beyond 1000 CNY/Mu; (2) For the F2E program, the most fiscally cost-efficient option arises when the subsidized electricity price is at 0.4-0.5 CNY/kWh, while further reductions of the prices to below 0.1 CNY/kWh result in a diminishing cost-benefit ratio; (3) Comprehensive cost-efficiency analysis reveals no significant link between financial burden and carbon emissions, but a positive correlation with habitat quality and an inverted U-shaped relationship with total economic income; (4) Pareto analysis identifies 18 optimal dual-policy combinations for balancing carbon footprint, habitat quality, and gross economic benefits; (5) Posterior Pareto optimization further refines the selection of a specific policy scheme for a given realistic scenario. The analytical framework of this paper helps policymakers design economically viable and environmentally sustainable policies, addressing global conservation challenges.'}",oai:arXiv.org:2412.07275v3,False,"[{'term': 'cs.CY', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Keyi Liu, Yufeng Chen, Liyan Xu, Xiao Zhang, Zilin Wang, Hailong Li, Yansheng Yang, Hong You, Dihua Li'}]","Keyi Liu, Yufeng Chen, Liyan Xu, Xiao Zhang, Zilin Wang, Hailong Li, Yansheng Yang, Hong You, Dihua Li","{'name': 'Keyi Liu, Yufeng Chen, Liyan Xu, Xiao Zhang, Zilin Wang, Hailong Li, Yansheng Yang, Hong You, Dihua Li'}",,
685,"Contextualized Counterspeech: Strategies for Adaptation, Personalization, and Evaluation","{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Contextualized Counterspeech: Strategies for Adaptation, Personalization, and Evaluation'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.07338'}]",https://arxiv.org/abs/2412.07338,"arXiv:2412.07338v2 Announce Type: replace 
Abstract: AI-generated counterspeech offers a promising and scalable strategy to curb online toxicity through direct replies that promote civil discourse. However, current counterspeech is one-size-fits-all, lacking adaptation to the moderation context and the users involved. We propose and evaluate multiple strategies for generating tailored counterspeech that is adapted to the moderation context and personalized for the moderated user. We instruct an LLaMA2-13B model to generate counterspeech, experimenting with various configurations based on different contextual information and fine-tuning strategies. We identify the configurations that generate persuasive counterspeech through a combination of quantitative indicators and human evaluations collected via a pre-registered mixed-design crowdsourcing experiment. Results show that contextualized counterspeech can significantly outperform state-of-the-art generic counterspeech in adequacy and persuasiveness, without compromising other characteristics. Our findings also reveal a poor correlation between quantitative indicators and human evaluations, suggesting that these methods assess different aspects and highlighting the need for nuanced evaluation methodologies. The effectiveness of contextualized AI-generated counterspeech and the divergence between human and algorithmic evaluations underscore the importance of increased human-AI collaboration in content moderation.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.07338v2 Announce Type: replace \nAbstract: AI-generated counterspeech offers a promising and scalable strategy to curb online toxicity through direct replies that promote civil discourse. However, current counterspeech is one-size-fits-all, lacking adaptation to the moderation context and the users involved. We propose and evaluate multiple strategies for generating tailored counterspeech that is adapted to the moderation context and personalized for the moderated user. We instruct an LLaMA2-13B model to generate counterspeech, experimenting with various configurations based on different contextual information and fine-tuning strategies. We identify the configurations that generate persuasive counterspeech through a combination of quantitative indicators and human evaluations collected via a pre-registered mixed-design crowdsourcing experiment. Results show that contextualized counterspeech can significantly outperform state-of-the-art generic counterspeech in adequacy and persuasiveness, without compromising other characteristics. Our findings also reveal a poor correlation between quantitative indicators and human evaluations, suggesting that these methods assess different aspects and highlighting the need for nuanced evaluation methodologies. The effectiveness of contextualized AI-generated counterspeech and the divergence between human and algorithmic evaluations underscore the importance of increased human-AI collaboration in content moderation.'}",oai:arXiv.org:2412.07338v2,False,"[{'term': 'cs.HC', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.SI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': ""Lorenzo Cima, Alessio Miaschi, Amaury Trujillo, Marco Avvenuti, Felice Dell'Orletta, Stefano Cresci""}]","Lorenzo Cima, Alessio Miaschi, Amaury Trujillo, Marco Avvenuti, Felice Dell'Orletta, Stefano Cresci","{'name': ""Lorenzo Cima, Alessio Miaschi, Amaury Trujillo, Marco Avvenuti, Felice Dell'Orletta, Stefano Cresci""}",,
686,Piece of Table: A Divide-and-Conquer Approach for Selecting Sub-Tables in Table Question Answering,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Piece of Table: A Divide-and-Conquer Approach for Selecting Sub-Tables in Table Question Answering'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.07629'}]",https://arxiv.org/abs/2412.07629,"arXiv:2412.07629v2 Announce Type: replace 
Abstract: Applying language models (LMs) to tables is challenging due to the inherent structural differences between two-dimensional tables and one-dimensional text for which the LMs were originally designed. Furthermore, when applying linearized tables to LMs, the maximum token lengths often imposed in self-attention calculations make it difficult to comprehensively understand the context spread across large tables. To address these challenges, we present PieTa (Piece of Table), a new framework for sub-table-based question answering (QA). PieTa operates through an iterative process of dividing tables into smaller windows, using LMs to select relevant cells within each window, and merging these cells into a sub-table. This multi-resolution approach captures dependencies across multiple rows and columns while avoiding the limitations caused by long context inputs. Instantiated as a simple iterative sub-table union algorithm, PieTa demonstrates improved performance over previous sub-table-based QA approaches.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.07629v2 Announce Type: replace \nAbstract: Applying language models (LMs) to tables is challenging due to the inherent structural differences between two-dimensional tables and one-dimensional text for which the LMs were originally designed. Furthermore, when applying linearized tables to LMs, the maximum token lengths often imposed in self-attention calculations make it difficult to comprehensively understand the context spread across large tables. To address these challenges, we present PieTa (Piece of Table), a new framework for sub-table-based question answering (QA). PieTa operates through an iterative process of dividing tables into smaller windows, using LMs to select relevant cells within each window, and merging these cells into a sub-table. This multi-resolution approach captures dependencies across multiple rows and columns while avoiding the limitations caused by long context inputs. Instantiated as a simple iterative sub-table union algorithm, PieTa demonstrates improved performance over previous sub-table-based QA approaches.'}",oai:arXiv.org:2412.07629v2,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by-nc-nd/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-nd/4.0/'}","[{'name': 'Wonjin Lee, Kyumin Kim, Sungjae Lee, Jihun Lee, Kwang In Kim'}]","Wonjin Lee, Kyumin Kim, Sungjae Lee, Jihun Lee, Kwang In Kim","{'name': 'Wonjin Lee, Kyumin Kim, Sungjae Lee, Jihun Lee, Kwang In Kim'}",,
687,RAZOR: Sharpening Knowledge by Cutting Bias with Unsupervised Text Rewriting,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'RAZOR: Sharpening Knowledge by Cutting Bias with Unsupervised Text Rewriting'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.07675'}]",https://arxiv.org/abs/2412.07675,"arXiv:2412.07675v3 Announce Type: replace 
Abstract: Despite the widespread use of LLMs due to their superior performance in various tasks, their high computational costs often lead potential users to opt for the pretraining-finetuning pipeline. However, biases prevalent in manually constructed datasets can introduce spurious correlations between tokens and labels, creating so-called shortcuts and hindering the generalizability of fine-tuned models. Existing debiasing methods often rely on prior knowledge of specific dataset biases, which is challenging to acquire a priori. We propose RAZOR (Rewriting And Zero-bias Optimization Refinement), a novel, unsupervised, and data-focused debiasing approach based on text rewriting for shortcut mitigation. RAZOR leverages LLMs to iteratively rewrite potentially biased text segments by replacing them with heuristically selected alternatives in a shortcut space defined by token statistics and positional information. This process aims to align surface-level text features more closely with diverse label distributions, thereby promoting the learning of genuine linguistic patterns. Compared with unsupervised SoTA models, RAZOR improves by 3.5% on the FEVER and 6.5% on MNLI and SNLI datasets according to the F1 score. Additionally, RAZOR effectively mitigates specific known biases, reducing bias-related terms by x2 without requiring prior bias information, a result that is on par with SoTA models that leverage prior information. Our work prioritizes data manipulation over architectural modifications, emphasizing the pivotal role of data quality in enhancing model performance and fairness. This research contributes to developing more robust evaluation benchmarks for debiasing methods by incorporating metrics for bias reduction and overall model efficacy.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.07675v3 Announce Type: replace \nAbstract: Despite the widespread use of LLMs due to their superior performance in various tasks, their high computational costs often lead potential users to opt for the pretraining-finetuning pipeline. However, biases prevalent in manually constructed datasets can introduce spurious correlations between tokens and labels, creating so-called shortcuts and hindering the generalizability of fine-tuned models. Existing debiasing methods often rely on prior knowledge of specific dataset biases, which is challenging to acquire a priori. We propose RAZOR (Rewriting And Zero-bias Optimization Refinement), a novel, unsupervised, and data-focused debiasing approach based on text rewriting for shortcut mitigation. RAZOR leverages LLMs to iteratively rewrite potentially biased text segments by replacing them with heuristically selected alternatives in a shortcut space defined by token statistics and positional information. This process aims to align surface-level text features more closely with diverse label distributions, thereby promoting the learning of genuine linguistic patterns. Compared with unsupervised SoTA models, RAZOR improves by 3.5% on the FEVER and 6.5% on MNLI and SNLI datasets according to the F1 score. Additionally, RAZOR effectively mitigates specific known biases, reducing bias-related terms by x2 without requiring prior bias information, a result that is on par with SoTA models that leverage prior information. Our work prioritizes data manipulation over architectural modifications, emphasizing the pivotal role of data quality in enhancing model performance and fairness. This research contributes to developing more robust evaluation benchmarks for debiasing methods by incorporating metrics for bias reduction and overall model efficacy.'}",oai:arXiv.org:2412.07675v3,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Shuo Yang, Bardh Prenkaj, Gjergji Kasneci'}]","Shuo Yang, Bardh Prenkaj, Gjergji Kasneci","{'name': 'Shuo Yang, Bardh Prenkaj, Gjergji Kasneci'}",,
688,Doubly-Universal Adversarial Perturbations: Deceiving Vision-Language Models Across Both Images and Text with a Single Perturbation,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Doubly-Universal Adversarial Perturbations: Deceiving Vision-Language Models Across Both Images and Text with a Single Perturbation'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.08108'}]",https://arxiv.org/abs/2412.08108,"arXiv:2412.08108v2 Announce Type: replace 
Abstract: Large Vision-Language Models (VLMs) have demonstrated remarkable performance across multimodal tasks by integrating vision encoders with large language models (LLMs). However, these models remain vulnerable to adversarial attacks. Among such attacks, Universal Adversarial Perturbations (UAPs) are especially powerful, as a single optimized perturbation can mislead the model across various input images. In this work, we introduce a novel UAP specifically designed for VLMs: the Doubly-Universal Adversarial Perturbation (Doubly-UAP), capable of universally deceiving VLMs across both image and text inputs. To successfully disrupt the vision encoder's fundamental process, we analyze the core components of the attention mechanism. After identifying value vectors in the middle-to-late layers as the most vulnerable, we optimize Doubly-UAP in a label-free manner with a frozen model. Despite being developed as a black-box to the LLM, Doubly-UAP achieves high attack success rates on VLMs, consistently outperforming baseline methods across vision-language tasks. Extensive ablation studies and analyses further demonstrate the robustness of Doubly-UAP and provide insights into how it influences internal attention mechanisms.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.08108v2 Announce Type: replace \nAbstract: Large Vision-Language Models (VLMs) have demonstrated remarkable performance across multimodal tasks by integrating vision encoders with large language models (LLMs). However, these models remain vulnerable to adversarial attacks. Among such attacks, Universal Adversarial Perturbations (UAPs) are especially powerful, as a single optimized perturbation can mislead the model across various input images. In this work, we introduce a novel UAP specifically designed for VLMs: the Doubly-Universal Adversarial Perturbation (Doubly-UAP), capable of universally deceiving VLMs across both image and text inputs. To successfully disrupt the vision encoder's fundamental process, we analyze the core components of the attention mechanism. After identifying value vectors in the middle-to-late layers as the most vulnerable, we optimize Doubly-UAP in a label-free manner with a frozen model. Despite being developed as a black-box to the LLM, Doubly-UAP achieves high attack success rates on VLMs, consistently outperforming baseline methods across vision-language tasks. Extensive ablation studies and analyses further demonstrate the robustness of Doubly-UAP and provide insights into how it influences internal attention mechanisms.""}",oai:arXiv.org:2412.08108v2,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.CL', 'scheme': None, 'label': None}, {'term': 'cs.CR', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Hee-Seon Kim, Minbeom Kim, Changick Kim'}]","Hee-Seon Kim, Minbeom Kim, Changick Kim","{'name': 'Hee-Seon Kim, Minbeom Kim, Changick Kim'}",,
689,Progressive Multi-granular Alignments for Grounded Reasoning in Large Vision-Language Models,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Progressive Multi-granular Alignments for Grounded Reasoning in Large Vision-Language Models'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.08125'}]",https://arxiv.org/abs/2412.08125,"arXiv:2412.08125v2 Announce Type: replace 
Abstract: Existing Large Vision-Language Models (LVLMs) excel at matching concepts across multi-modal inputs but struggle with compositional concepts and high-level relationships between entities. This paper introduces Progressive multi-granular Vision-Language alignments (PromViL), a novel framework to enhance LVLMs' ability in performing grounded compositional visual reasoning tasks. Our approach constructs a hierarchical structure of multi-modal alignments, ranging from simple to complex concepts. By progressively aligning textual descriptions with corresponding visual regions, our model learns to leverage contextual information from lower levels to inform higher-level reasoning. To facilitate this learning process, we introduce a data generation process that creates a novel dataset derived from Visual Genome, providing a wide range of nested compositional vision-language pairs. Experimental results demonstrate that our PromViL framework significantly outperforms baselines on various visual grounding and compositional question answering tasks. The code is available at: https://github.com/lqh52/PromViL.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.08125v2 Announce Type: replace \nAbstract: Existing Large Vision-Language Models (LVLMs) excel at matching concepts across multi-modal inputs but struggle with compositional concepts and high-level relationships between entities. This paper introduces Progressive multi-granular Vision-Language alignments (PromViL), a novel framework to enhance LVLMs' ability in performing grounded compositional visual reasoning tasks. Our approach constructs a hierarchical structure of multi-modal alignments, ranging from simple to complex concepts. By progressively aligning textual descriptions with corresponding visual regions, our model learns to leverage contextual information from lower levels to inform higher-level reasoning. To facilitate this learning process, we introduce a data generation process that creates a novel dataset derived from Visual Genome, providing a wide range of nested compositional vision-language pairs. Experimental results demonstrate that our PromViL framework significantly outperforms baselines on various visual grounding and compositional question answering tasks. The code is available at: https://github.com/lqh52/PromViL.""}",oai:arXiv.org:2412.08125v2,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.CL', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Quang-Hung Le, Long Hoang Dang, Ngan Le, Truyen Tran, Thao Minh Le'}]","Quang-Hung Le, Long Hoang Dang, Ngan Le, Truyen Tran, Thao Minh Le","{'name': 'Quang-Hung Le, Long Hoang Dang, Ngan Le, Truyen Tran, Thao Minh Le'}",,
690,DG-Mamba: Robust and Efficient Dynamic Graph Structure Learning with Selective State Space Models,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'DG-Mamba: Robust and Efficient Dynamic Graph Structure Learning with Selective State Space Models'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.08160'}]",https://arxiv.org/abs/2412.08160,"arXiv:2412.08160v4 Announce Type: replace 
Abstract: Dynamic graphs exhibit intertwined spatio-temporal evolutionary patterns, widely existing in the real world. Nevertheless, the structure incompleteness, noise, and redundancy result in poor robustness for Dynamic Graph Neural Networks (DGNNs). Dynamic Graph Structure Learning (DGSL) offers a promising way to optimize graph structures. However, aside from encountering unacceptable quadratic complexity, it overly relies on heuristic priors, making it hard to discover underlying predictive patterns. How to efficiently refine the dynamic structures, capture intrinsic dependencies, and learn robust representations, remains under-explored. In this work, we propose the novel DG-Mamba, a robust and efficient Dynamic Graph structure learning framework with the Selective State Space Models (Mamba). To accelerate the spatio-temporal structure learning, we propose a kernelized dynamic message-passing operator that reduces the quadratic time complexity to linear. To capture global intrinsic dynamics, we establish the dynamic graph as a self-contained system with State Space Model. By discretizing the system states with the cross-snapshot graph adjacency, we enable the long-distance dependencies capturing with the selective snapshot scan. To endow learned dynamic structures more expressive with informativeness, we propose the self-supervised Principle of Relevant Information for DGSL to regularize the most relevant yet least redundant information, enhancing global robustness. Extensive experiments demonstrate the superiority of the robustness and efficiency of our DG-Mamba compared with the state-of-the-art baselines against adversarial attacks.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.08160v4 Announce Type: replace \nAbstract: Dynamic graphs exhibit intertwined spatio-temporal evolutionary patterns, widely existing in the real world. Nevertheless, the structure incompleteness, noise, and redundancy result in poor robustness for Dynamic Graph Neural Networks (DGNNs). Dynamic Graph Structure Learning (DGSL) offers a promising way to optimize graph structures. However, aside from encountering unacceptable quadratic complexity, it overly relies on heuristic priors, making it hard to discover underlying predictive patterns. How to efficiently refine the dynamic structures, capture intrinsic dependencies, and learn robust representations, remains under-explored. In this work, we propose the novel DG-Mamba, a robust and efficient Dynamic Graph structure learning framework with the Selective State Space Models (Mamba). To accelerate the spatio-temporal structure learning, we propose a kernelized dynamic message-passing operator that reduces the quadratic time complexity to linear. To capture global intrinsic dynamics, we establish the dynamic graph as a self-contained system with State Space Model. By discretizing the system states with the cross-snapshot graph adjacency, we enable the long-distance dependencies capturing with the selective snapshot scan. To endow learned dynamic structures more expressive with informativeness, we propose the self-supervised Principle of Relevant Information for DGSL to regularize the most relevant yet least redundant information, enhancing global robustness. Extensive experiments demonstrate the superiority of the robustness and efficiency of our DG-Mamba compared with the state-of-the-art baselines against adversarial attacks.'}",oai:arXiv.org:2412.08160v4,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Haonan Yuan, Qingyun Sun, Zhaonan Wang, Xingcheng Fu, Cheng Ji, Yongjian Wang, Bo Jin, Jianxin Li'}]","Haonan Yuan, Qingyun Sun, Zhaonan Wang, Xingcheng Fu, Cheng Ji, Yongjian Wang, Bo Jin, Jianxin Li","{'name': 'Haonan Yuan, Qingyun Sun, Zhaonan Wang, Xingcheng Fu, Cheng Ji, Yongjian Wang, Bo Jin, Jianxin Li'}",,
691,How Does the Smoothness Approximation Method Facilitate Generalization for Federated Adversarial Learning?,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'How Does the Smoothness Approximation Method Facilitate Generalization for Federated Adversarial Learning?'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.08282'}]",https://arxiv.org/abs/2412.08282,"arXiv:2412.08282v2 Announce Type: replace 
Abstract: Federated Adversarial Learning (FAL) is a robust framework for resisting adversarial attacks on federated learning. Although some FAL studies have developed efficient algorithms, they primarily focus on convergence performance and overlook generalization. Generalization is crucial for evaluating algorithm performance on unseen data. However, generalization analysis is more challenging due to non-smooth adversarial loss functions. A common approach to addressing this issue is to leverage smoothness approximation. In this paper, we develop algorithm stability measures to evaluate the generalization performance of two popular FAL algorithms: \textit{Vanilla FAL (VFAL)} and {\it Slack FAL (SFAL)}, using three different smooth approximation methods: 1) \textit{Surrogate Smoothness Approximation (SSA)}, (2) \textit{Randomized Smoothness Approximation (RSA)}, and (3) \textit{Over-Parameterized Smoothness Approximation (OPSA)}. Based on our in-depth analysis, we answer the question of how to properly set the smoothness approximation method to mitigate generalization error in FAL. Moreover, we identify RSA as the most effective method for reducing generalization error. In highly data-heterogeneous scenarios, we also recommend employing SFAL to mitigate the deterioration of generalization performance caused by heterogeneity. Based on our theoretical results, we provide insights to help develop more efficient FAL algorithms, such as designing new metrics and dynamic aggregation rules to mitigate heterogeneity.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.08282v2 Announce Type: replace \nAbstract: Federated Adversarial Learning (FAL) is a robust framework for resisting adversarial attacks on federated learning. Although some FAL studies have developed efficient algorithms, they primarily focus on convergence performance and overlook generalization. Generalization is crucial for evaluating algorithm performance on unseen data. However, generalization analysis is more challenging due to non-smooth adversarial loss functions. A common approach to addressing this issue is to leverage smoothness approximation. In this paper, we develop algorithm stability measures to evaluate the generalization performance of two popular FAL algorithms: \\textit{Vanilla FAL (VFAL)} and {\\it Slack FAL (SFAL)}, using three different smooth approximation methods: 1) \\textit{Surrogate Smoothness Approximation (SSA)}, (2) \\textit{Randomized Smoothness Approximation (RSA)}, and (3) \\textit{Over-Parameterized Smoothness Approximation (OPSA)}. Based on our in-depth analysis, we answer the question of how to properly set the smoothness approximation method to mitigate generalization error in FAL. Moreover, we identify RSA as the most effective method for reducing generalization error. In highly data-heterogeneous scenarios, we also recommend employing SFAL to mitigate the deterioration of generalization performance caused by heterogeneity. Based on our theoretical results, we provide insights to help develop more efficient FAL algorithms, such as designing new metrics and dynamic aggregation rules to mitigate heterogeneity.'}",oai:arXiv.org:2412.08282v2,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Wenjun Ding, Ying An, Lixing Chen, Shichao Kan, Fan Wu, Zhe Qu'}]","Wenjun Ding, Ying An, Lixing Chen, Shichao Kan, Fan Wu, Zhe Qu","{'name': 'Wenjun Ding, Ying An, Lixing Chen, Shichao Kan, Fan Wu, Zhe Qu'}",,
692,Grimm: A Plug-and-Play Perturbation Rectifier for Graph Neural Networks Defending against Poisoning Attacks,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Grimm: A Plug-and-Play Perturbation Rectifier for Graph Neural Networks Defending against Poisoning Attacks'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.08555'}]",https://arxiv.org/abs/2412.08555,"arXiv:2412.08555v2 Announce Type: replace 
Abstract: Recent studies have revealed the vulnerability of graph neural networks (GNNs) to adversarial poisoning attacks on node classification tasks. Current defensive methods require substituting the original GNNs with defense models, regardless of the original's type. This approach, while targeting adversarial robustness, compromises the enhancements developed in prior research to boost GNNs' practical performance. Here we introduce Grimm, the first plug-and-play defense model. With just a minimal interface requirement for extracting features from any layer of the protected GNNs, Grimm is thus enabled to seamlessly rectify perturbations. Specifically, we utilize the feature trajectories (FTs) generated by GNNs, as they evolve through epochs, to reflect the training status of the networks. We then theoretically prove that the FTs of victim nodes will inevitably exhibit discriminable anomalies. Consequently, inspired by the natural parallelism between the biological nervous and immune systems, we construct Grimm, a comprehensive artificial immune system for GNNs. Grimm not only detects abnormal FTs and rectifies adversarial edges during training but also operates efficiently in parallel, thereby mirroring the concurrent functionalities of its biological counterparts. We experimentally confirm that Grimm offers four empirically validated advantages: 1) Harmlessness, as it does not actively interfere with GNN training; 2) Parallelism, ensuring monitoring, detection, and rectification functions operate independently of the GNN training process; 3) Generalizability, demonstrating compatibility with mainstream GNNs such as GCN, GAT, and GraphSAGE; and 4) Transferability, as the detectors for abnormal FTs can be efficiently transferred across different systems for one-step rectification.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.08555v2 Announce Type: replace \nAbstract: Recent studies have revealed the vulnerability of graph neural networks (GNNs) to adversarial poisoning attacks on node classification tasks. Current defensive methods require substituting the original GNNs with defense models, regardless of the original's type. This approach, while targeting adversarial robustness, compromises the enhancements developed in prior research to boost GNNs' practical performance. Here we introduce Grimm, the first plug-and-play defense model. With just a minimal interface requirement for extracting features from any layer of the protected GNNs, Grimm is thus enabled to seamlessly rectify perturbations. Specifically, we utilize the feature trajectories (FTs) generated by GNNs, as they evolve through epochs, to reflect the training status of the networks. We then theoretically prove that the FTs of victim nodes will inevitably exhibit discriminable anomalies. Consequently, inspired by the natural parallelism between the biological nervous and immune systems, we construct Grimm, a comprehensive artificial immune system for GNNs. Grimm not only detects abnormal FTs and rectifies adversarial edges during training but also operates efficiently in parallel, thereby mirroring the concurrent functionalities of its biological counterparts. We experimentally confirm that Grimm offers four empirically validated advantages: 1) Harmlessness, as it does not actively interfere with GNN training; 2) Parallelism, ensuring monitoring, detection, and rectification functions operate independently of the GNN training process; 3) Generalizability, demonstrating compatibility with mainstream GNNs such as GCN, GAT, and GraphSAGE; and 4) Transferability, as the detectors for abnormal FTs can be efficiently transferred across different systems for one-step rectification.""}",oai:arXiv.org:2412.08555v2,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Ao Liu, Wenshan Li, Beibei Li, Wengang Ma, Tao Li, Pan Zhou'}]","Ao Liu, Wenshan Li, Beibei Li, Wengang Ma, Tao Li, Pan Zhou","{'name': 'Ao Liu, Wenshan Li, Beibei Li, Wengang Ma, Tao Li, Pan Zhou'}",,
693,Optimized Gradient Clipping for Noisy Label Learning,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Optimized Gradient Clipping for Noisy Label Learning'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.08941'}]",https://arxiv.org/abs/2412.08941,"arXiv:2412.08941v3 Announce Type: replace 
Abstract: Previous research has shown that constraining the gradient of loss function with respect to model-predicted probabilities can enhance the model robustness against noisy labels. These methods typically specify a fixed optimal threshold for gradient clipping through validation data to obtain the desired robustness against noise. However, this common practice overlooks the dynamic distribution of gradients from both clean and noisy-labeled samples at different stages of training, significantly limiting the model capability to adapt to the variable nature of gradients throughout the training process. To address this issue, we propose a simple yet effective approach called Optimized Gradient Clipping (OGC), which dynamically adjusts the clipping threshold based on the ratio of noise gradients to clean gradients after clipping, estimated by modeling the distributions of clean and noisy samples. This approach allows us to modify the clipping threshold at each training step, effectively controlling the influence of noise gradients. Additionally, we provide statistical analysis to certify the noise-tolerance ability of OGC. Our extensive experiments across various types of label noise, including symmetric, asymmetric, instance-dependent, and real-world noise, demonstrate the effectiveness of our approach.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.08941v3 Announce Type: replace \nAbstract: Previous research has shown that constraining the gradient of loss function with respect to model-predicted probabilities can enhance the model robustness against noisy labels. These methods typically specify a fixed optimal threshold for gradient clipping through validation data to obtain the desired robustness against noise. However, this common practice overlooks the dynamic distribution of gradients from both clean and noisy-labeled samples at different stages of training, significantly limiting the model capability to adapt to the variable nature of gradients throughout the training process. To address this issue, we propose a simple yet effective approach called Optimized Gradient Clipping (OGC), which dynamically adjusts the clipping threshold based on the ratio of noise gradients to clean gradients after clipping, estimated by modeling the distributions of clean and noisy samples. This approach allows us to modify the clipping threshold at each training step, effectively controlling the influence of noise gradients. Additionally, we provide statistical analysis to certify the noise-tolerance ability of OGC. Our extensive experiments across various types of label noise, including symmetric, asymmetric, instance-dependent, and real-world noise, demonstrate the effectiveness of our approach.'}",oai:arXiv.org:2412.08941v3,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Xichen Ye, Yifan Wu, Weizhong Zhang, Xiaoqiang Li, Yifan Chen, Cheng Jin'}]","Xichen Ye, Yifan Wu, Weizhong Zhang, Xiaoqiang Li, Yifan Chen, Cheng Jin","{'name': 'Xichen Ye, Yifan Wu, Weizhong Zhang, Xiaoqiang Li, Yifan Chen, Cheng Jin'}",,
694,How to Re-enable PDE Loss for Physical Systems Modeling Under Partial Observation,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'How to Re-enable PDE Loss for Physical Systems Modeling Under Partial Observation'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.09116'}]",https://arxiv.org/abs/2412.09116,"arXiv:2412.09116v3 Announce Type: replace 
Abstract: In science and engineering, machine learning techniques are increasingly successful in physical systems modeling (predicting future states of physical systems). Effectively integrating PDE loss as a constraint of system transition can improve the model's prediction by overcoming generalization issues due to data scarcity, especially when data acquisition is costly. However, in many real-world scenarios, due to sensor limitations, the data we can obtain is often only partial observation, making the calculation of PDE loss seem to be infeasible, as the PDE loss heavily relies on high-resolution states. We carefully study this problem and propose a novel framework named Re-enable PDE Loss under Partial Observation (RPLPO). The key idea is that although enabling PDE loss to constrain system transition solely is infeasible, we can re-enable PDE loss by reconstructing the learnable high-resolution state and constraining system transition simultaneously. Specifically, RPLPO combines an encoding module for reconstructing learnable high-resolution states with a transition module for predicting future states. The two modules are jointly trained by data and PDE loss. We conduct experiments in various physical systems to demonstrate that RPLPO has significant improvement in generalization, even when observation is sparse, irregular, noisy, and PDE is inaccurate.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.09116v3 Announce Type: replace \nAbstract: In science and engineering, machine learning techniques are increasingly successful in physical systems modeling (predicting future states of physical systems). Effectively integrating PDE loss as a constraint of system transition can improve the model's prediction by overcoming generalization issues due to data scarcity, especially when data acquisition is costly. However, in many real-world scenarios, due to sensor limitations, the data we can obtain is often only partial observation, making the calculation of PDE loss seem to be infeasible, as the PDE loss heavily relies on high-resolution states. We carefully study this problem and propose a novel framework named Re-enable PDE Loss under Partial Observation (RPLPO). The key idea is that although enabling PDE loss to constrain system transition solely is infeasible, we can re-enable PDE loss by reconstructing the learnable high-resolution state and constraining system transition simultaneously. Specifically, RPLPO combines an encoding module for reconstructing learnable high-resolution states with a transition module for predicting future states. The two modules are jointly trained by data and PDE loss. We conduct experiments in various physical systems to demonstrate that RPLPO has significant improvement in generalization, even when observation is sparse, irregular, noisy, and PDE is inaccurate.""}",oai:arXiv.org:2412.09116v3,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Haodong Feng, Yue Wang, Dixia Fan'}]","Haodong Feng, Yue Wang, Dixia Fan","{'name': 'Haodong Feng, Yue Wang, Dixia Fan'}",,
695,RAD: Region-Aware Diffusion Models for Image Inpainting,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'RAD: Region-Aware Diffusion Models for Image Inpainting'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.09191'}]",https://arxiv.org/abs/2412.09191,"arXiv:2412.09191v3 Announce Type: replace 
Abstract: Diffusion models have achieved remarkable success in image generation, with applications broadening across various domains. Inpainting is one such application that can benefit significantly from diffusion models. Existing methods either hijack the reverse process of a pretrained diffusion model or cast the problem into a larger framework, \ie, conditioned generation. However, these approaches often require nested loops in the generation process or additional components for conditioning. In this paper, we present region-aware diffusion models (RAD) for inpainting with a simple yet effective reformulation of the vanilla diffusion models. RAD utilizes a different noise schedule for each pixel, which allows local regions to be generated asynchronously while considering the global image context. A plain reverse process requires no additional components, enabling RAD to achieve inference time up to 100 times faster than the state-of-the-art approaches. Moreover, we employ low-rank adaptation (LoRA) to fine-tune RAD based on other pretrained diffusion models, reducing computational burdens in training as well. Experiments demonstrated that RAD provides state-of-the-art results both qualitatively and quantitatively, on the FFHQ, LSUN Bedroom, and ImageNet datasets.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.09191v3 Announce Type: replace \nAbstract: Diffusion models have achieved remarkable success in image generation, with applications broadening across various domains. Inpainting is one such application that can benefit significantly from diffusion models. Existing methods either hijack the reverse process of a pretrained diffusion model or cast the problem into a larger framework, \\ie, conditioned generation. However, these approaches often require nested loops in the generation process or additional components for conditioning. In this paper, we present region-aware diffusion models (RAD) for inpainting with a simple yet effective reformulation of the vanilla diffusion models. RAD utilizes a different noise schedule for each pixel, which allows local regions to be generated asynchronously while considering the global image context. A plain reverse process requires no additional components, enabling RAD to achieve inference time up to 100 times faster than the state-of-the-art approaches. Moreover, we employ low-rank adaptation (LoRA) to fine-tune RAD based on other pretrained diffusion models, reducing computational burdens in training as well. Experiments demonstrated that RAD provides state-of-the-art results both qualitatively and quantitatively, on the FFHQ, LSUN Bedroom, and ImageNet datasets.'}",oai:arXiv.org:2412.09191v3,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by-sa/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-sa/4.0/'}","[{'name': 'Sora Kim, Sungho Suh, Minsik Lee'}]","Sora Kim, Sungho Suh, Minsik Lee","{'name': 'Sora Kim, Sungho Suh, Minsik Lee'}",,
696,Score and Distribution Matching Policy: Advanced Accelerated Visuomotor Policies via Matched Distillation,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Score and Distribution Matching Policy: Advanced Accelerated Visuomotor Policies via Matched Distillation'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.09265'}]",https://arxiv.org/abs/2412.09265,"arXiv:2412.09265v4 Announce Type: replace 
Abstract: Visual-motor policy learning has advanced with architectures like diffusion-based policies, known for modeling complex robotic trajectories. However, their prolonged inference times hinder high-frequency control tasks requiring real-time feedback. While consistency distillation (CD) accelerates inference, it introduces errors that compromise action quality. To address these limitations, we propose the Score and Distribution Matching Policy (SDM Policy), which transforms diffusion-based policies into single-step generators through a two-stage optimization process: score matching ensures alignment with true action distributions, and distribution matching minimizes KL divergence for consistency. A dual-teacher mechanism integrates a frozen teacher for stability and an unfrozen teacher for adversarial training, enhancing robustness and alignment with target distributions. Evaluated on a 57-task simulation benchmark, SDM Policy achieves a 6x inference speedup while having state-of-the-art action quality, providing an efficient and reliable framework for high-frequency robotic tasks.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.09265v4 Announce Type: replace \nAbstract: Visual-motor policy learning has advanced with architectures like diffusion-based policies, known for modeling complex robotic trajectories. However, their prolonged inference times hinder high-frequency control tasks requiring real-time feedback. While consistency distillation (CD) accelerates inference, it introduces errors that compromise action quality. To address these limitations, we propose the Score and Distribution Matching Policy (SDM Policy), which transforms diffusion-based policies into single-step generators through a two-stage optimization process: score matching ensures alignment with true action distributions, and distribution matching minimizes KL divergence for consistency. A dual-teacher mechanism integrates a frozen teacher for stability and an unfrozen teacher for adversarial training, enhancing robustness and alignment with target distributions. Evaluated on a 57-task simulation benchmark, SDM Policy achieves a 6x inference speedup while having state-of-the-art action quality, providing an efficient and reliable framework for high-frequency robotic tasks.'}",oai:arXiv.org:2412.09265v4,False,"[{'term': 'cs.RO', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'stat.ML', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Bofang Jia, Pengxiang Ding, Can Cui, Mingyang Sun, Pengfang Qian, Siteng Huang, Zhaoxin Fan, Donglin Wang'}]","Bofang Jia, Pengxiang Ding, Can Cui, Mingyang Sun, Pengfang Qian, Siteng Huang, Zhaoxin Fan, Donglin Wang","{'name': 'Bofang Jia, Pengxiang Ding, Can Cui, Mingyang Sun, Pengfang Qian, Siteng Huang, Zhaoxin Fan, Donglin Wang'}",,
697,From Bench to Bedside: A Review of Clinical Trials in Drug Discovery and Development,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'From Bench to Bedside: A Review of Clinical Trials in Drug Discovery and Development'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.09378'}]",https://arxiv.org/abs/2412.09378,"arXiv:2412.09378v2 Announce Type: replace 
Abstract: Clinical trials are an indispensable part of the drug development process, bridging the gap between basic research and clinical application. During the development of new drugs, clinical trials are used not only to evaluate the safety and efficacy of the drug but also to explore its dosage, treatment regimens, and potential side effects. This review discusses the various stages of clinical trials, including Phase I (safety assessment), Phase II (preliminary efficacy evaluation), Phase III (large-scale validation), and Phase IV (post-marketing surveillance), highlighting the characteristics of each phase and their interrelationships. Additionally, the paper addresses the major challenges encountered in clinical trials, such as ethical issues, subject recruitment difficulties, diversity and representativeness concerns, and proposes strategies for overcoming these challenges. With the advancement of technology, innovative technologies such as artificial intelligence, big data, and digitalization are gradually transforming clinical trial design and implementation, improving trial efficiency and data quality. The article also looks forward to the future of clinical trials, particularly the impact of emerging therapies such as gene therapy and immunotherapy on trial design, as well as the importance of regulatory reforms and global collaboration. In conclusion, the core role of clinical trials in drug development will continue to drive the progress of innovative drug development and clinical treatment.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.09378v2 Announce Type: replace \nAbstract: Clinical trials are an indispensable part of the drug development process, bridging the gap between basic research and clinical application. During the development of new drugs, clinical trials are used not only to evaluate the safety and efficacy of the drug but also to explore its dosage, treatment regimens, and potential side effects. This review discusses the various stages of clinical trials, including Phase I (safety assessment), Phase II (preliminary efficacy evaluation), Phase III (large-scale validation), and Phase IV (post-marketing surveillance), highlighting the characteristics of each phase and their interrelationships. Additionally, the paper addresses the major challenges encountered in clinical trials, such as ethical issues, subject recruitment difficulties, diversity and representativeness concerns, and proposes strategies for overcoming these challenges. With the advancement of technology, innovative technologies such as artificial intelligence, big data, and digitalization are gradually transforming clinical trial design and implementation, improving trial efficiency and data quality. The article also looks forward to the future of clinical trials, particularly the impact of emerging therapies such as gene therapy and immunotherapy on trial design, as well as the importance of regulatory reforms and global collaboration. In conclusion, the core role of clinical trials in drug development will continue to drive the progress of innovative drug development and clinical treatment.'}",oai:arXiv.org:2412.09378v2,False,"[{'term': 'cs.CY', 'scheme': None, 'label': None}, {'term': 'cs.CL', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Tianyang Wang, Ming Liu, Benji Peng, Xinyuan Song, Charles Zhang, Xintian Sun, Qian Niu, Junyu Liu, Silin Chen, Keyu Chen, Ming Li, Pohsun Feng, Ziqian Bi, Yunze Wang, Yichao Zhang, Cheng Fei, Lawrence KQ Yan'}]","Tianyang Wang, Ming Liu, Benji Peng, Xinyuan Song, Charles Zhang, Xintian Sun, Qian Niu, Junyu Liu, Silin Chen, Keyu Chen, Ming Li, Pohsun Feng, Ziqian Bi, Yunze Wang, Yichao Zhang, Cheng Fei, Lawrence KQ Yan","{'name': 'Tianyang Wang, Ming Liu, Benji Peng, Xinyuan Song, Charles Zhang, Xintian Sun, Qian Niu, Junyu Liu, Silin Chen, Keyu Chen, Ming Li, Pohsun Feng, Ziqian Bi, Yunze Wang, Yichao Zhang, Cheng Fei, Lawrence KQ Yan'}",,
698,SLAM3R: Real-Time Dense Scene Reconstruction from Monocular RGB Videos,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'SLAM3R: Real-Time Dense Scene Reconstruction from Monocular RGB Videos'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.09401'}]",https://arxiv.org/abs/2412.09401,"arXiv:2412.09401v2 Announce Type: replace 
Abstract: In this paper, we introduce SLAM3R, a novel and effective monocular RGB SLAM system for real-time and high-quality dense 3D reconstruction. SLAM3R provides an end-to-end solution by seamlessly integrating local 3D reconstruction and global coordinate registration through feed-forward neural networks. Given an input video, the system first converts it into overlapping clips using a sliding window mechanism. Unlike traditional pose optimization-based methods, SLAM3R directly regresses 3D pointmaps from RGB images in each window and progressively aligns and deforms these local pointmaps to create a globally consistent scene reconstruction - all without explicitly solving any camera parameters. Experiments across datasets consistently show that SLAM3R achieves state-of-the-art reconstruction accuracy and completeness while maintaining real-time performance at 20+ FPS. Code and weights at: https://github.com/PKU-VCL-3DV/SLAM3R.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.09401v2 Announce Type: replace \nAbstract: In this paper, we introduce SLAM3R, a novel and effective monocular RGB SLAM system for real-time and high-quality dense 3D reconstruction. SLAM3R provides an end-to-end solution by seamlessly integrating local 3D reconstruction and global coordinate registration through feed-forward neural networks. Given an input video, the system first converts it into overlapping clips using a sliding window mechanism. Unlike traditional pose optimization-based methods, SLAM3R directly regresses 3D pointmaps from RGB images in each window and progressively aligns and deforms these local pointmaps to create a globally consistent scene reconstruction - all without explicitly solving any camera parameters. Experiments across datasets consistently show that SLAM3R achieves state-of-the-art reconstruction accuracy and completeness while maintaining real-time performance at 20+ FPS. Code and weights at: https://github.com/PKU-VCL-3DV/SLAM3R.'}",oai:arXiv.org:2412.09401v2,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Yuzheng Liu, Siyan Dong, Shuzhe Wang, Yanchao Yang, Qingnan Fan, Baoquan Chen'}]","Yuzheng Liu, Siyan Dong, Shuzhe Wang, Yanchao Yang, Qingnan Fan, Baoquan Chen","{'name': 'Yuzheng Liu, Siyan Dong, Shuzhe Wang, Yanchao Yang, Qingnan Fan, Baoquan Chen'}",,
699,Can Modern LLMs Act as Agent Cores in Radiology Environments?,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Can Modern LLMs Act as Agent Cores in Radiology Environments?'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.09529'}]",https://arxiv.org/abs/2412.09529,"arXiv:2412.09529v2 Announce Type: replace 
Abstract: Advancements in large language models (LLMs) have paved the way for LLM-based agent systems that offer enhanced accuracy and interpretability across various domains. Radiology, with its complex analytical requirements, is an ideal field for the application of these agents. This paper aims to investigate the pre-requisite question for building concrete radiology agents which is, `Can modern LLMs act as agent cores in radiology environments?' To investigate it, we introduce RadABench with three-fold contributions: First, we present RadABench-Data, a comprehensive synthetic evaluation dataset for LLM-based agents, generated from an extensive taxonomy encompassing 6 anatomies, 5 imaging modalities, 10 tool categories, and 11 radiology tasks. Second, we propose RadABench-EvalPlat, a novel evaluation platform for agents featuring a prompt-driven workflow and the capability to simulate a wide range of radiology toolsets. Third, we assess the performance of 7 leading LLMs on our benchmark from 5 perspectives with multiple metrics. Our findings indicate that while current LLMs demonstrate strong capabilities in many areas, they are still not sufficiently advanced to serve as the central agent core in a fully operational radiology agent system. Additionally, we identify key factors influencing the performance of LLM-based agent cores, offering insights for clinicians on how to apply agent systems in real-world radiology practices effectively. All of our code and data are open-sourced in https://github.com/MAGIC-AI4Med/RadABench.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.09529v2 Announce Type: replace \nAbstract: Advancements in large language models (LLMs) have paved the way for LLM-based agent systems that offer enhanced accuracy and interpretability across various domains. Radiology, with its complex analytical requirements, is an ideal field for the application of these agents. This paper aims to investigate the pre-requisite question for building concrete radiology agents which is, `Can modern LLMs act as agent cores in radiology environments?' To investigate it, we introduce RadABench with three-fold contributions: First, we present RadABench-Data, a comprehensive synthetic evaluation dataset for LLM-based agents, generated from an extensive taxonomy encompassing 6 anatomies, 5 imaging modalities, 10 tool categories, and 11 radiology tasks. Second, we propose RadABench-EvalPlat, a novel evaluation platform for agents featuring a prompt-driven workflow and the capability to simulate a wide range of radiology toolsets. Third, we assess the performance of 7 leading LLMs on our benchmark from 5 perspectives with multiple metrics. Our findings indicate that while current LLMs demonstrate strong capabilities in many areas, they are still not sufficiently advanced to serve as the central agent core in a fully operational radiology agent system. Additionally, we identify key factors influencing the performance of LLM-based agent cores, offering insights for clinicians on how to apply agent systems in real-world radiology practices effectively. All of our code and data are open-sourced in https://github.com/MAGIC-AI4Med/RadABench.""}",oai:arXiv.org:2412.09529v2,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Qiaoyu Zheng, Chaoyi Wu, Pengcheng Qiu, Lisong Dai, Ya Zhang, Yanfeng Wang, Weidi Xie'}]","Qiaoyu Zheng, Chaoyi Wu, Pengcheng Qiu, Lisong Dai, Ya Zhang, Yanfeng Wang, Weidi Xie","{'name': 'Qiaoyu Zheng, Chaoyi Wu, Pengcheng Qiu, Lisong Dai, Ya Zhang, Yanfeng Wang, Weidi Xie'}",,
700,SimAvatar: Simulation-Ready Avatars with Layered Hair and Clothing,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'SimAvatar: Simulation-Ready Avatars with Layered Hair and Clothing'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.09545'}]",https://arxiv.org/abs/2412.09545,"arXiv:2412.09545v2 Announce Type: replace 
Abstract: We introduce SimAvatar, a framework designed to generate simulation-ready clothed 3D human avatars from a text prompt. Current text-driven human avatar generation methods either model hair, clothing, and the human body using a unified geometry or produce hair and garments that are not easily adaptable for simulation within existing simulation pipelines. The primary challenge lies in representing the hair and garment geometry in a way that allows leveraging established prior knowledge from foundational image diffusion models (e.g., Stable Diffusion) while being simulation-ready using either physics or neural simulators. To address this task, we propose a two-stage framework that combines the flexibility of 3D Gaussians with simulation-ready hair strands and garment meshes. Specifically, we first employ three text-conditioned 3D generative models to generate garment mesh, body shape and hair strands from the given text prompt. To leverage prior knowledge from foundational diffusion models, we attach 3D Gaussians to the body mesh, garment mesh, as well as hair strands and learn the avatar appearance through optimization. To drive the avatar given a pose sequence, we first apply physics simulators onto the garment meshes and hair strands. We then transfer the motion onto 3D Gaussians through carefully designed mechanisms for each body part. As a result, our synthesized avatars have vivid texture and realistic dynamic motion. To the best of our knowledge, our method is the first to produce highly realistic, fully simulation-ready 3D avatars, surpassing the capabilities of current approaches.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.09545v2 Announce Type: replace \nAbstract: We introduce SimAvatar, a framework designed to generate simulation-ready clothed 3D human avatars from a text prompt. Current text-driven human avatar generation methods either model hair, clothing, and the human body using a unified geometry or produce hair and garments that are not easily adaptable for simulation within existing simulation pipelines. The primary challenge lies in representing the hair and garment geometry in a way that allows leveraging established prior knowledge from foundational image diffusion models (e.g., Stable Diffusion) while being simulation-ready using either physics or neural simulators. To address this task, we propose a two-stage framework that combines the flexibility of 3D Gaussians with simulation-ready hair strands and garment meshes. Specifically, we first employ three text-conditioned 3D generative models to generate garment mesh, body shape and hair strands from the given text prompt. To leverage prior knowledge from foundational diffusion models, we attach 3D Gaussians to the body mesh, garment mesh, as well as hair strands and learn the avatar appearance through optimization. To drive the avatar given a pose sequence, we first apply physics simulators onto the garment meshes and hair strands. We then transfer the motion onto 3D Gaussians through carefully designed mechanisms for each body part. As a result, our synthesized avatars have vivid texture and realistic dynamic motion. To the best of our knowledge, our method is the first to produce highly realistic, fully simulation-ready 3D avatars, surpassing the capabilities of current approaches.'}",oai:arXiv.org:2412.09545v2,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.GR', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Xueting Li, Ye Yuan, Shalini De Mello, Gilles Daviet, Jonathan Leaf, Miles Macklin, Jan Kautz, Umar Iqbal'}]","Xueting Li, Ye Yuan, Shalini De Mello, Gilles Daviet, Jonathan Leaf, Miles Macklin, Jan Kautz, Umar Iqbal","{'name': 'Xueting Li, Ye Yuan, Shalini De Mello, Gilles Daviet, Jonathan Leaf, Miles Macklin, Jan Kautz, Umar Iqbal'}",,
701,GenEx: Generating an Explorable World,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'GenEx: Generating an Explorable World'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.09624'}]",https://arxiv.org/abs/2412.09624,"arXiv:2412.09624v3 Announce Type: replace 
Abstract: Understanding, navigating, and exploring the 3D physical real world has long been a central challenge in the development of artificial intelligence. In this work, we take a step toward this goal by introducing GenEx, a system capable of planning complex embodied world exploration, guided by its generative imagination that forms priors (expectations) about the surrounding environments. GenEx generates an entire 3D-consistent imaginative environment from as little as a single RGB image, bringing it to life through panoramic video streams. Leveraging scalable 3D world data curated from Unreal Engine, our generative model is rounded in the physical world. It captures a continuous 360-degree environment with little effort, offering a boundless landscape for AI agents to explore and interact with. GenEx achieves high-quality world generation, robust loop consistency over long trajectories, and demonstrates strong 3D capabilities such as consistency and active 3D mapping. Powered by generative imagination of the world, GPT-assisted agents are equipped to perform complex embodied tasks, including both goal-agnostic exploration and goal-driven navigation. These agents utilize predictive expectation regarding unseen parts of the physical world to refine their beliefs, simulate different outcomes based on potential decisions, and make more informed choices. In summary, we demonstrate that GenEx provides a transformative platform for advancing embodied AI in imaginative spaces and brings potential for extending these capabilities to real-world exploration.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.09624v3 Announce Type: replace \nAbstract: Understanding, navigating, and exploring the 3D physical real world has long been a central challenge in the development of artificial intelligence. In this work, we take a step toward this goal by introducing GenEx, a system capable of planning complex embodied world exploration, guided by its generative imagination that forms priors (expectations) about the surrounding environments. GenEx generates an entire 3D-consistent imaginative environment from as little as a single RGB image, bringing it to life through panoramic video streams. Leveraging scalable 3D world data curated from Unreal Engine, our generative model is rounded in the physical world. It captures a continuous 360-degree environment with little effort, offering a boundless landscape for AI agents to explore and interact with. GenEx achieves high-quality world generation, robust loop consistency over long trajectories, and demonstrates strong 3D capabilities such as consistency and active 3D mapping. Powered by generative imagination of the world, GPT-assisted agents are equipped to perform complex embodied tasks, including both goal-agnostic exploration and goal-driven navigation. These agents utilize predictive expectation regarding unseen parts of the physical world to refine their beliefs, simulate different outcomes based on potential decisions, and make more informed choices. In summary, we demonstrate that GenEx provides a transformative platform for advancing embodied AI in imaginative spaces and brings potential for extending these capabilities to real-world exploration.'}",oai:arXiv.org:2412.09624v3,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.RO', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Taiming Lu, Tianmin Shu, Junfei Xiao, Luoxin Ye, Jiahao Wang, Cheng Peng, Chen Wei, Daniel Khashabi, Rama Chellappa, Alan Yuille, Jieneng Chen'}]","Taiming Lu, Tianmin Shu, Junfei Xiao, Luoxin Ye, Jiahao Wang, Cheng Peng, Chen Wei, Daniel Khashabi, Rama Chellappa, Alan Yuille, Jieneng Chen","{'name': 'Taiming Lu, Tianmin Shu, Junfei Xiao, Luoxin Ye, Jiahao Wang, Cheng Peng, Chen Wei, Daniel Khashabi, Rama Chellappa, Alan Yuille, Jieneng Chen'}",,
702,AniSora: Exploring the Frontiers of Animation Video Generation in the Sora Era,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'AniSora: Exploring the Frontiers of Animation Video Generation in the Sora Era'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.10255'}]",https://arxiv.org/abs/2412.10255,"arXiv:2412.10255v3 Announce Type: replace 
Abstract: Animation has gained significant interest in the recent film and TV industry. Despite the success of advanced video generation models like Sora, Kling, and CogVideoX in generating natural videos, they lack the same effectiveness in handling animation videos. Evaluating animation video generation is also a great challenge due to its unique artist styles, violating the laws of physics and exaggerated motions. In this paper, we present a comprehensive system, AniSora, designed for animation video generation, which includes a data processing pipeline, a controllable generation model, and an evaluation dataset. Supported by the data processing pipeline with over 10M high-quality data, the generation model incorporates a spatiotemporal mask module to facilitate key animation production functions such as image-to-video generation, frame interpolation, and localized image-guided animation. We also collect an evaluation benchmark of 948 various animation videos, the evaluation on VBench and human double-blind test demonstrates consistency in character and motion, achieving state-of-the-art results in animation video generation. Our evaluation benchmark will be publicly available at https://github.com/bilibili/Index-anisora.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.10255v3 Announce Type: replace \nAbstract: Animation has gained significant interest in the recent film and TV industry. Despite the success of advanced video generation models like Sora, Kling, and CogVideoX in generating natural videos, they lack the same effectiveness in handling animation videos. Evaluating animation video generation is also a great challenge due to its unique artist styles, violating the laws of physics and exaggerated motions. In this paper, we present a comprehensive system, AniSora, designed for animation video generation, which includes a data processing pipeline, a controllable generation model, and an evaluation dataset. Supported by the data processing pipeline with over 10M high-quality data, the generation model incorporates a spatiotemporal mask module to facilitate key animation production functions such as image-to-video generation, frame interpolation, and localized image-guided animation. We also collect an evaluation benchmark of 948 various animation videos, the evaluation on VBench and human double-blind test demonstrates consistency in character and motion, achieving state-of-the-art results in animation video generation. Our evaluation benchmark will be publicly available at https://github.com/bilibili/Index-anisora.'}",oai:arXiv.org:2412.10255v3,False,"[{'term': 'cs.GR', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by-nc-sa/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-sa/4.0/'}","[{'name': 'Yudong Jiang, Baohan Xu, Siqian Yang, Mingyu Yin, Jing Liu, Chao Xu, Siqi Wang, Yidi Wu, Bingwen Zhu, Xinwen Zhang, Xingyu Zheng, Jixuan Xu, Yue Zhang, Jinlong Hou, Huyang Sun'}]","Yudong Jiang, Baohan Xu, Siqian Yang, Mingyu Yin, Jing Liu, Chao Xu, Siqi Wang, Yidi Wu, Bingwen Zhu, Xinwen Zhang, Xingyu Zheng, Jixuan Xu, Yue Zhang, Jinlong Hou, Huyang Sun","{'name': 'Yudong Jiang, Baohan Xu, Siqian Yang, Mingyu Yin, Jing Liu, Chao Xu, Siqi Wang, Yidi Wu, Bingwen Zhu, Xinwen Zhang, Xingyu Zheng, Jixuan Xu, Yue Zhang, Jinlong Hou, Huyang Sun'}",,
703,Shape error prediction in 5-axis machining using graph neural networks,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Shape error prediction in 5-axis machining using graph neural networks'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.10341'}]",https://arxiv.org/abs/2412.10341,"arXiv:2412.10341v2 Announce Type: replace 
Abstract: This paper presents an innovative method for predicting shape errors in 5-axis machining using graph neural networks. The graph structure is defined with nodes representing workpiece surface points and edges denoting the neighboring relationships. The dataset encompasses data from a material removal simulation, process data, and post-machining quality information. Experimental results show that the presented approach can generalize the shape error prediction for the investigated workpiece geometry. Moreover, by modelling spatial and temporal connections within the workpiece, the approach handles a low number of labels compared to non-graphical methods such as Support Vector Machines.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.10341v2 Announce Type: replace \nAbstract: This paper presents an innovative method for predicting shape errors in 5-axis machining using graph neural networks. The graph structure is defined with nodes representing workpiece surface points and edges denoting the neighboring relationships. The dataset encompasses data from a material removal simulation, process data, and post-machining quality information. Experimental results show that the presented approach can generalize the shape error prediction for the investigated workpiece geometry. Moreover, by modelling spatial and temporal connections within the workpiece, the approach handles a low number of labels compared to non-graphical methods such as Support Vector Machines.'}",oai:arXiv.org:2412.10341v2,False,"[{'term': 'eess.SY', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'cs.SY', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by-nc-nd/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-nd/4.0/'}","[{'name': 'Julia Huuk, Abheek Dhingra, Eirini Ntoutsi, Berend Denkena'}]","Julia Huuk, Abheek Dhingra, Eirini Ntoutsi, Berend Denkena","{'name': 'Julia Huuk, Abheek Dhingra, Eirini Ntoutsi, Berend Denkena'}",,
704,Evidence Contextualization and Counterfactual Attribution for Conversational QA over Heterogeneous Data with RAG Systems,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Evidence Contextualization and Counterfactual Attribution for Conversational QA over Heterogeneous Data with RAG Systems'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.10571'}]",https://arxiv.org/abs/2412.10571,"arXiv:2412.10571v2 Announce Type: replace 
Abstract: Retrieval Augmented Generation (RAG) works as a backbone for interacting with an enterprise's own data via Conversational Question Answering (ConvQA). In a RAG system, a retriever fetches passages from a collection in response to a question, which are then included in the prompt of a large language model (LLM) for generating a natural language (NL) answer. However, several RAG systems today suffer from two shortcomings: (i) retrieved passages usually contain their raw text and lack appropriate document context, negatively impacting both retrieval and answering quality; and (ii) attribution strategies that explain answer generation usually rely only on similarity between the answer and the retrieved passages, thereby only generating plausible but not causal explanations. In this work, we demonstrate RAGONITE, a RAG system that remedies the above concerns by: (i) contextualizing evidence with source metadata and surrounding text; and (ii) computing counterfactual attribution, a causal explanation approach where the contribution of an evidence to an answer is determined by the similarity of the original response to the answer obtained by removing that evidence. To evaluate our proposals, we release a new benchmark ConfQuestions, with 300 hand-created conversational questions, each in English and German, coupled with ground truth URLs, completed questions, and answers from 215 public Confluence pages, that are typical of enterprise wiki spaces with heterogeneous elements. Experiments with RAGONITE on ConfQuestions show the viability of our ideas: contextualization improves RAG performance, and counterfactual attribution is effective at explaining RAG answers.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.10571v2 Announce Type: replace \nAbstract: Retrieval Augmented Generation (RAG) works as a backbone for interacting with an enterprise's own data via Conversational Question Answering (ConvQA). In a RAG system, a retriever fetches passages from a collection in response to a question, which are then included in the prompt of a large language model (LLM) for generating a natural language (NL) answer. However, several RAG systems today suffer from two shortcomings: (i) retrieved passages usually contain their raw text and lack appropriate document context, negatively impacting both retrieval and answering quality; and (ii) attribution strategies that explain answer generation usually rely only on similarity between the answer and the retrieved passages, thereby only generating plausible but not causal explanations. In this work, we demonstrate RAGONITE, a RAG system that remedies the above concerns by: (i) contextualizing evidence with source metadata and surrounding text; and (ii) computing counterfactual attribution, a causal explanation approach where the contribution of an evidence to an answer is determined by the similarity of the original response to the answer obtained by removing that evidence. To evaluate our proposals, we release a new benchmark ConfQuestions, with 300 hand-created conversational questions, each in English and German, coupled with ground truth URLs, completed questions, and answers from 215 public Confluence pages, that are typical of enterprise wiki spaces with heterogeneous elements. Experiments with RAGONITE on ConfQuestions show the viability of our ideas: contextualization improves RAG performance, and counterfactual attribution is effective at explaining RAG answers.""}",oai:arXiv.org:2412.10571v2,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}, {'term': 'cs.IR', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Rishiraj Saha Roy, Joel Schlotthauer, Chris Hinze, Andreas Foltyn, Luzian Hahn, Fabian Kuech'}]","Rishiraj Saha Roy, Joel Schlotthauer, Chris Hinze, Andreas Foltyn, Luzian Hahn, Fabian Kuech","{'name': 'Rishiraj Saha Roy, Joel Schlotthauer, Chris Hinze, Andreas Foltyn, Luzian Hahn, Fabian Kuech'}",,
705,One Pixel is All I Need,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'One Pixel is All I Need'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.10681'}]",https://arxiv.org/abs/2412.10681,"arXiv:2412.10681v2 Announce Type: replace 
Abstract: Vision Transformers (ViTs) have achieved record-breaking performance in various visual tasks. However, concerns about their robustness against backdoor attacks have grown. Backdoor attacks involve associating a specific trigger with a target label, causing the model to predict the attacker-specified label when the trigger is present, while correctly identifying clean images.We found that ViTs exhibit higher attack success rates for quasi-triggers(patterns different from but similar to the original training triggers)compared to CNNs. Moreover, some backdoor features in clean samples can suppress the original trigger, making quasi-triggers more effective.To better understand and exploit these vulnerabilities, we developed a tool called the Perturbation Sensitivity Distribution Map (PSDM). PSDM computes and sums gradients over many inputs to show how sensitive the model is to small changes in the input. In ViTs, PSDM reveals a patch-like pattern where central pixels are more sensitive than edges. We use PSDM to guide the creation of quasi-triggers.Based on these findings, we designed ""WorstVIT,"" a simple yet effective data poisoning backdoor for ViT models. This attack requires an extremely low poisoning rate, trains for just one epoch, and modifies a single pixel to successfully attack all validation images.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.10681v2 Announce Type: replace \nAbstract: Vision Transformers (ViTs) have achieved record-breaking performance in various visual tasks. However, concerns about their robustness against backdoor attacks have grown. Backdoor attacks involve associating a specific trigger with a target label, causing the model to predict the attacker-specified label when the trigger is present, while correctly identifying clean images.We found that ViTs exhibit higher attack success rates for quasi-triggers(patterns different from but similar to the original training triggers)compared to CNNs. Moreover, some backdoor features in clean samples can suppress the original trigger, making quasi-triggers more effective.To better understand and exploit these vulnerabilities, we developed a tool called the Perturbation Sensitivity Distribution Map (PSDM). PSDM computes and sums gradients over many inputs to show how sensitive the model is to small changes in the input. In ViTs, PSDM reveals a patch-like pattern where central pixels are more sensitive than edges. We use PSDM to guide the creation of quasi-triggers.Based on these findings, we designed ""WorstVIT,"" a simple yet effective data poisoning backdoor for ViT models. This attack requires an extremely low poisoning rate, trains for just one epoch, and modifies a single pixel to successfully attack all validation images.'}",oai:arXiv.org:2412.10681v2,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Deng Siqin, Zhou Xiaoyi'}]","Deng Siqin, Zhou Xiaoyi","{'name': 'Deng Siqin, Zhou Xiaoyi'}",,
706,NeuralPLexer3: Accurate Biomolecular Complex Structure Prediction with Flow Models,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'NeuralPLexer3: Accurate Biomolecular Complex Structure Prediction with Flow Models'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.10743'}]",https://arxiv.org/abs/2412.10743,"arXiv:2412.10743v2 Announce Type: replace 
Abstract: Structure determination is essential to a mechanistic understanding of diseases and the development of novel therapeutics. Machine-learning-based structure prediction methods have made significant advancements by computationally predicting protein and bioassembly structures from sequences and molecular topology alone. Despite substantial progress in the field, challenges remain to deliver structure prediction models to real-world drug discovery. Here, we present NeuralPLexer3 -- a physics-inspired flow-based generative model that achieves state-of-the-art prediction accuracy on key biomolecular interaction types and improves training and sampling efficiency compared to its predecessors and alternative methodologies. Examined through newly developed benchmarking strategies, NeuralPLexer3 excels in vital areas that are crucial to structure-based drug design, such as physical validity and ligand-induced conformational changes.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.10743v2 Announce Type: replace \nAbstract: Structure determination is essential to a mechanistic understanding of diseases and the development of novel therapeutics. Machine-learning-based structure prediction methods have made significant advancements by computationally predicting protein and bioassembly structures from sequences and molecular topology alone. Despite substantial progress in the field, challenges remain to deliver structure prediction models to real-world drug discovery. Here, we present NeuralPLexer3 -- a physics-inspired flow-based generative model that achieves state-of-the-art prediction accuracy on key biomolecular interaction types and improves training and sampling efficiency compared to its predecessors and alternative methodologies. Examined through newly developed benchmarking strategies, NeuralPLexer3 excels in vital areas that are crucial to structure-based drug design, such as physical validity and ligand-induced conformational changes.'}",oai:arXiv.org:2412.10743v2,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'physics.chem-ph', 'scheme': None, 'label': None}, {'term': 'q-bio.BM', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Zhuoran Qiao, Feizhi Ding, Thomas Dresselhaus, Mia A. Rosenfeld, Xiaotian Han, Owen Howell, Aniketh Iyengar, Stephen Opalenski, Anders S. Christensen, Sai Krishna Sirumalla, Frederick R. Manby, Thomas F. Miller III, Matthew Welborn'}]","Zhuoran Qiao, Feizhi Ding, Thomas Dresselhaus, Mia A. Rosenfeld, Xiaotian Han, Owen Howell, Aniketh Iyengar, Stephen Opalenski, Anders S. Christensen, Sai Krishna Sirumalla, Frederick R. Manby, Thomas F. Miller III, Matthew Welborn","{'name': 'Zhuoran Qiao, Feizhi Ding, Thomas Dresselhaus, Mia A. Rosenfeld, Xiaotian Han, Owen Howell, Aniketh Iyengar, Stephen Opalenski, Anders S. Christensen, Sai Krishna Sirumalla, Frederick R. Manby, Thomas F. Miller III, Matthew Welborn'}",,
707,Distribution-Consistency-Guided Multi-modal Hashing,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Distribution-Consistency-Guided Multi-modal Hashing'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.11216'}]",https://arxiv.org/abs/2412.11216,"arXiv:2412.11216v2 Announce Type: replace 
Abstract: Multi-modal hashing methods have gained popularity due to their fast speed and low storage requirements. Among them, the supervised methods demonstrate better performance by utilizing labels as supervisory signals compared with unsupervised methods. Currently, for almost all supervised multi-modal hashing methods, there is a hidden assumption that training sets have no noisy labels. However, labels are often annotated incorrectly due to manual labeling in real-world scenarios, which will greatly harm the retrieval performance. To address this issue, we first discover a significant distribution consistency pattern through experiments, i.e., the 1-0 distribution of the presence or absence of each category in the label is consistent with the high-low distribution of similarity scores of the hash codes relative to category centers. Then, inspired by this pattern, we propose a novel Distribution-Consistency-Guided Multi-modal Hashing (DCGMH), which aims to filter and reconstruct noisy labels to enhance retrieval performance. Specifically, the proposed method first randomly initializes several category centers, which are used to compute the high-low distribution of similarity scores; Noisy and clean labels are then separately filtered out via the discovered distribution consistency pattern to mitigate the impact of noisy labels; Subsequently, a correction strategy, which is indirectly designed via the distribution consistency pattern, is applied to the filtered noisy labels, correcting high-confidence ones while treating low-confidence ones as unlabeled for unsupervised learning, thereby further enhancing the model's performance. Extensive experiments on three widely used datasets demonstrate the superiority of the proposed method compared to state-of-the-art baselines in multi-modal retrieval tasks. The code is available at https://github.com/LiuJinyu1229/DCGMH.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.11216v2 Announce Type: replace \nAbstract: Multi-modal hashing methods have gained popularity due to their fast speed and low storage requirements. Among them, the supervised methods demonstrate better performance by utilizing labels as supervisory signals compared with unsupervised methods. Currently, for almost all supervised multi-modal hashing methods, there is a hidden assumption that training sets have no noisy labels. However, labels are often annotated incorrectly due to manual labeling in real-world scenarios, which will greatly harm the retrieval performance. To address this issue, we first discover a significant distribution consistency pattern through experiments, i.e., the 1-0 distribution of the presence or absence of each category in the label is consistent with the high-low distribution of similarity scores of the hash codes relative to category centers. Then, inspired by this pattern, we propose a novel Distribution-Consistency-Guided Multi-modal Hashing (DCGMH), which aims to filter and reconstruct noisy labels to enhance retrieval performance. Specifically, the proposed method first randomly initializes several category centers, which are used to compute the high-low distribution of similarity scores; Noisy and clean labels are then separately filtered out via the discovered distribution consistency pattern to mitigate the impact of noisy labels; Subsequently, a correction strategy, which is indirectly designed via the distribution consistency pattern, is applied to the filtered noisy labels, correcting high-confidence ones while treating low-confidence ones as unlabeled for unsupervised learning, thereby further enhancing the model's performance. Extensive experiments on three widely used datasets demonstrate the superiority of the proposed method compared to state-of-the-art baselines in multi-modal retrieval tasks. The code is available at https://github.com/LiuJinyu1229/DCGMH.""}",oai:arXiv.org:2412.11216v2,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.IR', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Jin-Yu Liu, Xian-Ling Mao, Tian-Yi Che, Rong-Cheng Tu'}]","Jin-Yu Liu, Xian-Ling Mao, Tian-Yi Che, Rong-Cheng Tu","{'name': 'Jin-Yu Liu, Xian-Ling Mao, Tian-Yi Che, Rong-Cheng Tu'}",,
708,A Syntactic Approach to Computing Complete and Sound Abstraction in the Situation Calculus,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'A Syntactic Approach to Computing Complete and Sound Abstraction in the Situation Calculus'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.11217'}]",https://arxiv.org/abs/2412.11217,"arXiv:2412.11217v2 Announce Type: replace 
Abstract: Abstraction is an important and useful concept in the field of artificial intelligence. To the best of our knowledge, there is no syntactic method to compute a sound and complete abstraction from a given low-level basic action theory and a refinement mapping. This paper aims to address this issue.To this end, we first present a variant of situation calculus,namely linear integer situation calculus, which serves as the formalization of high-level basic action theory. We then migrate Banihashemi, De Giacomo, and Lesp\'erance's abstraction framework to one from linear integer situation calculus to extended situation calculus. Furthermore, we identify a class of Golog programs, namely guarded actions,that is used to restrict low-level Golog programs, and impose some restrictions on refinement mappings. Finally, we design a syntactic approach to computing a sound and complete abstraction from a low-level basic action theory and a restricted refinement mapping.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.11217v2 Announce Type: replace \nAbstract: Abstraction is an important and useful concept in the field of artificial intelligence. To the best of our knowledge, there is no syntactic method to compute a sound and complete abstraction from a given low-level basic action theory and a refinement mapping. This paper aims to address this issue.To this end, we first present a variant of situation calculus,namely linear integer situation calculus, which serves as the formalization of high-level basic action theory. We then migrate Banihashemi, De Giacomo, and Lesp\\'erance's abstraction framework to one from linear integer situation calculus to extended situation calculus. Furthermore, we identify a class of Golog programs, namely guarded actions,that is used to restrict low-level Golog programs, and impose some restrictions on refinement mappings. Finally, we design a syntactic approach to computing a sound and complete abstraction from a low-level basic action theory and a restricted refinement mapping.""}",oai:arXiv.org:2412.11217v2,False,"[{'term': 'cs.LO', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Liangda Fang, Xiaoman Wang, Zhang Chen, Kailun Luo, Zhenhe Cui, Quanlong Guan'}]","Liangda Fang, Xiaoman Wang, Zhang Chen, Kailun Luo, Zhenhe Cui, Quanlong Guan","{'name': 'Liangda Fang, Xiaoman Wang, Zhang Chen, Kailun Luo, Zhenhe Cui, Quanlong Guan'}",,
709,TrimLLM: Progressive Layer Dropping for Domain-Specific LLMs,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'TrimLLM: Progressive Layer Dropping for Domain-Specific LLMs'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.11242'}]",https://arxiv.org/abs/2412.11242,"arXiv:2412.11242v2 Announce Type: replace 
Abstract: Specializing large language models (LLMs) for local deployment in domain-specific use cases is necessary for strong performance while meeting latency and privacy constraints. However, conventional task-specific adaptation approaches do not show simultaneous memory saving and inference speedup at deployment time. Practical compression techniques like quantization and pruning require dedicated hardware or kernel support to achieve measured inference speedup. We develop TrimLLM based on the layer-wise specialization phenomenon we empirically observed and verified on contemporary LLMs. TrimLLM reduces the depth of LLMs via progressive layer dropping. We show it retains LLMs' capacity in specific domains and achieves inference speedup irrespective of hardware and deep learning frameworks. We evaluated TrimLLM on LLMs of various sizes for inference; models adapted on medical, legal, and financial datasets all demonstrate $2.1-5.7\times$ inference speedup on consumer GPUs and up to $3.1\times$ speedup on A100 when compared to state-of-the-art model compression algorithms, with no loss in accuracy at 50$\sim$60\% model compression ratio.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.11242v2 Announce Type: replace \nAbstract: Specializing large language models (LLMs) for local deployment in domain-specific use cases is necessary for strong performance while meeting latency and privacy constraints. However, conventional task-specific adaptation approaches do not show simultaneous memory saving and inference speedup at deployment time. Practical compression techniques like quantization and pruning require dedicated hardware or kernel support to achieve measured inference speedup. We develop TrimLLM based on the layer-wise specialization phenomenon we empirically observed and verified on contemporary LLMs. TrimLLM reduces the depth of LLMs via progressive layer dropping. We show it retains LLMs' capacity in specific domains and achieves inference speedup irrespective of hardware and deep learning frameworks. We evaluated TrimLLM on LLMs of various sizes for inference; models adapted on medical, legal, and financial datasets all demonstrate $2.1-5.7\\times$ inference speedup on consumer GPUs and up to $3.1\\times$ speedup on A100 when compared to state-of-the-art model compression algorithms, with no loss in accuracy at 50$\\sim$60\\% model compression ratio.""}",oai:arXiv.org:2412.11242v2,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.CL', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Lanxiang Hu, Tajana Rosing, Hao Zhang'}]","Lanxiang Hu, Tajana Rosing, Hao Zhang","{'name': 'Lanxiang Hu, Tajana Rosing, Hao Zhang'}",,
710,Temporal Logic Control for Nonlinear Stochastic Systems Under Unknown Disturbances,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Temporal Logic Control for Nonlinear Stochastic Systems Under Unknown Disturbances'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.11343'}]",https://arxiv.org/abs/2412.11343,"arXiv:2412.11343v2 Announce Type: replace 
Abstract: In this paper, we present a novel framework to synthesize robust strategies for discrete-time nonlinear systems with random disturbances that are unknown, against temporal logic specifications. The proposed framework is data-driven and abstraction-based: leveraging observations of the system, our approach learns a high-confidence abstraction of the system in the form of an uncertain Markov decision process (UMDP). The uncertainty in the resulting UMDP is used to formally account for both the error in abstracting the system and for the uncertainty coming from the data. Critically, we show that for any given state-action pair in the resulting UMDP, the uncertainty in the transition probabilities can be represented as a convex polytope obtained by a two-layer state discretization and concentration inequalities. This allows us to obtain tighter uncertainty estimates compared to existing approaches, and guarantees efficiency, as we tailor a synthesis algorithm exploiting the structure of this UMDP. We empirically validate our approach on several case studies, showing substantially improved performance compared to the state-of-the-art.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.11343v2 Announce Type: replace \nAbstract: In this paper, we present a novel framework to synthesize robust strategies for discrete-time nonlinear systems with random disturbances that are unknown, against temporal logic specifications. The proposed framework is data-driven and abstraction-based: leveraging observations of the system, our approach learns a high-confidence abstraction of the system in the form of an uncertain Markov decision process (UMDP). The uncertainty in the resulting UMDP is used to formally account for both the error in abstracting the system and for the uncertainty coming from the data. Critically, we show that for any given state-action pair in the resulting UMDP, the uncertainty in the transition probabilities can be represented as a convex polytope obtained by a two-layer state discretization and concentration inequalities. This allows us to obtain tighter uncertainty estimates compared to existing approaches, and guarantees efficiency, as we tailor a synthesis algorithm exploiting the structure of this UMDP. We empirically validate our approach on several case studies, showing substantially improved performance compared to the state-of-the-art.'}",oai:arXiv.org:2412.11343v2,False,"[{'term': 'eess.SY', 'scheme': None, 'label': None}, {'term': 'cs.SY', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Ibon Gracia, Luca Laurenti, Manuel Mazo Jr., Alessandro Abate, Morteza Lahijanian'}]","Ibon Gracia, Luca Laurenti, Manuel Mazo Jr., Alessandro Abate, Morteza Lahijanian","{'name': 'Ibon Gracia, Luca Laurenti, Manuel Mazo Jr., Alessandro Abate, Morteza Lahijanian'}",,
711,TRAIL: Trust-Aware Client Scheduling for Semi-Decentralized Federated Learning,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'TRAIL: Trust-Aware Client Scheduling for Semi-Decentralized Federated Learning'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.11448'}]",https://arxiv.org/abs/2412.11448,"arXiv:2412.11448v3 Announce Type: replace 
Abstract: Due to the sensitivity of data, Federated Learning (FL) is employed to enable distributed machine learning while safeguarding data privacy and accommodating the requirements of various devices. However, in the context of semi-decentralized FL, clients' communication and training states are dynamic. This variability arises from local training fluctuations, heterogeneous data distributions, and intermittent client participation. Most existing studies primarily focus on stable client states, neglecting the dynamic challenges inherent in real-world scenarios. To tackle this issue, we propose a TRust-Aware clIent scheduLing mechanism called TRAIL, which assesses client states and contributions, enhancing model training efficiency through selective client participation. We focus on a semi-decentralized FL framework where edge servers and clients train a shared global model using unreliable intra-cluster model aggregation and inter-cluster model consensus. First, we propose an adaptive hidden semi-Markov model to estimate clients' communication states and contributions. Next, we address a client-server association optimization problem to minimize global training loss. Using convergence analysis, we propose a greedy client scheduling algorithm. Finally, our experiments conducted on real-world datasets demonstrate that TRAIL outperforms state-of-the-art baselines, achieving an improvement of 8.7% in test accuracy and a reduction of 15.3% in training loss.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.11448v3 Announce Type: replace \nAbstract: Due to the sensitivity of data, Federated Learning (FL) is employed to enable distributed machine learning while safeguarding data privacy and accommodating the requirements of various devices. However, in the context of semi-decentralized FL, clients' communication and training states are dynamic. This variability arises from local training fluctuations, heterogeneous data distributions, and intermittent client participation. Most existing studies primarily focus on stable client states, neglecting the dynamic challenges inherent in real-world scenarios. To tackle this issue, we propose a TRust-Aware clIent scheduLing mechanism called TRAIL, which assesses client states and contributions, enhancing model training efficiency through selective client participation. We focus on a semi-decentralized FL framework where edge servers and clients train a shared global model using unreliable intra-cluster model aggregation and inter-cluster model consensus. First, we propose an adaptive hidden semi-Markov model to estimate clients' communication states and contributions. Next, we address a client-server association optimization problem to minimize global training loss. Using convergence analysis, we propose a greedy client scheduling algorithm. Finally, our experiments conducted on real-world datasets demonstrate that TRAIL outperforms state-of-the-art baselines, achieving an improvement of 8.7% in test accuracy and a reduction of 15.3% in training loss.""}",oai:arXiv.org:2412.11448v3,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.DC', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Gangqiang Hu, Jianfeng Lu, Jianmin Han, Shuqin Cao, Jing Liu, Hao Fu'}]","Gangqiang Hu, Jianfeng Lu, Jianmin Han, Shuqin Cao, Jing Liu, Hao Fu","{'name': 'Gangqiang Hu, Jianfeng Lu, Jianmin Han, Shuqin Cao, Jing Liu, Hao Fu'}",,
712,RoMeO: Robust Metric Visual Odometry,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'RoMeO: Robust Metric Visual Odometry'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.11530'}]",https://arxiv.org/abs/2412.11530,"arXiv:2412.11530v2 Announce Type: replace 
Abstract: Visual odometry (VO) aims to estimate camera poses from visual inputs -- a fundamental building block for many applications such as VR/AR and robotics. This work focuses on monocular RGB VO where the input is a monocular RGB video without IMU or 3D sensors. Existing approaches lack robustness under this challenging scenario and fail to generalize to unseen data (especially outdoors); they also cannot recover metric-scale poses. We propose Robust Metric Visual Odometry (RoMeO), a novel method that resolves these issues leveraging priors from pre-trained depth models. RoMeO incorporates both monocular metric depth and multi-view stereo (MVS) models to recover metric-scale, simplify correspondence search, provide better initialization and regularize optimization. Effective strategies are proposed to inject noise during training and adaptively filter noisy depth priors, which ensure the robustness of RoMeO on in-the-wild data. As shown in Fig.1, RoMeO advances the state-of-the-art (SOTA) by a large margin across 6 diverse datasets covering both indoor and outdoor scenes. Compared to the current SOTA DPVO, RoMeO reduces the relative (align the trajectory scale with GT) and absolute trajectory errors both by >50%. The performance gain also transfers to the full SLAM pipeline (with global BA & loop closure). Code will be released upon acceptance.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.11530v2 Announce Type: replace \nAbstract: Visual odometry (VO) aims to estimate camera poses from visual inputs -- a fundamental building block for many applications such as VR/AR and robotics. This work focuses on monocular RGB VO where the input is a monocular RGB video without IMU or 3D sensors. Existing approaches lack robustness under this challenging scenario and fail to generalize to unseen data (especially outdoors); they also cannot recover metric-scale poses. We propose Robust Metric Visual Odometry (RoMeO), a novel method that resolves these issues leveraging priors from pre-trained depth models. RoMeO incorporates both monocular metric depth and multi-view stereo (MVS) models to recover metric-scale, simplify correspondence search, provide better initialization and regularize optimization. Effective strategies are proposed to inject noise during training and adaptively filter noisy depth priors, which ensure the robustness of RoMeO on in-the-wild data. As shown in Fig.1, RoMeO advances the state-of-the-art (SOTA) by a large margin across 6 diverse datasets covering both indoor and outdoor scenes. Compared to the current SOTA DPVO, RoMeO reduces the relative (align the trajectory scale with GT) and absolute trajectory errors both by >50%. The performance gain also transfers to the full SLAM pipeline (with global BA & loop closure). Code will be released upon acceptance.'}",oai:arXiv.org:2412.11530v2,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Junda Cheng, Zhipeng Cai, Zhaoxing Zhang, Wei Yin, Matthias Muller, Michael Paulitsch, Xin Yang'}]","Junda Cheng, Zhipeng Cai, Zhaoxing Zhang, Wei Yin, Matthias Muller, Michael Paulitsch, Xin Yang","{'name': 'Junda Cheng, Zhipeng Cai, Zhaoxing Zhang, Wei Yin, Matthias Muller, Michael Paulitsch, Xin Yang'}",,
713,THESAURUS: Contrastive Graph Clustering by Swapping Fused Gromov-Wasserstein Couplings,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'THESAURUS: Contrastive Graph Clustering by Swapping Fused Gromov-Wasserstein Couplings'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.11550'}]",https://arxiv.org/abs/2412.11550,"arXiv:2412.11550v2 Announce Type: replace 
Abstract: Graph node clustering is a fundamental unsupervised task. Existing methods typically train an encoder through selfsupervised learning and then apply K-means to the encoder output. Some methods use this clustering result directly as the final assignment, while others initialize centroids based on this initial clustering and then finetune both the encoder and these learnable centroids. However, due to their reliance on K-means, these methods inherit its drawbacks when the cluster separability of encoder output is low, facing challenges from the Uniform Effect and Cluster Assimilation. We summarize three reasons for the low cluster separability in existing methods: (1) lack of contextual information prevents discrimination between similar nodes from different clusters; (2) training tasks are not sufficiently aligned with the downstream clustering task; (3) the cluster information in the graph structure is not appropriately exploited. To address these issues, we propose conTrastive grapH clustEring by SwApping fUsed gRomov-wasserstein coUplingS (THESAURUS). Our method introduces semantic prototypes to provide contextual information, and employs a cross-view assignment prediction pretext task that aligns well with the downstream clustering task. Additionally, it utilizes Gromov-Wasserstein Optimal Transport (GW-OT) along with the proposed prototype graph to thoroughly exploit cluster information in the graph structure. To adapt to diverse real-world data, THESAURUS updates the prototype graph and the prototype marginal distribution in OT by using momentum. Extensive experiments demonstrate that THESAURUS achieves higher cluster separability than the prior art, effectively mitigating the Uniform Effect and Cluster Assimilation issues","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.11550v2 Announce Type: replace \nAbstract: Graph node clustering is a fundamental unsupervised task. Existing methods typically train an encoder through selfsupervised learning and then apply K-means to the encoder output. Some methods use this clustering result directly as the final assignment, while others initialize centroids based on this initial clustering and then finetune both the encoder and these learnable centroids. However, due to their reliance on K-means, these methods inherit its drawbacks when the cluster separability of encoder output is low, facing challenges from the Uniform Effect and Cluster Assimilation. We summarize three reasons for the low cluster separability in existing methods: (1) lack of contextual information prevents discrimination between similar nodes from different clusters; (2) training tasks are not sufficiently aligned with the downstream clustering task; (3) the cluster information in the graph structure is not appropriately exploited. To address these issues, we propose conTrastive grapH clustEring by SwApping fUsed gRomov-wasserstein coUplingS (THESAURUS). Our method introduces semantic prototypes to provide contextual information, and employs a cross-view assignment prediction pretext task that aligns well with the downstream clustering task. Additionally, it utilizes Gromov-Wasserstein Optimal Transport (GW-OT) along with the proposed prototype graph to thoroughly exploit cluster information in the graph structure. To adapt to diverse real-world data, THESAURUS updates the prototype graph and the prototype marginal distribution in OT by using momentum. Extensive experiments demonstrate that THESAURUS achieves higher cluster separability than the prior art, effectively mitigating the Uniform Effect and Cluster Assimilation issues'}",oai:arXiv.org:2412.11550v2,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Bowen Deng, Tong Wang, Lele Fu, Sheng Huang, Chuan Chen, Tao Zhang'}]","Bowen Deng, Tong Wang, Lele Fu, Sheng Huang, Chuan Chen, Tao Zhang","{'name': 'Bowen Deng, Tong Wang, Lele Fu, Sheng Huang, Chuan Chen, Tao Zhang'}",,
714,StrandHead: Text to Strand-Disentangled 3D Head Avatars Using Hair Geometric Priors,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'StrandHead: Text to Strand-Disentangled 3D Head Avatars Using Hair Geometric Priors'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.11586'}]",https://arxiv.org/abs/2412.11586,"arXiv:2412.11586v2 Announce Type: replace 
Abstract: While haircut indicates distinct personality, existing avatar generation methods fail to model practical hair due to the general or entangled representation. We propose StrandHead, a novel text to 3D head avatar generation method capable of generating disentangled 3D hair with strand representation. Without using 3D data for supervision, we demonstrate that realistic hair strands can be generated from prompts by distilling 2D generative diffusion models. To this end, we propose a series of reliable priors on shape initialization, geometric primitives, and statistical haircut features, leading to a stable optimization and text-aligned performance. Extensive experiments show that StrandHead achieves the state-of-the-art reality and diversity of generated 3D head and hair. The generated 3D hair can also be easily implemented in the Unreal Engine for physical simulation and other applications. The code will be available at https://xiaokunsun.github.io/StrandHead.github.io.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.11586v2 Announce Type: replace \nAbstract: While haircut indicates distinct personality, existing avatar generation methods fail to model practical hair due to the general or entangled representation. We propose StrandHead, a novel text to 3D head avatar generation method capable of generating disentangled 3D hair with strand representation. Without using 3D data for supervision, we demonstrate that realistic hair strands can be generated from prompts by distilling 2D generative diffusion models. To this end, we propose a series of reliable priors on shape initialization, geometric primitives, and statistical haircut features, leading to a stable optimization and text-aligned performance. Extensive experiments show that StrandHead achieves the state-of-the-art reality and diversity of generated 3D head and hair. The generated 3D hair can also be easily implemented in the Unreal Engine for physical simulation and other applications. The code will be available at https://xiaokunsun.github.io/StrandHead.github.io.'}",oai:arXiv.org:2412.11586v2,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Xiaokun Sun, Zeyu Cai, Ying Tai, Jian Yang, Zhenyu Zhang'}]","Xiaokun Sun, Zeyu Cai, Ying Tai, Jian Yang, Zhenyu Zhang","{'name': 'Xiaokun Sun, Zeyu Cai, Ying Tai, Jian Yang, Zhenyu Zhang'}",,
715,Smoothness Really Matters: A Simple Yet Effective Approach for Unsupervised Graph Domain Adaptation,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Smoothness Really Matters: A Simple Yet Effective Approach for Unsupervised Graph Domain Adaptation'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.11654'}]",https://arxiv.org/abs/2412.11654,"arXiv:2412.11654v2 Announce Type: replace 
Abstract: Unsupervised Graph Domain Adaptation (UGDA) seeks to bridge distribution shifts between domains by transferring knowledge from labeled source graphs to given unlabeled target graphs. Existing UGDA methods primarily focus on aligning features in the latent space learned by graph neural networks (GNNs) across domains, often overlooking structural shifts, resulting in limited effectiveness when addressing structurally complex transfer scenarios. Given the sensitivity of GNNs to local structural features, even slight discrepancies between source and target graphs could lead to significant shifts in node embeddings, thereby reducing the effectiveness of knowledge transfer. To address this issue, we introduce a novel approach for UGDA called Target-Domain Structural Smoothing (TDSS). TDSS is a simple and effective method designed to perform structural smoothing directly on the target graph, thereby mitigating structural distribution shifts and ensuring the consistency of node representations. Specifically, by integrating smoothing techniques with neighborhood sampling, TDSS maintains the structural coherence of the target graph while mitigating the risk of over-smoothing. Our theoretical analysis shows that TDSS effectively reduces target risk by improving model smoothness. Empirical results on three real-world datasets demonstrate that TDSS outperforms recent state-of-the-art baselines, achieving significant improvements across six transfer scenarios. The code is available in https://github.com/cwei01/TDSS.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.11654v2 Announce Type: replace \nAbstract: Unsupervised Graph Domain Adaptation (UGDA) seeks to bridge distribution shifts between domains by transferring knowledge from labeled source graphs to given unlabeled target graphs. Existing UGDA methods primarily focus on aligning features in the latent space learned by graph neural networks (GNNs) across domains, often overlooking structural shifts, resulting in limited effectiveness when addressing structurally complex transfer scenarios. Given the sensitivity of GNNs to local structural features, even slight discrepancies between source and target graphs could lead to significant shifts in node embeddings, thereby reducing the effectiveness of knowledge transfer. To address this issue, we introduce a novel approach for UGDA called Target-Domain Structural Smoothing (TDSS). TDSS is a simple and effective method designed to perform structural smoothing directly on the target graph, thereby mitigating structural distribution shifts and ensuring the consistency of node representations. Specifically, by integrating smoothing techniques with neighborhood sampling, TDSS maintains the structural coherence of the target graph while mitigating the risk of over-smoothing. Our theoretical analysis shows that TDSS effectively reduces target risk by improving model smoothness. Empirical results on three real-world datasets demonstrate that TDSS outperforms recent state-of-the-art baselines, achieving significant improvements across six transfer scenarios. The code is available in https://github.com/cwei01/TDSS.'}",oai:arXiv.org:2412.11654v2,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Wei Chen, Guo Ye, Yakun Wang, Zhao Zhang, Libang Zhang, Daxin Wang, Zhiqiang Zhang, Fuzhen Zhuang'}]","Wei Chen, Guo Ye, Yakun Wang, Zhao Zhang, Libang Zhang, Daxin Wang, Zhiqiang Zhang, Fuzhen Zhuang","{'name': 'Wei Chen, Guo Ye, Yakun Wang, Zhao Zhang, Libang Zhang, Daxin Wang, Zhiqiang Zhang, Fuzhen Zhuang'}",,
716,Beyond Dataset Creation: Critical View of Annotation Variation and Bias Probing of a Dataset for Online Radical Content Detection,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Beyond Dataset Creation: Critical View of Annotation Variation and Bias Probing of a Dataset for Online Radical Content Detection'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.11745'}]",https://arxiv.org/abs/2412.11745,"arXiv:2412.11745v2 Announce Type: replace 
Abstract: The proliferation of radical content on online platforms poses significant risks, including inciting violence and spreading extremist ideologies. Despite ongoing research, existing datasets and models often fail to address the complexities of multilingual and diverse data. To bridge this gap, we introduce a publicly available multilingual dataset annotated with radicalization levels, calls for action, and named entities in English, French, and Arabic. This dataset is pseudonymized to protect individual privacy while preserving contextual information. Beyond presenting our freely available dataset, we analyze the annotation process, highlighting biases and disagreements among annotators and their implications for model performance. Additionally, we use synthetic data to investigate the influence of socio-demographic traits on annotation patterns and model predictions. Our work offers a comprehensive examination of the challenges and opportunities in building robust datasets for radical content detection, emphasizing the importance of fairness and transparency in model development.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.11745v2 Announce Type: replace \nAbstract: The proliferation of radical content on online platforms poses significant risks, including inciting violence and spreading extremist ideologies. Despite ongoing research, existing datasets and models often fail to address the complexities of multilingual and diverse data. To bridge this gap, we introduce a publicly available multilingual dataset annotated with radicalization levels, calls for action, and named entities in English, French, and Arabic. This dataset is pseudonymized to protect individual privacy while preserving contextual information. Beyond presenting our freely available dataset, we analyze the annotation process, highlighting biases and disagreements among annotators and their implications for model performance. Additionally, we use synthetic data to investigate the influence of socio-demographic traits on annotation patterns and model predictions. Our work offers a comprehensive examination of the challenges and opportunities in building robust datasets for radical content detection, emphasizing the importance of fairness and transparency in model development.'}",oai:arXiv.org:2412.11745v2,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': ""Arij Riabi, Virginie Mouilleron, Menel Mahamdi, Wissam Antoun, Djam\\'e Seddah""}]","Arij Riabi, Virginie Mouilleron, Menel Mahamdi, Wissam Antoun, Djam\'e Seddah","{'name': ""Arij Riabi, Virginie Mouilleron, Menel Mahamdi, Wissam Antoun, Djam\\'e Seddah""}",,
717,ProsodyFM: Unsupervised Phrasing and Intonation Control for Intelligible Speech Synthesis,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'ProsodyFM: Unsupervised Phrasing and Intonation Control for Intelligible Speech Synthesis'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.11795'}]",https://arxiv.org/abs/2412.11795,"arXiv:2412.11795v2 Announce Type: replace 
Abstract: Prosody contains rich information beyond the literal meaning of words, which is crucial for the intelligibility of speech. Current models still fall short in phrasing and intonation; they not only miss or misplace breaks when synthesizing long sentences with complex structures but also produce unnatural intonation. We propose ProsodyFM, a prosody-aware text-to-speech synthesis (TTS) model with a flow-matching (FM) backbone that aims to enhance the phrasing and intonation aspects of prosody. ProsodyFM introduces two key components: a Phrase Break Encoder to capture initial phrase break locations, followed by a Duration Predictor for the flexible adjustment of break durations; and a Terminal Intonation Encoder which learns a bank of intonation shape tokens combined with a novel Pitch Processor for more robust modeling of human-perceived intonation change. ProsodyFM is trained with no explicit prosodic labels and yet can uncover a broad spectrum of break durations and intonation patterns. Experimental results demonstrate that ProsodyFM can effectively improve the phrasing and intonation aspects of prosody, thereby enhancing the overall intelligibility compared to four state-of-the-art (SOTA) models. Out-of-distribution experiments show that this prosody improvement can further bring ProsodyFM superior generalizability for unseen complex sentences and speakers. Our case study intuitively illustrates the powerful and fine-grained controllability of ProsodyFM over phrasing and intonation.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.11795v2 Announce Type: replace \nAbstract: Prosody contains rich information beyond the literal meaning of words, which is crucial for the intelligibility of speech. Current models still fall short in phrasing and intonation; they not only miss or misplace breaks when synthesizing long sentences with complex structures but also produce unnatural intonation. We propose ProsodyFM, a prosody-aware text-to-speech synthesis (TTS) model with a flow-matching (FM) backbone that aims to enhance the phrasing and intonation aspects of prosody. ProsodyFM introduces two key components: a Phrase Break Encoder to capture initial phrase break locations, followed by a Duration Predictor for the flexible adjustment of break durations; and a Terminal Intonation Encoder which learns a bank of intonation shape tokens combined with a novel Pitch Processor for more robust modeling of human-perceived intonation change. ProsodyFM is trained with no explicit prosodic labels and yet can uncover a broad spectrum of break durations and intonation patterns. Experimental results demonstrate that ProsodyFM can effectively improve the phrasing and intonation aspects of prosody, thereby enhancing the overall intelligibility compared to four state-of-the-art (SOTA) models. Out-of-distribution experiments show that this prosody improvement can further bring ProsodyFM superior generalizability for unseen complex sentences and speakers. Our case study intuitively illustrates the powerful and fine-grained controllability of ProsodyFM over phrasing and intonation.'}",oai:arXiv.org:2412.11795v2,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}, {'term': 'cs.SD', 'scheme': None, 'label': None}, {'term': 'eess.AS', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Xiangheng He, Junjie Chen, Zixing Zhang, Bj\\""orn W. Schuller'}]","Xiangheng He, Junjie Chen, Zixing Zhang, Bj\""orn W. Schuller","{'name': 'Xiangheng He, Junjie Chen, Zixing Zhang, Bj\\""orn W. Schuller'}",,
718,Does VLM Classification Benefit from LLM Description Semantics?,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Does VLM Classification Benefit from LLM Description Semantics?'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.11917'}]",https://arxiv.org/abs/2412.11917,"arXiv:2412.11917v3 Announce Type: replace 
Abstract: Accurately describing images with text is a foundation of explainable AI. Vision-Language Models (VLMs) like CLIP have recently addressed this by aligning images and texts in a shared embedding space, expressing semantic similarities between vision and language embeddings. VLM classification can be improved with descriptions generated by Large Language Models (LLMs). However, it is difficult to determine the contribution of actual description semantics, as the performance gain may also stem from a semantic-agnostic ensembling effect, where multiple modified text prompts act as a noisy test-time augmentation for the original one. We propose an alternative evaluation scenario to decide if a performance boost of LLM-generated descriptions is caused by such a noise augmentation effect or rather by genuine description semantics. The proposed scenario avoids noisy test-time augmentation and ensures that genuine, distinctive descriptions cause the performance boost. Furthermore, we propose a training-free method for selecting discriminative descriptions that work independently of classname-ensembling effects. Our approach identifies descriptions that effectively differentiate classes within a local CLIP label neighborhood, improving classification accuracy across seven datasets. Additionally, we provide insights into the explainability of description-based image classification with VLMs.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.11917v3 Announce Type: replace \nAbstract: Accurately describing images with text is a foundation of explainable AI. Vision-Language Models (VLMs) like CLIP have recently addressed this by aligning images and texts in a shared embedding space, expressing semantic similarities between vision and language embeddings. VLM classification can be improved with descriptions generated by Large Language Models (LLMs). However, it is difficult to determine the contribution of actual description semantics, as the performance gain may also stem from a semantic-agnostic ensembling effect, where multiple modified text prompts act as a noisy test-time augmentation for the original one. We propose an alternative evaluation scenario to decide if a performance boost of LLM-generated descriptions is caused by such a noise augmentation effect or rather by genuine description semantics. The proposed scenario avoids noisy test-time augmentation and ensures that genuine, distinctive descriptions cause the performance boost. Furthermore, we propose a training-free method for selecting discriminative descriptions that work independently of classname-ensembling effects. Our approach identifies descriptions that effectively differentiate classes within a local CLIP label neighborhood, improving classification accuracy across seven datasets. Additionally, we provide insights into the explainability of description-based image classification with VLMs.'}",oai:arXiv.org:2412.11917v3,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Pingchuan Ma, Lennart Rietdorf, Dmytro Kotovenko, Vincent Tao Hu, Bj\\""orn Ommer'}]","Pingchuan Ma, Lennart Rietdorf, Dmytro Kotovenko, Vincent Tao Hu, Bj\""orn Ommer","{'name': 'Pingchuan Ma, Lennart Rietdorf, Dmytro Kotovenko, Vincent Tao Hu, Bj\\""orn Ommer'}",,
719,Reliable Breast Cancer Molecular Subtype Prediction based on uncertainty-aware Bayesian Deep Learning by Mammography,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Reliable Breast Cancer Molecular Subtype Prediction based on uncertainty-aware Bayesian Deep Learning by Mammography'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.11953'}]",https://arxiv.org/abs/2412.11953,"arXiv:2412.11953v2 Announce Type: replace 
Abstract: Breast cancer is a heterogeneous disease with different molecular subtypes, clinical behavior, treatment responses as well as survival outcomes. The development of a reliable, accurate, available and inexpensive method to predict the molecular subtypes using medical images plays an important role in the diagnosis and prognosis of breast cancer. Recently, deep learning methods have shown good performance in the breast cancer classification tasks using various medical images. Despite all that success, classical deep learning cannot deliver the predictive uncertainty. The uncertainty represents the validity of the predictions. Therefore, the high predicted uncertainty might cause a negative effect in the accurate diagnosis of breast cancer molecular subtypes. To overcome this, uncertainty quantification methods are used to determine the predictive uncertainty. Accordingly, in this study, we proposed an uncertainty-aware Bayesian deep learning model using the full mammogram images. In addition, to increase the performance of the multi-class molecular subtype classification task, we proposed a novel hierarchical classification strategy, named the two-stage classification strategy. The separate AUC of the proposed model for each subtype was 0.71, 0.75 and 0.86 for HER2-enriched, luminal and triple-negative classes, respectively. The proposed model not only has a comparable performance to other studies in the field of breast cancer molecular subtypes prediction, even using full mammography images, but it is also more reliable, due to quantify the predictive uncertainty.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.11953v2 Announce Type: replace \nAbstract: Breast cancer is a heterogeneous disease with different molecular subtypes, clinical behavior, treatment responses as well as survival outcomes. The development of a reliable, accurate, available and inexpensive method to predict the molecular subtypes using medical images plays an important role in the diagnosis and prognosis of breast cancer. Recently, deep learning methods have shown good performance in the breast cancer classification tasks using various medical images. Despite all that success, classical deep learning cannot deliver the predictive uncertainty. The uncertainty represents the validity of the predictions. Therefore, the high predicted uncertainty might cause a negative effect in the accurate diagnosis of breast cancer molecular subtypes. To overcome this, uncertainty quantification methods are used to determine the predictive uncertainty. Accordingly, in this study, we proposed an uncertainty-aware Bayesian deep learning model using the full mammogram images. In addition, to increase the performance of the multi-class molecular subtype classification task, we proposed a novel hierarchical classification strategy, named the two-stage classification strategy. The separate AUC of the proposed model for each subtype was 0.71, 0.75 and 0.86 for HER2-enriched, luminal and triple-negative classes, respectively. The proposed model not only has a comparable performance to other studies in the field of breast cancer molecular subtypes prediction, even using full mammography images, but it is also more reliable, due to quantify the predictive uncertainty.'}",oai:arXiv.org:2412.11953v2,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Mohaddeseh Chegini, Ali Mahloojifar'}]","Mohaddeseh Chegini, Ali Mahloojifar","{'name': 'Mohaddeseh Chegini, Ali Mahloojifar'}",,
720,Leveraging Group Classification with Descending Soft Labeling for Deep Imbalanced Regression,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Leveraging Group Classification with Descending Soft Labeling for Deep Imbalanced Regression'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.12327'}]",https://arxiv.org/abs/2412.12327,"arXiv:2412.12327v2 Announce Type: replace 
Abstract: Deep imbalanced regression (DIR), where the target values have a highly skewed distribution and are also continuous, is an intriguing yet under-explored problem in machine learning.
  While recent works have already shown that incorporating various classification-based regularizers can produce enhanced outcomes, the role of classification remains elusive in DIR.
  Moreover, such regularizers (e.g., contrastive penalties) merely focus on learning discriminative features of data, which inevitably results in ignorance of either continuity or similarity across the data.
  To address these issues, we first bridge the connection between the objectives of DIR and classification from a Bayesian perspective.
  Consequently, this motivates us to decompose the objective of DIR into a combination of classification and regression tasks, which naturally guides us toward a divide-and-conquer manner to solve the DIR problem.
  Specifically, by aggregating the data at nearby labels into the same groups, we introduce an ordinal group-aware contrastive learning loss along with a multi-experts regressor to tackle the different groups of data thereby maintaining the data continuity.
  Meanwhile, considering the similarity between the groups, we also propose a symmetric descending soft labeling strategy to exploit the intrinsic similarity across the data, which allows classification to facilitate regression more effectively.
  Extensive experiments on real-world datasets also validate the effectiveness of our method.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.12327v2 Announce Type: replace \nAbstract: Deep imbalanced regression (DIR), where the target values have a highly skewed distribution and are also continuous, is an intriguing yet under-explored problem in machine learning.\n  While recent works have already shown that incorporating various classification-based regularizers can produce enhanced outcomes, the role of classification remains elusive in DIR.\n  Moreover, such regularizers (e.g., contrastive penalties) merely focus on learning discriminative features of data, which inevitably results in ignorance of either continuity or similarity across the data.\n  To address these issues, we first bridge the connection between the objectives of DIR and classification from a Bayesian perspective.\n  Consequently, this motivates us to decompose the objective of DIR into a combination of classification and regression tasks, which naturally guides us toward a divide-and-conquer manner to solve the DIR problem.\n  Specifically, by aggregating the data at nearby labels into the same groups, we introduce an ordinal group-aware contrastive learning loss along with a multi-experts regressor to tackle the different groups of data thereby maintaining the data continuity.\n  Meanwhile, considering the similarity between the groups, we also propose a symmetric descending soft labeling strategy to exploit the intrinsic similarity across the data, which allows classification to facilitate regression more effectively.\n  Extensive experiments on real-world datasets also validate the effectiveness of our method.'}",oai:arXiv.org:2412.12327v2,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/publicdomain/zero/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/publicdomain/zero/1.0/'}","[{'name': 'Ruizhi Pu, Gezheng Xu, Ruiyi Fang, Binkun Bao, Charles X. Ling, Boyu Wang'}]","Ruizhi Pu, Gezheng Xu, Ruiyi Fang, Binkun Bao, Charles X. Ling, Boyu Wang","{'name': 'Ruizhi Pu, Gezheng Xu, Ruiyi Fang, Binkun Bao, Charles X. Ling, Boyu Wang'}",,
721,RemoteTrimmer: Adaptive Structural Pruning for Remote Sensing Image Classification,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'RemoteTrimmer: Adaptive Structural Pruning for Remote Sensing Image Classification'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.12603'}]",https://arxiv.org/abs/2412.12603,"arXiv:2412.12603v2 Announce Type: replace 
Abstract: Since high resolution remote sensing image classification often requires a relatively high computation complexity, lightweight models tend to be practical and efficient. Model pruning is an effective method for model compression. However, existing methods rarely take into account the specificity of remote sensing images, resulting in significant accuracy loss after pruning. To this end, we propose an effective structural pruning approach for remote sensing image classification. Specifically, a pruning strategy that amplifies the differences in channel importance of the model is introduced. Then an adaptive mining loss function is designed for the fine-tuning process of the pruned model. Finally, we conducted experiments on two remote sensing classification datasets. The experimental results demonstrate that our method achieves minimal accuracy loss after compressing remote sensing classification models, achieving state-of-the-art (SoTA) performance.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.12603v2 Announce Type: replace \nAbstract: Since high resolution remote sensing image classification often requires a relatively high computation complexity, lightweight models tend to be practical and efficient. Model pruning is an effective method for model compression. However, existing methods rarely take into account the specificity of remote sensing images, resulting in significant accuracy loss after pruning. To this end, we propose an effective structural pruning approach for remote sensing image classification. Specifically, a pruning strategy that amplifies the differences in channel importance of the model is introduced. Then an adaptive mining loss function is designed for the fine-tuning process of the pruned model. Finally, we conducted experiments on two remote sensing classification datasets. The experimental results demonstrate that our method achieves minimal accuracy loss after compressing remote sensing classification models, achieving state-of-the-art (SoTA) performance.'}",oai:arXiv.org:2412.12603v2,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by-sa/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-sa/4.0/'}","[{'name': 'Guangwenjie Zou, Liang Yao, Fan Liu, Chuanyi Zhang, Xin Li, Ning Chen, Shengxiang Xu, Jun Zhou'}]","Guangwenjie Zou, Liang Yao, Fan Liu, Chuanyi Zhang, Xin Li, Ning Chen, Shengxiang Xu, Jun Zhou","{'name': 'Guangwenjie Zou, Liang Yao, Fan Liu, Chuanyi Zhang, Xin Li, Ning Chen, Shengxiang Xu, Jun Zhou'}",,
722,Dyn-HaMR: Recovering 4D Interacting Hand Motion from a Dynamic Camera,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Dyn-HaMR: Recovering 4D Interacting Hand Motion from a Dynamic Camera'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.12861'}]",https://arxiv.org/abs/2412.12861,"arXiv:2412.12861v2 Announce Type: replace 
Abstract: We propose Dyn-HaMR, to the best of our knowledge, the first approach to reconstruct 4D global hand motion from monocular videos recorded by dynamic cameras in the wild. Reconstructing accurate 3D hand meshes from monocular videos is a crucial task for understanding human behaviour, with significant applications in augmented and virtual reality (AR/VR). However, existing methods for monocular hand reconstruction typically rely on a weak perspective camera model, which simulates hand motion within a limited camera frustum. As a result, these approaches struggle to recover the full 3D global trajectory and often produce noisy or incorrect depth estimations, particularly when the video is captured by dynamic or moving cameras, which is common in egocentric scenarios. Our Dyn-HaMR consists of a multi-stage, multi-objective optimization pipeline, that factors in (i) simultaneous localization and mapping (SLAM) to robustly estimate relative camera motion, (ii) an interacting-hand prior for generative infilling and to refine the interaction dynamics, ensuring plausible recovery under (self-)occlusions, and (iii) hierarchical initialization through a combination of state-of-the-art hand tracking methods. Through extensive evaluations on both in-the-wild and indoor datasets, we show that our approach significantly outperforms state-of-the-art methods in terms of 4D global mesh recovery. This establishes a new benchmark for hand motion reconstruction from monocular video with moving cameras. Our project page is at https://dyn-hamr.github.io/.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.12861v2 Announce Type: replace \nAbstract: We propose Dyn-HaMR, to the best of our knowledge, the first approach to reconstruct 4D global hand motion from monocular videos recorded by dynamic cameras in the wild. Reconstructing accurate 3D hand meshes from monocular videos is a crucial task for understanding human behaviour, with significant applications in augmented and virtual reality (AR/VR). However, existing methods for monocular hand reconstruction typically rely on a weak perspective camera model, which simulates hand motion within a limited camera frustum. As a result, these approaches struggle to recover the full 3D global trajectory and often produce noisy or incorrect depth estimations, particularly when the video is captured by dynamic or moving cameras, which is common in egocentric scenarios. Our Dyn-HaMR consists of a multi-stage, multi-objective optimization pipeline, that factors in (i) simultaneous localization and mapping (SLAM) to robustly estimate relative camera motion, (ii) an interacting-hand prior for generative infilling and to refine the interaction dynamics, ensuring plausible recovery under (self-)occlusions, and (iii) hierarchical initialization through a combination of state-of-the-art hand tracking methods. Through extensive evaluations on both in-the-wild and indoor datasets, we show that our approach significantly outperforms state-of-the-art methods in terms of 4D global mesh recovery. This establishes a new benchmark for hand motion reconstruction from monocular video with moving cameras. Our project page is at https://dyn-hamr.github.io/.'}",oai:arXiv.org:2412.12861v2,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by-sa/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-sa/4.0/'}","[{'name': 'Zhengdi Yu, Stefanos Zafeiriou, Tolga Birdal'}]","Zhengdi Yu, Stefanos Zafeiriou, Tolga Birdal","{'name': 'Zhengdi Yu, Stefanos Zafeiriou, Tolga Birdal'}",,
723,Attentive Eraser: Unleashing Diffusion Model's Object Removal Potential via Self-Attention Redirection Guidance,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""Attentive Eraser: Unleashing Diffusion Model's Object Removal Potential via Self-Attention Redirection Guidance""}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.12974'}]",https://arxiv.org/abs/2412.12974,"arXiv:2412.12974v3 Announce Type: replace 
Abstract: Recently, diffusion models have emerged as promising newcomers in the field of generative models, shining brightly in image generation. However, when employed for object removal tasks, they still encounter issues such as generating random artifacts and the incapacity to repaint foreground object areas with appropriate content after removal. To tackle these problems, we propose Attentive Eraser, a tuning-free method to empower pre-trained diffusion models for stable and effective object removal. Firstly, in light of the observation that the self-attention maps influence the structure and shape details of the generated images, we propose Attention Activation and Suppression (ASS), which re-engineers the self-attention mechanism within the pre-trained diffusion models based on the given mask, thereby prioritizing the background over the foreground object during the reverse generation process. Moreover, we introduce Self-Attention Redirection Guidance (SARG), which utilizes the self-attention redirected by ASS to guide the generation process, effectively removing foreground objects within the mask while simultaneously generating content that is both plausible and coherent. Experiments demonstrate the stability and effectiveness of Attentive Eraser in object removal across a variety of pre-trained diffusion models, outperforming even training-based methods. Furthermore, Attentive Eraser can be implemented in various diffusion model architectures and checkpoints, enabling excellent scalability. Code is available at https://github.com/Anonym0u3/AttentiveEraser.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.12974v3 Announce Type: replace \nAbstract: Recently, diffusion models have emerged as promising newcomers in the field of generative models, shining brightly in image generation. However, when employed for object removal tasks, they still encounter issues such as generating random artifacts and the incapacity to repaint foreground object areas with appropriate content after removal. To tackle these problems, we propose Attentive Eraser, a tuning-free method to empower pre-trained diffusion models for stable and effective object removal. Firstly, in light of the observation that the self-attention maps influence the structure and shape details of the generated images, we propose Attention Activation and Suppression (ASS), which re-engineers the self-attention mechanism within the pre-trained diffusion models based on the given mask, thereby prioritizing the background over the foreground object during the reverse generation process. Moreover, we introduce Self-Attention Redirection Guidance (SARG), which utilizes the self-attention redirected by ASS to guide the generation process, effectively removing foreground objects within the mask while simultaneously generating content that is both plausible and coherent. Experiments demonstrate the stability and effectiveness of Attentive Eraser in object removal across a variety of pre-trained diffusion models, outperforming even training-based methods. Furthermore, Attentive Eraser can be implemented in various diffusion model architectures and checkpoints, enabling excellent scalability. Code is available at https://github.com/Anonym0u3/AttentiveEraser.'}",oai:arXiv.org:2412.12974v3,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Wenhao Sun, Benlei Cui, Xue-Mei Dong, Jingqun Tang'}]","Wenhao Sun, Benlei Cui, Xue-Mei Dong, Jingqun Tang","{'name': 'Wenhao Sun, Benlei Cui, Xue-Mei Dong, Jingqun Tang'}",,
724,Accuracy Limits as a Barrier to Biometric System Security,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Accuracy Limits as a Barrier to Biometric System Security'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.13099'}]",https://arxiv.org/abs/2412.13099,"arXiv:2412.13099v2 Announce Type: replace 
Abstract: Biometric systems are widely used for identity verification and identification, including authentication (i.e., one-to-one matching to verify a claimed identity) and identification (i.e., one-to-many matching to find a subject in a database). The matching process relies on measuring similarities or dissimilarities between a fresh biometric template and enrolled templates. The False Match Rate FMR is a key metric for assessing the accuracy and reliability of such systems. This paper analyzes biometric systems based on their FMR, with two main contributions. First, we explore untargeted attacks, where an adversary aims to impersonate any user within a database. We determine the number of trials required for an attacker to successfully impersonate a user and derive the critical population size (i.e., the maximum number of users in the database) required to maintain a given level of security. Furthermore, we compute the critical FMR value needed to ensure resistance against untargeted attacks as the database size increases. Second, we revisit the biometric birthday problem to evaluate the approximate and exact probabilities that two users in a database collide (i.e., can impersonate each other). Based on this analysis, we derive both the approximate critical population size and the critical FMR value needed to bound the likelihood of such collisions occurring with a given probability. These thresholds offer insights for designing systems that mitigate the risk of impersonation and collisions, particularly in large-scale biometric databases. Our findings indicate that current biometric systems fail to deliver sufficient accuracy to achieve an adequate security level against untargeted attacks, even in small-scale databases. Moreover, state-of-the-art systems face significant challenges in addressing the biometric birthday problem, especially as database sizes grow.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.13099v2 Announce Type: replace \nAbstract: Biometric systems are widely used for identity verification and identification, including authentication (i.e., one-to-one matching to verify a claimed identity) and identification (i.e., one-to-many matching to find a subject in a database). The matching process relies on measuring similarities or dissimilarities between a fresh biometric template and enrolled templates. The False Match Rate FMR is a key metric for assessing the accuracy and reliability of such systems. This paper analyzes biometric systems based on their FMR, with two main contributions. First, we explore untargeted attacks, where an adversary aims to impersonate any user within a database. We determine the number of trials required for an attacker to successfully impersonate a user and derive the critical population size (i.e., the maximum number of users in the database) required to maintain a given level of security. Furthermore, we compute the critical FMR value needed to ensure resistance against untargeted attacks as the database size increases. Second, we revisit the biometric birthday problem to evaluate the approximate and exact probabilities that two users in a database collide (i.e., can impersonate each other). Based on this analysis, we derive both the approximate critical population size and the critical FMR value needed to bound the likelihood of such collisions occurring with a given probability. These thresholds offer insights for designing systems that mitigate the risk of impersonation and collisions, particularly in large-scale biometric databases. Our findings indicate that current biometric systems fail to deliver sufficient accuracy to achieve an adequate security level against untargeted attacks, even in small-scale databases. Moreover, state-of-the-art systems face significant challenges in addressing the biometric birthday problem, especially as database sizes grow.'}",oai:arXiv.org:2412.13099v2,False,"[{'term': 'cs.CR', 'scheme': None, 'label': None}, {'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Axel Durbet, Paul-Marie Grollemund, Pascal Lafourcade, Kevin Thiry-Atighehchi'}]","Axel Durbet, Paul-Marie Grollemund, Pascal Lafourcade, Kevin Thiry-Atighehchi","{'name': 'Axel Durbet, Paul-Marie Grollemund, Pascal Lafourcade, Kevin Thiry-Atighehchi'}",,
725,C-FedRAG: A Confidential Federated Retrieval-Augmented Generation System,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'C-FedRAG: A Confidential Federated Retrieval-Augmented Generation System'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.13163'}]",https://arxiv.org/abs/2412.13163,"arXiv:2412.13163v2 Announce Type: replace 
Abstract: Organizations seeking to utilize Large Language Models (LLMs) for knowledge querying and analysis often encounter challenges in maintaining an LLM fine-tuned on targeted, up-to-date information that keeps answers relevant and grounded. Retrieval Augmented Generation (RAG) has quickly become a feasible solution for organizations looking to overcome the challenges of maintaining proprietary models and to help reduce LLM hallucinations in their query responses. However, RAG comes with its own issues regarding scaling data pipelines across tiered-access and disparate data sources. In many scenarios, it is necessary to query beyond a single data silo to provide richer and more relevant context for an LLM. Analyzing data sources within and across organizational trust boundaries is often limited by complex data-sharing policies that prohibit centralized data storage, therefore, inhibit the fast and effective setup and scaling of RAG solutions. In this paper, we introduce Confidential Computing (CC) techniques as a solution for secure Federated Retrieval Augmented Generation (FedRAG). Our proposed Confidential FedRAG system (C-FedRAG) enables secure connection and scaling of a RAG workflows across a decentralized network of data providers by ensuring context confidentiality. We also demonstrate how to implement a C-FedRAG system using the NVIDIA FLARE SDK and assess its performance using the MedRAG toolkit and MIRAGE benchmarking dataset.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.13163v2 Announce Type: replace \nAbstract: Organizations seeking to utilize Large Language Models (LLMs) for knowledge querying and analysis often encounter challenges in maintaining an LLM fine-tuned on targeted, up-to-date information that keeps answers relevant and grounded. Retrieval Augmented Generation (RAG) has quickly become a feasible solution for organizations looking to overcome the challenges of maintaining proprietary models and to help reduce LLM hallucinations in their query responses. However, RAG comes with its own issues regarding scaling data pipelines across tiered-access and disparate data sources. In many scenarios, it is necessary to query beyond a single data silo to provide richer and more relevant context for an LLM. Analyzing data sources within and across organizational trust boundaries is often limited by complex data-sharing policies that prohibit centralized data storage, therefore, inhibit the fast and effective setup and scaling of RAG solutions. In this paper, we introduce Confidential Computing (CC) techniques as a solution for secure Federated Retrieval Augmented Generation (FedRAG). Our proposed Confidential FedRAG system (C-FedRAG) enables secure connection and scaling of a RAG workflows across a decentralized network of data providers by ensuring context confidentiality. We also demonstrate how to implement a C-FedRAG system using the NVIDIA FLARE SDK and assess its performance using the MedRAG toolkit and MIRAGE benchmarking dataset.'}",oai:arXiv.org:2412.13163v2,False,"[{'term': 'cs.DC', 'scheme': None, 'label': None}, {'term': 'cs.IR', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Parker Addison, Minh-Tuan H. Nguyen, Tomislav Medan, Jinali Shah, Mohammad T. Manzari, Brendan McElrone, Laksh Lalwani, Aboli More, Smita Sharma, Holger R. Roth, Isaac Yang, Chester Chen, Daguang Xu, Yan Cheng, Andrew Feng, Ziyue Xu'}]","Parker Addison, Minh-Tuan H. Nguyen, Tomislav Medan, Jinali Shah, Mohammad T. Manzari, Brendan McElrone, Laksh Lalwani, Aboli More, Smita Sharma, Holger R. Roth, Isaac Yang, Chester Chen, Daguang Xu, Yan Cheng, Andrew Feng, Ziyue Xu","{'name': 'Parker Addison, Minh-Tuan H. Nguyen, Tomislav Medan, Jinali Shah, Mohammad T. Manzari, Brendan McElrone, Laksh Lalwani, Aboli More, Smita Sharma, Holger R. Roth, Isaac Yang, Chester Chen, Daguang Xu, Yan Cheng, Andrew Feng, Ziyue Xu'}",,
726,SafeDrive: Knowledge- and Data-Driven Risk-Sensitive Decision-Making for Autonomous Vehicles with Large Language Models,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'SafeDrive: Knowledge- and Data-Driven Risk-Sensitive Decision-Making for Autonomous Vehicles with Large Language Models'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.13238'}]",https://arxiv.org/abs/2412.13238,"arXiv:2412.13238v2 Announce Type: replace 
Abstract: Recent advancements in autonomous vehicles (AVs) use Large Language Models (LLMs) to perform well in normal driving scenarios. However, ensuring safety in dynamic, high-risk environments and managing safety-critical long-tail events remain significant challenges. To address these issues, we propose SafeDrive, a knowledge- and data-driven risk-sensitive decision-making framework to enhance AV safety and adaptability. The proposed framework introduces a modular system comprising: (1) a Risk Module for quantifying multi-factor coupled risks involving driver, vehicle, and road interactions; (2) a Memory Module for storing and retrieving typical scenarios to improve adaptability; (3) a LLM-powered Reasoning Module for context-aware safety decision-making; and (4) a Reflection Module for refining decisions through iterative learning. By integrating knowledge-driven insights with adaptive learning mechanisms, the framework ensures robust decision-making under uncertain conditions. Extensive evaluations on real-world traffic datasets, including highways (HighD), intersections (InD), and roundabouts (RounD), validate the framework's ability to enhance decision-making safety (achieving a 100% safety rate), replicate human-like driving behaviors (with decision alignment exceeding 85%), and adapt effectively to unpredictable scenarios. SafeDrive establishes a novel paradigm for integrating knowledge- and data-driven methods, highlighting significant potential to improve safety and adaptability of autonomous driving in high-risk traffic scenarios. Project Page: https://mezzi33.github.io/SafeDrive/","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.13238v2 Announce Type: replace \nAbstract: Recent advancements in autonomous vehicles (AVs) use Large Language Models (LLMs) to perform well in normal driving scenarios. However, ensuring safety in dynamic, high-risk environments and managing safety-critical long-tail events remain significant challenges. To address these issues, we propose SafeDrive, a knowledge- and data-driven risk-sensitive decision-making framework to enhance AV safety and adaptability. The proposed framework introduces a modular system comprising: (1) a Risk Module for quantifying multi-factor coupled risks involving driver, vehicle, and road interactions; (2) a Memory Module for storing and retrieving typical scenarios to improve adaptability; (3) a LLM-powered Reasoning Module for context-aware safety decision-making; and (4) a Reflection Module for refining decisions through iterative learning. By integrating knowledge-driven insights with adaptive learning mechanisms, the framework ensures robust decision-making under uncertain conditions. Extensive evaluations on real-world traffic datasets, including highways (HighD), intersections (InD), and roundabouts (RounD), validate the framework's ability to enhance decision-making safety (achieving a 100% safety rate), replicate human-like driving behaviors (with decision alignment exceeding 85%), and adapt effectively to unpredictable scenarios. SafeDrive establishes a novel paradigm for integrating knowledge- and data-driven methods, highlighting significant potential to improve safety and adaptability of autonomous driving in high-risk traffic scenarios. Project Page: https://mezzi33.github.io/SafeDrive/""}",oai:arXiv.org:2412.13238v2,False,"[{'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.ET', 'scheme': None, 'label': None}, {'term': 'cs.RO', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Zhiyuan Zhou, Heye Huang, Boqi Li, Shiyue Zhao, Yao Mu, Jianqiang Wang'}]","Zhiyuan Zhou, Heye Huang, Boqi Li, Shiyue Zhao, Yao Mu, Jianqiang Wang","{'name': 'Zhiyuan Zhou, Heye Huang, Boqi Li, Shiyue Zhao, Yao Mu, Jianqiang Wang'}",,
727,Scene Modeling of Autonomous Vehicles Avoiding Stationary and Moving Vehicles on Narrow Roads,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Scene Modeling of Autonomous Vehicles Avoiding Stationary and Moving Vehicles on Narrow Roads'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.13305'}]",https://arxiv.org/abs/2412.13305,"arXiv:2412.13305v2 Announce Type: replace 
Abstract: Navigating narrow roads with oncoming vehicles is a significant challenge that has garnered considerable public interest. These scenarios often involve sections that cannot accommodate two moving vehicles simultaneously due to the presence of stationary vehicles or limited road width. Autonomous vehicles must therefore profoundly comprehend their surroundings to identify passable areas and execute sophisticated maneuvers. To address this issue, this paper presents a comprehensive model for such an intricate scenario. The primary contribution is the principle of road width occupancy minimization, which models the narrow road problem and identifies candidate meeting gaps. Additionally, the concept of homology classes is introduced to help initialize and optimize candidate trajectories, while evaluation strategies are developed to select the optimal gap and most efficient trajectory. Qualitative and quantitative simulations demonstrate that the proposed approach, SM-NR, achieves high scene pass rates, efficient movement, and robust decisions. Experiments conducted in tiny gap scenarios and conflict scenarios reveal that the autonomous vehicle can robustly select meeting gaps and trajectories, compromising flexibly for safety while advancing bravely for efficiency.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.13305v2 Announce Type: replace \nAbstract: Navigating narrow roads with oncoming vehicles is a significant challenge that has garnered considerable public interest. These scenarios often involve sections that cannot accommodate two moving vehicles simultaneously due to the presence of stationary vehicles or limited road width. Autonomous vehicles must therefore profoundly comprehend their surroundings to identify passable areas and execute sophisticated maneuvers. To address this issue, this paper presents a comprehensive model for such an intricate scenario. The primary contribution is the principle of road width occupancy minimization, which models the narrow road problem and identifies candidate meeting gaps. Additionally, the concept of homology classes is introduced to help initialize and optimize candidate trajectories, while evaluation strategies are developed to select the optimal gap and most efficient trajectory. Qualitative and quantitative simulations demonstrate that the proposed approach, SM-NR, achieves high scene pass rates, efficient movement, and robust decisions. Experiments conducted in tiny gap scenarios and conflict scenarios reveal that the autonomous vehicle can robustly select meeting gaps and trajectories, compromising flexibly for safety while advancing bravely for efficiency.'}",oai:arXiv.org:2412.13305v2,False,"[{'term': 'cs.RO', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Qianyi Zhang, Jinzheng Guang, Zhenzhong Cao, Jingtai Liu'}]","Qianyi Zhang, Jinzheng Guang, Zhenzhong Cao, Jingtai Liu","{'name': 'Qianyi Zhang, Jinzheng Guang, Zhenzhong Cao, Jingtai Liu'}",,
728,Multi-view Granular-ball Contrastive Clustering,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Multi-view Granular-ball Contrastive Clustering'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.13550'}]",https://arxiv.org/abs/2412.13550,"arXiv:2412.13550v2 Announce Type: replace 
Abstract: Previous multi-view contrastive learning methods typically operate at two scales: instance-level and cluster-level. Instance-level approaches construct positive and negative pairs based on sample correspondences, aiming to bring positive pairs closer and push negative pairs further apart in the latent space. Cluster-level methods focus on calculating cluster assignments for samples under each view and maximize view consensus by reducing distribution discrepancies, e.g., minimizing KL divergence or maximizing mutual information. However, these two types of methods either introduce false negatives, leading to reduced model discriminability, or overlook local structures and cannot measure relationships between clusters across views explicitly. To this end, we propose a method named Multi-view Granular-ball Contrastive Clustering (MGBCC). MGBCC segments the sample set into coarse-grained granular balls, and establishes associations between intra-view and cross-view granular balls. These associations are reinforced in a shared latent space, thereby achieving multi-granularity contrastive learning. Granular balls lie between instances and clusters, naturally preserving the local topological structure of the sample set. We conduct extensive experiments to validate the effectiveness of the proposed method.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.13550v2 Announce Type: replace \nAbstract: Previous multi-view contrastive learning methods typically operate at two scales: instance-level and cluster-level. Instance-level approaches construct positive and negative pairs based on sample correspondences, aiming to bring positive pairs closer and push negative pairs further apart in the latent space. Cluster-level methods focus on calculating cluster assignments for samples under each view and maximize view consensus by reducing distribution discrepancies, e.g., minimizing KL divergence or maximizing mutual information. However, these two types of methods either introduce false negatives, leading to reduced model discriminability, or overlook local structures and cannot measure relationships between clusters across views explicitly. To this end, we propose a method named Multi-view Granular-ball Contrastive Clustering (MGBCC). MGBCC segments the sample set into coarse-grained granular balls, and establishes associations between intra-view and cross-view granular balls. These associations are reinforced in a shared latent space, thereby achieving multi-granularity contrastive learning. Granular balls lie between instances and clusters, naturally preserving the local topological structure of the sample set. We conduct extensive experiments to validate the effectiveness of the proposed method.'}",oai:arXiv.org:2412.13550v2,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Peng Su, Shudong Huang, Weihong Ma, Deng Xiong, Jiancheng Lv'}]","Peng Su, Shudong Huang, Weihong Ma, Deng Xiong, Jiancheng Lv","{'name': 'Peng Su, Shudong Huang, Weihong Ma, Deng Xiong, Jiancheng Lv'}",,
729,Revisiting Interactions of Multiple Driver States in Heterogenous Population and Cognitive Tasks,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Revisiting Interactions of Multiple Driver States in Heterogenous Population and Cognitive Tasks'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.13574'}]",https://arxiv.org/abs/2412.13574,"arXiv:2412.13574v2 Announce Type: replace 
Abstract: In real-world driving scenarios, multiple states occur simultaneously due to individual differences and environmental factors, complicating the analysis and estimation of driver states. Previous studies, limited by experimental design and analytical methods, may not be able to disentangle the relationships among multiple driver states and environmental factors. This paper introduces the Double Machine Learning (DML) analysis method to the field of driver state analysis to tackle this challenge. To train and test the DML model, a driving simulator experiment with 42 participants was conducted. All participants drove SAE level-3 vehicles and conducted three types of cognitive tasks in a 3-hour driving experiment. Drivers' subjective cognitive load and drowsiness levels were collected throughout the experiment. Then, we isolated individual and environmental factors affecting driver state variations and the factors affecting drivers' physiological and eye-tracking metrics when they are under specific states. The results show that our approach successfully decoupled and inferred the complex causal relationships between multiple types of drowsiness and cognitive load. Additionally, we identified key physiological and eye-tracking indicators in the presence of multiple driver states and under the influence of a single state, excluding the influence of other driver states, environmental factors, and individual characteristics. Our causal inference analytical framework can offer new insights for subsequent analysis of drivers' states. Further, the updated causal relation graph based on the DML analysis can provide theoretical bases for driver state monitoring based on physiological and eye-tracking measures.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.13574v2 Announce Type: replace \nAbstract: In real-world driving scenarios, multiple states occur simultaneously due to individual differences and environmental factors, complicating the analysis and estimation of driver states. Previous studies, limited by experimental design and analytical methods, may not be able to disentangle the relationships among multiple driver states and environmental factors. This paper introduces the Double Machine Learning (DML) analysis method to the field of driver state analysis to tackle this challenge. To train and test the DML model, a driving simulator experiment with 42 participants was conducted. All participants drove SAE level-3 vehicles and conducted three types of cognitive tasks in a 3-hour driving experiment. Drivers' subjective cognitive load and drowsiness levels were collected throughout the experiment. Then, we isolated individual and environmental factors affecting driver state variations and the factors affecting drivers' physiological and eye-tracking metrics when they are under specific states. The results show that our approach successfully decoupled and inferred the complex causal relationships between multiple types of drowsiness and cognitive load. Additionally, we identified key physiological and eye-tracking indicators in the presence of multiple driver states and under the influence of a single state, excluding the influence of other driver states, environmental factors, and individual characteristics. Our causal inference analytical framework can offer new insights for subsequent analysis of drivers' states. Further, the updated causal relation graph based on the DML analysis can provide theoretical bases for driver state monitoring based on physiological and eye-tracking measures.""}",oai:arXiv.org:2412.13574v2,False,"[{'term': 'cs.HC', 'scheme': None, 'label': None}, {'term': 'stat.AP', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Jiyao Wang, Ange Wang, Song Yan, Dengbo He, Kaishun Wu'}]","Jiyao Wang, Ange Wang, Song Yan, Dengbo He, Kaishun Wu","{'name': 'Jiyao Wang, Ange Wang, Song Yan, Dengbo He, Kaishun Wu'}",,
730,Sign-IDD: Iconicity Disentangled Diffusion for Sign Language Production,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Sign-IDD: Iconicity Disentangled Diffusion for Sign Language Production'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.13609'}]",https://arxiv.org/abs/2412.13609,"arXiv:2412.13609v2 Announce Type: replace 
Abstract: Sign Language Production (SLP) aims to generate semantically consistent sign videos from textual statements, where the conversion from textual glosses to sign poses (G2P) is a crucial step. Existing G2P methods typically treat sign poses as discrete three-dimensional coordinates and directly fit them, which overlooks the relative positional relationships among joints. To this end, we provide a new perspective, constraining joint associations and gesture details by modeling the limb bones to improve the accuracy and naturalness of the generated poses. In this work, we propose a pioneering iconicity disentangled diffusion framework, termed Sign-IDD, specifically designed for SLP. Sign-IDD incorporates a novel Iconicity Disentanglement (ID) module to bridge the gap between relative positions among joints. The ID module disentangles the conventional 3D joint representation into a 4D bone representation, comprising the 3D spatial direction vector and 1D spatial distance vector between adjacent joints. Additionally, an Attribute Controllable Diffusion (ACD) module is introduced to further constrain joint associations, in which the attribute separation layer aims to separate the bone direction and length attributes, and the attribute control layer is designed to guide the pose generation by leveraging the above attributes. The ACD module utilizes the gloss embeddings as semantic conditions and finally generates sign poses from noise embeddings. Extensive experiments on PHOENIX14T and USTC-CSL datasets validate the effectiveness of our method. The code is available at: https://github.com/NaVi-start/Sign-IDD.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.13609v2 Announce Type: replace \nAbstract: Sign Language Production (SLP) aims to generate semantically consistent sign videos from textual statements, where the conversion from textual glosses to sign poses (G2P) is a crucial step. Existing G2P methods typically treat sign poses as discrete three-dimensional coordinates and directly fit them, which overlooks the relative positional relationships among joints. To this end, we provide a new perspective, constraining joint associations and gesture details by modeling the limb bones to improve the accuracy and naturalness of the generated poses. In this work, we propose a pioneering iconicity disentangled diffusion framework, termed Sign-IDD, specifically designed for SLP. Sign-IDD incorporates a novel Iconicity Disentanglement (ID) module to bridge the gap between relative positions among joints. The ID module disentangles the conventional 3D joint representation into a 4D bone representation, comprising the 3D spatial direction vector and 1D spatial distance vector between adjacent joints. Additionally, an Attribute Controllable Diffusion (ACD) module is introduced to further constrain joint associations, in which the attribute separation layer aims to separate the bone direction and length attributes, and the attribute control layer is designed to guide the pose generation by leveraging the above attributes. The ACD module utilizes the gloss embeddings as semantic conditions and finally generates sign poses from noise embeddings. Extensive experiments on PHOENIX14T and USTC-CSL datasets validate the effectiveness of our method. The code is available at: https://github.com/NaVi-start/Sign-IDD.'}",oai:arXiv.org:2412.13609v2,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.MM', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Shengeng Tang, Jiayi He, Dan Guo, Yanyan Wei, Feng Li, Richang Hong'}]","Shengeng Tang, Jiayi He, Dan Guo, Yanyan Wei, Feng Li, Richang Hong","{'name': 'Shengeng Tang, Jiayi He, Dan Guo, Yanyan Wei, Feng Li, Richang Hong'}",,
731,G-VEval: A Versatile Metric for Evaluating Image and Video Captions Using GPT-4o,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'G-VEval: A Versatile Metric for Evaluating Image and Video Captions Using GPT-4o'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.13647'}]",https://arxiv.org/abs/2412.13647,"arXiv:2412.13647v2 Announce Type: replace 
Abstract: Evaluation metric of visual captioning is important yet not thoroughly explored. Traditional metrics like BLEU, METEOR, CIDEr, and ROUGE often miss semantic depth, while trained metrics such as CLIP-Score, PAC-S, and Polos are limited in zero-shot scenarios. Advanced Language Model-based metrics also struggle with aligning to nuanced human preferences. To address these issues, we introduce G-VEval, a novel metric inspired by G-Eval and powered by the new GPT-4o. G-VEval uses chain-of-thought reasoning in large multimodal models and supports three modes: reference-free, reference-only, and combined, accommodating both video and image inputs. We also propose MSVD-Eval, a new dataset for video captioning evaluation, to establish a more transparent and consistent framework for both human experts and evaluation metrics. It is designed to address the lack of clear criteria in existing datasets by introducing distinct dimensions of Accuracy, Completeness, Conciseness, and Relevance (ACCR). Extensive results show that G-VEval outperforms existing methods in correlation with human annotations, as measured by Kendall tau-b and Kendall tau-c. This provides a flexible solution for diverse captioning tasks and suggests a straightforward yet effective approach for large language models to understand video content, paving the way for advancements in automated captioning. Codes are available at https://github.com/ztangaj/gveval","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.13647v2 Announce Type: replace \nAbstract: Evaluation metric of visual captioning is important yet not thoroughly explored. Traditional metrics like BLEU, METEOR, CIDEr, and ROUGE often miss semantic depth, while trained metrics such as CLIP-Score, PAC-S, and Polos are limited in zero-shot scenarios. Advanced Language Model-based metrics also struggle with aligning to nuanced human preferences. To address these issues, we introduce G-VEval, a novel metric inspired by G-Eval and powered by the new GPT-4o. G-VEval uses chain-of-thought reasoning in large multimodal models and supports three modes: reference-free, reference-only, and combined, accommodating both video and image inputs. We also propose MSVD-Eval, a new dataset for video captioning evaluation, to establish a more transparent and consistent framework for both human experts and evaluation metrics. It is designed to address the lack of clear criteria in existing datasets by introducing distinct dimensions of Accuracy, Completeness, Conciseness, and Relevance (ACCR). Extensive results show that G-VEval outperforms existing methods in correlation with human annotations, as measured by Kendall tau-b and Kendall tau-c. This provides a flexible solution for diverse captioning tasks and suggests a straightforward yet effective approach for large language models to understand video content, paving the way for advancements in automated captioning. Codes are available at https://github.com/ztangaj/gveval'}",oai:arXiv.org:2412.13647v2,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.CL', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Tony Cheng Tong, Sirui He, Zhiwen Shao, Dit-Yan Yeung'}]","Tony Cheng Tong, Sirui He, Zhiwen Shao, Dit-Yan Yeung","{'name': 'Tony Cheng Tong, Sirui He, Zhiwen Shao, Dit-Yan Yeung'}",,
732,"Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for Fast, Memory Efficient, and Long Context Finetuning and Inference","{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for Fast, Memory Efficient, and Long Context Finetuning and Inference'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.13663'}]",https://arxiv.org/abs/2412.13663,"arXiv:2412.13663v2 Announce Type: replace 
Abstract: Encoder-only transformer models such as BERT offer a great performance-size tradeoff for retrieval and classification tasks with respect to larger decoder-only models. Despite being the workhorse of numerous production pipelines, there have been limited Pareto improvements to BERT since its release. In this paper, we introduce ModernBERT, bringing modern model optimizations to encoder-only models and representing a major Pareto improvement over older encoders. Trained on 2 trillion tokens with a native 8192 sequence length, ModernBERT models exhibit state-of-the-art results on a large pool of evaluations encompassing diverse classification tasks and both single and multi-vector retrieval on different domains (including code). In addition to strong downstream performance, ModernBERT is also the most speed and memory efficient encoder and is designed for inference on common GPUs.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.13663v2 Announce Type: replace \nAbstract: Encoder-only transformer models such as BERT offer a great performance-size tradeoff for retrieval and classification tasks with respect to larger decoder-only models. Despite being the workhorse of numerous production pipelines, there have been limited Pareto improvements to BERT since its release. In this paper, we introduce ModernBERT, bringing modern model optimizations to encoder-only models and representing a major Pareto improvement over older encoders. Trained on 2 trillion tokens with a native 8192 sequence length, ModernBERT models exhibit state-of-the-art results on a large pool of evaluations encompassing diverse classification tasks and both single and multi-vector retrieval on different domains (including code). In addition to strong downstream performance, ModernBERT is also the most speed and memory efficient encoder and is designed for inference on common GPUs.'}",oai:arXiv.org:2412.13663v2,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Benjamin Warner, Antoine Chaffin, Benjamin Clavi\\\'e, Orion Weller, Oskar Hallstr\\""om, Said Taghadouini, Alexis Gallagher, Raja Biswas, Faisal Ladhak, Tom Aarsen, Nathan Cooper, Griffin Adams, Jeremy Howard, Iacopo Poli'}]","Benjamin Warner, Antoine Chaffin, Benjamin Clavi\'e, Orion Weller, Oskar Hallstr\""om, Said Taghadouini, Alexis Gallagher, Raja Biswas, Faisal Ladhak, Tom Aarsen, Nathan Cooper, Griffin Adams, Jeremy Howard, Iacopo Poli","{'name': 'Benjamin Warner, Antoine Chaffin, Benjamin Clavi\\\'e, Orion Weller, Oskar Hallstr\\""om, Said Taghadouini, Alexis Gallagher, Raja Biswas, Faisal Ladhak, Tom Aarsen, Nathan Cooper, Griffin Adams, Jeremy Howard, Iacopo Poli'}",,
733,A2H: A UI Converter from Android to HarmonyOS Platform,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'A2H: A UI Converter from Android to HarmonyOS Platform'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.13693'}]",https://arxiv.org/abs/2412.13693,"arXiv:2412.13693v2 Announce Type: replace 
Abstract: With the growing importance of smartphones, developers face the challenge of creating separate applications for multiple platforms (e.g., Android, iOS, and HarmonyOS), leading to increased development costs and longer iteration cycles. One potential solution is to develop an app on one platform and then automatically convert it to other platforms, reducing the need for separate development efforts. However, migrating user interfaces (UIs) between platforms is particularly challenging due to significant differences in layout structures and development paradigms, such as the disparity between XML layout files in Android and ArkUI framework in HarmonyOS. Manual conversion of UIs is time-consuming, error-prone, and inefficient, necessitating an automated solution to streamline the process and enable seamless migration from Android to HarmonyOS. To address this challenge, we propose the A2H Converter, an automated tool for migrating Android UIs to HarmonyOS. The tool employs an large language model (LLM)-driven multi-agent framework to convert Android XML layouts into HarmonyOS ArkUI layouts. Using the RAG combing with decision rules, the system maps Android UI components to ArkUI equivalents, while a reflective mechanism continuously improves conversion accuracy. A2H Converter handles project-level layouts, ensuring consistency across multiple files and addressing complex UI logic. Experiments on six Android applications collected from GitHub demonstrate that our A2H Converter achieves a migration success rate of over 90.1%, 89.3%, and 89.2% at the component, page, and project levels, respectively. The demo video is available at. The tool is available at http://124.70.54.129:37860/.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.13693v2 Announce Type: replace \nAbstract: With the growing importance of smartphones, developers face the challenge of creating separate applications for multiple platforms (e.g., Android, iOS, and HarmonyOS), leading to increased development costs and longer iteration cycles. One potential solution is to develop an app on one platform and then automatically convert it to other platforms, reducing the need for separate development efforts. However, migrating user interfaces (UIs) between platforms is particularly challenging due to significant differences in layout structures and development paradigms, such as the disparity between XML layout files in Android and ArkUI framework in HarmonyOS. Manual conversion of UIs is time-consuming, error-prone, and inefficient, necessitating an automated solution to streamline the process and enable seamless migration from Android to HarmonyOS. To address this challenge, we propose the A2H Converter, an automated tool for migrating Android UIs to HarmonyOS. The tool employs an large language model (LLM)-driven multi-agent framework to convert Android XML layouts into HarmonyOS ArkUI layouts. Using the RAG combing with decision rules, the system maps Android UI components to ArkUI equivalents, while a reflective mechanism continuously improves conversion accuracy. A2H Converter handles project-level layouts, ensuring consistency across multiple files and addressing complex UI logic. Experiments on six Android applications collected from GitHub demonstrate that our A2H Converter achieves a migration success rate of over 90.1%, 89.3%, and 89.2% at the component, page, and project levels, respectively. The demo video is available at. The tool is available at http://124.70.54.129:37860/.'}",oai:arXiv.org:2412.13693v2,False,"[{'term': 'cs.SE', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Chen Wang, Lina Gong, Yujun Huang, Di Cui, Mingqiang Wei'}]","Chen Wang, Lina Gong, Yujun Huang, Di Cui, Mingqiang Wei","{'name': 'Chen Wang, Lina Gong, Yujun Huang, Di Cui, Mingqiang Wei'}",,
734,Typhoon 2: A Family of Open Text and Multimodal Thai Large Language Models,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Typhoon 2: A Family of Open Text and Multimodal Thai Large Language Models'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.13702'}]",https://arxiv.org/abs/2412.13702,"arXiv:2412.13702v2 Announce Type: replace 
Abstract: This paper introduces Typhoon 2, a series of text and multimodal large language models optimized for the Thai language. The series includes models for text, vision, and audio. Typhoon2-Text builds on state-of-the-art open models, such as Llama 3 and Qwen2, and we perform continual pre-training on a mixture of English and Thai data. We employ post-training techniques to enhance Thai language performance while preserving the base models' original capabilities. We release text models across a range of sizes, from 1 to 70 billion parameters, available in both base and instruction-tuned variants. To guardrail text generation, we release Typhoon2-Safety, a classifier enhanced for Thai cultures and language. Typhoon2-Vision improves Thai document understanding while retaining general visual capabilities, such as image captioning. Typhoon2-Audio introduces an end-to-end speech-to-speech model architecture capable of processing audio, speech, and text inputs and generating both text and speech outputs.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.13702v2 Announce Type: replace \nAbstract: This paper introduces Typhoon 2, a series of text and multimodal large language models optimized for the Thai language. The series includes models for text, vision, and audio. Typhoon2-Text builds on state-of-the-art open models, such as Llama 3 and Qwen2, and we perform continual pre-training on a mixture of English and Thai data. We employ post-training techniques to enhance Thai language performance while preserving the base models' original capabilities. We release text models across a range of sizes, from 1 to 70 billion parameters, available in both base and instruction-tuned variants. To guardrail text generation, we release Typhoon2-Safety, a classifier enhanced for Thai cultures and language. Typhoon2-Vision improves Thai document understanding while retaining general visual capabilities, such as image captioning. Typhoon2-Audio introduces an end-to-end speech-to-speech model architecture capable of processing audio, speech, and text inputs and generating both text and speech outputs.""}",oai:arXiv.org:2412.13702v2,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Kunat Pipatanakul, Potsawee Manakul, Natapong Nitarach, Warit Sirichotedumrong, Surapon Nonesung, Teetouch Jaknamon, Parinthapat Pengpun, Pittawat Taveekitworachai, Adisai Na-Thalang, Sittipong Sripaisarnmongkol, Krisanapong Jirayoot, Kasima Tharnpipitchai'}]","Kunat Pipatanakul, Potsawee Manakul, Natapong Nitarach, Warit Sirichotedumrong, Surapon Nonesung, Teetouch Jaknamon, Parinthapat Pengpun, Pittawat Taveekitworachai, Adisai Na-Thalang, Sittipong Sripaisarnmongkol, Krisanapong Jirayoot, Kasima Tharnpipitchai","{'name': 'Kunat Pipatanakul, Potsawee Manakul, Natapong Nitarach, Warit Sirichotedumrong, Surapon Nonesung, Teetouch Jaknamon, Parinthapat Pengpun, Pittawat Taveekitworachai, Adisai Na-Thalang, Sittipong Sripaisarnmongkol, Krisanapong Jirayoot, Kasima Tharnpipitchai'}",,
735,3D Registration in 30 Years: A Survey,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': '3D Registration in 30 Years: A Survey'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.13735'}]",https://arxiv.org/abs/2412.13735,"arXiv:2412.13735v2 Announce Type: replace 
Abstract: 3D point cloud registration is a fundamental problem in computer vision, computer graphics, robotics, remote sensing, and etc. Over the last thirty years, we have witnessed the amazing advancement in this area with numerous kinds of solutions. Although a handful of relevant surveys have been conducted, their coverage is still limited. In this work, we present a comprehensive survey on 3D point cloud registration, covering a set of sub-areas such as pairwise coarse registration, pairwise fine registration, multi-view registration, cross-scale registration, and multi-instance registration. The datasets, evaluation metrics, method taxonomy, discussions of the merits and demerits, insightful thoughts of future directions are comprehensively presented in this survey. The regularly updated project page of the survey is available at https://github.com/Amyyyy11/3D-Registration-in-30-Years-A-Survey.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.13735v2 Announce Type: replace \nAbstract: 3D point cloud registration is a fundamental problem in computer vision, computer graphics, robotics, remote sensing, and etc. Over the last thirty years, we have witnessed the amazing advancement in this area with numerous kinds of solutions. Although a handful of relevant surveys have been conducted, their coverage is still limited. In this work, we present a comprehensive survey on 3D point cloud registration, covering a set of sub-areas such as pairwise coarse registration, pairwise fine registration, multi-view registration, cross-scale registration, and multi-instance registration. The datasets, evaluation metrics, method taxonomy, discussions of the merits and demerits, insightful thoughts of future directions are comprehensively presented in this survey. The regularly updated project page of the survey is available at https://github.com/Amyyyy11/3D-Registration-in-30-Years-A-Survey.'}",oai:arXiv.org:2412.13735v2,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': ""Jiaqi Yang, Chu'ai Zhang, Zhengbao Wang, Xinyue Cao, Xuan Ouyang, Xiyu Zhang, Zhenxuan Zeng, Zhao Zeng, Borui Lu, Zhiyi Xia, Qian Zhang, Yulan Guo, Yanning Zhang""}]","Jiaqi Yang, Chu'ai Zhang, Zhengbao Wang, Xinyue Cao, Xuan Ouyang, Xiyu Zhang, Zhenxuan Zeng, Zhao Zeng, Borui Lu, Zhiyi Xia, Qian Zhang, Yulan Guo, Yanning Zhang","{'name': ""Jiaqi Yang, Chu'ai Zhang, Zhengbao Wang, Xinyue Cao, Xuan Ouyang, Xiyu Zhang, Zhenxuan Zeng, Zhao Zeng, Borui Lu, Zhiyi Xia, Qian Zhang, Yulan Guo, Yanning Zhang""}",,
736,LLM-SEM: A Sentiment-Based Student Engagement Metric Using LLMS for E-Learning Platforms,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'LLM-SEM: A Sentiment-Based Student Engagement Metric Using LLMS for E-Learning Platforms'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.13765'}]",https://arxiv.org/abs/2412.13765,"arXiv:2412.13765v2 Announce Type: replace 
Abstract: Current methods for analyzing student engagement in e-learning platforms, including automated systems, often struggle with challenges such as handling fuzzy sentiment in text comments and relying on limited metadata. Traditional approaches, such as surveys and questionnaires, also face issues like small sample sizes and scalability. In this paper, we introduce LLM-SEM (Language Model-Based Student Engagement Metric), a novel approach that leverages video metadata and sentiment analysis of student comments to measure engagement. By utilizing recent Large Language Models (LLMs), we generate high-quality sentiment predictions to mitigate text fuzziness and normalize key features such as views and likes. Our holistic method combines comprehensive metadata with sentiment polarity scores to gauge engagement at both the course and lesson levels. Extensive experiments were conducted to evaluate various LLM models, demonstrating the effectiveness of LLM-SEM in providing a scalable and accurate measure of student engagement. We fine-tuned TXLM-RoBERTa using human-annotated sentiment datasets to enhance prediction accuracy and utilized LLama 3B, and Gemma 9B from Ollama.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.13765v2 Announce Type: replace \nAbstract: Current methods for analyzing student engagement in e-learning platforms, including automated systems, often struggle with challenges such as handling fuzzy sentiment in text comments and relying on limited metadata. Traditional approaches, such as surveys and questionnaires, also face issues like small sample sizes and scalability. In this paper, we introduce LLM-SEM (Language Model-Based Student Engagement Metric), a novel approach that leverages video metadata and sentiment analysis of student comments to measure engagement. By utilizing recent Large Language Models (LLMs), we generate high-quality sentiment predictions to mitigate text fuzziness and normalize key features such as views and likes. Our holistic method combines comprehensive metadata with sentiment polarity scores to gauge engagement at both the course and lesson levels. Extensive experiments were conducted to evaluate various LLM models, demonstrating the effectiveness of LLM-SEM in providing a scalable and accurate measure of student engagement. We fine-tuned TXLM-RoBERTa using human-annotated sentiment datasets to enhance prediction accuracy and utilized LLama 3B, and Gemma 9B from Ollama.'}",oai:arXiv.org:2412.13765v2,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by-sa/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-sa/4.0/'}","[{'name': 'Ali Hamdi, Ahmed Abdelmoneim Mazrou, Mohamed Shaltout'}]","Ali Hamdi, Ahmed Abdelmoneim Mazrou, Mohamed Shaltout","{'name': 'Ali Hamdi, Ahmed Abdelmoneim Mazrou, Mohamed Shaltout'}",,
737,"M$^3$-VOS: Multi-Phase, Multi-Transition, and Multi-Scenery Video Object Segmentation","{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'M$^3$-VOS: Multi-Phase, Multi-Transition, and Multi-Scenery Video Object Segmentation'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.13803'}]",https://arxiv.org/abs/2412.13803,"arXiv:2412.13803v2 Announce Type: replace 
Abstract: Intelligent robots need to interact with diverse objects across various environments. The appearance and state of objects frequently undergo complex transformations depending on the object properties, e.g., phase transitions. However, in the vision community, segmenting dynamic objects with phase transitions is overlooked. In light of this, we introduce the concept of phase in segmentation, which categorizes real-world objects based on their visual characteristics and potential morphological and appearance changes. Then, we present a new benchmark, Multi-Phase, Multi-Transition, and Multi-Scenery Video Object Segmentation (M$^3$-VOS), to verify the ability of models to understand object phases, which consists of 479 high-resolution videos spanning over 10 distinct everyday scenarios. It provides dense instance mask annotations that capture both object phases and their transitions. We evaluate state-of-the-art methods on M$^3$-VOS, yielding several key insights. Notably, current appearancebased approaches show significant room for improvement when handling objects with phase transitions. The inherent changes in disorder suggest that the predictive performance of the forward entropy-increasing process can be improved through a reverse entropy-reducing process. These findings lead us to propose ReVOS, a new plug-andplay model that improves its performance by reversal refinement. Our data and code will be publicly available at https://zixuan-chen.github.io/M-cubeVOS.github.io/.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.13803v2 Announce Type: replace \nAbstract: Intelligent robots need to interact with diverse objects across various environments. The appearance and state of objects frequently undergo complex transformations depending on the object properties, e.g., phase transitions. However, in the vision community, segmenting dynamic objects with phase transitions is overlooked. In light of this, we introduce the concept of phase in segmentation, which categorizes real-world objects based on their visual characteristics and potential morphological and appearance changes. Then, we present a new benchmark, Multi-Phase, Multi-Transition, and Multi-Scenery Video Object Segmentation (M$^3$-VOS), to verify the ability of models to understand object phases, which consists of 479 high-resolution videos spanning over 10 distinct everyday scenarios. It provides dense instance mask annotations that capture both object phases and their transitions. We evaluate state-of-the-art methods on M$^3$-VOS, yielding several key insights. Notably, current appearancebased approaches show significant room for improvement when handling objects with phase transitions. The inherent changes in disorder suggest that the predictive performance of the forward entropy-increasing process can be improved through a reverse entropy-reducing process. These findings lead us to propose ReVOS, a new plug-andplay model that improves its performance by reversal refinement. Our data and code will be publicly available at https://zixuan-chen.github.io/M-cubeVOS.github.io/.'}",oai:arXiv.org:2412.13803v2,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by-nc-sa/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-sa/4.0/'}","[{'name': 'Zixuan Chen, Jiaxin Li, Liming Tan, Yejie Guo, Junxuan Liang, Cewu Lu, Yong-Lu Li'}]","Zixuan Chen, Jiaxin Li, Liming Tan, Yejie Guo, Junxuan Liang, Cewu Lu, Yong-Lu Li","{'name': 'Zixuan Chen, Jiaxin Li, Liming Tan, Yejie Guo, Junxuan Liang, Cewu Lu, Yong-Lu Li'}",,
738,From Expectation to Habit: Why Do Software Practitioners Adopt Fairness Toolkits?,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'From Expectation to Habit: Why Do Software Practitioners Adopt Fairness Toolkits?'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.13846'}]",https://arxiv.org/abs/2412.13846,"arXiv:2412.13846v2 Announce Type: replace 
Abstract: As the adoption of machine learning (ML) systems continues to grow across industries, concerns about fairness and bias in these systems have taken center stage. Fairness toolkits, designed to mitigate bias in ML models, serve as critical tools for addressing these ethical concerns. However, their adoption in the context of software development remains underexplored, especially regarding the cognitive and behavioral factors driving their usage. As a deeper understanding of these factors could be pivotal in refining tool designs and promoting broader adoption, this study investigates the factors influencing the adoption of fairness toolkits from an individual perspective. Guided by the Unified Theory of Acceptance and Use of Technology (UTAUT2), we examined the factors shaping the intention to adopt and actual use of fairness toolkits. Specifically, we employed Partial Least Squares Structural Equation Modeling (PLS-SEM) to analyze data from a survey study involving practitioners in the software industry. Our findings reveal that performance expectancy and habit are the primary drivers of fairness toolkit adoption. These insights suggest that by emphasizing the effectiveness of these tools in mitigating bias and fostering habitual use, organizations can encourage wider adoption. Practical recommendations include improving toolkit usability, integrating bias mitigation processes into routine development workflows, and providing ongoing support to ensure professionals see clear benefits from regular use.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.13846v2 Announce Type: replace \nAbstract: As the adoption of machine learning (ML) systems continues to grow across industries, concerns about fairness and bias in these systems have taken center stage. Fairness toolkits, designed to mitigate bias in ML models, serve as critical tools for addressing these ethical concerns. However, their adoption in the context of software development remains underexplored, especially regarding the cognitive and behavioral factors driving their usage. As a deeper understanding of these factors could be pivotal in refining tool designs and promoting broader adoption, this study investigates the factors influencing the adoption of fairness toolkits from an individual perspective. Guided by the Unified Theory of Acceptance and Use of Technology (UTAUT2), we examined the factors shaping the intention to adopt and actual use of fairness toolkits. Specifically, we employed Partial Least Squares Structural Equation Modeling (PLS-SEM) to analyze data from a survey study involving practitioners in the software industry. Our findings reveal that performance expectancy and habit are the primary drivers of fairness toolkit adoption. These insights suggest that by emphasizing the effectiveness of these tools in mitigating bias and fostering habitual use, organizations can encourage wider adoption. Practical recommendations include improving toolkit usability, integrating bias mitigation processes into routine development workflows, and providing ongoing support to ensure professionals see clear benefits from regular use.'}",oai:arXiv.org:2412.13846v2,False,"[{'term': 'cs.SE', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Gianmario Voria, Stefano Lambiase, Maria Concetta Schiavone, Gemma Catolino, Fabio Palomba'}]","Gianmario Voria, Stefano Lambiase, Maria Concetta Schiavone, Gemma Catolino, Fabio Palomba","{'name': 'Gianmario Voria, Stefano Lambiase, Maria Concetta Schiavone, Gemma Catolino, Fabio Palomba'}",,
739,Towards an identity management solution on Arweave,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Towards an identity management solution on Arweave'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.13865'}]",https://arxiv.org/abs/2412.13865,"arXiv:2412.13865v2 Announce Type: replace 
Abstract: Traditional identity management systems, often centralized, face challenges around privacy, data security, and user control, leaving users vulnerable to data breaches and misuse. This paper explores the potential of using the Arweave network to develop an identity management solution. By harnessing Arweave's permanent storage, our solution offers the users a Self-Sovereign Identity (SSI) framework, that uses Decentralized Identifiers (DIDs) and Verifiable Credentials (VCs) to allow individuals and other entities to create, own, and manage their digital identities. Further, the solution integrates privacy-preserving technologies, including zero-knowledge proofs and the BBS(+) signature scheme, enabling selective disclosure. This approach ultimately enhances user privacy and supports compliance with European Union legislation and regulatory standards like the General Data Protection Regulation (GDPR) by design.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.13865v2 Announce Type: replace \nAbstract: Traditional identity management systems, often centralized, face challenges around privacy, data security, and user control, leaving users vulnerable to data breaches and misuse. This paper explores the potential of using the Arweave network to develop an identity management solution. By harnessing Arweave's permanent storage, our solution offers the users a Self-Sovereign Identity (SSI) framework, that uses Decentralized Identifiers (DIDs) and Verifiable Credentials (VCs) to allow individuals and other entities to create, own, and manage their digital identities. Further, the solution integrates privacy-preserving technologies, including zero-knowledge proofs and the BBS(+) signature scheme, enabling selective disclosure. This approach ultimately enhances user privacy and supports compliance with European Union legislation and regulatory standards like the General Data Protection Regulation (GDPR) by design.""}",oai:arXiv.org:2412.13865v2,False,"[{'term': 'cs.CR', 'scheme': None, 'label': None}, {'term': 'cs.ET', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Andreea Elena Dragnoiu, Ruxandra F. Olimid'}]","Andreea Elena Dragnoiu, Ruxandra F. Olimid","{'name': 'Andreea Elena Dragnoiu, Ruxandra F. Olimid'}",,
740,A Black-Box Evaluation Framework for Semantic Robustness in Bird's Eye View Detection,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""A Black-Box Evaluation Framework for Semantic Robustness in Bird's Eye View Detection""}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.13913'}]",https://arxiv.org/abs/2412.13913,"arXiv:2412.13913v2 Announce Type: replace 
Abstract: Camera-based Bird's Eye View (BEV) perception models receive increasing attention for their crucial role in autonomous driving, a domain where concerns about the robustness and reliability of deep learning have been raised. While only a few works have investigated the effects of randomly generated semantic perturbations, aka natural corruptions, on the multi-view BEV detection task, we develop a black-box robustness evaluation framework that adversarially optimises three common semantic perturbations: geometric transformation, colour shifting, and motion blur, to deceive BEV models, serving as the first approach in this emerging field. To address the challenge posed by optimising the semantic perturbation, we design a smoothed, distance-based surrogate function to replace the mAP metric and introduce SimpleDIRECT, a deterministic optimisation algorithm that utilises observed slopes to guide the optimisation process. By comparing with randomised perturbation and two optimisation baselines, we demonstrate the effectiveness of the proposed framework. Additionally, we provide a benchmark on the semantic robustness of ten recent BEV models. The results reveal that PolarFormer, which emphasises geometric information from multi-view images, exhibits the highest robustness, whereas BEVDet is fully compromised, with its precision reduced to zero.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.13913v2 Announce Type: replace \nAbstract: Camera-based Bird's Eye View (BEV) perception models receive increasing attention for their crucial role in autonomous driving, a domain where concerns about the robustness and reliability of deep learning have been raised. While only a few works have investigated the effects of randomly generated semantic perturbations, aka natural corruptions, on the multi-view BEV detection task, we develop a black-box robustness evaluation framework that adversarially optimises three common semantic perturbations: geometric transformation, colour shifting, and motion blur, to deceive BEV models, serving as the first approach in this emerging field. To address the challenge posed by optimising the semantic perturbation, we design a smoothed, distance-based surrogate function to replace the mAP metric and introduce SimpleDIRECT, a deterministic optimisation algorithm that utilises observed slopes to guide the optimisation process. By comparing with randomised perturbation and two optimisation baselines, we demonstrate the effectiveness of the proposed framework. Additionally, we provide a benchmark on the semantic robustness of ten recent BEV models. The results reveal that PolarFormer, which emphasises geometric information from multi-view images, exhibits the highest robustness, whereas BEVDet is fully compromised, with its precision reduced to zero.""}",oai:arXiv.org:2412.13913v2,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Fu Wang, Yanghao Zhang, Xiangyu Yin, Guangliang Cheng, Zeyu Fu, Xiaowei Huang, Wenjie Ruan'}]","Fu Wang, Yanghao Zhang, Xiangyu Yin, Guangliang Cheng, Zeyu Fu, Xiaowei Huang, Wenjie Ruan","{'name': 'Fu Wang, Yanghao Zhang, Xiangyu Yin, Guangliang Cheng, Zeyu Fu, Xiaowei Huang, Wenjie Ruan'}",,
741,Towards an optimised evaluation of teachers' discourse: The case of engaging messages,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""Towards an optimised evaluation of teachers' discourse: The case of engaging messages""}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14011'}]",https://arxiv.org/abs/2412.14011,"arXiv:2412.14011v2 Announce Type: replace 
Abstract: Evaluating teachers' skills is crucial for enhancing education quality and student outcomes. Teacher discourse, significantly influencing student performance, is a key component. However, coding this discourse can be laborious. This study addresses this issue by introducing a new methodology for optimising the assessment of teacher discourse. The research consisted of two studies, both within the framework of engaging messages used by secondary education teachers. The first study involved training two large language models on real-world examples from audio-recorded lessons over two academic years to identify and classify the engaging messages from the lessons' transcripts. This resulted in sensitivities of 84.31% and 91.11%, and specificities of 97.69% and 86.36% in identification and classification, respectively. The second study applied these models to transcripts of audio-recorded lessons from a third academic year to examine the frequency and distribution of message types by educational level and moment of the academic year. Results showed teachers predominantly use messages emphasising engagement benefits, linked to improved outcomes, while one-third highlighted non-engagement disadvantages, associated with increased anxiety. The use of engaging messages declined in Grade 12 and towards the academic year's end. These findings suggest potential interventions to optimise engaging message use, enhancing teaching quality and student outcomes.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14011v2 Announce Type: replace \nAbstract: Evaluating teachers' skills is crucial for enhancing education quality and student outcomes. Teacher discourse, significantly influencing student performance, is a key component. However, coding this discourse can be laborious. This study addresses this issue by introducing a new methodology for optimising the assessment of teacher discourse. The research consisted of two studies, both within the framework of engaging messages used by secondary education teachers. The first study involved training two large language models on real-world examples from audio-recorded lessons over two academic years to identify and classify the engaging messages from the lessons' transcripts. This resulted in sensitivities of 84.31% and 91.11%, and specificities of 97.69% and 86.36% in identification and classification, respectively. The second study applied these models to transcripts of audio-recorded lessons from a third academic year to examine the frequency and distribution of message types by educational level and moment of the academic year. Results showed teachers predominantly use messages emphasising engagement benefits, linked to improved outcomes, while one-third highlighted non-engagement disadvantages, associated with increased anxiety. The use of engaging messages declined in Grade 12 and towards the academic year's end. These findings suggest potential interventions to optimise engaging message use, enhancing teaching quality and student outcomes.""}",oai:arXiv.org:2412.14011v2,False,"[{'term': 'cs.CL', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Samuel Falcon, Jaime Leon'}]","Samuel Falcon, Jaime Leon","{'name': 'Samuel Falcon, Jaime Leon'}",,
742,Online MDP with Transition Prototypes: A Robust Adaptive Approach,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Online MDP with Transition Prototypes: A Robust Adaptive Approach'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14075'}]",https://arxiv.org/abs/2412.14075,"arXiv:2412.14075v2 Announce Type: replace 
Abstract: In this work, we consider an online robust Markov Decision Process (MDP) where we have the information of finitely many prototypes of the underlying transition kernel. We consider an adaptively updated ambiguity set of the prototypes and propose an algorithm that efficiently identifies the true underlying transition kernel while guaranteeing the performance of the corresponding robust policy. To be more specific, we provide a sublinear regret of the subsequent optimal robust policy. We also provide an early stopping mechanism and a worst-case performance bound of the value function. In numerical experiments, we demonstrate that our method outperforms existing approaches, particularly in the early stage with limited data. This work contributes to robust MDPs by considering possible prior information about the underlying transition probability and online learning, offering both theoretical insights and practical algorithms for improved decision-making under uncertainty.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14075v2 Announce Type: replace \nAbstract: In this work, we consider an online robust Markov Decision Process (MDP) where we have the information of finitely many prototypes of the underlying transition kernel. We consider an adaptively updated ambiguity set of the prototypes and propose an algorithm that efficiently identifies the true underlying transition kernel while guaranteeing the performance of the corresponding robust policy. To be more specific, we provide a sublinear regret of the subsequent optimal robust policy. We also provide an early stopping mechanism and a worst-case performance bound of the value function. In numerical experiments, we demonstrate that our method outperforms existing approaches, particularly in the early stage with limited data. This work contributes to robust MDPs by considering possible prior information about the underlying transition probability and online learning, offering both theoretical insights and practical algorithms for improved decision-making under uncertainty.'}",oai:arXiv.org:2412.14075v2,False,"[{'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Shuo Sun, Meng Qi, Zuo-Jun Max Shen'}]","Shuo Sun, Meng Qi, Zuo-Jun Max Shen","{'name': 'Shuo Sun, Meng Qi, Zuo-Jun Max Shen'}",,
743,GaraMoSt: Parallel Multi-Granularity Motion and Structural Modeling for Efficient Multi-Frame Interpolation in DSA Images,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'GaraMoSt: Parallel Multi-Granularity Motion and Structural Modeling for Efficient Multi-Frame Interpolation in DSA Images'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14118'}]",https://arxiv.org/abs/2412.14118,"arXiv:2412.14118v2 Announce Type: replace 
Abstract: The rapid and accurate direct multi-frame interpolation method for Digital Subtraction Angiography (DSA) images is crucial for reducing radiation and providing real-time assistance to physicians for precise diagnostics and treatment. DSA images contain complex vascular structures and various motions. Applying natural scene Video Frame Interpolation (VFI) methods results in motion artifacts, structural dissipation, and blurriness. Recently, MoSt-DSA has specifically addressed these issues for the first time and achieved SOTA results. However, MoSt-DSA's focus on real-time performance leads to insufficient suppression of high-frequency noise and incomplete filtering of low-frequency noise in the generated images. To address these issues within the same computational time scale, we propose GaraMoSt. Specifically, we optimize the network pipeline with a parallel design and propose a module named MG-MSFE. MG-MSFE extracts frame-relative motion and structural features at various granularities in a fully convolutional parallel manner and supports independent, flexible adjustment of context-aware granularity at different scales, thus enhancing computational efficiency and accuracy. Extensive experiments demonstrate that GaraMoSt achieves the SOTA performance in accuracy, robustness, visual effects, and noise suppression, comprehensively surpassing MoSt-DSA and other natural scene VFI methods. The code and models are available at https://github.com/ZyoungXu/GaraMoSt.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.14118v2 Announce Type: replace \nAbstract: The rapid and accurate direct multi-frame interpolation method for Digital Subtraction Angiography (DSA) images is crucial for reducing radiation and providing real-time assistance to physicians for precise diagnostics and treatment. DSA images contain complex vascular structures and various motions. Applying natural scene Video Frame Interpolation (VFI) methods results in motion artifacts, structural dissipation, and blurriness. Recently, MoSt-DSA has specifically addressed these issues for the first time and achieved SOTA results. However, MoSt-DSA's focus on real-time performance leads to insufficient suppression of high-frequency noise and incomplete filtering of low-frequency noise in the generated images. To address these issues within the same computational time scale, we propose GaraMoSt. Specifically, we optimize the network pipeline with a parallel design and propose a module named MG-MSFE. MG-MSFE extracts frame-relative motion and structural features at various granularities in a fully convolutional parallel manner and supports independent, flexible adjustment of context-aware granularity at different scales, thus enhancing computational efficiency and accuracy. Extensive experiments demonstrate that GaraMoSt achieves the SOTA performance in accuracy, robustness, visual effects, and noise suppression, comprehensively surpassing MoSt-DSA and other natural scene VFI methods. The code and models are available at https://github.com/ZyoungXu/GaraMoSt.""}",oai:arXiv.org:2412.14118v2,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://creativecommons.org/licenses/by-nc-sa/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-sa/4.0/'}","[{'name': 'Ziyang Xu, Huangxuan Zhao, Wenyu Liu, Xinggang Wang'}]","Ziyang Xu, Huangxuan Zhao, Wenyu Liu, Xinggang Wang","{'name': 'Ziyang Xu, Huangxuan Zhao, Wenyu Liu, Xinggang Wang'}",,
744,FashionComposer: Compositional Fashion Image Generation,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'FashionComposer: Compositional Fashion Image Generation'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14168'}]",https://arxiv.org/abs/2412.14168,"arXiv:2412.14168v2 Announce Type: replace 
Abstract: We present FashionComposer for compositional fashion image generation. Unlike previous methods, FashionComposer is highly flexible. It takes multi-modal input (i.e., text prompt, parametric human model, garment image, and face image) and supports personalizing the appearance, pose, and figure of the human and assigning multiple garments in one pass. To achieve this, we first develop a universal framework capable of handling diverse input modalities. We construct scaled training data to enhance the model's robust compositional capabilities. To accommodate multiple reference images (garments and faces) seamlessly, we organize these references in a single image as an ""asset library"" and employ a reference UNet to extract appearance features. To inject the appearance features into the correct pixels in the generated result, we propose subject-binding attention. It binds the appearance features from different ""assets"" with the corresponding text features. In this way, the model could understand each asset according to their semantics, supporting arbitrary numbers and types of reference images. As a comprehensive solution, FashionComposer also supports many other applications like human album generation, diverse virtual try-on tasks, etc.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14168v2 Announce Type: replace \nAbstract: We present FashionComposer for compositional fashion image generation. Unlike previous methods, FashionComposer is highly flexible. It takes multi-modal input (i.e., text prompt, parametric human model, garment image, and face image) and supports personalizing the appearance, pose, and figure of the human and assigning multiple garments in one pass. To achieve this, we first develop a universal framework capable of handling diverse input modalities. We construct scaled training data to enhance the model\'s robust compositional capabilities. To accommodate multiple reference images (garments and faces) seamlessly, we organize these references in a single image as an ""asset library"" and employ a reference UNet to extract appearance features. To inject the appearance features into the correct pixels in the generated result, we propose subject-binding attention. It binds the appearance features from different ""assets"" with the corresponding text features. In this way, the model could understand each asset according to their semantics, supporting arbitrary numbers and types of reference images. As a comprehensive solution, FashionComposer also supports many other applications like human album generation, diverse virtual try-on tasks, etc.'}",oai:arXiv.org:2412.14168v2,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Sihui Ji, Yiyang Wang, Xi Chen, Xiaogang Xu, Hao Luo, Hengshuang Zhao'}]","Sihui Ji, Yiyang Wang, Xi Chen, Xiaogang Xu, Hao Luo, Hengshuang Zhao","{'name': 'Sihui Ji, Yiyang Wang, Xi Chen, Xiaogang Xu, Hao Luo, Hengshuang Zhao'}",,
745,E-CAR: Efficient Continuous Autoregressive Image Generation via Multistage Modeling,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'E-CAR: Efficient Continuous Autoregressive Image Generation via Multistage Modeling'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14170'}]",https://arxiv.org/abs/2412.14170,"arXiv:2412.14170v2 Announce Type: replace 
Abstract: Recent advances in autoregressive (AR) models with continuous tokens for image generation show promising results by eliminating the need for discrete tokenization. However, these models face efficiency challenges due to their sequential token generation nature and reliance on computationally intensive diffusion-based sampling. We present ECAR (Efficient Continuous Auto-Regressive Image Generation via Multistage Modeling), an approach that addresses these limitations through two intertwined innovations: (1) a stage-wise continuous token generation strategy that reduces computational complexity and provides progressively refined token maps as hierarchical conditions, and (2) a multistage flow-based distribution modeling method that transforms only partial-denoised distributions at each stage comparing to complete denoising in normal diffusion models. Holistically, ECAR operates by generating tokens at increasing resolutions while simultaneously denoising the image at each stage. This design not only reduces token-to-image transformation cost by a factor of the stage number but also enables parallel processing at the token level. Our approach not only enhances computational efficiency but also aligns naturally with image generation principles by operating in continuous token space and following a hierarchical generation process from coarse to fine details. Experimental results demonstrate that ECAR achieves comparable image quality to DiT Peebles & Xie [2023] while requiring 10$\times$ FLOPs reduction and 5$\times$ speedup to generate a 256$\times$256 image.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14170v2 Announce Type: replace \nAbstract: Recent advances in autoregressive (AR) models with continuous tokens for image generation show promising results by eliminating the need for discrete tokenization. However, these models face efficiency challenges due to their sequential token generation nature and reliance on computationally intensive diffusion-based sampling. We present ECAR (Efficient Continuous Auto-Regressive Image Generation via Multistage Modeling), an approach that addresses these limitations through two intertwined innovations: (1) a stage-wise continuous token generation strategy that reduces computational complexity and provides progressively refined token maps as hierarchical conditions, and (2) a multistage flow-based distribution modeling method that transforms only partial-denoised distributions at each stage comparing to complete denoising in normal diffusion models. Holistically, ECAR operates by generating tokens at increasing resolutions while simultaneously denoising the image at each stage. This design not only reduces token-to-image transformation cost by a factor of the stage number but also enables parallel processing at the token level. Our approach not only enhances computational efficiency but also aligns naturally with image generation principles by operating in continuous token space and following a hierarchical generation process from coarse to fine details. Experimental results demonstrate that ECAR achieves comparable image quality to DiT Peebles & Xie [2023] while requiring 10$\\times$ FLOPs reduction and 5$\\times$ speedup to generate a 256$\\times$256 image.'}",oai:arXiv.org:2412.14170v2,False,"[{'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Zhihang Yuan, Yuzhang Shang, Hanling Zhang, Tongcheng Fang, Rui Xie, Bingxin Xu, Yan Yan, Shengen Yan, Guohao Dai, Yu Wang'}]","Zhihang Yuan, Yuzhang Shang, Hanling Zhang, Tongcheng Fang, Rui Xie, Bingxin Xu, Yan Yan, Shengen Yan, Guohao Dai, Yu Wang","{'name': 'Zhihang Yuan, Yuzhang Shang, Hanling Zhang, Tongcheng Fang, Rui Xie, Bingxin Xu, Yan Yan, Shengen Yan, Guohao Dai, Yu Wang'}",,
746,Asymptotic expansion for the Hartman-Watson distribution,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Asymptotic expansion for the Hartman-Watson distribution'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2001.09579'}]",https://arxiv.org/abs/2001.09579,"arXiv:2001.09579v3 Announce Type: replace-cross 
Abstract: The Hartman-Watson distribution with density $f_r(t)$ is a probability distribution defined on $t \geq 0$ which appears in several problems of applied probability. The density of this distribution is expressed in terms of an integral $\theta(r,t)$ which is difficult to evaluate numerically for small $t\to 0$. Using saddle point methods, we obtain the first two terms of the $t\to 0$ expansion of $\theta(\rho/t,t)$ at fixed $\rho >0$. An error bound is obtained by numerical estimates of the integrand, which is furthermore uniform in $\rho$. As an application we obtain the leading asymptotics of the density of the time average of the geometric Brownian motion as $t\to 0$. This has the form $\mathbb{P}(\frac{1}{t} \int_0^t e^{2(B_s+\mu s)} ds \in da) \sim (2\pi t)^{-1/2} g(a,\mu) e^{-\frac{1}{t} J(a)} da/a$, with an exponent $J(a)$ which reproduces the known result obtained previously using Large Deviations theory.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2001.09579v3 Announce Type: replace-cross \nAbstract: The Hartman-Watson distribution with density $f_r(t)$ is a probability distribution defined on $t \\geq 0$ which appears in several problems of applied probability. The density of this distribution is expressed in terms of an integral $\\theta(r,t)$ which is difficult to evaluate numerically for small $t\\to 0$. Using saddle point methods, we obtain the first two terms of the $t\\to 0$ expansion of $\\theta(\\rho/t,t)$ at fixed $\\rho >0$. An error bound is obtained by numerical estimates of the integrand, which is furthermore uniform in $\\rho$. As an application we obtain the leading asymptotics of the density of the time average of the geometric Brownian motion as $t\\to 0$. This has the form $\\mathbb{P}(\\frac{1}{t} \\int_0^t e^{2(B_s+\\mu s)} ds \\in da) \\sim (2\\pi t)^{-1/2} g(a,\\mu) e^{-\\frac{1}{t} J(a)} da/a$, with an exponent $J(a)$ which reproduces the known result obtained previously using Large Deviations theory.'}",oai:arXiv.org:2001.09579v3,False,"[{'term': 'math.PR', 'scheme': None, 'label': None}, {'term': 'cs.NA', 'scheme': None, 'label': None}, {'term': 'math.NA', 'scheme': None, 'label': None}, {'term': 'q-fin.MF', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace-cross,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}",[{'name': 'Dan Pirjol'}],Dan Pirjol,{'name': 'Dan Pirjol'},10.1007/s11009-020-09827-5,
747,Deep Learning-based Non-Intrusive Multi-Objective Speech Assessment Model with Cross-Domain Features,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Deep Learning-based Non-Intrusive Multi-Objective Speech Assessment Model with Cross-Domain Features'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2111.02363'}]",https://arxiv.org/abs/2111.02363,"arXiv:2111.02363v5 Announce Type: replace-cross 
Abstract: In this study, we propose a cross-domain multi-objective speech assessment model called MOSA-Net, which can estimate multiple speech assessment metrics simultaneously. Experimental results show that MOSA-Net can improve the linear correlation coefficient (LCC) by 0.026 (0.990 vs 0.964 in seen noise environments) and 0.012 (0.969 vs 0.957 in unseen noise environments) in perceptual evaluation of speech quality (PESQ) prediction, compared to Quality-Net, an existing single-task model for PESQ prediction, and improve LCC by 0.021 (0.985 vs 0.964 in seen noise environments) and 0.047 (0.836 vs 0.789 in unseen noise environments) in short-time objective intelligibility (STOI) prediction, compared to STOI-Net (based on CRNN), an existing single-task model for STOI prediction. Moreover, MOSA-Net, originally trained to assess objective scores, can be used as a pre-trained model to be effectively adapted to an assessment model for predicting subjective quality and intelligibility scores with a limited amount of training data. Experimental results show that MOSA-Net can improve LCC by 0.018 (0.805 vs 0.787) in mean opinion score (MOS) prediction, compared to MOS-SSL, a strong single-task model for MOS prediction. In light of the confirmed prediction capability, we further adopt the latent representations of MOSA-Net to guide the speech enhancement (SE) process and derive a quality-intelligibility (QI)-aware SE (QIA-SE) approach accordingly. Experimental results show that QIA-SE provides superior enhancement performance compared with the baseline SE system in terms of objective evaluation metrics and qualitative evaluation test. For example, QIA-SE can improve PESQ by 0.301 (2.953 vs 2.652 in seen noise environments) and 0.18 (2.658 vs 2.478 in unseen noise environments) over a CNN-based baseline SE model.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2111.02363v5 Announce Type: replace-cross \nAbstract: In this study, we propose a cross-domain multi-objective speech assessment model called MOSA-Net, which can estimate multiple speech assessment metrics simultaneously. Experimental results show that MOSA-Net can improve the linear correlation coefficient (LCC) by 0.026 (0.990 vs 0.964 in seen noise environments) and 0.012 (0.969 vs 0.957 in unseen noise environments) in perceptual evaluation of speech quality (PESQ) prediction, compared to Quality-Net, an existing single-task model for PESQ prediction, and improve LCC by 0.021 (0.985 vs 0.964 in seen noise environments) and 0.047 (0.836 vs 0.789 in unseen noise environments) in short-time objective intelligibility (STOI) prediction, compared to STOI-Net (based on CRNN), an existing single-task model for STOI prediction. Moreover, MOSA-Net, originally trained to assess objective scores, can be used as a pre-trained model to be effectively adapted to an assessment model for predicting subjective quality and intelligibility scores with a limited amount of training data. Experimental results show that MOSA-Net can improve LCC by 0.018 (0.805 vs 0.787) in mean opinion score (MOS) prediction, compared to MOS-SSL, a strong single-task model for MOS prediction. In light of the confirmed prediction capability, we further adopt the latent representations of MOSA-Net to guide the speech enhancement (SE) process and derive a quality-intelligibility (QI)-aware SE (QIA-SE) approach accordingly. Experimental results show that QIA-SE provides superior enhancement performance compared with the baseline SE system in terms of objective evaluation metrics and qualitative evaluation test. For example, QIA-SE can improve PESQ by 0.301 (2.953 vs 2.652 in seen noise environments) and 0.18 (2.658 vs 2.478 in unseen noise environments) over a CNN-based baseline SE model.'}",oai:arXiv.org:2111.02363v5,False,"[{'term': 'eess.AS', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'cs.SD', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace-cross,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Ryandhimas E. Zezario, Szu-Wei Fu, Fei Chen, Chiou-Shann Fuh, Hsin-Min Wang, Yu Tsao'}]","Ryandhimas E. Zezario, Szu-Wei Fu, Fei Chen, Chiou-Shann Fuh, Hsin-Min Wang, Yu Tsao","{'name': 'Ryandhimas E. Zezario, Szu-Wei Fu, Fei Chen, Chiou-Shann Fuh, Hsin-Min Wang, Yu Tsao'}",,
748,Modeling Short-Range Microwave Networks to Scale Superconducting Quantum Computation,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Modeling Short-Range Microwave Networks to Scale Superconducting Quantum Computation'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2201.08825'}]",https://arxiv.org/abs/2201.08825,"arXiv:2201.08825v4 Announce Type: replace-cross 
Abstract: A core challenge for superconducting quantum computers is to scale up the number of qubits in each processor without increasing noise or cross-talk. Distributed quantum computing across small qubit arrays, known as chiplets, can address these challenges in a scalable manner. We propose a chiplet architecture over microwave links with potential to exceed monolithic performance on near-term hardware. Our methods of modeling and evaluating the chiplet architecture bridge the physical and network layers in these processors. We find evidence that distributing computation across chiplets may reduce the overall error rates associated with moving data across the device, despite higher error figures for transfers across links. Preliminary analyses suggest that latency is not substantially impacted, and that at least some applications and architectures may avoid bottlenecks around chiplet boundaries. In the long-term, short-range networks may underlie quantum computers just as local area networks underlie classical datacenters and supercomputers today.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2201.08825v4 Announce Type: replace-cross \nAbstract: A core challenge for superconducting quantum computers is to scale up the number of qubits in each processor without increasing noise or cross-talk. Distributed quantum computing across small qubit arrays, known as chiplets, can address these challenges in a scalable manner. We propose a chiplet architecture over microwave links with potential to exceed monolithic performance on near-term hardware. Our methods of modeling and evaluating the chiplet architecture bridge the physical and network layers in these processors. We find evidence that distributing computation across chiplets may reduce the overall error rates associated with moving data across the device, despite higher error figures for transfers across links. Preliminary analyses suggest that latency is not substantially impacted, and that at least some applications and architectures may avoid bottlenecks around chiplet boundaries. In the long-term, short-range networks may underlie quantum computers just as local area networks underlie classical datacenters and supercomputers today.'}",oai:arXiv.org:2201.08825v4,False,"[{'term': 'quant-ph', 'scheme': None, 'label': None}, {'term': 'cs.AR', 'scheme': None, 'label': None}, {'term': 'cs.DC', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace-cross,http://creativecommons.org/licenses/by-nc-sa/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-sa/4.0/'}","[{'name': 'Nicholas LaRacuente, Kaitlin N. Smith, Poolad Imany, Kevin L. Silverman, Frederic T. Chong'}]","Nicholas LaRacuente, Kaitlin N. Smith, Poolad Imany, Kevin L. Silverman, Frederic T. Chong","{'name': 'Nicholas LaRacuente, Kaitlin N. Smith, Poolad Imany, Kevin L. Silverman, Frederic T. Chong'}",,
749,Fault-tolerant Locating-Dominating Sets on the Infinite King Grid,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Fault-tolerant Locating-Dominating Sets on the Infinite King Grid'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2201.09399'}]",https://arxiv.org/abs/2201.09399,"arXiv:2201.09399v2 Announce Type: replace-cross 
Abstract: Let $G$ be a graph of a network system with vertices, $V(G)$, representing physical locations and edges, $E(G)$, representing informational connectivity. A \emph{locating-dominating (LD)} set $S \subseteq V(G)$ is a subset of vertices representing detectors capable of sensing an ""intruder"" at precisely their location or somewhere in their open-neighborhood -- an LD set must be capable of locating an intruder anywhere in the graph. We explore three types of fault-tolerant LD sets: \emph{redundant LD} sets, which allow a detector to be removed, \emph{error-detecting LD} sets, which allow at most one false negative, and \emph{error-correcting LD} sets, which allow at most one error (false positive or negative). In particular, we determine lower and upper bounds for the minimum density of these three fault-tolerant locating-dominating sets in the \emph{infinite king grid}.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2201.09399v2 Announce Type: replace-cross \nAbstract: Let $G$ be a graph of a network system with vertices, $V(G)$, representing physical locations and edges, $E(G)$, representing informational connectivity. A \\emph{locating-dominating (LD)} set $S \\subseteq V(G)$ is a subset of vertices representing detectors capable of sensing an ""intruder"" at precisely their location or somewhere in their open-neighborhood -- an LD set must be capable of locating an intruder anywhere in the graph. We explore three types of fault-tolerant LD sets: \\emph{redundant LD} sets, which allow a detector to be removed, \\emph{error-detecting LD} sets, which allow at most one false negative, and \\emph{error-correcting LD} sets, which allow at most one error (false positive or negative). In particular, we determine lower and upper bounds for the minimum density of these three fault-tolerant locating-dominating sets in the \\emph{infinite king grid}.'}",oai:arXiv.org:2201.09399v2,False,"[{'term': 'math.CO', 'scheme': None, 'label': None}, {'term': 'cs.DM', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace-cross,http://creativecommons.org/licenses/by-nc-nd/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-nd/4.0/'}","[{'name': 'Devin Jean, Suk Seo'}]","Devin Jean, Suk Seo","{'name': 'Devin Jean, Suk Seo'}",,
750,Holdouts set for safe predictive model updating,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Holdouts set for safe predictive model updating'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2202.06374'}]",https://arxiv.org/abs/2202.06374,"arXiv:2202.06374v5 Announce Type: replace-cross 
Abstract: Predictive risk scores for adverse outcomes are increasingly crucial in guiding health interventions. Such scores may need to be periodically updated due to change in the distributions they model. However, directly updating risk scores used to guide intervention can lead to biased risk estimates. To address this, we propose updating using a `holdout set' - a subset of the population that does not receive interventions guided by the risk score. Balancing the holdout set size is essential to ensure good performance of the updated risk score whilst minimising the number of held out samples. We prove that this approach reduces adverse outcome frequency to an asymptotically optimal level and argue that often there is no competitive alternative. We describe conditions under which an optimal holdout size (OHS) can be readily identified, and introduce parametric and semi-parametric algorithms for OHS estimation. We apply our methods to the ASPRE risk score for pre-eclampsia to recommend a plan for updating it in the presence of change in the underlying data distribution. We show that, in order to minimise the number of pre-eclampsia cases over time, this is best achieved using a holdout set of around 10,000 individuals.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2202.06374v5 Announce Type: replace-cross \nAbstract: Predictive risk scores for adverse outcomes are increasingly crucial in guiding health interventions. Such scores may need to be periodically updated due to change in the distributions they model. However, directly updating risk scores used to guide intervention can lead to biased risk estimates. To address this, we propose updating using a `holdout set' - a subset of the population that does not receive interventions guided by the risk score. Balancing the holdout set size is essential to ensure good performance of the updated risk score whilst minimising the number of held out samples. We prove that this approach reduces adverse outcome frequency to an asymptotically optimal level and argue that often there is no competitive alternative. We describe conditions under which an optimal holdout size (OHS) can be readily identified, and introduce parametric and semi-parametric algorithms for OHS estimation. We apply our methods to the ASPRE risk score for pre-eclampsia to recommend a plan for updating it in the presence of change in the underlying data distribution. We show that, in order to minimise the number of pre-eclampsia cases over time, this is best achieved using a holdout set of around 10,000 individuals.""}",oai:arXiv.org:2202.06374v5,False,"[{'term': 'stat.ML', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'stat.ME', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace-cross,http://creativecommons.org/licenses/by-sa/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-sa/4.0/'}","[{'name': 'Sami Haidar-Wehbe, Samuel R Emerson, Louis J M Aslett, James Liley'}]","Sami Haidar-Wehbe, Samuel R Emerson, Louis J M Aslett, James Liley","{'name': 'Sami Haidar-Wehbe, Samuel R Emerson, Louis J M Aslett, James Liley'}",,
751,Generative Adversarial Networks for Image Super-Resolution: A Survey,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Generative Adversarial Networks for Image Super-Resolution: A Survey'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2204.13620'}]",https://arxiv.org/abs/2204.13620,"arXiv:2204.13620v3 Announce Type: replace-cross 
Abstract: Single image super-resolution (SISR) has played an important role in the field of image processing. Recent generative adversarial networks (GANs) can achieve excellent results on low-resolution images with small samples. However, there are little literatures summarizing different GANs in SISR. In this paper, we conduct a comparative study of GANs from different perspectives. We first take a look at developments of GANs. Second, we present popular architectures for GANs in big and small samples for image applications. Then, we analyze motivations, implementations and differences of GANs based optimization methods and discriminative learning for image super-resolution in terms of supervised, semi-supervised and unsupervised manners, where these GANs are analyzed via integrating different network architectures, prior knowledge, loss functions and multiple tasks. Next, we compare performance of these popular GANs on public datasets via quantitative and qualitative analysis in SISR. Finally, we highlight challenges of GANs and potential research points for SISR.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2204.13620v3 Announce Type: replace-cross \nAbstract: Single image super-resolution (SISR) has played an important role in the field of image processing. Recent generative adversarial networks (GANs) can achieve excellent results on low-resolution images with small samples. However, there are little literatures summarizing different GANs in SISR. In this paper, we conduct a comparative study of GANs from different perspectives. We first take a look at developments of GANs. Second, we present popular architectures for GANs in big and small samples for image applications. Then, we analyze motivations, implementations and differences of GANs based optimization methods and discriminative learning for image super-resolution in terms of supervised, semi-supervised and unsupervised manners, where these GANs are analyzed via integrating different network architectures, prior knowledge, loss functions and multiple tasks. Next, we compare performance of these popular GANs on public datasets via quantitative and qualitative analysis in SISR. Finally, we highlight challenges of GANs and potential research points for SISR.'}",oai:arXiv.org:2204.13620v3,False,"[{'term': 'eess.IV', 'scheme': None, 'label': None}, {'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace-cross,http://creativecommons.org/licenses/by-nc-nd/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-nd/4.0/'}","[{'name': 'Chunwei Tian, Xuanyu Zhang, Qi Zhu, Bob Zhang, Jerry Chun-Wei Lin'}]","Chunwei Tian, Xuanyu Zhang, Qi Zhu, Bob Zhang, Jerry Chun-Wei Lin","{'name': 'Chunwei Tian, Xuanyu Zhang, Qi Zhu, Bob Zhang, Jerry Chun-Wei Lin'}",,
752,Label propagation on binomial random graphs,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Label propagation on binomial random graphs'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2302.03569'}]",https://arxiv.org/abs/2302.03569,"arXiv:2302.03569v4 Announce Type: replace-cross 
Abstract: We study the behavior of a label propagation algorithm (LPA) on the Erd\H{o}s-R\'enyi random graph $\mathcal{G}(n,p)$. Initially, given a network, each vertex starts with a random label in the interval $[0,1]$. Then, in each round of LPA, every vertex switches its label to the majority label in its neighborhood (including its own label). At the first round, ties are broken towards smaller labels, while at each of the next rounds, ties are broken uniformly at random. The algorithm terminates once all labels stay the same in two consecutive iterations. LPA is successfully used in practice for detecting communities in networks (corresponding to vertex sets with the same label after termination of the algorithm). Perhaps surprisingly, LPA's performance on dense random graphs is hard to analyze, and so far convergence to consensus was known only when $np\ge n^{3/4+\varepsilon}$, where LPA converges in three rounds. By defining an alternative label attribution procedure which converges to the label propagation algorithm after three rounds, a careful multi-stage exposure of the edges allows us to break the $n^{3/4+\varepsilon}$ barrier and show that, when $np \ge n^{5/8+\varepsilon}$, a.a.s.\ the algorithm terminates with a single label. Moreover, we show that, if $np\gg n^{2/3}$, a.a.s.\ this label is the smallest one, whereas if $n^{5/8+\varepsilon}\le np\ll n^{2/3}$, the surviving label is a.a.s.\ not the smallest one. En passant, we show a presumably new monotonicity lemma for Binomial random variables that might be of independent interest.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2302.03569v4 Announce Type: replace-cross \nAbstract: We study the behavior of a label propagation algorithm (LPA) on the Erd\\H{o}s-R\\'enyi random graph $\\mathcal{G}(n,p)$. Initially, given a network, each vertex starts with a random label in the interval $[0,1]$. Then, in each round of LPA, every vertex switches its label to the majority label in its neighborhood (including its own label). At the first round, ties are broken towards smaller labels, while at each of the next rounds, ties are broken uniformly at random. The algorithm terminates once all labels stay the same in two consecutive iterations. LPA is successfully used in practice for detecting communities in networks (corresponding to vertex sets with the same label after termination of the algorithm). Perhaps surprisingly, LPA's performance on dense random graphs is hard to analyze, and so far convergence to consensus was known only when $np\\ge n^{3/4+\\varepsilon}$, where LPA converges in three rounds. By defining an alternative label attribution procedure which converges to the label propagation algorithm after three rounds, a careful multi-stage exposure of the edges allows us to break the $n^{3/4+\\varepsilon}$ barrier and show that, when $np \\ge n^{5/8+\\varepsilon}$, a.a.s.\\ the algorithm terminates with a single label. Moreover, we show that, if $np\\gg n^{2/3}$, a.a.s.\\ this label is the smallest one, whereas if $n^{5/8+\\varepsilon}\\le np\\ll n^{2/3}$, the surviving label is a.a.s.\\ not the smallest one. En passant, we show a presumably new monotonicity lemma for Binomial random variables that might be of independent interest.""}",oai:arXiv.org:2302.03569v4,False,"[{'term': 'math.PR', 'scheme': None, 'label': None}, {'term': 'cs.CC', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace-cross,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Marcos Kiwi, Lyuben Lichev, Dieter Mitsche, Pawe{\\l} Pra{\\l}at'}]","Marcos Kiwi, Lyuben Lichev, Dieter Mitsche, Pawe{\l} Pra{\l}at","{'name': 'Marcos Kiwi, Lyuben Lichev, Dieter Mitsche, Pawe{\\l} Pra{\\l}at'}",,
753,Mixed Semi-Supervised Generalized-Linear-Regression with Applications to Deep-Learning and Interpolators,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Mixed Semi-Supervised Generalized-Linear-Regression with Applications to Deep-Learning and Interpolators'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2302.09526'}]",https://arxiv.org/abs/2302.09526,"arXiv:2302.09526v4 Announce Type: replace-cross 
Abstract: We present a methodology for using unlabeled data to design semi supervised learning (SSL) methods that improve the prediction performance of supervised learning for regression tasks. The main idea is to design different mechanisms for integrating the unlabeled data, and include in each of them a mixing parameter $\alpha$, controlling the weight given to the unlabeled data. Focusing on Generalized Linear Models (GLM) and linear interpolators classes of models, we analyze the characteristics of different mixing mechanisms, and prove that in all cases, it is invariably beneficial to integrate the unlabeled data with some nonzero mixing ratio $\alpha>0$, in terms of predictive performance. Moreover, we provide a rigorous framework to estimate the best mixing ratio $\alpha^*$ where mixed SSL delivers the best predictive performance, while using the labeled and unlabeled data on hand.
  The effectiveness of our methodology in delivering substantial improvement compared to the standard supervised models, in a variety of settings, is demonstrated empirically through extensive simulation, in a manner that supports the theoretical analysis. We also demonstrate the applicability of our methodology (with some intuitive modifications) to improve more complex models, such as deep neural networks, in real-world regression tasks.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2302.09526v4 Announce Type: replace-cross \nAbstract: We present a methodology for using unlabeled data to design semi supervised learning (SSL) methods that improve the prediction performance of supervised learning for regression tasks. The main idea is to design different mechanisms for integrating the unlabeled data, and include in each of them a mixing parameter $\\alpha$, controlling the weight given to the unlabeled data. Focusing on Generalized Linear Models (GLM) and linear interpolators classes of models, we analyze the characteristics of different mixing mechanisms, and prove that in all cases, it is invariably beneficial to integrate the unlabeled data with some nonzero mixing ratio $\\alpha>0$, in terms of predictive performance. Moreover, we provide a rigorous framework to estimate the best mixing ratio $\\alpha^*$ where mixed SSL delivers the best predictive performance, while using the labeled and unlabeled data on hand.\n  The effectiveness of our methodology in delivering substantial improvement compared to the standard supervised models, in a variety of settings, is demonstrated empirically through extensive simulation, in a manner that supports the theoretical analysis. We also demonstrate the applicability of our methodology (with some intuitive modifications) to improve more complex models, such as deep neural networks, in real-world regression tasks.'}",oai:arXiv.org:2302.09526v4,False,"[{'term': 'stat.ME', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'stat.ML', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace-cross,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Oren Yuval, Saharon Rosset'}]","Oren Yuval, Saharon Rosset","{'name': 'Oren Yuval, Saharon Rosset'}",,
754,Toward Falsifying Causal Graphs Using a Permutation-Based Test,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Toward Falsifying Causal Graphs Using a Permutation-Based Test'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2305.09565'}]",https://arxiv.org/abs/2305.09565,"arXiv:2305.09565v2 Announce Type: replace-cross 
Abstract: Understanding causal relationships among the variables of a system is paramount to explain and control its behavior. For many real-world systems, however, the true causal graph is not readily available and one must resort to predictions made by algorithms or domain experts. Therefore, metrics that quantitatively assess the goodness of a causal graph provide helpful checks before using it in downstream tasks. Existing metrics provide an $\textit{absolute}$ number of inconsistencies between the graph and the observed data, and without a baseline, practitioners are left to answer the hard question of how many such inconsistencies are acceptable or expected. Here, we propose a novel consistency metric by constructing a baseline through node permutations. By comparing the number of inconsistencies with those on the baseline, we derive an interpretable metric that captures whether the graph is significantly better than random. Evaluating on both simulated and real data sets from various domains, including biology and cloud monitoring, we demonstrate that the true graph is not falsified by our metric, whereas the wrong graphs given by a hypothetical user are likely to be falsified.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2305.09565v2 Announce Type: replace-cross \nAbstract: Understanding causal relationships among the variables of a system is paramount to explain and control its behavior. For many real-world systems, however, the true causal graph is not readily available and one must resort to predictions made by algorithms or domain experts. Therefore, metrics that quantitatively assess the goodness of a causal graph provide helpful checks before using it in downstream tasks. Existing metrics provide an $\\textit{absolute}$ number of inconsistencies between the graph and the observed data, and without a baseline, practitioners are left to answer the hard question of how many such inconsistencies are acceptable or expected. Here, we propose a novel consistency metric by constructing a baseline through node permutations. By comparing the number of inconsistencies with those on the baseline, we derive an interpretable metric that captures whether the graph is significantly better than random. Evaluating on both simulated and real data sets from various domains, including biology and cloud monitoring, we demonstrate that the true graph is not falsified by our metric, whereas the wrong graphs given by a hypothetical user are likely to be falsified.'}",oai:arXiv.org:2305.09565v2,False,"[{'term': 'stat.ML', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace-cross,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Elias Eulig, Atalanti A. Mastakouri, Patrick Bl\\""obaum, Michaela Hardt, Dominik Janzing'}]","Elias Eulig, Atalanti A. Mastakouri, Patrick Bl\""obaum, Michaela Hardt, Dominik Janzing","{'name': 'Elias Eulig, Atalanti A. Mastakouri, Patrick Bl\\""obaum, Michaela Hardt, Dominik Janzing'}",,
755,PUGAN: Physical Model-Guided Underwater Image Enhancement Using GAN with Dual-Discriminators,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'PUGAN: Physical Model-Guided Underwater Image Enhancement Using GAN with Dual-Discriminators'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2306.08918'}]",https://arxiv.org/abs/2306.08918,"arXiv:2306.08918v2 Announce Type: replace-cross 
Abstract: Due to the light absorption and scattering induced by the water medium, underwater images usually suffer from some degradation problems, such as low contrast, color distortion, and blurring details, which aggravate the difficulty of downstream underwater understanding tasks. Therefore, how to obtain clear and visually pleasant images has become a common concern of people, and the task of underwater image enhancement (UIE) has also emerged as the times require. Among existing UIE methods, Generative Adversarial Networks (GANs) based methods perform well in visual aesthetics, while the physical model-based methods have better scene adaptability. Inheriting the advantages of the above two types of models, we propose a physical model-guided GAN model for UIE in this paper, referred to as PUGAN. The entire network is under the GAN architecture. On the one hand, we design a Parameters Estimation subnetwork (Par-subnet) to learn the parameters for physical model inversion, and use the generated color enhancement image as auxiliary information for the Two-Stream Interaction Enhancement sub-network (TSIE-subnet). Meanwhile, we design a Degradation Quantization (DQ) module in TSIE-subnet to quantize scene degradation, thereby achieving reinforcing enhancement of key regions. On the other hand, we design the Dual-Discriminators for the style-content adversarial constraint, promoting the authenticity and visual aesthetics of the results. Extensive experiments on three benchmark datasets demonstrate that our PUGAN outperforms state-of-the-art methods in both qualitative and quantitative metrics.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2306.08918v2 Announce Type: replace-cross \nAbstract: Due to the light absorption and scattering induced by the water medium, underwater images usually suffer from some degradation problems, such as low contrast, color distortion, and blurring details, which aggravate the difficulty of downstream underwater understanding tasks. Therefore, how to obtain clear and visually pleasant images has become a common concern of people, and the task of underwater image enhancement (UIE) has also emerged as the times require. Among existing UIE methods, Generative Adversarial Networks (GANs) based methods perform well in visual aesthetics, while the physical model-based methods have better scene adaptability. Inheriting the advantages of the above two types of models, we propose a physical model-guided GAN model for UIE in this paper, referred to as PUGAN. The entire network is under the GAN architecture. On the one hand, we design a Parameters Estimation subnetwork (Par-subnet) to learn the parameters for physical model inversion, and use the generated color enhancement image as auxiliary information for the Two-Stream Interaction Enhancement sub-network (TSIE-subnet). Meanwhile, we design a Degradation Quantization (DQ) module in TSIE-subnet to quantize scene degradation, thereby achieving reinforcing enhancement of key regions. On the other hand, we design the Dual-Discriminators for the style-content adversarial constraint, promoting the authenticity and visual aesthetics of the results. Extensive experiments on three benchmark datasets demonstrate that our PUGAN outperforms state-of-the-art methods in both qualitative and quantitative metrics.'}",oai:arXiv.org:2306.08918v2,False,"[{'term': 'eess.IV', 'scheme': None, 'label': None}, {'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace-cross,http://creativecommons.org/licenses/by-nc-sa/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-sa/4.0/'}","[{'name': 'Runmin Cong, Wenyu Yang, Wei Zhang, Chongyi Li, Chun-Le Guo, Qingming Huang, Sam Kwong'}]","Runmin Cong, Wenyu Yang, Wei Zhang, Chongyi Li, Chun-Le Guo, Qingming Huang, Sam Kwong","{'name': 'Runmin Cong, Wenyu Yang, Wei Zhang, Chongyi Li, Chun-Le Guo, Qingming Huang, Sam Kwong'}",,
756,Carbon-Aware Optimal Power Flow,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Carbon-Aware Optimal Power Flow'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2308.03240'}]",https://arxiv.org/abs/2308.03240,"arXiv:2308.03240v3 Announce Type: replace-cross 
Abstract: To facilitate effective decarbonization of the electric power sector, this paper introduces the generic Carbon-aware Optimal Power Flow (C-OPF) method for power system decision-making that considers demand-side carbon accounting and emission management. Built upon the classic optimal power flow (OPF) model, the C-OPF method incorporates carbon emission flow equations and constraints, as well as carbon-related objectives, to jointly optimize power flow and carbon flow. In particular, this paper establishes the feasibility and solution uniqueness of the carbon emission flow equations, and proposes modeling and linearization techniques to address the issues of undetermined power flow directions and bilinear terms in the C-OPF model. Additionally, two novel carbon emission models, together with the carbon accounting schemes, for energy storage systems are developed and integrated into the C-OPF model. Numerical simulations demonstrate the characteristics and effectiveness of the C-OPF method, in comparison with OPF solutions.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2308.03240v3 Announce Type: replace-cross \nAbstract: To facilitate effective decarbonization of the electric power sector, this paper introduces the generic Carbon-aware Optimal Power Flow (C-OPF) method for power system decision-making that considers demand-side carbon accounting and emission management. Built upon the classic optimal power flow (OPF) model, the C-OPF method incorporates carbon emission flow equations and constraints, as well as carbon-related objectives, to jointly optimize power flow and carbon flow. In particular, this paper establishes the feasibility and solution uniqueness of the carbon emission flow equations, and proposes modeling and linearization techniques to address the issues of undetermined power flow directions and bilinear terms in the C-OPF model. Additionally, two novel carbon emission models, together with the carbon accounting schemes, for energy storage systems are developed and integrated into the C-OPF model. Numerical simulations demonstrate the characteristics and effectiveness of the C-OPF method, in comparison with OPF solutions.'}",oai:arXiv.org:2308.03240v3,False,"[{'term': 'math.OC', 'scheme': None, 'label': None}, {'term': 'cs.SY', 'scheme': None, 'label': None}, {'term': 'eess.SY', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace-cross,http://creativecommons.org/licenses/by-nc-nd/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-nd/4.0/'}","[{'name': 'Xin Chen, Andy Sun, Wenbo Shi, Na Li'}]","Xin Chen, Andy Sun, Wenbo Shi, Na Li","{'name': 'Xin Chen, Andy Sun, Wenbo Shi, Na Li'}",,
757,InstructMol: Multi-Modal Integration for Building a Versatile and Reliable Molecular Assistant in Drug Discovery,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'InstructMol: Multi-Modal Integration for Building a Versatile and Reliable Molecular Assistant in Drug Discovery'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2311.16208'}]",https://arxiv.org/abs/2311.16208,"arXiv:2311.16208v2 Announce Type: replace-cross 
Abstract: The rapid evolution of artificial intelligence in drug discovery encounters challenges with generalization and extensive training, yet Large Language Models (LLMs) offer promise in reshaping interactions with complex molecular data. Our novel contribution, InstructMol, a multi-modal LLM, effectively aligns molecular structures with natural language via an instruction-tuning approach, utilizing a two-stage training strategy that adeptly combines limited domain-specific data with molecular and textual information. InstructMol showcases substantial performance improvements in drug discovery-related molecular tasks, surpassing leading LLMs and significantly reducing the gap with specialized models, thereby establishing a robust foundation for a versatile and dependable drug discovery assistant.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2311.16208v2 Announce Type: replace-cross \nAbstract: The rapid evolution of artificial intelligence in drug discovery encounters challenges with generalization and extensive training, yet Large Language Models (LLMs) offer promise in reshaping interactions with complex molecular data. Our novel contribution, InstructMol, a multi-modal LLM, effectively aligns molecular structures with natural language via an instruction-tuning approach, utilizing a two-stage training strategy that adeptly combines limited domain-specific data with molecular and textual information. InstructMol showcases substantial performance improvements in drug discovery-related molecular tasks, surpassing leading LLMs and significantly reducing the gap with specialized models, thereby establishing a robust foundation for a versatile and dependable drug discovery assistant.'}",oai:arXiv.org:2311.16208v2,False,"[{'term': 'q-bio.BM', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace-cross,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'He Cao, Zijing Liu, Xingyu Lu, Yuan Yao, Yu Li'}]","He Cao, Zijing Liu, Xingyu Lu, Yuan Yao, Yu Li","{'name': 'He Cao, Zijing Liu, Xingyu Lu, Yuan Yao, Yu Li'}",,
758,Risk-Aware Control of Discrete-Time Stochastic Systems: Integrating Kalman Filter and Worst-case CVaR in Control Barrier Functions,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Risk-Aware Control of Discrete-Time Stochastic Systems: Integrating Kalman Filter and Worst-case CVaR in Control Barrier Functions'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2312.15638'}]",https://arxiv.org/abs/2312.15638,"arXiv:2312.15638v4 Announce Type: replace-cross 
Abstract: This paper proposes control approaches for discrete-time linear systems subject to stochastic disturbances. It employs Kalman filter to estimate the mean and covariance of the state propagation, and the worst-case conditional value-at-risk (CVaR) to quantify the tail risk using the estimated mean and covariance. The quantified risk is then integrated into a control barrier function (CBF) to derive constraints for controller synthesis, addressing tail risks near safe set boundaries. Two optimization-based control methods are presented using the obtained constraints for half-space and ellipsoidal safe sets, respectively. The effectiveness of the obtained results is demonstrated using numerical simulations.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2312.15638v4 Announce Type: replace-cross \nAbstract: This paper proposes control approaches for discrete-time linear systems subject to stochastic disturbances. It employs Kalman filter to estimate the mean and covariance of the state propagation, and the worst-case conditional value-at-risk (CVaR) to quantify the tail risk using the estimated mean and covariance. The quantified risk is then integrated into a control barrier function (CBF) to derive constraints for controller synthesis, addressing tail risks near safe set boundaries. Two optimization-based control methods are presented using the obtained constraints for half-space and ellipsoidal safe sets, respectively. The effectiveness of the obtained results is demonstrated using numerical simulations.'}",oai:arXiv.org:2312.15638v4,False,"[{'term': 'math.OC', 'scheme': None, 'label': None}, {'term': 'cs.SY', 'scheme': None, 'label': None}, {'term': 'eess.SY', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace-cross,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}",[{'name': 'Masako Kishida'}],Masako Kishida,{'name': 'Masako Kishida'},,
759,Decentralized real-time iterations for distributed NMPC,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Decentralized real-time iterations for distributed NMPC'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2401.14898'}]",https://arxiv.org/abs/2401.14898,"arXiv:2401.14898v2 Announce Type: replace-cross 
Abstract: This article presents a Real-Time Iteration (RTI) scheme for distributed Nonlinear Model Predictive Control (NMPC). The scheme transfers the well-known RTI approach, a key enabler for many industrial real-time NMPC implementations, to the setting of cooperative distributed control. At each sampling instant, one outer iteration of a bi-level decentralized Sequential Quadratic Programming (dSQP) method is applied to a centralized optimal control problem. This ensures that real-time requirements are met and it facilitates cooperation between subsystems. Combining novel dSQP convergence results with RTI stability guarantees, we prove local exponential stability under standard assumptions on the MPC design with and without terminal constraints. The proposed scheme only requires neighbor-to-neighbor communication and avoids a central coordinator. A numerical example with coupled inverted pendulums demonstrates the efficacy of the approach.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2401.14898v2 Announce Type: replace-cross \nAbstract: This article presents a Real-Time Iteration (RTI) scheme for distributed Nonlinear Model Predictive Control (NMPC). The scheme transfers the well-known RTI approach, a key enabler for many industrial real-time NMPC implementations, to the setting of cooperative distributed control. At each sampling instant, one outer iteration of a bi-level decentralized Sequential Quadratic Programming (dSQP) method is applied to a centralized optimal control problem. This ensures that real-time requirements are met and it facilitates cooperation between subsystems. Combining novel dSQP convergence results with RTI stability guarantees, we prove local exponential stability under standard assumptions on the MPC design with and without terminal constraints. The proposed scheme only requires neighbor-to-neighbor communication and avoids a central coordinator. A numerical example with coupled inverted pendulums demonstrates the efficacy of the approach.'}",oai:arXiv.org:2401.14898v2,False,"[{'term': 'math.OC', 'scheme': None, 'label': None}, {'term': 'cs.SY', 'scheme': None, 'label': None}, {'term': 'eess.SY', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace-cross,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'G\\""osta Stomberg, Alexander Engelmann, Moritz Diehl, Timm Faulwasser'}]","G\""osta Stomberg, Alexander Engelmann, Moritz Diehl, Timm Faulwasser","{'name': 'G\\""osta Stomberg, Alexander Engelmann, Moritz Diehl, Timm Faulwasser'}",,
760,Arbitrary Polynomial Separations in Trainable Quantum Machine Learning,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Arbitrary Polynomial Separations in Trainable Quantum Machine Learning'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2402.08606'}]",https://arxiv.org/abs/2402.08606,"arXiv:2402.08606v2 Announce Type: replace-cross 
Abstract: Recent theoretical results in quantum machine learning have demonstrated a general trade-off between the expressive power of quantum neural networks (QNNs) and their trainability; as a corollary of these results, practical exponential separations in expressive power over classical machine learning models are believed to be infeasible as such QNNs take a time to train that is exponential in the model size. We here circumvent these negative results by constructing a hierarchy of efficiently trainable QNNs that exhibit unconditionally provable, polynomial memory separations of arbitrary constant degree over classical neural networks -- including state-of-the-art models, such as Transformers -- in performing a classical sequence modeling task. This construction is also computationally efficient, as each unit cell of the introduced class of QNNs only has constant gate complexity. We show that contextuality -- informally, a quantitative notion of semantic ambiguity -- is the source of the expressivity separation, suggesting that other learning tasks with this property may be a natural setting for the use of quantum learning algorithms.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2402.08606v2 Announce Type: replace-cross \nAbstract: Recent theoretical results in quantum machine learning have demonstrated a general trade-off between the expressive power of quantum neural networks (QNNs) and their trainability; as a corollary of these results, practical exponential separations in expressive power over classical machine learning models are believed to be infeasible as such QNNs take a time to train that is exponential in the model size. We here circumvent these negative results by constructing a hierarchy of efficiently trainable QNNs that exhibit unconditionally provable, polynomial memory separations of arbitrary constant degree over classical neural networks -- including state-of-the-art models, such as Transformers -- in performing a classical sequence modeling task. This construction is also computationally efficient, as each unit cell of the introduced class of QNNs only has constant gate complexity. We show that contextuality -- informally, a quantitative notion of semantic ambiguity -- is the source of the expressivity separation, suggesting that other learning tasks with this property may be a natural setting for the use of quantum learning algorithms.'}",oai:arXiv.org:2402.08606v2,False,"[{'term': 'quant-ph', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace-cross,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Eric R. Anschuetz, Xun Gao'}]","Eric R. Anschuetz, Xun Gao","{'name': 'Eric R. Anschuetz, Xun Gao'}",,
761,CAP: A General Algorithm for Online Selective Conformal Prediction with FCR Control,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'CAP: A General Algorithm for Online Selective Conformal Prediction with FCR Control'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2403.07728'}]",https://arxiv.org/abs/2403.07728,"arXiv:2403.07728v3 Announce Type: replace-cross 
Abstract: We study the problem of post-selection predictive inference in an online fashion. To avoid devoting resources to unimportant units, a preliminary selection of the current individual before reporting its prediction interval is common and meaningful in online predictive tasks. Since the online selection causes a temporal multiplicity in the selected prediction intervals, it is important to control the real-time false coverage-statement rate (FCR) which measures the overall miscoverage level. We develop a general framework named CAP (Calibration after Adaptive Pick) that performs an adaptive pick rule on historical data to construct a calibration set if the current individual is selected and then outputs a conformal prediction interval for the unobserved label. We provide tractable procedures for constructing the calibration set for popular online selection rules. We proved that CAP can achieve an exact selection-conditional coverage guarantee in the finite-sample and distribution-free regimes. To account for the distribution shift in online data, we also embed CAP into some recent dynamic conformal prediction algorithms and show that the proposed method can deliver long-run FCR control. Numerical results on both synthetic and real data corroborate that CAP can effectively control FCR around the target level and yield more narrowed prediction intervals over existing baselines across various settings.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2403.07728v3 Announce Type: replace-cross \nAbstract: We study the problem of post-selection predictive inference in an online fashion. To avoid devoting resources to unimportant units, a preliminary selection of the current individual before reporting its prediction interval is common and meaningful in online predictive tasks. Since the online selection causes a temporal multiplicity in the selected prediction intervals, it is important to control the real-time false coverage-statement rate (FCR) which measures the overall miscoverage level. We develop a general framework named CAP (Calibration after Adaptive Pick) that performs an adaptive pick rule on historical data to construct a calibration set if the current individual is selected and then outputs a conformal prediction interval for the unobserved label. We provide tractable procedures for constructing the calibration set for popular online selection rules. We proved that CAP can achieve an exact selection-conditional coverage guarantee in the finite-sample and distribution-free regimes. To account for the distribution shift in online data, we also embed CAP into some recent dynamic conformal prediction algorithms and show that the proposed method can deliver long-run FCR control. Numerical results on both synthetic and real data corroborate that CAP can effectively control FCR around the target level and yield more narrowed prediction intervals over existing baselines across various settings.'}",oai:arXiv.org:2403.07728v3,False,"[{'term': 'stat.ML', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'stat.ME', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace-cross,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Yajie Bao, Yuyang Huo, Haojie Ren, Changliang Zou'}]","Yajie Bao, Yuyang Huo, Haojie Ren, Changliang Zou","{'name': 'Yajie Bao, Yuyang Huo, Haojie Ren, Changliang Zou'}",,
762,Image Classification with Rotation-Invariant Variational Quantum Circuits,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Image Classification with Rotation-Invariant Variational Quantum Circuits'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2403.15031'}]",https://arxiv.org/abs/2403.15031,"arXiv:2403.15031v2 Announce Type: replace-cross 
Abstract: Variational quantum algorithms are gaining attention as an early application of Noisy Intermediate-Scale Quantum (NISQ) devices. One of the main problems of variational methods lies in the phenomenon of Barren Plateaus, present in the optimization of variational parameters. Adding geometric inductive bias to the quantum models has been proposed as a potential solution to mitigate this problem, leading to a new field called Geometric Quantum Machine Learning. In this work, an equivariant architecture for variational quantum classifiers is introduced to create a label-invariant model for image classification with $C_4$ rotational label symmetry. The equivariant circuit is benchmarked against two different architectures, and it is experimentally observed that the geometric approach boosts the model's performance. Finally, a classical equivariant convolution operation is proposed to extend the quantum model for the processing of larger images, employing the resources available in NISQ devices.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2403.15031v2 Announce Type: replace-cross \nAbstract: Variational quantum algorithms are gaining attention as an early application of Noisy Intermediate-Scale Quantum (NISQ) devices. One of the main problems of variational methods lies in the phenomenon of Barren Plateaus, present in the optimization of variational parameters. Adding geometric inductive bias to the quantum models has been proposed as a potential solution to mitigate this problem, leading to a new field called Geometric Quantum Machine Learning. In this work, an equivariant architecture for variational quantum classifiers is introduced to create a label-invariant model for image classification with $C_4$ rotational label symmetry. The equivariant circuit is benchmarked against two different architectures, and it is experimentally observed that the geometric approach boosts the model's performance. Finally, a classical equivariant convolution operation is proposed to extend the quantum model for the processing of larger images, employing the resources available in NISQ devices.""}",oai:arXiv.org:2403.15031v2,False,"[{'term': 'quant-ph', 'scheme': None, 'label': None}, {'term': 'cs.CV', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace-cross,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': ""Paul San Sebastian, Mikel Ca\\~nizo, Rom\\'an Or\\'us""}]","Paul San Sebastian, Mikel Ca\~nizo, Rom\'an Or\'us","{'name': ""Paul San Sebastian, Mikel Ca\\~nizo, Rom\\'an Or\\'us""}",,
763,Covering convection with thermal blankets: fluid-structure interactions in thermal convection,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Covering convection with thermal blankets: fluid-structure interactions in thermal convection'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2404.01172'}]",https://arxiv.org/abs/2404.01172,"arXiv:2404.01172v3 Announce Type: replace-cross 
Abstract: The continental plates of Earth are known to drift over a geophysical timescale, and their interactions have lead to some of the most spectacular geoformations of our planet while also causing natural disasters such as earthquakes and volcanic activity. Understanding the dynamics of interacting continental plates is thus significant. In this work, we present a fluid mechanical investigation of the plate motion, interaction, and dynamics. Through numerical experiments, we examine the coupling between a convective fluid and plates floating on top of it. With physical modeling, we show the coupling is both mechanical and thermal, leading to the thermal blanket effect: the floating plate is not only transported by the fluid flow beneath, it also prevents the heat from leaving the fluid, leading to a convective flow that further affects the plate motion. By adding several plates to such a coupled fluid-structure interaction, we also investigate how floating plates interact with each other and show that, under proper conditions, small plates can converge into a supercontinent.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2404.01172v3 Announce Type: replace-cross \nAbstract: The continental plates of Earth are known to drift over a geophysical timescale, and their interactions have lead to some of the most spectacular geoformations of our planet while also causing natural disasters such as earthquakes and volcanic activity. Understanding the dynamics of interacting continental plates is thus significant. In this work, we present a fluid mechanical investigation of the plate motion, interaction, and dynamics. Through numerical experiments, we examine the coupling between a convective fluid and plates floating on top of it. With physical modeling, we show the coupling is both mechanical and thermal, leading to the thermal blanket effect: the floating plate is not only transported by the fluid flow beneath, it also prevents the heat from leaving the fluid, leading to a convective flow that further affects the plate motion. By adding several plates to such a coupled fluid-structure interaction, we also investigate how floating plates interact with each other and show that, under proper conditions, small plates can converge into a supercontinent.'}",oai:arXiv.org:2404.01172v3,False,"[{'term': 'physics.flu-dyn', 'scheme': None, 'label': None}, {'term': 'cs.NA', 'scheme': None, 'label': None}, {'term': 'math.NA', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace-cross,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}",[{'name': 'Jinzi Mac Huang'}],Jinzi Mac Huang,{'name': 'Jinzi Mac Huang'},,
764,Feature selection in linear SVMs via a hard cardinality constraint: a scalable SDP decomposition approach,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Feature selection in linear SVMs via a hard cardinality constraint: a scalable SDP decomposition approach'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2404.10099'}]",https://arxiv.org/abs/2404.10099,"arXiv:2404.10099v2 Announce Type: replace-cross 
Abstract: In this paper, we study the embedded feature selection problem in linear Support Vector Machines (SVMs), in which a cardinality constraint is employed, leading to an interpretable classification model. The problem is NP-hard due to the presence of the cardinality constraint, even though the original linear SVM amounts to a problem solvable in polynomial time. To handle the hard problem, we first introduce two mixed-integer formulations for which novel semidefinite relaxations are proposed. Exploiting the sparsity pattern of the relaxations, we decompose the problems and obtain equivalent relaxations in a much smaller cone, making the conic approaches scalable. To make the best usage of the decomposed relaxations, we propose heuristics using the information of its optimal solution. Moreover, an exact procedure is proposed by solving a sequence of mixed-integer decomposed semidefinite optimization problems. Numerical results on classical benchmarking datasets are reported, showing the efficiency and effectiveness of our approach.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2404.10099v2 Announce Type: replace-cross \nAbstract: In this paper, we study the embedded feature selection problem in linear Support Vector Machines (SVMs), in which a cardinality constraint is employed, leading to an interpretable classification model. The problem is NP-hard due to the presence of the cardinality constraint, even though the original linear SVM amounts to a problem solvable in polynomial time. To handle the hard problem, we first introduce two mixed-integer formulations for which novel semidefinite relaxations are proposed. Exploiting the sparsity pattern of the relaxations, we decompose the problems and obtain equivalent relaxations in a much smaller cone, making the conic approaches scalable. To make the best usage of the decomposed relaxations, we propose heuristics using the information of its optimal solution. Moreover, an exact procedure is proposed by solving a sequence of mixed-integer decomposed semidefinite optimization problems. Numerical results on classical benchmarking datasets are reported, showing the efficiency and effectiveness of our approach.'}",oai:arXiv.org:2404.10099v2,False,"[{'term': 'math.OC', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace-cross,http://creativecommons.org/licenses/by-nc-nd/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by-nc-nd/4.0/'}","[{'name': ""Immanuel Bomze, Federico D'Onofrio, Laura Palagi, Bo Peng""}]","Immanuel Bomze, Federico D'Onofrio, Laura Palagi, Bo Peng","{'name': ""Immanuel Bomze, Federico D'Onofrio, Laura Palagi, Bo Peng""}",,
765,Online Policy Learning and Inference by Matrix Completion,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Online Policy Learning and Inference by Matrix Completion'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2404.17398'}]",https://arxiv.org/abs/2404.17398,"arXiv:2404.17398v2 Announce Type: replace-cross 
Abstract: Is it possible to make online decisions when personalized covariates are unavailable? We take a collaborative-filtering approach for decision-making based on collective preferences. By assuming low-dimensional latent features, we formulate the covariate-free decision-making problem as a matrix completion bandit. We propose a policy learning procedure that combines an $\varepsilon$-greedy policy for decision-making with an online gradient descent algorithm for bandit parameter estimation. Our novel two-phase design balances policy learning accuracy and regret performance. For policy inference, we develop an online debiasing method based on inverse propensity weighting and establish its asymptotic normality. Our methods are applied to data from the San Francisco parking pricing project, revealing intriguing discoveries and outperforming the benchmark policy.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2404.17398v2 Announce Type: replace-cross \nAbstract: Is it possible to make online decisions when personalized covariates are unavailable? We take a collaborative-filtering approach for decision-making based on collective preferences. By assuming low-dimensional latent features, we formulate the covariate-free decision-making problem as a matrix completion bandit. We propose a policy learning procedure that combines an $\\varepsilon$-greedy policy for decision-making with an online gradient descent algorithm for bandit parameter estimation. Our novel two-phase design balances policy learning accuracy and regret performance. For policy inference, we develop an online debiasing method based on inverse propensity weighting and establish its asymptotic normality. Our methods are applied to data from the San Francisco parking pricing project, revealing intriguing discoveries and outperforming the benchmark policy.'}",oai:arXiv.org:2404.17398v2,False,"[{'term': 'stat.ML', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace-cross,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Congyuan Duan, Jingyang Li, Dong Xia'}]","Congyuan Duan, Jingyang Li, Dong Xia","{'name': 'Congyuan Duan, Jingyang Li, Dong Xia'}",,
766,Iterative Methods for Full-Scale Gaussian Process Approximations for Large Spatial Data,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Iterative Methods for Full-Scale Gaussian Process Approximations for Large Spatial Data'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2405.14492'}]",https://arxiv.org/abs/2405.14492,"arXiv:2405.14492v2 Announce Type: replace-cross 
Abstract: Gaussian processes are flexible probabilistic regression models which are widely used in statistics and machine learning. However, a drawback is their limited scalability to large data sets. To alleviate this, we consider full-scale approximations (FSAs) that combine predictive process methods and covariance tapering, thus approximating both global and local structures. We show how iterative methods can be used to reduce the computational costs for calculating likelihoods, gradients, and predictive distributions with FSAs. We introduce a novel preconditioner and show that it accelerates the conjugate gradient method's convergence speed and mitigates its sensitivity with respect to the FSA parameters and the eigenvalue structure of the original covariance matrix, and we demonstrate empirically that it outperforms a state-of-the-art pivoted Cholesky preconditioner. Further, we present a novel, accurate, and fast way to calculate predictive variances relying on stochastic estimations and iterative methods. In both simulated and real-world data experiments, we find that our proposed methodology achieves the same accuracy as Cholesky-based computations with a substantial reduction in computational time. Finally, we also compare different approaches for determining inducing points in predictive process and FSA models. All methods are implemented in a free C++ software library with high-level Python and R packages.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2405.14492v2 Announce Type: replace-cross \nAbstract: Gaussian processes are flexible probabilistic regression models which are widely used in statistics and machine learning. However, a drawback is their limited scalability to large data sets. To alleviate this, we consider full-scale approximations (FSAs) that combine predictive process methods and covariance tapering, thus approximating both global and local structures. We show how iterative methods can be used to reduce the computational costs for calculating likelihoods, gradients, and predictive distributions with FSAs. We introduce a novel preconditioner and show that it accelerates the conjugate gradient method's convergence speed and mitigates its sensitivity with respect to the FSA parameters and the eigenvalue structure of the original covariance matrix, and we demonstrate empirically that it outperforms a state-of-the-art pivoted Cholesky preconditioner. Further, we present a novel, accurate, and fast way to calculate predictive variances relying on stochastic estimations and iterative methods. In both simulated and real-world data experiments, we find that our proposed methodology achieves the same accuracy as Cholesky-based computations with a substantial reduction in computational time. Finally, we also compare different approaches for determining inducing points in predictive process and FSA models. All methods are implemented in a free C++ software library with high-level Python and R packages.""}",oai:arXiv.org:2405.14492v2,False,"[{'term': 'stat.ME', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'stat.ML', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace-cross,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Tim Gyger, Reinhard Furrer, Fabio Sigrist'}]","Tim Gyger, Reinhard Furrer, Fabio Sigrist","{'name': 'Tim Gyger, Reinhard Furrer, Fabio Sigrist'}",,
767,Multipartite Entanglement Routing as a Hypergraph Immersion Problem,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Multipartite Entanglement Routing as a Hypergraph Immersion Problem'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2406.13452'}]",https://arxiv.org/abs/2406.13452,"arXiv:2406.13452v2 Announce Type: replace-cross 
Abstract: Multipartite entanglement, linking multiple nodes simultaneously, is a higher-order correlation that offers advantages over pairwise connections in quantum networks (QNs). Creating reliable, large-scale multipartite entanglement requires entanglement routing, a process that combines local, short-distance connections into a long-distance connection, which can be considered as a transformation of network topology. Here, we address the question of whether a QN can be topologically transformed into another via entanglement routing. Our key result is an exact mapping from multipartite entanglement routing to Nash-Williams's graph immersion problem, extended to hypergraphs. This generalized hypergraph immersion problem introduces a partial order between QN topologies, permitting certain topological transformations while precluding others, offering discerning insights into the design and manipulation of higher-order network topologies in QNs.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2406.13452v2 Announce Type: replace-cross \nAbstract: Multipartite entanglement, linking multiple nodes simultaneously, is a higher-order correlation that offers advantages over pairwise connections in quantum networks (QNs). Creating reliable, large-scale multipartite entanglement requires entanglement routing, a process that combines local, short-distance connections into a long-distance connection, which can be considered as a transformation of network topology. Here, we address the question of whether a QN can be topologically transformed into another via entanglement routing. Our key result is an exact mapping from multipartite entanglement routing to Nash-Williams's graph immersion problem, extended to hypergraphs. This generalized hypergraph immersion problem introduces a partial order between QN topologies, permitting certain topological transformations while precluding others, offering discerning insights into the design and manipulation of higher-order network topologies in QNs.""}",oai:arXiv.org:2406.13452v2,False,"[{'term': 'quant-ph', 'scheme': None, 'label': None}, {'term': 'cs.DM', 'scheme': None, 'label': None}, {'term': 'cs.SI', 'scheme': None, 'label': None}, {'term': 'physics.comp-ph', 'scheme': None, 'label': None}, {'term': 'physics.soc-ph', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace-cross,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Yu Tian, Yuefei Liu, Xiangyi Meng'}]","Yu Tian, Yuefei Liu, Xiangyi Meng","{'name': 'Yu Tian, Yuefei Liu, Xiangyi Meng'}",,
768,An independence of the MIN principle from the PHP principle,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'An independence of the MIN principle from the PHP principle'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2406.14930'}]",https://arxiv.org/abs/2406.14930,"arXiv:2406.14930v3 Announce Type: replace-cross 
Abstract: We extend the typical forcing of M. M\""uller and derive conditions on the forcing frame for which generic expansions preserve injective/bijective pigeonhole principle for polynomial-time computable graphs of functions. Applying this machinery, we show that the bounded arithmetic theory $\forall \textsf{T}^1_2(\textsf{PV}(\alpha))$ augmented by the polynomial-time injective pigeonhole principle does not prove the linear ordering, tournament, and dual weak pigeonhole principles.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2406.14930v3 Announce Type: replace-cross \nAbstract: We extend the typical forcing of M. M\\""uller and derive conditions on the forcing frame for which generic expansions preserve injective/bijective pigeonhole principle for polynomial-time computable graphs of functions. Applying this machinery, we show that the bounded arithmetic theory $\\forall \\textsf{T}^1_2(\\textsf{PV}(\\alpha))$ augmented by the polynomial-time injective pigeonhole principle does not prove the linear ordering, tournament, and dual weak pigeonhole principles.'}",oai:arXiv.org:2406.14930v3,False,"[{'term': 'math.LO', 'scheme': None, 'label': None}, {'term': 'cs.LO', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace-cross,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}",[{'name': 'Mykyta Narusevych'}],Mykyta Narusevych,{'name': 'Mykyta Narusevych'},,
769,Quantum Curriculum Learning,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Quantum Curriculum Learning'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2407.02419'}]",https://arxiv.org/abs/2407.02419,"arXiv:2407.02419v3 Announce Type: replace-cross 
Abstract: Quantum machine learning (QML) requires significant quantum resources to address practical real-world problems. When the underlying quantum information exhibits hierarchical structures in the data, limitations persist in training complexity and generalization. Research should prioritize both the efficient design of quantum architectures and the development of learning strategies to optimize resource usage. We propose a framework called quantum curriculum learning (Q-CurL) for quantum data, where the curriculum introduces simpler tasks or data to the learning model before progressing to more challenging ones. Q-CurL exhibits robustness to noise and data limitations, which is particularly relevant for current and near-term noisy intermediate-scale quantum devices. We achieve this through a curriculum design based on quantum data density ratios and a dynamic learning schedule that prioritizes the most informative quantum data. Empirical evidence shows that Q-CurL significantly enhances training convergence and generalization for unitary learning and improves the robustness of quantum phase recognition tasks. Q-CurL is effective with broad physical learning applications in condensed matter physics and quantum chemistry.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2407.02419v3 Announce Type: replace-cross \nAbstract: Quantum machine learning (QML) requires significant quantum resources to address practical real-world problems. When the underlying quantum information exhibits hierarchical structures in the data, limitations persist in training complexity and generalization. Research should prioritize both the efficient design of quantum architectures and the development of learning strategies to optimize resource usage. We propose a framework called quantum curriculum learning (Q-CurL) for quantum data, where the curriculum introduces simpler tasks or data to the learning model before progressing to more challenging ones. Q-CurL exhibits robustness to noise and data limitations, which is particularly relevant for current and near-term noisy intermediate-scale quantum devices. We achieve this through a curriculum design based on quantum data density ratios and a dynamic learning schedule that prioritizes the most informative quantum data. Empirical evidence shows that Q-CurL significantly enhances training convergence and generalization for unitary learning and improves the robustness of quantum phase recognition tasks. Q-CurL is effective with broad physical learning applications in condensed matter physics and quantum chemistry.'}",oai:arXiv.org:2407.02419v3,False,"[{'term': 'quant-ph', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'stat.ML', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace-cross,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Quoc Hoan Tran, Yasuhiro Endo, Hirotaka Oshima'}]","Quoc Hoan Tran, Yasuhiro Endo, Hirotaka Oshima","{'name': 'Quoc Hoan Tran, Yasuhiro Endo, Hirotaka Oshima'}",,
770,AI-Driven Mobility Management for High-Speed Railway Communications: Compressed Measurements and Proactive Handover,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'AI-Driven Mobility Management for High-Speed Railway Communications: Compressed Measurements and Proactive Handover'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2407.04336'}]",https://arxiv.org/abs/2407.04336,"arXiv:2407.04336v2 Announce Type: replace-cross 
Abstract: High-speed railway (HSR) communications are pivotal for ensuring rail safety, operations, maintenance, and delivering passenger information services. The high speed of trains creates rapidly time-varying wireless channels, increases the signaling overhead, and reduces the system throughput, making it difficult to meet the growing and stringent needs of HSR applications. In this article, we explore artificial intelligence (AI)-based beam-level and cell-level mobility management suitable for HSR communications. Particularly, we propose a compressed spatial multi-beam measurements scheme via compressive sensing for beam-level mobility management in HSR communications. In comparison to traditional down-sampling spatial beam measurements, this method leads to improved spatial-temporal beam prediction accuracy with the same measurement overhead. Moreover, we propose a novel AI-based proactive handover scheme to predict handover events and reduce radio link failure (RLF) rates in HSR communications. Compared with the traditional event A3-based handover mechanism, the proposed approach significantly reduces the RLF rates which saves 50% beam measurement overhead.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2407.04336v2 Announce Type: replace-cross \nAbstract: High-speed railway (HSR) communications are pivotal for ensuring rail safety, operations, maintenance, and delivering passenger information services. The high speed of trains creates rapidly time-varying wireless channels, increases the signaling overhead, and reduces the system throughput, making it difficult to meet the growing and stringent needs of HSR applications. In this article, we explore artificial intelligence (AI)-based beam-level and cell-level mobility management suitable for HSR communications. Particularly, we propose a compressed spatial multi-beam measurements scheme via compressive sensing for beam-level mobility management in HSR communications. In comparison to traditional down-sampling spatial beam measurements, this method leads to improved spatial-temporal beam prediction accuracy with the same measurement overhead. Moreover, we propose a novel AI-based proactive handover scheme to predict handover events and reduce radio link failure (RLF) rates in HSR communications. Compared with the traditional event A3-based handover mechanism, the proposed approach significantly reduces the RLF rates which saves 50% beam measurement overhead.'}",oai:arXiv.org:2407.04336v2,False,"[{'term': 'eess.SP', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace-cross,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Wen Li, Wei Chen, Shiyue Wang, Yuanyuan Zhang, Michail Matthaiou, Bo Ai'}]","Wen Li, Wei Chen, Shiyue Wang, Yuanyuan Zhang, Michail Matthaiou, Bo Ai","{'name': 'Wen Li, Wei Chen, Shiyue Wang, Yuanyuan Zhang, Michail Matthaiou, Bo Ai'}",,
771,CUAOA: A Novel CUDA-Accelerated Simulation Framework for the QAOA,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'CUAOA: A Novel CUDA-Accelerated Simulation Framework for the QAOA'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2407.13012'}]",https://arxiv.org/abs/2407.13012,"arXiv:2407.13012v2 Announce Type: replace-cross 
Abstract: The Quantum Approximate Optimization Algorithm (QAOA) is a prominent quantum algorithm designed to find approximate solutions to combinatorial optimization problems, which are challenging for classical computers. In the current era, where quantum hardware is constrained by noise and limited qubit availability, simulating the QAOA remains essential for research. However, existing state-of-the-art simulation frameworks suffer from long execution times or lack comprehensive functionality, usability, and versatility, often requiring users to implement essential features themselves. Additionally, these frameworks are primarily restricted to Python, limiting their use in safer and faster languages like Rust, which offer, e.g., advanced parallelization capabilities. In this paper, we develop a GPU accelerated QAOA simulation framework utilizing the NVIDIA CUDA toolkit. This framework offers a complete interface for QAOA simulations, enabling the calculation of (exact) expectation values, direct access to the statevector, fast sampling, and high-performance optimization methods using an advanced state-of-the-art gradient calculation technique. The framework is designed for use in Python and Rust, providing flexibility for integration into a wide range of applications, including those requiring fast algorithm implementations leveraging QAOA at its core. The new framework's performance is rigorously benchmarked on the MaxCut problem and compared against the current state-of-the-art general-purpose quantum circuit simulation frameworks Qiskit and Pennylane as well as the specialized QAOA simulation tool QOKit. Our evaluation shows that our approach outperforms the existing state-of-the-art solutions in terms of runtime up to multiple orders of magnitude. Our implementation is publicly available at https://github.com/JFLXB/cuaoa and Zenodo.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2407.13012v2 Announce Type: replace-cross \nAbstract: The Quantum Approximate Optimization Algorithm (QAOA) is a prominent quantum algorithm designed to find approximate solutions to combinatorial optimization problems, which are challenging for classical computers. In the current era, where quantum hardware is constrained by noise and limited qubit availability, simulating the QAOA remains essential for research. However, existing state-of-the-art simulation frameworks suffer from long execution times or lack comprehensive functionality, usability, and versatility, often requiring users to implement essential features themselves. Additionally, these frameworks are primarily restricted to Python, limiting their use in safer and faster languages like Rust, which offer, e.g., advanced parallelization capabilities. In this paper, we develop a GPU accelerated QAOA simulation framework utilizing the NVIDIA CUDA toolkit. This framework offers a complete interface for QAOA simulations, enabling the calculation of (exact) expectation values, direct access to the statevector, fast sampling, and high-performance optimization methods using an advanced state-of-the-art gradient calculation technique. The framework is designed for use in Python and Rust, providing flexibility for integration into a wide range of applications, including those requiring fast algorithm implementations leveraging QAOA at its core. The new framework's performance is rigorously benchmarked on the MaxCut problem and compared against the current state-of-the-art general-purpose quantum circuit simulation frameworks Qiskit and Pennylane as well as the specialized QAOA simulation tool QOKit. Our evaluation shows that our approach outperforms the existing state-of-the-art solutions in terms of runtime up to multiple orders of magnitude. Our implementation is publicly available at https://github.com/JFLXB/cuaoa and Zenodo.""}",oai:arXiv.org:2407.13012v2,False,"[{'term': 'quant-ph', 'scheme': None, 'label': None}, {'term': 'cs.ET', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace-cross,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Jonas Stein, Jonas Blenninger, David Bucher, Peter J. Eder, Elif \\c{C}etiner, Maximilian Zorn, Claudia Linnhoff-Popien'}]","Jonas Stein, Jonas Blenninger, David Bucher, Peter J. Eder, Elif \c{C}etiner, Maximilian Zorn, Claudia Linnhoff-Popien","{'name': 'Jonas Stein, Jonas Blenninger, David Bucher, Peter J. Eder, Elif \\c{C}etiner, Maximilian Zorn, Claudia Linnhoff-Popien'}",,
772,Langevin dynamics for high-dimensional optimization: the case of multi-spiked tensor PCA,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Langevin dynamics for high-dimensional optimization: the case of multi-spiked tensor PCA'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2408.06401'}]",https://arxiv.org/abs/2408.06401,"arXiv:2408.06401v2 Announce Type: replace-cross 
Abstract: We study nonconvex optimization in high dimensions through Langevin dynamics, focusing on the multi-spiked tensor PCA problem. This tensor estimation problem involves recovering $r$ hidden signal vectors (spikes) from noisy Gaussian tensor observations using maximum likelihood estimation. We study the number of samples required for Langevin dynamics to efficiently recover the spikes and determine the necessary separation condition on the signal-to-noise ratios (SNRs) for exact recovery, distinguishing the cases $p \ge 3$ and $p=2$, where $p$ denotes the order of the tensor. In particular, we show that the sample complexity required for recovering the spike associated with the largest SNR matches the well-known algorithmic threshold for the single-spike case, while this threshold degrades when recovering all $r$ spikes. As a key step, we provide a detailed characterization of the trajectory and interactions of low-dimensional projections that capture the high-dimensional dynamics.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2408.06401v2 Announce Type: replace-cross \nAbstract: We study nonconvex optimization in high dimensions through Langevin dynamics, focusing on the multi-spiked tensor PCA problem. This tensor estimation problem involves recovering $r$ hidden signal vectors (spikes) from noisy Gaussian tensor observations using maximum likelihood estimation. We study the number of samples required for Langevin dynamics to efficiently recover the spikes and determine the necessary separation condition on the signal-to-noise ratios (SNRs) for exact recovery, distinguishing the cases $p \\ge 3$ and $p=2$, where $p$ denotes the order of the tensor. In particular, we show that the sample complexity required for recovering the spike associated with the largest SNR matches the well-known algorithmic threshold for the single-spike case, while this threshold degrades when recovering all $r$ spikes. As a key step, we provide a detailed characterization of the trajectory and interactions of low-dimensional projections that capture the high-dimensional dynamics.'}",oai:arXiv.org:2408.06401v2,False,"[{'term': 'stat.ML', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'math.PR', 'scheme': None, 'label': None}, {'term': 'math.ST', 'scheme': None, 'label': None}, {'term': 'stat.TH', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace-cross,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': ""G\\'erard Ben Arous, C\\'edric Gerbelot, Vanessa Piccolo""}]","G\'erard Ben Arous, C\'edric Gerbelot, Vanessa Piccolo","{'name': ""G\\'erard Ben Arous, C\\'edric Gerbelot, Vanessa Piccolo""}",,
773,OCTCube-M: A 3D multimodal optical coherence tomography foundation model for retinal and systemic diseases with cross-cohort and cross-device validation,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'OCTCube-M: A 3D multimodal optical coherence tomography foundation model for retinal and systemic diseases with cross-cohort and cross-device validation'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2408.11227'}]",https://arxiv.org/abs/2408.11227,"arXiv:2408.11227v2 Announce Type: replace-cross 
Abstract: We present OCTCube-M, a 3D OCT-based multi-modal foundation model for jointly analyzing OCT and en face images. OCTCube-M first developed OCTCube, a 3D foundation model pre-trained on 26,685 3D OCT volumes encompassing 1.62 million 2D OCT images. It then exploits a novel multi-modal contrastive learning framework COEP to integrate other retinal imaging modalities, such as fundus autofluorescence and infrared retinal imaging, into OCTCube, efficiently extending it into multi-modal foundation models. OCTCube achieves best performance on predicting 8 retinal diseases, demonstrating strong generalizability on cross-cohort, cross-device and cross-modality prediction. OCTCube can also predict cross-organ nodule malignancy (CT) and low cardiac ejection fraction as well as systemic diseases, such as diabetes and hypertension, revealing its wide applicability beyond retinal diseases. We further develop OCTCube-IR using COEP with 26,685 OCT and IR image pairs. OCTCube-IR can accurately retrieve between OCT and IR images, allowing joint analysis between 3D and 2D retinal imaging modalities. Finally, we trained a tri-modal foundation model OCTCube-EF from 4 million 2D OCT images and 400K en face retinal images. OCTCube-EF attains the best performance on predicting the growth rate of geographic atrophy (GA) across datasets collected from 6 multi-center global trials conducted in 23 countries. This improvement is statistically equivalent to running a clinical trial with more than double the size of the original study. Our analysis based on another retrospective case study reveals OCTCube-EF's ability to avoid false positive Phase-III results according to its accurate treatment effect estimation on the Phase-II results. In sum, OCTCube-M is a 3D multi-modal foundation model framework that integrates OCT and other retinal imaging modalities revealing substantial diagnostic and prognostic benefits.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2408.11227v2 Announce Type: replace-cross \nAbstract: We present OCTCube-M, a 3D OCT-based multi-modal foundation model for jointly analyzing OCT and en face images. OCTCube-M first developed OCTCube, a 3D foundation model pre-trained on 26,685 3D OCT volumes encompassing 1.62 million 2D OCT images. It then exploits a novel multi-modal contrastive learning framework COEP to integrate other retinal imaging modalities, such as fundus autofluorescence and infrared retinal imaging, into OCTCube, efficiently extending it into multi-modal foundation models. OCTCube achieves best performance on predicting 8 retinal diseases, demonstrating strong generalizability on cross-cohort, cross-device and cross-modality prediction. OCTCube can also predict cross-organ nodule malignancy (CT) and low cardiac ejection fraction as well as systemic diseases, such as diabetes and hypertension, revealing its wide applicability beyond retinal diseases. We further develop OCTCube-IR using COEP with 26,685 OCT and IR image pairs. OCTCube-IR can accurately retrieve between OCT and IR images, allowing joint analysis between 3D and 2D retinal imaging modalities. Finally, we trained a tri-modal foundation model OCTCube-EF from 4 million 2D OCT images and 400K en face retinal images. OCTCube-EF attains the best performance on predicting the growth rate of geographic atrophy (GA) across datasets collected from 6 multi-center global trials conducted in 23 countries. This improvement is statistically equivalent to running a clinical trial with more than double the size of the original study. Our analysis based on another retrospective case study reveals OCTCube-EF's ability to avoid false positive Phase-III results according to its accurate treatment effect estimation on the Phase-II results. In sum, OCTCube-M is a 3D multi-modal foundation model framework that integrates OCT and other retinal imaging modalities revealing substantial diagnostic and prognostic benefits.""}",oai:arXiv.org:2408.11227v2,False,"[{'term': 'eess.IV', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.CV', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace-cross,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Zixuan Liu, Hanwen Xu, Addie Woicik, Linda G. Shapiro, Marian Blazes, Yue Wu, Verena Steffen, Catherine Cukras, Cecilia S. Lee, Miao Zhang, Aaron Y. Lee, Sheng Wang'}]","Zixuan Liu, Hanwen Xu, Addie Woicik, Linda G. Shapiro, Marian Blazes, Yue Wu, Verena Steffen, Catherine Cukras, Cecilia S. Lee, Miao Zhang, Aaron Y. Lee, Sheng Wang","{'name': 'Zixuan Liu, Hanwen Xu, Addie Woicik, Linda G. Shapiro, Marian Blazes, Yue Wu, Verena Steffen, Catherine Cukras, Cecilia S. Lee, Miao Zhang, Aaron Y. Lee, Sheng Wang'}",,
774,Hybridization of Persistent Homology with Neural Networks for Time-Series Prediction: A Case Study in Wave Height,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Hybridization of Persistent Homology with Neural Networks for Time-Series Prediction: A Case Study in Wave Height'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2409.01519'}]",https://arxiv.org/abs/2409.01519,"arXiv:2409.01519v3 Announce Type: replace-cross 
Abstract: Time-series prediction is an active area of research across various fields, often challenged by the fluctuating influence of short-term and long-term factors. In this study, we introduce a feature engineering method that enhances the predictive performance of neural network models. Specifically, we leverage computational topology techniques to derive valuable topological features from input data, boosting the predictive accuracy of our models. Our focus is on predicting wave heights, utilizing models based on topological features within feedforward neural networks (FNNs), recurrent neural networks (RNNs), long short-term memory networks (LSTM), and RNNs with gated recurrent units (GRU). For time-ahead predictions, the enhancements in $R^2$ score were significant for FNNs, RNNs, LSTM, and GRU models. Additionally, these models also showed significant reductions in maximum errors and mean squared errors.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2409.01519v3 Announce Type: replace-cross \nAbstract: Time-series prediction is an active area of research across various fields, often challenged by the fluctuating influence of short-term and long-term factors. In this study, we introduce a feature engineering method that enhances the predictive performance of neural network models. Specifically, we leverage computational topology techniques to derive valuable topological features from input data, boosting the predictive accuracy of our models. Our focus is on predicting wave heights, utilizing models based on topological features within feedforward neural networks (FNNs), recurrent neural networks (RNNs), long short-term memory networks (LSTM), and RNNs with gated recurrent units (GRU). For time-ahead predictions, the enhancements in $R^2$ score were significant for FNNs, RNNs, LSTM, and GRU models. Additionally, these models also showed significant reductions in maximum errors and mean squared errors.'}",oai:arXiv.org:2409.01519v3,False,"[{'term': 'stat.ML', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace-cross,http://creativecommons.org/publicdomain/zero/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/publicdomain/zero/1.0/'}","[{'name': 'Zixin Lin, Nur Fariha Syaqina Zulkepli, Mohd Shareduwan Mohd Kasihmuddin, R. U. Gobithaasan'}]","Zixin Lin, Nur Fariha Syaqina Zulkepli, Mohd Shareduwan Mohd Kasihmuddin, R. U. Gobithaasan","{'name': 'Zixin Lin, Nur Fariha Syaqina Zulkepli, Mohd Shareduwan Mohd Kasihmuddin, R. U. Gobithaasan'}",,
775,Nonstationary Sparse Spectral Permanental Process,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Nonstationary Sparse Spectral Permanental Process'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2410.03581'}]",https://arxiv.org/abs/2410.03581,"arXiv:2410.03581v3 Announce Type: replace-cross 
Abstract: Existing permanental processes often impose constraints on kernel types or stationarity, limiting the model's expressiveness. To overcome these limitations, we propose a novel approach utilizing the sparse spectral representation of nonstationary kernels. This technique relaxes the constraints on kernel types and stationarity, allowing for more flexible modeling while reducing computational complexity to the linear level. Additionally, we introduce a deep kernel variant by hierarchically stacking multiple spectral feature mappings, further enhancing the model's expressiveness to capture complex patterns in data. Experimental results on both synthetic and real-world datasets demonstrate the effectiveness of our approach, particularly in scenarios with pronounced data nonstationarity. Additionally, ablation studies are conducted to provide insights into the impact of various hyperparameters on model performance.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2410.03581v3 Announce Type: replace-cross \nAbstract: Existing permanental processes often impose constraints on kernel types or stationarity, limiting the model's expressiveness. To overcome these limitations, we propose a novel approach utilizing the sparse spectral representation of nonstationary kernels. This technique relaxes the constraints on kernel types and stationarity, allowing for more flexible modeling while reducing computational complexity to the linear level. Additionally, we introduce a deep kernel variant by hierarchically stacking multiple spectral feature mappings, further enhancing the model's expressiveness to capture complex patterns in data. Experimental results on both synthetic and real-world datasets demonstrate the effectiveness of our approach, particularly in scenarios with pronounced data nonstationarity. Additionally, ablation studies are conducted to provide insights into the impact of various hyperparameters on model performance.""}",oai:arXiv.org:2410.03581v3,False,"[{'term': 'stat.ML', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace-cross,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Zicheng Sun, Yixuan Zhang, Zenan Ling, Xuhui Fan, Feng Zhou'}]","Zicheng Sun, Yixuan Zhang, Zenan Ling, Xuhui Fan, Feng Zhou","{'name': 'Zicheng Sun, Yixuan Zhang, Zenan Ling, Xuhui Fan, Feng Zhou'}",,
776,"Truth, beauty, and goodness in grand unification: a machine learning approach","{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Truth, beauty, and goodness in grand unification: a machine learning approach'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2411.06718'}]",https://arxiv.org/abs/2411.06718,"arXiv:2411.06718v2 Announce Type: replace-cross 
Abstract: We investigate the flavour sector of the supersymmetric $SU(5)$ Grand Unified Theory (GUT) model using machine learning techniques. The minimal $SU(5)$ model is known to predict fermion masses that disagree with observed values in nature. There are two well-known approaches to address this issue: one involves introducing a 45-representation Higgs field, while the other employs a higher-dimensional operator involving the 24-representation GUT Higgs field. We compare these two approaches by numerically optimising a loss function, defined as the ratio of determinants of mass matrices. Our findings indicate that the 24-Higgs approach achieves the observed fermion masses with smaller modifications to the original minimal $SU(5)$ model.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2411.06718v2 Announce Type: replace-cross \nAbstract: We investigate the flavour sector of the supersymmetric $SU(5)$ Grand Unified Theory (GUT) model using machine learning techniques. The minimal $SU(5)$ model is known to predict fermion masses that disagree with observed values in nature. There are two well-known approaches to address this issue: one involves introducing a 45-representation Higgs field, while the other employs a higher-dimensional operator involving the 24-representation GUT Higgs field. We compare these two approaches by numerically optimising a loss function, defined as the ratio of determinants of mass matrices. Our findings indicate that the 24-Higgs approach achieves the observed fermion masses with smaller modifications to the original minimal $SU(5)$ model.'}",oai:arXiv.org:2411.06718v2,False,"[{'term': 'hep-ph', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'hep-th', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace-cross,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Shinsuke Kawai, Nobuchika Okada'}]","Shinsuke Kawai, Nobuchika Okada","{'name': 'Shinsuke Kawai, Nobuchika Okada'}",,
777,Efficient Classical Computation of Single-Qubit Marginal Measurement Probabilities to Simulate Certain Classes of Quantum Algorithms,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Efficient Classical Computation of Single-Qubit Marginal Measurement Probabilities to Simulate Certain Classes of Quantum Algorithms'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2411.06822'}]",https://arxiv.org/abs/2411.06822,"arXiv:2411.06822v3 Announce Type: replace-cross 
Abstract: Classical simulations of quantum circuits are essential for verifying and benchmarking quantum algorithms, particularly for large circuits, where computational demands increase exponentially with the number of qubits. Among available methods, the classical simulation of quantum circuits inspired by density functional theory -- the so-called QC-DFT method, shows promise for large circuit simulations as it approximates the quantum circuits using single-qubit reduced density matrices to model multi-qubit systems. However, the QC-DFT method performs very poorly when dealing with multi-qubit gates. In this work, we introduce a novel CNOT ""functional"" that leverages neural networks to generate unitary transformations, effectively mitigating the simulation errors observed in the original QC-DFT method. For random circuit simulations, our modified QC-DFT enables efficient computation of single-qubit marginal measurement probabilities, or single-qubit probability (SQPs), and achieves lower SQP errors and higher fidelities than the original QC-DFT method. Despite some limitations in capturing full entanglement and joint probability distributions, we find potential applications of SQPs in simulating Shor's and Grover's algorithms for specific solution classes. These findings advance the capabilities of classical simulations for some quantum problems and provide insights into managing entanglement and gate errors in practical quantum computing.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2411.06822v3 Announce Type: replace-cross \nAbstract: Classical simulations of quantum circuits are essential for verifying and benchmarking quantum algorithms, particularly for large circuits, where computational demands increase exponentially with the number of qubits. Among available methods, the classical simulation of quantum circuits inspired by density functional theory -- the so-called QC-DFT method, shows promise for large circuit simulations as it approximates the quantum circuits using single-qubit reduced density matrices to model multi-qubit systems. However, the QC-DFT method performs very poorly when dealing with multi-qubit gates. In this work, we introduce a novel CNOT ""functional"" that leverages neural networks to generate unitary transformations, effectively mitigating the simulation errors observed in the original QC-DFT method. For random circuit simulations, our modified QC-DFT enables efficient computation of single-qubit marginal measurement probabilities, or single-qubit probability (SQPs), and achieves lower SQP errors and higher fidelities than the original QC-DFT method. Despite some limitations in capturing full entanglement and joint probability distributions, we find potential applications of SQPs in simulating Shor\'s and Grover\'s algorithms for specific solution classes. These findings advance the capabilities of classical simulations for some quantum problems and provide insights into managing entanglement and gate errors in practical quantum computing.'}",oai:arXiv.org:2411.06822v3,False,"[{'term': 'quant-ph', 'scheme': None, 'label': None}, {'term': 'cs.DM', 'scheme': None, 'label': None}, {'term': 'cs.DS', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace-cross,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': ""Santana Y. Pradata, M 'Anin N. 'Azhiim, Hendry M. Lim, Ahmad R. T. Nugraha""}]","Santana Y. Pradata, M 'Anin N. 'Azhiim, Hendry M. Lim, Ahmad R. T. Nugraha","{'name': ""Santana Y. Pradata, M 'Anin N. 'Azhiim, Hendry M. Lim, Ahmad R. T. Nugraha""}",,
778,Radiance Field Delta Video Compression in Edge-Enabled Vehicular Metaverse,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Radiance Field Delta Video Compression in Edge-Enabled Vehicular Metaverse'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2411.11857'}]",https://arxiv.org/abs/2411.11857,"arXiv:2411.11857v3 Announce Type: replace-cross 
Abstract: Connected and autonomous vehicles (CAVs) offload computationally intensive tasks to multi-access edge computing (MEC) servers via vehicle-to-infrastructure (V2I) communication, enabling applications within the vehicular metaverse, which transforms physical environment into the digital space enabling advanced analysis or predictive modeling. A core challenge is physical-to-virtual (P2V) synchronization through digital twins (DTs), reliant on MEC networks and ultra-reliable low-latency communication (URLLC). To address this, we introduce radiance field (RF) delta video compression (RFDVC), which uses RF-encoder and RF-decoder architecture using distributed RFs as DTs storing photorealistic 3D urban scenes in compressed form. This method extracts differences between CAV-frame capturing actual traffic and RF-frame capturing empty scene from the same camera pose in batches encoded and transmitted over the MEC network. Experiments show data savings up to 71% against H.264 codec and 44% against H.265 codec under different conditions as lighting changes, and rain. RFDVC also demonstrates resilience to transmission errors, achieving up to +0.29 structural similarity index measure (SSIM) improvement at block error rate (BLER) = 0.35 in non-rainy and +0.25 at BLER = 0.2 in rainy conditions, ensuring superior visual quality compared to standard video coding (VC) methods across various conditions.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2411.11857v3 Announce Type: replace-cross \nAbstract: Connected and autonomous vehicles (CAVs) offload computationally intensive tasks to multi-access edge computing (MEC) servers via vehicle-to-infrastructure (V2I) communication, enabling applications within the vehicular metaverse, which transforms physical environment into the digital space enabling advanced analysis or predictive modeling. A core challenge is physical-to-virtual (P2V) synchronization through digital twins (DTs), reliant on MEC networks and ultra-reliable low-latency communication (URLLC). To address this, we introduce radiance field (RF) delta video compression (RFDVC), which uses RF-encoder and RF-decoder architecture using distributed RFs as DTs storing photorealistic 3D urban scenes in compressed form. This method extracts differences between CAV-frame capturing actual traffic and RF-frame capturing empty scene from the same camera pose in batches encoded and transmitted over the MEC network. Experiments show data savings up to 71% against H.264 codec and 44% against H.265 codec under different conditions as lighting changes, and rain. RFDVC also demonstrates resilience to transmission errors, achieving up to +0.29 structural similarity index measure (SSIM) improvement at block error rate (BLER) = 0.35 in non-rainy and +0.25 at BLER = 0.2 in rainy conditions, ensuring superior visual quality compared to standard video coding (VC) methods across various conditions.'}",oai:arXiv.org:2411.11857v3,False,"[{'term': 'eess.SP', 'scheme': None, 'label': None}, {'term': 'cs.IT', 'scheme': None, 'label': None}, {'term': 'cs.NI', 'scheme': None, 'label': None}, {'term': 'math.IT', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace-cross,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': ""Mat\\'u\\v{s} Dopiriak, Eugen \\v{S}lapak, Juraj Gazda, Devendra S. Gurjar, Mohammad Abdullah Al Faruque, Marco Levorato""}]","Mat\'u\v{s} Dopiriak, Eugen \v{S}lapak, Juraj Gazda, Devendra S. Gurjar, Mohammad Abdullah Al Faruque, Marco Levorato","{'name': ""Mat\\'u\\v{s} Dopiriak, Eugen \\v{S}lapak, Juraj Gazda, Devendra S. Gurjar, Mohammad Abdullah Al Faruque, Marco Levorato""}",,
779,The Data-Driven Censored Newsvendor Problem,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'The Data-Driven Censored Newsvendor Problem'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.01763'}]",https://arxiv.org/abs/2412.01763,"arXiv:2412.01763v2 Announce Type: replace-cross 
Abstract: We study a censored variant of the data-driven newsvendor problem, where the decision-maker must select an ordering quantity that minimizes expected overage and underage costs based only on offline censored sales data, rather than historical demand realizations. Our goal is to understand how the degree of historical demand censoring affects the performance of any learning algorithm for this problem. To isolate this impact, we adopt a distributionally robust optimization framework, evaluating policies according to their worst-case regret over an ambiguity set of distributions. This set is defined by the largest historical order quantity (the observable boundary of the dataset), and contains all distributions matching the true demand distribution up to this boundary, while allowing them to be arbitrary afterwards. We demonstrate a spectrum of achievability under demand censoring by deriving a natural necessary and sufficient condition under which vanishing regret is an achievable goal. In regimes in which it is not, we exactly characterize the information loss due to censoring: an insurmountable lower bound on the performance of any policy, even when the decision-maker has access to infinitely many demand samples. We then leverage these sharp characterizations to propose a natural robust algorithm that adapts to the historical level of demand censoring. We derive finite-sample guarantees for this algorithm across all possible censoring regimes and show its near-optimality with matching lower bounds (up to polylogarithmic factors). We moreover demonstrate its robust performance via extensive numerical experiments on both synthetic and real-world datasets.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.01763v2 Announce Type: replace-cross \nAbstract: We study a censored variant of the data-driven newsvendor problem, where the decision-maker must select an ordering quantity that minimizes expected overage and underage costs based only on offline censored sales data, rather than historical demand realizations. Our goal is to understand how the degree of historical demand censoring affects the performance of any learning algorithm for this problem. To isolate this impact, we adopt a distributionally robust optimization framework, evaluating policies according to their worst-case regret over an ambiguity set of distributions. This set is defined by the largest historical order quantity (the observable boundary of the dataset), and contains all distributions matching the true demand distribution up to this boundary, while allowing them to be arbitrary afterwards. We demonstrate a spectrum of achievability under demand censoring by deriving a natural necessary and sufficient condition under which vanishing regret is an achievable goal. In regimes in which it is not, we exactly characterize the information loss due to censoring: an insurmountable lower bound on the performance of any policy, even when the decision-maker has access to infinitely many demand samples. We then leverage these sharp characterizations to propose a natural robust algorithm that adapts to the historical level of demand censoring. We derive finite-sample guarantees for this algorithm across all possible censoring regimes and show its near-optimality with matching lower bounds (up to polylogarithmic factors). We moreover demonstrate its robust performance via extensive numerical experiments on both synthetic and real-world datasets.'}",oai:arXiv.org:2412.01763v2,False,"[{'term': 'math.OC', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'stat.ML', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace-cross,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Chamsi Hssaine, Sean R. Sinclair'}]","Chamsi Hssaine, Sean R. Sinclair","{'name': 'Chamsi Hssaine, Sean R. Sinclair'}",,
780,Samudra: An AI Global Ocean Emulator for Climate,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Samudra: An AI Global Ocean Emulator for Climate'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.03795'}]",https://arxiv.org/abs/2412.03795,"arXiv:2412.03795v2 Announce Type: replace-cross 
Abstract: AI emulators for forecasting have emerged as powerful tools that can outperform conventional numerical predictions. The next frontier is to build emulators for long climate simulations with skill across a range of spatiotemporal scales, a particularly important goal for the ocean. Our work builds a skillful global emulator of the ocean component of a state-of-the-art climate model. We emulate key ocean variables, sea surface height, horizontal velocities, temperature, and salinity, across their full depth. We use a modified ConvNeXt UNet architecture trained on multidepth levels of ocean data. We show that the ocean emulator - Samudra - which exhibits no drift relative to the truth, can reproduce the depth structure of ocean variables and their interannual variability. Samudra is stable for centuries and 150 times faster than the original ocean model. Samudra struggles to capture the correct magnitude of the forcing trends and simultaneously remains stable, requiring further work.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.03795v2 Announce Type: replace-cross \nAbstract: AI emulators for forecasting have emerged as powerful tools that can outperform conventional numerical predictions. The next frontier is to build emulators for long climate simulations with skill across a range of spatiotemporal scales, a particularly important goal for the ocean. Our work builds a skillful global emulator of the ocean component of a state-of-the-art climate model. We emulate key ocean variables, sea surface height, horizontal velocities, temperature, and salinity, across their full depth. We use a modified ConvNeXt UNet architecture trained on multidepth levels of ocean data. We show that the ocean emulator - Samudra - which exhibits no drift relative to the truth, can reproduce the depth structure of ocean variables and their interannual variability. Samudra is stable for centuries and 150 times faster than the original ocean model. Samudra struggles to capture the correct magnitude of the forcing trends and simultaneously remains stable, requiring further work.'}",oai:arXiv.org:2412.03795v2,False,"[{'term': 'physics.ao-ph', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace-cross,http://creativecommons.org/licenses/by/4.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://creativecommons.org/licenses/by/4.0/'}","[{'name': 'Surya Dheeshjith, Adam Subel, Alistair Adcroft, Julius Busecke, Carlos Fernandez-Granda, Shubham Gupta, Laure Zanna'}]","Surya Dheeshjith, Adam Subel, Alistair Adcroft, Julius Busecke, Carlos Fernandez-Granda, Shubham Gupta, Laure Zanna","{'name': 'Surya Dheeshjith, Adam Subel, Alistair Adcroft, Julius Busecke, Carlos Fernandez-Granda, Shubham Gupta, Laure Zanna'}",,
781,A subgradient splitting algorithm for optimization on nonpositively curved metric spaces,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'A subgradient splitting algorithm for optimization on nonpositively curved metric spaces'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.06730'}]",https://arxiv.org/abs/2412.06730,"arXiv:2412.06730v2 Announce Type: replace-cross 
Abstract: Many of the primal ingredients of convex optimization extend naturally from Euclidean to Hadamard spaces $\unicode{x2014}$ nonpositively curved metric spaces like Euclidean, Hilbert, and hyperbolic spaces, metric trees, and more general CAT(0) cubical complexes. Linear structure, however, and the duality theory it supports are absent. Nonetheless, we introduce a new type of subgradient for convex functions on Hadamard spaces, based on Busemann functions. This notion supports a splitting subgradient method with guaranteed complexity bounds. In particular, the algorithm solves $p$-mean problems in general Hadamard spaces: we illustrate by computing medians in BHV tree space.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.06730v2 Announce Type: replace-cross \nAbstract: Many of the primal ingredients of convex optimization extend naturally from Euclidean to Hadamard spaces $\\unicode{x2014}$ nonpositively curved metric spaces like Euclidean, Hilbert, and hyperbolic spaces, metric trees, and more general CAT(0) cubical complexes. Linear structure, however, and the duality theory it supports are absent. Nonetheless, we introduce a new type of subgradient for convex functions on Hadamard spaces, based on Busemann functions. This notion supports a splitting subgradient method with guaranteed complexity bounds. In particular, the algorithm solves $p$-mean problems in general Hadamard spaces: we illustrate by computing medians in BHV tree space.'}",oai:arXiv.org:2412.06730v2,False,"[{'term': 'math.OC', 'scheme': None, 'label': None}, {'term': 'cs.CG', 'scheme': None, 'label': None}, {'term': 'cs.DS', 'scheme': None, 'label': None}, {'term': 'cs.NA', 'scheme': None, 'label': None}, {'term': 'math.NA', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace-cross,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': ""Ariel Goodwin, Adrian S. Lewis, Genaro L\\'opez-Acedo, Adriana Nicolae""}]","Ariel Goodwin, Adrian S. Lewis, Genaro L\'opez-Acedo, Adriana Nicolae","{'name': ""Ariel Goodwin, Adrian S. Lewis, Genaro L\\'opez-Acedo, Adriana Nicolae""}",,
782,On the metric mean dimensions of saturated sets,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'On the metric mean dimensions of saturated sets'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.09218'}]",https://arxiv.org/abs/2412.09218,"arXiv:2412.09218v2 Announce Type: replace-cross 
Abstract: Saturated set and its reduced case, the set of generic points, constitute two significant types of fractal-like sets in multifractal analysis of dynamical systems. In the context of infinite entropy systems, this paper aims to give some qualitative aspects of saturated sets and the set of generic points in both topological and measure-theoretic perspectives. For systems with specification property, we establish the certain variational principles for saturated sets in terms of Bowen and packing metric mean dimensions, and show the upper capacity metric mean dimension of saturated sets have full metric mean dimension. All results are useful for understanding the topological structures of dynamical systems with infinite topological entropy.
  As applications, we further exhibit some qualitative aspects of metric mean dimensions of level sets and the set of mean Li-Yorke pairs in infinite entropy systems.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.09218v2 Announce Type: replace-cross \nAbstract: Saturated set and its reduced case, the set of generic points, constitute two significant types of fractal-like sets in multifractal analysis of dynamical systems. In the context of infinite entropy systems, this paper aims to give some qualitative aspects of saturated sets and the set of generic points in both topological and measure-theoretic perspectives. For systems with specification property, we establish the certain variational principles for saturated sets in terms of Bowen and packing metric mean dimensions, and show the upper capacity metric mean dimension of saturated sets have full metric mean dimension. All results are useful for understanding the topological structures of dynamical systems with infinite topological entropy.\n  As applications, we further exhibit some qualitative aspects of metric mean dimensions of level sets and the set of mean Li-Yorke pairs in infinite entropy systems.'}",oai:arXiv.org:2412.09218v2,False,"[{'term': 'math.DS', 'scheme': None, 'label': None}, {'term': 'cs.IT', 'scheme': None, 'label': None}, {'term': 'math.IT', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace-cross,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Yong Ji, Junye Li, Rui Yang'}]","Yong Ji, Junye Li, Rui Yang","{'name': 'Yong Ji, Junye Li, Rui Yang'}",,
783,Generalization Analysis for Deep Contrastive Representation Learning,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Generalization Analysis for Deep Contrastive Representation Learning'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.12014'}]",https://arxiv.org/abs/2412.12014,"arXiv:2412.12014v2 Announce Type: replace-cross 
Abstract: In this paper, we present generalization bounds for the unsupervised risk in the Deep Contrastive Representation Learning framework, which employs deep neural networks as representation functions. We approach this problem from two angles. On the one hand, we derive a parameter-counting bound that scales with the overall size of the neural networks. On the other hand, we provide a norm-based bound that scales with the norms of neural networks' weight matrices. Ignoring logarithmic factors, the bounds are independent of $k$, the size of the tuples provided for contrastive learning. To the best of our knowledge, this property is only shared by one other work, which employed a different proof strategy and suffers from very strong exponential dependence on the depth of the network which is due to a use of the peeling technique. Our results circumvent this by leveraging powerful results on covering numbers with respect to uniform norms over samples. In addition, we utilize loss augmentation techniques to further reduce the dependency on matrix norms and the implicit dependence on network depth. In fact, our techniques allow us to produce many bounds for the contrastive learning setting with similar architectural dependencies as in the study of the sample complexity of ordinary loss functions, thereby bridging the gap between the learning theories of contrastive learning and DNNs.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': ""arXiv:2412.12014v2 Announce Type: replace-cross \nAbstract: In this paper, we present generalization bounds for the unsupervised risk in the Deep Contrastive Representation Learning framework, which employs deep neural networks as representation functions. We approach this problem from two angles. On the one hand, we derive a parameter-counting bound that scales with the overall size of the neural networks. On the other hand, we provide a norm-based bound that scales with the norms of neural networks' weight matrices. Ignoring logarithmic factors, the bounds are independent of $k$, the size of the tuples provided for contrastive learning. To the best of our knowledge, this property is only shared by one other work, which employed a different proof strategy and suffers from very strong exponential dependence on the depth of the network which is due to a use of the peeling technique. Our results circumvent this by leveraging powerful results on covering numbers with respect to uniform norms over samples. In addition, we utilize loss augmentation techniques to further reduce the dependency on matrix norms and the implicit dependence on network depth. In fact, our techniques allow us to produce many bounds for the contrastive learning setting with similar architectural dependencies as in the study of the sample complexity of ordinary loss functions, thereby bridging the gap between the learning theories of contrastive learning and DNNs.""}",oai:arXiv.org:2412.12014v2,False,"[{'term': 'stat.ML', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace-cross,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}","[{'name': 'Nong Minh Hieu, Antoine Ledent, Yunwen Lei, Cheng Yeaw Ku'}]","Nong Minh Hieu, Antoine Ledent, Yunwen Lei, Cheng Yeaw Ku","{'name': 'Nong Minh Hieu, Antoine Ledent, Yunwen Lei, Cheng Yeaw Ku'}",,
784,Gauss-Newton Dynamics for Neural Networks: A Riemannian Optimization Perspective,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'Gauss-Newton Dynamics for Neural Networks: A Riemannian Optimization Perspective'}","[{'rel': 'alternate', 'type': 'text/html', 'href': 'https://arxiv.org/abs/2412.14031'}]",https://arxiv.org/abs/2412.14031,"arXiv:2412.14031v2 Announce Type: replace-cross 
Abstract: We analyze the convergence of Gauss-Newton dynamics for training neural networks with smooth activation functions. In the underparameterized regime, the Gauss-Newton gradient flow induces a Riemannian gradient flow on a low-dimensional, smooth, embedded submanifold of the Euclidean output space. Using tools from Riemannian optimization, we prove \emph{last-iterate} convergence of the Riemannian gradient flow to the optimal in-class predictor at an \emph{exponential rate} that is independent of the conditioning of the Gram matrix, \emph{without} requiring explicit regularization. We further characterize the critical impacts of the neural network scaling factor and the initialization on the convergence behavior. In the overparameterized regime, we show that the Levenberg-Marquardt dynamics with an appropriately chosen damping factor yields robustness to ill-conditioned kernels, analogous to the underparameterized regime. These findings demonstrate the potential of Gauss-Newton methods for efficiently optimizing neural networks, particularly in ill-conditioned problems where kernel and Gram matrices have small singular values.","{'type': 'text/html', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'arXiv:2412.14031v2 Announce Type: replace-cross \nAbstract: We analyze the convergence of Gauss-Newton dynamics for training neural networks with smooth activation functions. In the underparameterized regime, the Gauss-Newton gradient flow induces a Riemannian gradient flow on a low-dimensional, smooth, embedded submanifold of the Euclidean output space. Using tools from Riemannian optimization, we prove \\emph{last-iterate} convergence of the Riemannian gradient flow to the optimal in-class predictor at an \\emph{exponential rate} that is independent of the conditioning of the Gram matrix, \\emph{without} requiring explicit regularization. We further characterize the critical impacts of the neural network scaling factor and the initialization on the convergence behavior. In the overparameterized regime, we show that the Levenberg-Marquardt dynamics with an appropriately chosen damping factor yields robustness to ill-conditioned kernels, analogous to the underparameterized regime. These findings demonstrate the potential of Gauss-Newton methods for efficiently optimizing neural networks, particularly in ill-conditioned problems where kernel and Gram matrices have small singular values.'}",oai:arXiv.org:2412.14031v2,False,"[{'term': 'math.OC', 'scheme': None, 'label': None}, {'term': 'cs.AI', 'scheme': None, 'label': None}, {'term': 'cs.LG', 'scheme': None, 'label': None}, {'term': 'cs.SY', 'scheme': None, 'label': None}, {'term': 'eess.SY', 'scheme': None, 'label': None}, {'term': 'stat.ML', 'scheme': None, 'label': None}]","Fri, 20 Dec 2024 00:00:00 -0500","time.struct_time(tm_year=2024, tm_mon=12, tm_mday=20, tm_hour=5, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=355, tm_isdst=0)",replace-cross,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"{'type': 'text/plain', 'language': None, 'base': 'https://rss.arxiv.org/rss/cs', 'value': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/'}",[{'name': 'Semih Cayci'}],Semih Cayci,{'name': 'Semih Cayci'},,
